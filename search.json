[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Projects\n\n\n\n\n\n현재 진행중인 프로젝트\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    ADP 실기 준비 - try 2\n                    on-going\n                \n                \n                    Started: 2026-02-05\n                    \n                     Ends: 2026-05-22\n                    \n                    Calculating...\n                \n                \n                    자격증 데이터 분석 python\n                \n                다시 가봅시다.\n            \n        \n        \n        \n            \n                \n                    정보처리기사\n                    on-going\n                \n                \n                    Started: 2025-12-19\n                    \n                     Ends: 2026-06-12\n                    \n                    Calculating...\n                \n                \n                    자격증 정보처리기사\n                \n                빠르게 따 봅시다\n            \n        \n        \n\n\n\n\n\n\n\n\nCategories\n\n\n\n\n\n관심 분야\n\n\n\n\n    \n    \n    \n\n\n\n    \n    \n        \n            Block Chain\n        \n        \n        \n            Deep Learning\n        \n        \n        \n            42 Seoul\n        \n        \n        \n            Rust\n        \n        \n        \n            진로 준비\n        \n        \n\n\n\n\n\n\n\n\nArchives\n\n\n\n\n\n완료된 프로젝트\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    학부 3학년 2학기\n                    completed\n                \n                \n                    Started: 2025-09-01\n                    \n                     Ended: 2025-12-20\n                    \n                    Calculating...\n                \n                \n                    산업공학 학부\n                \n                3학년 2학기 학부 할 일 총 정리\n            \n        \n        \n        \n            \n                \n                    ADP 실기 준비 - try 1\n                    completed\n                \n                \n                    Started: 2025-06-16\n                    \n                     Ended: 2025-10-18\n                    \n                    Calculating...\n                \n                \n                    자격증 데이터 분석 python\n                \n                ADP 실기를 준비해 봅시다.\n            \n        \n        \n        \n            \n                \n                    SQLD 준비\n                    completed\n                \n                \n                    Started: 2025-08-19\n                    \n                     Ended: 2025-08-23\n                    \n                    Calculating...\n                \n                \n                    자격증 sql\n                \n                혹시 모르니 준비해 봅시다.\n            \n        \n        \n        \n            \n                \n                    토익 스피킹 준비\n                    completed\n                \n                \n                    Started: 2025-07-19\n                    \n                     Ended: 2025-07-26\n                    \n                    Calculating...\n                \n                \n                    영어 자격증\n                \n                토익 스피킹을 준비해 봅시다\n            \n        \n        \n        \n            \n                \n                    학부 3학년 1학기\n                    completed\n                \n                \n                    Started: 2024-12-21\n                    \n                     Ended: 2025-06-20\n                    \n                    Calculating...\n                \n                \n                    산업공학 학부\n                \n                3학년 1학기 학부 할 일 총 정리\n            \n        \n        \n        \n            \n                \n                    ADP 필기 준비\n                    completed\n                \n                \n                    Started: 2025-02-02\n                    \n                     Ended: 2025-02-22\n                    \n                    Calculating...\n                \n                \n                    자격증 데이터 분석\n                \n                과연 2번째 도전은 성공할 것인가\n            \n        \n        \n        \n            \n                \n                    2학년 2학기 학부 정리\n                    completed\n                \n                \n                    Started: 2024-09-02\n                    \n                     Ended: 2024-12-20\n                    \n                    Calculating...\n                \n                \n                    산업공학 학부\n                \n                2학년 2학기 학부 개념 정리\n            \n        \n        \n        \n            \n                \n                    AWS SAA 준비\n                    completed\n                \n                \n                    Started: 2024-04-15\n                    \n                     Ended: 2024-05-22\n                    \n                    Calculating...\n                \n                \n                    자격증 cloud\n                \n                AWS SAA를 준비해 봅시다.\n            \n        \n        \n        \n            \n                \n                    TOFEL 준비\n                    failed\n                \n                \n                    Started: None\n                    \n                     Ended: None\n                    \n                    Calculating...\n                \n                \n                    English\n                \n                준비해 봅시다\n            \n        \n        \n        \n            \n                \n                    OPIc 준비\n                    failed\n                \n                \n                    Started: 2025-06-16\n                    \n                     Ended: 2025-07-07\n                    \n                    Calculating...\n                \n                \n                    영어 자격증\n                \n                OPIc을 준비해 봅시다\n            \n        \n        \n\n\n\n보관중인 자료\n\n\n\n\n    \n    \n        \n            Blog\n        \n        \n        \n            Hadoop\n        \n        \n        \n            vault\n        \n        \n        \n            Kaggle\n        \n        \n        \n            선형대수\n        \n        \n        \n            Helm\n        \n        \n        \n            Terraform\n        \n        \n        \n            Machine Learning\n        \n        \n        \n            k8s\n        \n        \n        \n            Problem Solving\n        \n        \n        \n            AirFlow\n        \n        \n        \n            금융\n        \n        \n        \n            ROS\n        \n        \n        \n            독서"
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#한국은행",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#한국은행",
    "title": "관심 분야 JD",
    "section": "한국은행",
    "text": "한국은행\n\n\n\n채용 절차\n\n\n\n채용 인원: 컴퓨터공학 15명 이내\n어학 성적 우대: TOEIC, TOEFL(iBT), TEPS 중\n필기 출제:\n\n전공 학술: 소프트웨어공학, 데이터베이스, 컴퓨터구조, 데이터통신, 정보보호, 운영체제, 자료구조, 인공지능, 기계학습\n논술: 주요 경제 / 금융 이슈, 인문학 등\n기출\n\n면접 전형:\n\n1차 실무 면접: 집단 토론, 심층 면접\n2차 집행 간부 면접",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#한국-투자증권",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#한국-투자증권",
    "title": "관심 분야 JD",
    "section": "한국 투자증권",
    "text": "한국 투자증권\n\n자기소개서 문항\n\n성장과정:가족사항, 학창시절, 교우관계, 생활습관, 자신에게 크게 영향을 미친 사건 등을 포함하여 구체적으로 작성 (최대 500자)\n실패 혹은 좌절을 극복한 사례와 이를 통해 얻은 교훈은? (최대 300자)\n증권업을 선택하게된 이유와 증권사 중 당사를 선택한 이유 (최대 300자)\n지원한 분야는 어떤 일을 한다고 생각하는가? (최대 200자)\n지원분야에 본인이 적합한 이유를 증명하시오. (최대 500자)\n당신의 인생계획(Life Plan)에서 꿈은 무엇이고, 그 꿈을 이루기 위해 회사가 어떻게 도움을 줄 수 있는지 서술하시오. (최대 500자)",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#금융-감독원",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#금융-감독원",
    "title": "관심 분야 JD",
    "section": "금융 감독원",
    "text": "금융 감독원\n\n채용 인원: IT 7명\n\n - 어학 성적 우대: TOEIC 730, TOEFL(iBT) 79, TEPS 368 만점 - 1차 필기 시험: NCS 직업 기초 시험 - 2차 필기 시험 - 전공 시험 - 논술 - 1차 면접 전형 - 실무진, 외부위원 개별 면접, 집단 토론 - 2차 면접 전형 - 임원, 외부위원 개별 면접",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#ibk-기업은행",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#ibk-기업은행",
    "title": "관심 분야 JD",
    "section": "IBK 기업은행",
    "text": "IBK 기업은행\n  - ADP를 꼭 따자 - 산업공학 석사 우대 - 금융권 공동채용 박람회 우수면접자 우대\n\n\n\n직무 소개\n\n\n\n\n\n전형 방법\n\n\n\n필기 시험\n\n직업 기초(객관식 40문항): 의사소통, 문제 해결, 자원관리, 조직 이해, 수리, 정보\n직무 수행(객관식 30문항, 주관식 5문항): 데이터베이스, 데이터분석, 인공지능 모델링, 블록체인, 시사 등\n\n실기 시험\n\n실기시험 전, AI역량검사 및 인성검사(온라인) 실시 예정\n(공통) 개인발표, 토론, 인터뷰\n(디지털, IT) 코딩테스트\n\n면접 시험\n\n多대多 질의응답을 통해 인성, 윤리의식, 직무·조직적합도 등의 평가항목을 기준으로 종합평가(평가위원 점수합계)\n\n\n\n자기소개 문항\n\nIBK와 미래를 함께해 나갈 지원자님을 환영합니다! 다양한 회사들 중에서 IBK를 선택하신 이유가 궁금한데요, 지원동기에 대해 편안하게 이야기해 주세요. (1500자 이내)\n지원자님의 여러 장점 중 “팀웍”에 대해 듣고 싶어요. 최선의 결과를 이끌어내기 위해 팀원으로서 했던 역할에 대해 구체적인 경험을 들어 말씀해 주세요. (1500자 이내)\n지원자님이 생각하는 본인의 단점에 대해 알고 싶어요. 그리고 그것을 극복하기 위해 기울이셨던 노력에 대해서도 자유롭게 전달해 주세요. (1500자 이내)\n은행원이라는 직업이 지원자님께 왜 어울리는지 궁금합니다. 지원자님만이 갖고 있는 차별화된 스토리를 저희에게 들려 주세요. (1500자 이내)",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/금융/00.html#우리-은행",
    "href": "posts/01_projects/진로준비/notes/금융/00.html#우리-은행",
    "title": "관심 분야 JD",
    "section": "우리 은행",
    "text": "우리 은행\n\n\n\n직무 내용\n\n\n\n신입 행원 연수: 약 1년 이내 영업점 근무 후, 유관 부서 및 본부 부서 배치. 인력 운영 및 업무상 필요에 따라 변동될 수 있음\n\n(이거 잘못하면 내 커리어 개같이 멸망할 수도 있겠는데)\n\nTECH 직무 관련 전공 우대 (구체적으로 무슨 전공인지 모르겠음)\n2025 금융권 공동채용 박람회 우수면접자 우대\n\n\n\n\n우대 자격증\n\n\n\n\n\n공인 영어\n\n\n\n\n\n전형 방법\n\n\n\n서류 전형: 입행 지원서 심사 및 AI 역량검사\n\nAI 역량검사: 자기보고, 전략게임, 영상 면접\n\n1차 면접: 기본 역량 면접\n2차 면접:\n\n인사이트 면접(PT, 포트폴리오), 참여형 팀워크 프로그램, 직무 / 인성 면접\n코딩테스트: 알고리즘 및 SQL\n\n최종 면접: 심화 역량 면접 (blind 면접. 그럼 어떻게 본다는거지?)\n\n\n자기소개 문항\n\n우리은행 및 해당 부문에 지원한 동기와 입행 후 이루고 싶은 목표를 구체적으로 서술해 주세요. (800자 이내)\n본인 성격 중 은행원 업무와 가장 맞지 않는 부분이 무엇이라고 생각하며, 그 이유를 구체적으로 설명해 주세요. (800자 이내)\n과거에 했던 선택이나 행동 중, 후회하는 사례를 구체적으로 작성해 주세요.(800자 이내)\n본인이 IT 직무에서 다른 지원자와 차별화된 경쟁력을 갖추었다고 생각하는 부분을 구체적인 사례를 근거로 설명해 주세요. (800자 이내)",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html",
    "title": "대학원 준비",
    "section": "",
    "text": "숭실대 성적 확정 7월 5일",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#서울대",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#서울대",
    "title": "대학원 준비",
    "section": "서울대",
    "text": "서울대\n\n중복지원 불가\nTeps 327 이상\n\n\n데이터사이언스\n\n입학지원서 접수: 10.07 - 10.11 17:00 까지\n서류 제출: 10.14 17:00 까지\n1차 합격자 발표: 10.28 18:00\n면접: 11.01\n최종 합격자 발표: 11.21 18:00\n\n\n면접이 제일 중요해 보임\n데이터사이언스 면접은 데이터사이언스에 관련된 3가지 주제 중 지원자가 2가지 주제를 선택해 품.\n데이터사이언스는 1차 서류에서 탈락할 가능성이 높아 보임\n그렇다고 서울대 산업공학 대학원이 만만한건 절대 아님",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#카이스트",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#카이스트",
    "title": "대학원 준비",
    "section": "카이스트",
    "text": "카이스트\n\n온라인 원서접수: 6.27 - 7.9 17:30 까지\n서류 제출: 7.11 18:00 까지\n1단계 전형 합격자 발표: 8.07 14:00 이후\n면접: 8.12 - 8.14 중 진행\n최종 합격자 발표: 9.18 14:00 이후\n\n\n2 지망 산업및시스템공학과 지원 가능\nTeps 326 이상, TOEIC 720 이상\n전형료 10만원\n산업공학 컨택은 면접 다 끝나고 있다\n데이터사이언스 사전 컨택은 강력히 권장된다고 한다.",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#포항공대",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#포항공대",
    "title": "대학원 준비",
    "section": "포항공대",
    "text": "포항공대",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#연세대",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#연세대",
    "title": "대학원 준비",
    "section": "연세대",
    "text": "연세대\n\n1개 학과에만 지원해야 함\n영어 성적 안봄\n\n\n원서접수: 10. 8 10:00 ~ 10. 16 17:00\n구술/실기시험대상자 발표: 11. 8 17:00\n구술시험 및 실기시험: 11. 16\n최종합격자 발표: 12. 6. 17:00\n\n\n스마트시스템 연구실 - 김우주",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/02.html#고려대",
    "href": "posts/01_projects/진로준비/notes/대학원/02.html#고려대",
    "title": "대학원 준비",
    "section": "고려대",
    "text": "고려대",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#인프라-엔지니어",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#인프라-엔지니어",
    "title": "관심 분야 JD",
    "section": "인프라 / 엔지니어",
    "text": "인프라 / 엔지니어",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#computer-vision-품질검사",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#computer-vision-품질검사",
    "title": "관심 분야 JD",
    "section": "Computer Vision / 품질검사",
    "text": "Computer Vision / 품질검사\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 현대오토에버\n\n\n\n\n\n\n2025 상반기 현대트랜시스\n\n\n\n\n\n\n2025 상반기 에스엘",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#자율주행",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#자율주행",
    "title": "관심 분야 JD",
    "section": "자율주행",
    "text": "자율주행\n \n\n\n\n\n2025 상반기 KAI\n\n\n\n\n\n\n2025 상반기 현대트랜시스",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#llm",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#llm",
    "title": "관심 분야 JD",
    "section": "LLM",
    "text": "LLM\n\n\n\n2024 LG화학 기반기술 연구소",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/00.html#기타",
    "href": "posts/01_projects/진로준비/notes/대학원/00.html#기타",
    "title": "관심 분야 JD",
    "section": "기타",
    "text": "기타\n\n\n\n2025 상반기 HD한국조선해양\n\n\n\n \n\n \n\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 삼성물산\n\n\n\n\n\n\n2024 하반기 LG U+\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 네이버\n\n\n\n\n\n\n2025 상반기 네이버",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html",
    "href": "posts/01_projects/toeic/index.html",
    "title": "Toeic 준비",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2026-01-12\n        종료일: 2026-02-12\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html#details",
    "href": "posts/01_projects/toeic/index.html#details",
    "title": "Toeic 준비",
    "section": "Details",
    "text": "Details\n빠르게 끝내 봅시다.",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html#tasks",
    "href": "posts/01_projects/toeic/index.html#tasks",
    "title": "Toeic 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html#참고-자료",
    "href": "posts/01_projects/toeic/index.html#참고-자료",
    "title": "Toeic 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/index.html#related-posts",
    "href": "posts/01_projects/toeic/index.html#related-posts",
    "title": "Toeic 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/index.html",
    "href": "posts/02_categories/machine_learning/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "machine learning 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/index.html#details",
    "href": "posts/02_categories/machine_learning/index.html#details",
    "title": "Machine Learning",
    "section": "",
    "text": "machine learning 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/index.html#tasks",
    "href": "posts/02_categories/machine_learning/index.html#tasks",
    "title": "Machine Learning",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/index.html#참고-자료",
    "href": "posts/02_categories/machine_learning/index.html#참고-자료",
    "title": "Machine Learning",
    "section": "참고 자료",
    "text": "참고 자료\n\n이 책\nudemy machine learning 강의",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/index.html#related-posts",
    "href": "posts/02_categories/machine_learning/index.html#related-posts",
    "title": "Machine Learning",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/17.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/17.html#preprocessing",
    "title": "Eclat",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/16.csv', header=None)\ntransactions = []\nfor i in range(0, len(dataset)):\n    transactions.append([str(dataset.values[i, j]) for j in range(0, len(dataset.columns))])",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Eclat"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/17.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/17.html#modeling",
    "title": "Eclat",
    "section": "Modeling",
    "text": "Modeling\n\nfrom apyori import apriori\n\nrules = apriori(transactions=transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2, max_length=2)\n\n\nresults = list(rules)\nresults\n\n[RelationRecord(items=frozenset({'light cream', 'chicken'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n RelationRecord(items=frozenset({'mushroom cream sauce', 'escalope'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n RelationRecord(items=frozenset({'honey', 'fromage blanc'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n RelationRecord(items=frozenset({'herb & pepper', 'ground beef'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n RelationRecord(items=frozenset({'tomato sauce', 'ground beef'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n RelationRecord(items=frozenset({'olive oil', 'light cream'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n RelationRecord(items=frozenset({'olive oil', 'whole wheat pasta'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n RelationRecord(items=frozenset({'pasta', 'shrimp'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]\n\n\n\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    return list(zip(lhs, rhs, supports))\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Product 1', 'Product 2', 'Support'])\nresultsinDataFrame\n\n\n\n\n\n\n\n\nProduct 1\nProduct 2\nSupport\n\n\n\n\n0\nlight cream\nchicken\n0.004533\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n\n\n2\npasta\nescalope\n0.005866\n\n\n3\nfromage blanc\nhoney\n0.003333\n\n\n4\nherb & pepper\nground beef\n0.015998\n\n\n5\ntomato sauce\nground beef\n0.005333\n\n\n6\nlight cream\nolive oil\n0.003200\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n\n\n8\npasta\nshrimp\n0.005066\n\n\n\n\n\n\n\n\nresultsinDataFrame.nlargest(n=10, columns='Support')\n\n\n\n\n\n\n\n\nProduct 1\nProduct 2\nSupport\n\n\n\n\n4\nherb & pepper\nground beef\n0.015998\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n\n\n2\npasta\nescalope\n0.005866\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n\n\n5\ntomato sauce\nground beef\n0.005333\n\n\n8\npasta\nshrimp\n0.005066\n\n\n0\nlight cream\nchicken\n0.004533\n\n\n3\nfromage blanc\nhoney\n0.003333\n\n\n6\nlight cream\nolive oil\n0.003200",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Eclat"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/01.html#load-library-and-data",
    "href": "posts/02_categories/machine_learning/notes/01.html#load-library-and-data",
    "title": "data preprocessing",
    "section": "Load Library and data",
    "text": "Load Library and data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/00-data.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nx\n\narray([['France', 44.0, 72000.0],\n       ['Spain', 27.0, 48000.0],\n       ['Germany', 30.0, 54000.0],\n       ['Spain', 38.0, 61000.0],\n       ['Germany', 40.0, nan],\n       ['France', 35.0, 58000.0],\n       ['Spain', nan, 52000.0],\n       ['France', 48.0, 79000.0],\n       ['Germany', 50.0, 83000.0],\n       ['France', 37.0, 67000.0]], dtype=object)\n\n\n\ny\n\narray(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n      dtype=object)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/01.html#taking-care-of-missing-data",
    "href": "posts/02_categories/machine_learning/notes/01.html#taking-care-of-missing-data",
    "title": "data preprocessing",
    "section": "Taking care of Missing data",
    "text": "Taking care of Missing data\n\ndelete\nreplace\n\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(x[:, 1:3])\nx[:, 1:3] = imputer.transform(x[:, 1:3])\nprint(x)\n\n[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 63777.77777777778]\n ['France' 35.0 58000.0]\n ['Spain' 38.77777777777778 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/01.html#encoding-cagegorical-data",
    "href": "posts/02_categories/machine_learning/notes/01.html#encoding-cagegorical-data",
    "title": "data preprocessing",
    "section": "Encoding Cagegorical data",
    "text": "Encoding Cagegorical data\n\n단순히 categorical 변수를 1, 2, 3으로 변형하면 순서가 고려된 것으로 간주될 수 있다.\n그래서 [0, 0, 1], [1, 0, 1] 이런 식으로 one hot encoding을 진행한다.\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\nprint(x)\n\n[[1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(y)\nprint(y)\n\n[0 1 0 0 1 1 0 1 0 1]",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/01.html#split-dataset-into-training-set-and-test-set",
    "href": "posts/02_categories/machine_learning/notes/01.html#split-dataset-into-training-set-and-test-set",
    "title": "data preprocessing",
    "section": "Split dataset into training set and test set",
    "text": "Split dataset into training set and test set\n\nfeature scaling 이전에 진행되어야함. (test set은 모델이 모르는 정보가 되야하기 때문)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/01.html#feature-scaling",
    "href": "posts/02_categories/machine_learning/notes/01.html#feature-scaling",
    "title": "data preprocessing",
    "section": "feature scaling",
    "text": "feature scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = sc.transform(X_test[:, 3:])\n\n\nprint(X_train)\n\n[[0.0 1.0 0.0 1.589843519888049 1.6688118414569673]\n [0.0 1.0 0.0 0.11473097566202425 -0.017053551664523183]\n [0.0 0.0 1.0 -1.8029153318318079 -1.4008274581573077]\n [0.0 0.0 1.0 -0.06556055752115642 -1.0500115382013906]\n [1.0 0.0 0.0 -0.32780278760578313 0.26554816163329864]\n [0.0 0.0 1.0 -0.18029153318318067 -0.260675718300577]\n [1.0 0.0 0.0 1.294821011042844 1.31799592150105]\n [1.0 0.0 0.0 -0.622825296450988 -0.5237876582675148]]\n\n\n\nprint(X_test)\n\n[[0.0 1.0 0.0 -1.3603815685640004 -0.874603578223432]\n [1.0 0.0 0.0 0.7047759933524341 0.704068061578195]]",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/11.html#독립성-가정이-필요한-수학적-이유",
    "href": "posts/02_categories/machine_learning/notes/11.html#독립성-가정이-필요한-수학적-이유",
    "title": "Naive Bayes",
    "section": "독립성 가정이 필요한 수학적 이유",
    "text": "독립성 가정이 필요한 수학적 이유\nNaive Bayes는 베이즈 정리를 기반으로 합니다. 클래스 \\(C\\)와 특성 벡터 \\(X = (x_1, x_2, ..., x_n)\\)이 있을 때, 베이즈 정리는 다음과 같습니다:\n\\[P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\\]\n여기서 \\(P(C|X)\\)는 특성 \\(X\\)가 주어졌을 때 클래스 \\(C\\)일 확률입니다. 문제는 \\(P(X|C)\\)를 계산하기가 어렵다는 점입니다. 특성이 많을수록 가능한 \\(X\\) 조합의 수가 기하급수적으로 증가하기 때문입니다.\n이 문제를 해결하기 위해 Naive Bayes는 모든 특성이 서로 조건부 독립이라고 가정합니다. 즉:\n\\[P(x_i|C, x_1, x_2, ..., x_{i-1}, x_{i+1}, ..., x_n) = P(x_i|C)\\]\n이 독립성 가정을 통해 \\(P(X|C)\\)를 다음과 같이 단순화할 수 있습니다:\n\\[P(X|C) = P(x_1, x_2, ..., x_n|C) = P(x_1|C) \\cdot P(x_2|C) \\cdot ... \\cdot P(x_n|C) = \\prod_{i=1}^{n} P(x_i|C)\\]\n이렇게 특성 간 독립성을 가정함으로써 복잡한 결합 확률을 개별 특성의 확률들의 곱으로 계산할 수 있게 되어 계산이 매우 단순해집니다. 이것이 바로 Naive Bayes에서 “naive(순진한)” 독립성 가정이 반드시 필요한 이유입니다.\n이제 Naive Bayes에서 독립성 가정이 필요한 이유가 수학적으로 명확하게 설명되었습니다. 이 설명을 통해 알 수 있듯이:\n\nNaive Bayes는 베이즈 정리를 사용하여 P(C|X)를 계산합니다.\n문제는 P(X|C)를 계산하는 것이 복잡하다는 점입니다.\n독립성 가정을 통해 P(X|C)를 개별 특성들의 조건부 확률 곱으로 단순화할 수 있습니다.\n이 단순화가 없다면, 특성의 조합이 많아질수록 계산이 기하급수적으로 복잡해집니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/11.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/11.html#preprocessing",
    "title": "Naive Bayes",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/11.html#modeling---linear",
    "href": "posts/02_categories/machine_learning/notes/11.html#modeling---linear",
    "title": "Naive Bayes",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GaussianNB?Documentation for GaussianNBiFittedGaussianNB()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/11.html#predict",
    "href": "posts/02_categories/machine_learning/notes/11.html#predict",
    "title": "Naive Bayes",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[59  4]\n [ 7 30]]\n\n\n0.89",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/11.html#predict-1",
    "href": "posts/02_categories/machine_learning/notes/11.html#predict-1",
    "title": "Naive Bayes",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[59  4]\n [ 7 30]]\n\n\n0.89",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/18.html",
    "href": "posts/02_categories/machine_learning/notes/18.html",
    "title": "Upper Confidence Bound",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Upper Confidence Bound"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/04.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/04.html#preprocessing",
    "title": "Polynorminal Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/04.html#linear-regression-model",
    "href": "posts/02_categories/machine_learning/notes/04.html#linear-regression-model",
    "title": "Polynorminal Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(x, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/04.html#polynorminal-linear-regression",
    "href": "posts/02_categories/machine_learning/notes/04.html#polynorminal-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Polynorminal Linear Regression",
    "text": "Polynorminal Linear Regression\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=4)\nx_poly = poly.fit_transform(x)\nregressor2 = LinearRegression()\nregressor2.fit(x_poly, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/04.html#visualize-linear-regression",
    "href": "posts/02_categories/machine_learning/notes/04.html#visualize-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Visualize Linear Regression",
    "text": "Visualize Linear Regression\n\nplt.scatter(x, y, color='red')\nplt.plot(x, regressor.predict(x), color='blue')\nplt.title('Linear Regression Model')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/04.html#visualize-poly-linear-regression",
    "href": "posts/02_categories/machine_learning/notes/04.html#visualize-poly-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Visualize Poly Linear Regression",
    "text": "Visualize Poly Linear Regression\n\nplt.scatter(x, y, color='red')\nplt.plot(x, regressor2.predict(x_poly), color='blue')\nplt.title('Poly Linear Regression Model')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/13.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/13.html#preprocessing",
    "title": "Random Forest",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/13.html#modeling---linear",
    "href": "posts/02_categories/machine_learning/notes/13.html#modeling---linear",
    "title": "Random Forest",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators=10, criterion='entropy')\nclassifier.fit(x_train, y_train)\n\nRandomForestClassifier(criterion='entropy', n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(criterion='entropy', n_estimators=10)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/13.html#predict",
    "href": "posts/02_categories/machine_learning/notes/13.html#predict",
    "title": "Random Forest",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[60  9]\n [ 1 30]]\n\n\n0.9",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/13.html#predict-1",
    "href": "posts/02_categories/machine_learning/notes/13.html#predict-1",
    "title": "Random Forest",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[60  9]\n [ 1 30]]\n\n\n0.9",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/08.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/08.html#preprocessing",
    "title": "Logistic Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/08.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/08.html#modeling",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(x_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/08.html#predict",
    "href": "posts/02_categories/machine_learning/notes/08.html#predict",
    "title": "Logistic Regression",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[68  5]\n [ 7 20]]\n\n\n0.88",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/02.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/02.html#preprocessing",
    "title": "Simple Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/02-data.csv')\n\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/02.html#train",
    "href": "posts/02_categories/machine_learning/notes/02.html#train",
    "title": "Simple Linear Regression",
    "section": "train",
    "text": "train\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/02.html#predict",
    "href": "posts/02_categories/machine_learning/notes/02.html#predict",
    "title": "Simple Linear Regression",
    "section": "predict",
    "text": "predict\n\ny_pred = regressor.predict(X_test)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/02.html#visualize",
    "href": "posts/02_categories/machine_learning/notes/02.html#visualize",
    "title": "Simple Linear Regression",
    "section": "visualize",
    "text": "visualize\n\nplt.scatter(X_train, y_train, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (training set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.scatter(X_test, y_test, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (test set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/02.html#evaluate",
    "href": "posts/02_categories/machine_learning/notes/02.html#evaluate",
    "title": "Simple Linear Regression",
    "section": "evaluate",
    "text": "evaluate\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n0.9488625193250544",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/14.html#k-means-algorithm",
    "href": "posts/02_categories/machine_learning/notes/14.html#k-means-algorithm",
    "title": "k-means clustering",
    "section": "K-means++ algorithm",
    "text": "K-means++ algorithm\n\n시작점을 잘 선택하여 수렴 속도를 높이는 알고리즘\n초기 중심점을 선택할 때, 멀리 떨어진 중심점을 선택하도록 함\n\n첫 번째 중심점을 랜덤하게 선택\n나머지 중심점을 선택할 때, 각 데이터 포인트와 가장 먼 중심점을 선택\nk개의 중심점을 선택할 때까지 반복",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/14.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/14.html#preprocessing",
    "title": "k-means clustering",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/14.csv')\nx = dataset.iloc[:, [3, 4]].values",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/14.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/14.html#modeling",
    "title": "k-means clustering",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n  cluster = KMeans(n_clusters=i, init='k-means++')\n  cluster.fit(x)\n  wcss.append(cluster.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of Cluster')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster = KMeans(n_clusters=5, init='k-means++')\ny_kmeans = cluster.fit_predict(x)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/14.html#visualize",
    "href": "posts/02_categories/machine_learning/notes/14.html#visualize",
    "title": "k-means clustering",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], c='red', label='Cluster 1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], c='pink', label='Cluster 2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], c='blue', label='Cluster 3')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], c='purple', label='Cluster 4')\nplt.scatter(x[y_kmeans == 4, 0], x[y_kmeans == 4, 1], c='cyan', label='Cluster 5')\nplt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], s=100, c='black', label='Centroids')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/index.html",
    "href": "posts/02_categories/42_seoul/index.html",
    "title": "42 Seoul",
    "section": "",
    "text": "42 seoul에서 진행한 프로젝트들에 대한 노트 모음입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/index.html#details",
    "href": "posts/02_categories/42_seoul/index.html#details",
    "title": "42 Seoul",
    "section": "",
    "text": "42 seoul에서 진행한 프로젝트들에 대한 노트 모음입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/index.html#tasks",
    "href": "posts/02_categories/42_seoul/index.html#tasks",
    "title": "42 Seoul",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/index.html#related-posts",
    "href": "posts/02_categories/42_seoul/index.html#related-posts",
    "title": "42 Seoul",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/01.html#intro",
    "href": "posts/02_categories/42_seoul/notes/01.html#intro",
    "title": "ft_transcendence - github action",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul 공통과정 6서클 과제\n\n\n42 Seoul 공통과정의 마지막 과제입니다. 이 프로젝트는 개발자가 선호하는 라이브러리와 프레임워크를 자유롭게 선택하여 구현할 수 있다는 점이 특징입니다.\n대형 협업 과제인 만큼, 과제에 명시되어있지 않지만 협업을 위한 툴도 공부해서 다양하게 적용해볼 수 있는 좋은 과제인것 같습니다. 저같은 경우에는 coursera, udemy 강의를 통해 agile 협업 방식과 github에서의 적용 방법에 대해 공부를 했고, 프로젝트 진행에 있어서 꽤 도움이 됐던걸로 기억합니다. 사실 프로젝트를 진행하다보니, agile 방식을 온전히 다 적용하기엔 적합하지 않다고 판단했지만, Kanban Board로 프로젝트를 관리하는 것 같은 부분은 꽤 유용하게 활용할 수 있었습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/01.html#프로젝트-및-구현-설명",
    "href": "posts/02_categories/42_seoul/notes/01.html#프로젝트-및-구현-설명",
    "title": "ft_transcendence - github action",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\n개요\n과제 명세서\n해당 과제는 실시간 Pong 게임 매칭 웹사이트를 만드는게 목표입니다. 저는 이번 프로젝트에서 github action 설정, User Management Backend 설계와 42 API를 이용한 OAuth 인증, JWT 구현, Game History를 Block Chain으로 저장하는 파트를 담당했습니다.\n참고한 자료는 다음과 같습니다:\n\nGoogle Agile Project 관리\nGithub Action Docs\nGithub CLI Docs\nDjango udemy 강좌\nDjango Rest Framework Docs\nDjango Simple JWT\nJWT Token 탈취 대응 시나리오\nmicro service에서 JWT 활용 방법\nRefresh Token을 사용해야 하는 이유\nCookie에서의 same site 옵션\nBitcoin 백서\nSolidity Udemy 강의\nSolidity Docs\nnomad coder 블록체인 시리즈\n블록체인 강의\n\n\n\n\n\n\n\n이 포스팅에서는 github action setting, jwt, block chain 부분만 다루겠습니다.\n전체 코드는 비공개 되어있는 상태입니다.\n\n\n\n\n\nGithub Action Setting\ngithub를 이용해서 agile 방법론을 적용할 수 있도록 의도했고, 자동화와 template을 이용해 통일성 있는 구조를 유지하려고 했습니다.\n1. 회의를 통해 진행해야 하는 작업을 Kanban board에 정리한다.\n\n\n\nGithub Kanban Board\n\n\n각각의 column에는 다음과 같은 내용이 들어갑니다.\n\nDiscussion: 논의가 필요한 작업. 개개인이 자유롭게 올릴 수 있습니다\nBacklog: Discussion에 있는 내용 중 구현하기로 회의에서 정한 작업\nReady: Back log에 있는 작업 중 이번 Sprint에서 구현할 작업들\nIn Progress: Ready에 있는 작업 중 누군가가 작업중인 것\nDone: master branch에 merge가 완료된 작업\n\n자세한 내용은 meeting 부분을 참고해 주세요.\n참고로 Disccusion에 작업을 올리는 방법은 template에 맞게 issue를 올리면 됩니다.\n\n\n\nDiscussion template\n\n\n아래와 같이 설정 파일을 만들어서 ‘.github/ISSUE_TEMPLATE/’ 폴더 안에 저장하면 issue create 시 자동으로 template이 뜨게 할 수 있습니다.\nname: New discussion\ndescription: new discussion\ntitle: \"[DISCUSSION]\"\nlabels: [\"enhancement\"]\nprojects: [\"org_name/5\"]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        해당 기능과 관련된 request가 이미 존재하는지 확인해주세요.\n  - type: textarea\n    id: story\n    attributes:\n      label: Story\n      description: 해당 기능에 대한 설명이나 필요한 배경을 작성해주세요.\n      placeholder: 자유로운 양식으로 작성해주세요.\n    validations:\n      required: true\n2. Kanban board를 보고 개인이 능동적으로 고유 브랜치에 작업을 진행한다.\n\n\n\n빨간 밑줄 부분을 설정해줍니다.\n\n\nKanban board의 Ready section에 있는 작업을 클릭해서 들어간 후, assignees를 본인으로 선택해서 작업하면 됩니다. task completion criteria라는 내용이 보이는데, 이는 회의를 통해 결정하는 것으로, 나중에 작업이 완료되고 pull request 시, 평가자가 작업에 완성도에 대해 판단할 수 있는 기준으로 제공됩니다.\n자동화 코드는 아래와 같이 구현했습니다.\nname: Create branch\non:\n  issues:\n    types: [ assigned ]\n  pull_request:\n    types: [ opened, closed ]\njobs:\n  create_issue_branch_job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Generate token\n        id: generate_token\n        uses: tibdex/github-app-token@v2\n        with:\n          app_id: ${{ secrets.APP_ID }}\n          private_key: ${{ secrets.PRIVATE_KEY }}\n\n      # gh 명령어를 이용해 project의 상태를 In progress로 수정해줍니다.\n      - name: Project in-progress\n        if: github.event.action == 'assigned'\n        run: |\n          PROJECT_ID=$(gh project view 5 --owner organization-for-practice --format=json --jq '.id')\n          ITEM_ID=$(gh project item-list 5 --owner organization-for-practice --format=json --jq \".items[] | select(.content.number == ${NUMBER}) | .id\")\n          FIELD_ID=$(gh project field-list 5 --owner organization-for-practice --format=json --jq '.fields[2].id')\n          SINGLE_ID=$(gh project field-list 5 --owner organization-for-practice --format=json --jq '.fields[2].options[] | select(.name == \"In progress\") | .id')\n          gh project item-edit --id ${ITEM_ID} --field-id ${FIELD_ID} --single-select-option-id ${SINGLE_ID} --project-id ${PROJECT_ID}\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          NUMBER: ${{ github.event.issue.number }}\n\n      # assign한 작업에 대한 branch를 새로 만들어줍니다.\n      - name: Create Issue Branch\n        uses: robvanderleek/create-issue-branch@main\n        env:\n          GITHUB_TOKEN: ${{ steps.generate_token.outputs.token }}\n위의 코드는 assign한 작업을 Ready column에서 In progress column으로 옮겨주고, 자동으로 작업할 branch를 만들어줍니다.\nbranch 자동 생성은 이 workflow를 사용하였고, 적용 시 아래와 같이 브랜치가 생성됩니다.\nautoLinkIssue: true\nautoCloseIssue: true\nbranchName: tiny\ncommentMessage: |\n  \\\"${branchName}\\\" branch 생성 완료.\n  해당 branch를 통해서 main에 pull request 올려주세요.\nbranches:\n  - label: 'task list'\n    prefix: feature/${issue.title[12,27],}/\n    copyIssueAssigneeToPR: true\n  - label: 'bug'\n    prefix: hot_fix/${issue.title[6,21],}/\n    copyIssueAssigneeToPR: true\n  - label: '*'\n    skip: true\n위의 config 파일을 작성해주면 아래와 같이 브랜치가 생성됩니다.\n\n\n\n자동 생성된 branch\n\n\n이름도 자동으로 생성되게 해서 convention을 지켜야 한다는 부담을 줄여줬습니다.\n3. 작업이 완료되면, 모든 조건을 충족하는지 확인한 후, master에 merge 한다.\n\n\n\npull request 화면\n\n\n작업이 완료됬다고 판단되면 위 화면과 같이 pull request를 생성하고, Reviewer를 설정해주면 됩니다.\n\n\n\ntask completion criteria\n\n\n그러면 이전에 설정했던 기준들이 자동으로 불러와지고, 모든 항목에 체크가 완료되어야 merge를 할 수 있게 설정했습니다. 구현 코드는 아래와 같습니다.\nname: Master merge rutine\non:\n  pull_request_target:\n    types: [ opened, synchronize ]\n    branches:\n      - master\nenv:\n  PR_NUM: ${{ github.event.pull_request.number }}\n  GH_REPO: ${{ github.repository }}\njobs:\n  get_checklist:\n    runs-on: ubuntu-latest\n    if: github.event.action == 'opened'\n    steps:\n      - name: Generate token\n        id: generate_token\n        uses: tibdex/github-app-token@v2\n        with:\n          app_id: ${{ secrets.APP_ID }}\n          private_key: ${{ secrets.PRIVATE_KEY }}\n      - name: Get issue\n        id: issue_num\n        env:\n          BRANCH: ${{ github.event.pull_request.head.ref }}\n        run: |\n          echo $BRANCH | grep -o 'feature\\/.*\\/i[0-9]\\+' || echo $BRANCH | grep -o 'hot_fix\\/.*\\/i[0-9]\\+'\n          TMP=$(echo $BRANCH | grep -o 'i[0-9]\\+')\n          echo \"NUMBER=${TMP#i}\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Get issue body\n        id: issue_body\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          NUM: ${{ steps.issue_num.outputs.number }}\n        run: |\n          echo \"CONTENTS&lt;&lt;EOF\" &gt;&gt; $GITHUB_OUTPUT\n          gh issue view ${NUM} --json body --jq '.body' &gt;&gt; $GITHUB_OUTPUT\n          echo \"EOF\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Update checklist\n        run: |\n          gh pr comment $PR_NUM --body \"${BODY}\"\n        env:\n          GH_TOKEN: ${{ steps.generate_token.outputs.token }}\n          BODY: \"${{ steps.issue_body.outputs.contents }}\"\nmerge가 완료된 branch는 자동으로 삭제가 되도록 설정을 해주었습니다.\n이제 아래는 실제 프로젝트를 진행할 때 만들었던 rule들입니다.\n\n1. work flow\ngithub flow로 진행됩니다.\n\n\n\ngithub flow\n\n\n\n매 작업은 master branch의 HEAD를 기반으로 이루어집니다.\npr을 올리지 않는 개인 작업용 local branch는 자유롭게 생성해주세요.\nmaster에 직접적인 push는 관리자를 제외하고는 불가능합니다.\nmaster에 대한 merge는 squash merge로 진행됩니다.\n그 외의 merge는 rebase로 진행해주세요.\n\n\n\n2. work\n\nkanban board의 'Ready' 섹션에서 하나를 정해서 새로운 기능에 대한 작업을 진행해주세요.\n선택한 작업은 assignees에 자신의 팀원을 등록 후, Start Date를 해당 날짜로 설정해주세요.\nassignees 등록이 완료되면 자동으로 target branch가 생성됩니다.\n해당 branch에 팀원들이 필요한 기능들을 자유로운 방식으로 구현한 후, master branch에 merge 해주세요.\n단, 해당 branch에 대한 merge는 rebase로 진행해주세요.\nhot_fix issue나, new feature request issue는 discussion의 필요성이 있을 경우에 등록해주세요.\n작업 중, 현재 작업하는 범위 외에서 추가적인 기능이 필요할 경우 관련 issue에 comment를 남기거나, reopen 해주세요.\n\n\n\n3. commit message convention\n아래의 명령어를 입력해주세요\ngit config commit.template .github/COMMIT_MESSAGE_TEMPLATE\n이후, -m 옵션 없이 ’git commit’으로 message를 입력해주세요.\n\n\nCOMMIT_MESSAGE_TEMPLATE\n\n# commit message template\n# ▼ &lt;Title&gt; 작성\n\n# ▼ &lt;빈 줄&gt;\n\n# ▼ &lt;body&gt; 작성\n\n# ▼ &lt;빈 줄&gt;\n\n# ▼ &lt;footer&gt; 작성\n\n\n# About Convention\n#   &lt;Title&gt;\n#       - 필수로 입력해주세요\n#       - 형식: &lt;type&gt;: &lt;short summary&gt;\n#\n#       &lt;type&gt;\n#           - config: 설정 관련 파일 작성 또는 변경\n#           - docs: 문서 변경사항\n#           - feat: 새로운 기능\n#           - fix: 버그 수정\n#           - refactor: 기능 추가나 버그 수정이 아닌 변경 사항\n#           - remove: 코드나 파일 제거\n#           - style: 스타일 작성 또는 수정\n#           - test: 누락된 테스트 추가 또는 기존 테스트 수정\n#           - core: 기능 구현 외 시스템 관련 작업\n#\n#       &lt;short summary&gt;\n#           - 변경 사항에 대한 간단한 설명\n#           - 첫글자 소문자, 현재 시제, 명령문으로 마지막에 .(마침표) 없이 작성\n#\n#   &lt;body&gt;\n#       - 선택적으로 입력 해주세요\n#       - 현재 시제, 명령문으로 작성\n#       - 변경 사항의 동기(왜)를 설명\n#       - 변경 효과를 설명하기 위해 이전 동작과 현재 동작의 비교를 포함할 수 있음\n#\n#   &lt;footer&gt;\n#       - 선택적으로 입력 해주세요\n#       - 해당 commit과 관련된 task의 issue 번호들을 적어주세요\n#       - 'bug'나 'task list' label이 붙은 issue는 제외해주세요\n#       - ex) closes #&lt;issue 번호&gt; closes #&lt;issue 번호&gt; ...\n\n\n\n\n\n\n\ncommit message template은 이 사이트를 참고해서 만들었습니다.\n\n\n\n\n\n4. pull request\n\npull request는 500줄의 코드를 넘어가지 않게 작성 바랍니다.\n모든 check list를 통과한 request만 master에 merge 가능합니다.\nreviewers에는 해당 작업과 관련된 domain의 팀원을 선택해주세요. 최소 1명 이상의 동료에게 평가를 받은 request만 merge 가능합니다.\n\n\n\n5. meeting\n\ndaily meeting\n\n매일 정해진 시간에 팀원들은 각각 다음과 같은 사안에 대해 논의합니다.\n\n개인이 어제 작업한 내용\n개인이 오늘 작업할 내용\n개인이 현재 도움이 필요한 내용\n\n이후, 새로운 내용이 추가된 ('Disccusion' 섹션에 있는) issue 중 다음과 같은 내용에 대해 논의합니다.\n\n해당 issue가 유효한가\n추가적으로 필요하거나 필요 없는 내용\n해당 issue의 priority (매우 급함 / 급함 / 안 급함)\n해당 issue의 estimate (작업하는데 필요한 노력의 정량적인 수치)\n\n추가적으로, project의 'Back log' 항목에서 'Ready' 항목으로 추가해야 할 작업에 대해 논의하거나 'Ready' 항목에서 'Back log' 항목으로 제외할 작업에 대해 논의할 수 있습니다.\n\nsprint planning / retrospective\n\n2주에 한번 진행.\n이전 sprint에 대한 평가와 이후 sprint를 위한 계획을 세웁니다.\n\nplanning\n\nProject의 'Back log' 항목 중 본격적으로 작업을 진행할 항목을 정합니다.\ndaily meeting 시간을 조정할 수 있습니다.\n\nretrospective\n\n이전 sprint의 문제점에 대해 서로 의논해봅니다.\n\n\n\n\n\n\n\n\n\n프로젝트를 하다보니, 생각보다 진행 속도가 빨라서 2주에 한번 진행하는 sprint는 유명무실해져버렸습니다. 실제로는 daily meeting만 진행을 했습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/01.html#outro",
    "href": "posts/02_categories/42_seoul/notes/01.html#outro",
    "title": "ft_transcendence - github action",
    "section": "outro",
    "text": "outro\n내용이 너무 길어져서 2편에 계속 포스팅 하겠습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "ft_transcendence - github action"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/09.html#intro",
    "href": "posts/02_categories/42_seoul/notes/09.html#intro",
    "title": "cloud-1 코드 설명",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul outer 과제\n\n\n개념 설명에 이어서 진행하도록 하겠습니다.\n\n\n\n\n\n\n전체 코드는 github repo에서 확인하실 수 있습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/09.html#프로젝트-및-구현-설명",
    "href": "posts/02_categories/42_seoul/notes/09.html#프로젝트-및-구현-설명",
    "title": "cloud-1 코드 설명",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\npre requirements\n이 프로젝트를 진행하기 위해 필요한 것들은 다음과 같습니다.\n\nAWS IAM 계정\nPacker\nTerraform\nAnsible\njq\nboto3\n\n\n\nbuild\n최종 build는 (42 seoul 사람에게 익숙한) makefile을 사용했습니다.\n\n\n\n\n\n\n제가 아직 로컬에서 돌려볼만한 다른 build 툴을 배우지 않아서 makefile을 사용하긴 했지만, 사실 c언어도 아니고..이 과제 구현에서 이 tool이 그렇게 어울리진 않은거 같긴 합니다.\n\n\n\n\n\n.env\n\n# only 1 line variable is allowed\n\nAWS_REGION=\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=\nSERVER_INSTANCE_COUNT=\n\n# public subnet에 접근할 수 있는 ip address를 지정해줍니다.\nSSH_IP=\n\n# public subnet에 접근할 때 사용할 ssh key path를 지정해줍니다.\nSSH_PUBLIC_KEY_PATH=\nSSH_PRIVATE_KEY_PATH=\n\n# docker compose setting\nMYSQL_USER=\nMYSQL_PASSWORD=\nMYSQL_ROOT_PASSWORD=\nDATABASE_NAME=\nSITE_TITLE=\nADMIN_NAME=\nADMIN_PASSWORD=\nADMIN_EMAIL=\nUSER_NAME=\nUSER_PASSWORD=\nUSER_EMAIL=\n\n\n\nMakefile\n\n# .env의 내용들을 makefile의 변수로 load 해줍니다.\n\ninclude .env\nexport\n\n먼저 필요한 변수들을 모두 .env에 저장해 한번에 관리할 수 있게 구현했습니다. 저장된 .env 내용은 makefile에서 위의 명령어로 불러와 build 명령어 실행시 사용할 수 있게 했습니다.\nmakefile이 .env 파일을 읽을 때 한 줄씩 읽기 때문에, 위의 방식으로 구현하면 여러 줄에 걸친 환경변수는 사용하기 어려울 수 있습니다. (그럴땐 그냥 makefile 말고 다른 tool을 쓰면 됩니다)\n\n\nMakefile\n\n.PHONY: provision deploy all destroy re build_ami\n\nall: build_ami provision deploy\n\nbuild_ami: packer\n    packer init $(PACKER_PATH)/database.pkr.hcl\n    @PKR_VAR_AWS_REGION=$(AWS_REGION) \\\n    PKR_VAR_MYSQL_USER=$(MYSQL_USER) \\\n    PKR_VAR_MYSQL_PASSWORD=$(MYSQL_PASSWORD) \\\n    PKR_VAR_DATABASE_NAME=$(DATABASE_NAME) \\\n    PKR_VAR_MYSQL_ROOT_PASSWORD=$(MYSQL_ROOT_PASSWORD) \\\n    packer build $(PACKER_PATH)/database.pkr.hcl\n\nprovision: build_ami terraform\n    terraform -chdir=$(PROVISION_PATH) init\n    @TF_VAR_AWS_REGION=$(AWS_REGION) \\\n    TF_VAR_SERVER_INSTANCE_COUNT=$(SERVER_INSTANCE_COUNT) \\\n    TF_VAR_SSH_IP=$(SSH_IP) \\\n    TF_VAR_SSH_PUBLIC_KEY_PATH=$(SSH_PUBLIC_KEY_PATH) \\\n    terraform -chdir=$(PROVISION_PATH) apply -auto-approve\n\ndeploy: ansible\n    @DB_PRIVATE_IP=\"$(shell terraform -chdir=$(PROVISION_PATH) output -json db_private_ip | jq -r '.[]' | tr '\\n' ' ')\" \\\n    ANSIBLE_HOST_KEY_CHECKING=False \\\n    ANSIBLE_REMOTE_USER=ubuntu \\\n    AWS_DEFAULT_REGION=$(AWS_REGION) \\\n    ANSIBLE_PYTHON_INTERPRETER=auto_silent \\\n    ansible-playbook \\\n    -i $(DEPLOY_PATH)/inventories \\\n    --private-key=$(SSH_PRIVATE_KEY_PATH) \\\n    $(DEPLOY_PATH)/server.yml \n\nbuild 과정은 ami 생성, provision, ansible deploy 순서로 진행됩니다.\n각 과정에 필요한 변수들은 명령어 수행 시 환경변수로 제공해줍니다. 대표적으로 ansible의 경우, provision 이후 생성된 database ec2의 private ip를 전달하고 있습니다.\n\n\nPacker 코드\n이 프로젝트에서는 데이터베이스 서버를 Private subnet에 위치시키고, Public subnet의 EC2만 이 데이터베이스에 접근할 수 있도록 설계했습니다. Private subnet에 있는 서버는 SSH 접근이 제한되기 때문에 Ansible로 직접 설정하기는 어렵습니다. 이런 경우 Packer로 미리 설정된 AMI를 생성하는 방법을 생각해볼 수 있습니다.\n구현한 Packer 파일 구조는 아래와 같습니다.\npacker/\n├── database.pkr.hcl\n└── ansible/\n    ├── _requirements/                      # docker compose setting files\n    ├── roles/setting_docker/tasks\n    │   └── main.yml\n    └── database.yml                        # playbook\n먼저 기본 이미지로 Ubuntu 20.04를 사용하도록 작성했습니다.\n\n\ndatabase.pkr.hcl\n\nsource \"amazon-ebs\" \"database\" {\n  region  = var.AWS_REGION\n  profile = \"default\"\n\n  ami_name      = \"hyunghki-database-${formatdate(\"YYYYMMDDhhmmss\", timestamp())}\"\n  instance_type = \"t2.micro\"\n  source_ami_filter {\n    filters = {\n      name                = \"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"\n      root-device-type    = \"ebs\"\n      virtualization-type = \"hvm\"\n    }\n    most_recent = true\n    owners      = [\"099720109477\"]\n  }\n  ssh_username = \"ubuntu\"\n}\n\nPacker는 기본적으로 이미지 생성을 위한 최소한의 기능만 제공하지만, 다양한 플러그인을 지원합니다. 여기서는 Ansible 플러그인을 사용하여 데이터베이스 서버 설정을 자동화했습니다.\n\n\ndatabase.pkr.hcl\n\nbuild {\n  sources = [\"source.amazon-ebs.database\"]\n\n  provisioner \"ansible\" {\n    playbook_file = \"${path.root}/ansible/database.yml\"\n    user = \"ubuntu\"\n    ansible_env_vars = [\n      \"ANSIBLE_HOST_KEY_CHECKING=False\",\n      \"MYSQL_USER=${var.MYSQL_USER}\",\n      \"MYSQL_PASSWORD=${var.MYSQL_PASSWORD}\",\n      \"DATABASE_NAME=${var.DATABASE_NAME}\",\n      \"MYSQL_ROOT_PASSWORD=${var.MYSQL_ROOT_PASSWORD}\",\n      \"ANSIBLE_PYTHON_INTERPRETER=auto_silent\"\n    ]\n  }\n}\n\n\n\nansible/database.yml\n\n- hosts: all\n  gather_facts: false\n  become: true\n  roles:\n    # docker compose를 machine에 설치해줍니다.\n    - role: setting_docker\n\n  tasks:\n    # docker compose에 필요한 파일들을 옮겨줍니다.\n    - name: copy_requirements\n      copy:\n        src: \"./_requirements/\"\n        dest: \"/home/{{ ansible_user }}/app/\"\n        mode: '0755'\n        directory_mode: '0755'\n\n    # 적절한 환경변수와 함께 docker compose 명령어를 실행합니다.\n    - name: execute docker compose\n      shell:\n        cmd: docker-compose up -d\n        chdir: \"/home/{{ ansible_user }}/app/\"\n      environment:\n        MYSQL_USER: \"{{ lookup('env', 'MYSQL_USER') }}\"\n        MYSQL_PASSWORD: \"{{ lookup('env', 'MYSQL_PASSWORD') }}\"\n        DATABASE_NAME: \"{{ lookup('env', 'DATABASE_NAME') }}\"\n        MYSQL_ROOT_PASSWORD: \"{{ lookup('env', 'MYSQL_ROOT_PASSWORD') }}\"\n\n이렇게 Ansible과 Packer를 조합하면 멱등성이 보장되는 안정적인 서버 이미지를 생성할 수 있습니다.\n참고로 packer에서 ansible plugin을 사용할 때 taget host를 ami가 build되는 임시 EC2로 간주하기 때문에, inventory는 사용하지 않습니다. 자세한 내용은 ansible part를 참고해주세요.\n\n\nTerraform 코드\n이제 본격적으로 provision을 해보겠습니다. 잠시 전체적인 구조를 다시 한번 보겠습니다.\n\n\n\n구현 aws 구조\n\n\n필요한 리소스는 VPC, subnet, security group, ec2 입니다.\nserver ec2와 database ec2는 환경변수 SERVER_INSTANCE_COUNT에 지정된 갯수 만큼 생성됩니다. database ec2는 이전 단계에서 생성한 ami를 사용해줍니다.\npublic, private subnet의 갯수는 임의로 생성했습니다.\n파일 구조는 아래와 같습니다.\nterraform/\n├── main/\n│   ├── main.tf\n│   ├── data.tf\n│   ├── output.tf\n│   └── variables.tf\n└── modules/network/\n    ├── main.tf\n    ├── output.tf\n    └── variables.tf\nmain.tf에서는 aws_instance를 생성하고, 그 외 VPC, subnet과 같은 리소스는 network module로 분리해서 생성했습니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_vpc\" \"main_vpc\" {\n  cidr_block           = \"10.0.0.0/16\"\n  instance_tenancy     = \"default\"\n  enable_dns_hostnames = \"true\"\n}\n\nresource \"aws_subnet\" \"public-1\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.1.0/24\"\n  map_public_ip_on_launch = \"true\"\n  availability_zone       = \"${var.AWS_REGION}a\"\n}\n\nresource \"aws_subnet\" \"public-2\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.2.0/24\"\n  map_public_ip_on_launch = \"true\"\n  availability_zone       = \"${var.AWS_REGION}c\"\n}\n\nresource \"aws_subnet\" \"private\" {\n  vpc_id                  = aws_vpc.main_vpc.id\n  cidr_block              = \"10.0.3.0/24\"\n  map_public_ip_on_launch = \"false\"\n  availability_zone       = \"${var.AWS_REGION}a\"\n}\n\n먼저 VPC와 subnet을 생성합니다.\ncidr_block은 private ip 중에서 겹치지 않는 범위로 지정해줍니다.\n\n\n\n\n\n\nPrivate IP ranges\n\n\n\n\nClass A: 10.0.0.0–10.255.255.255\nClass B: 172.16.0.0–172.31.255.255\nClass C: 192.168.0.0–192.168.255.255\n\n\n\nPublic subnet이 인터넷과 통신하기 위해서는 Internet Gateway와 Route Table이 필요합니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_internet_gateway\" \"gate_way\" {\n  vpc_id = aws_vpc.main_vpc.id\n}\n\nresource \"aws_route_table\" \"public_route_table\" {\n  vpc_id = aws_vpc.main_vpc.id\n\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.gate_way.id\n  }\n}\n\nresource \"aws_route_table_association\" \"public-1\" {\n  subnet_id      = aws_subnet.public-1.id\n  route_table_id = aws_route_table.public_route_table.id\n}\n\nresource \"aws_route_table_association\" \"public-2\" {\n  subnet_id      = aws_subnet.public-2.id\n  route_table_id = aws_route_table.public_route_table.id\n}\n\n모든 외부 트래픽을 Internet Gateway로 보내도록 Route Table을 설정하고, 이를 두 개의 Public subnet에 연결했습니다.\n참고로 VPC 내부 통신은 자동으로 라우팅됩니다. 같은 VPC 안에 있는 리소스들은 VPC의 기본 라우팅 테이블을 통해 서로 통신할 수 있기 때문에 내부 통신을 위한 route table은 따로 생성하지 않았습니다.\n\n\nmodules/network/main.tf\n\nresource \"aws_security_group\" \"server_sg\" {\n  vpc_id = aws_vpc.main_vpc.id\n  name   = \"server_sg\"\n\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = var.SSH_CIDR_BLOCKS\n  }\n\n  ingress {\n    from_port   = 80\n    to_port     = 80\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  ingress {\n    from_port   = 443\n    to_port     = 443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\nresource \"aws_security_group\" \"database_sg\" {\n  vpc_id = aws_vpc.main_vpc.id\n  name   = \"efs_sg\"\n\n  ingress {\n    from_port       = 3306\n    to_port         = 3306\n    protocol        = \"tcp\"\n    security_groups = [aws_security_group.server_sg.id]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\n마지막으로 security group입니다.\nserver ec2의 ssh 접근은 환경변수를 통해 ansible을 실행하는 머신의 ip에서만 접근 가능하도록 설정해줬습니다.\ndatabase ec2는 server ec2만 접근할 수 있도록 설정했습니다.\n\n\nmain/main.tf\n\n# 사용자가 지정한 경로의 ssh key를 사용해 ec2에 접근 가능하도록 설정했습니다.\nresource \"aws_key_pair\" \"my_labtop\" {\n  key_name   = \"my_labtop\"\n  public_key = file(var.SSH_PUBLIC_KEY_PATH)\n}\n\nmodule \"network\" {\n  source = \"../modules/network\"\n\n  AWS_REGION           = var.AWS_REGION\n  SSH_CIDR_BLOCKS      = [\"${var.SSH_IP}/32\"]\n}\n\nresource \"aws_instance\" \"server\" {\n  count         = var.SERVER_INSTANCE_COUNT\n  ami           = data.aws_ami.latest_ubuntu.id\n  instance_type = \"t2.micro\"\n\n  vpc_security_group_ids = [module.network.server_sg_id]\n  # subnet은 2개를 번걸아가면서 사용하도록 설정했습니다.\n  subnet_id              = module.network.public_subnets[count.index % 2]\n\n  key_name = aws_key_pair.my_labtop.key_name\n  tags = {\n    Name = \"serverNode\"\n  }\n}\n\nresource \"aws_instance\" \"database\" {\n  count         = var.SERVER_INSTANCE_COUNT\n  ami           = data.aws_ami.database_ami.id\n  instance_type = \"t2.micro\"\n\n  vpc_security_group_ids = [module.network.database_sg_id]\n  subnet_id              = module.network.private_subnets\n\n  key_name = aws_key_pair.my_labtop.key_name\n  tags = {\n    Name = \"dbNode\"\n  }\n}\n\n최종적으로 main.tf에서 network module을 불러와서 필요한 리소스를 생성한 후, server와 database ec2를 생성했습니다.\n\n\nmain/data.tf\n\ndata \"aws_ami\" \"latest_ubuntu\" {\n  most_recent = true\n\n  filter {\n    name   = \"name\"\n    values = [\"ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n\n  owners = [\"099720109477\"]\n}\n\ndata \"aws_ami\" \"database_ami\" {\n  most_recent = true\n  owners = [\"self\"]\n  filter {\n    name = \"name\"\n    values = [\"hyunghki-database-*\"]\n  }\n  filter {\n    name = \"root-device-type\"\n    values = [\"ebs\"]\n  }\n  filter {\n    name = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\nserver ec2는 기본 ubuntu 20.04 이미지를 사용하고, database ec2는 이전에 생성한 ami를 사용했습니다.\n\n\nansible 코드\n이제 필요한 설정을 진행하겠습니다.\n파일 구조는 아래와 같습니다.\nterraform/\n├── _requirements/                      # docker compose setting files\n├── inventories/\n│   └── aws_ec2.yml\n├── roles/setting_docker/tasks\n│   └── main.yml\n└── server.yml\n먼저 용어를 알아야 합니다.\n\nInventory (인벤토리)\n인벤토리는 Ansible이 관리할 호스트(서버)의 목록입니다. 호스트를 그룹으로 묶어 관리할 수 있습니다.\nPlaybook (플레이북)\n플레이북은 Ansible에서 작업을 정의하는 YAML 파일입니다. 플레이북은 하나 이상의 플레이로 구성되며, 각 플레이는 특정 호스트 그룹에 대해 수행할 작업(task)을 정의합니다.\nRole (롤)\n롤은 Ansible에서 재사용 가능한 구성 단위입니다. 플레이북을 모듈화하고 구조화하여 재사용성을 높이는 데 사용됩니다.\n\nInventory에서 server 그룹을 정의한 후, playbook으로 docker compose 환경을 설정하겠습니다.\n\n\naws_ec2.yml\n\nplugin: aws_ec2\nkeyed_groups:\n  - key: tags\ncompose:\n  ansible_host: public_ip_address\nleading_separator: False\nfilters:\n  instance-state-name: running\n\nAWS EC2 동적 인벤토리 설정입니다. Terraform으로 생성한 EC2 인스턴스들을 자동으로 관리할 수 있습니다.\n\n\nserver.yml\n\n- hosts: \"Name_serverNode\"\n  gather_facts: false\n  become: true\n  roles:\n    - role: setting_docker\n  tasks:\n    - name: copy_requirements\n      copy:\n        src: \"./_requirements/\"\n        dest: \"/home/{{ ansible_user }}/app/\"\n        mode: '0755'\n        directory_mode: '0755'\n\n    - name: Split array values from DB_PRIVATE_IP\n      set_fact:\n        target: \"{{ lookup('env', 'DB_PRIVATE_IP') | split(' ') }}\"\n\n    - name: execute docker compose\n      shell:\n        cmd: docker-compose up -d\n        chdir: \"/home/{{ ansible_user }}/app/\"\n      environment:\n        DOMAIN_NAME: \"{{ ansible_host }}\"\n        MYSQL_USER: \"{{ lookup('env', 'MYSQL_USER') }}\"\n        MYSQL_PASSWORD: \"{{ lookup('env', 'MYSQL_PASSWORD') }}\"\n        DATABASE_NAME: \"{{ lookup('env', 'DATABASE_NAME') }}\"\n        SITE_TITLE: \"{{ lookup('env', 'SITE_TITLE') }}\"\n        ADMIN_NAME: \"{{ lookup('env', 'ADMIN_NAME') }}\"\n        ADMIN_PASSWORD: \"{{ lookup('env', 'ADMIN_PASSWORD') }}\"\n        ADMIN_EMAIL: \"{{ lookup('env', 'ADMIN_EMAIL') }}\"\n        USER_NAME: \"{{ lookup('env', 'USER_NAME') }}\"\n        USER_PASSWORD: \"{{ lookup('env', 'USER_PASSWORD') }}\"\n        USER_EMAIL: \"{{ lookup('env', 'USER_EMAIL') }}\"\n        DB_PRIVATE_IP: \"{{ target[ansible_play_hosts.index(inventory_hostname)] }}\"\n\n    - name: all done message\n      debug:\n        msg: \"https://{{ ansible_host }}\"\n\n’Name’이 ’serverNode’인 인스턴스들만 선택하여 설정을 진행하겠습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/09.html#실행",
    "href": "posts/02_categories/42_seoul/notes/09.html#실행",
    "title": "cloud-1 코드 설명",
    "section": "실행",
    "text": "실행\n먼저 .env 파일에 환경변수를 설정해줍니다.\nip 정보도 알아낸 후, SSH_IP에 설정해줍니다.\n\n\n\nnaver에 내 ip 검색\n\n\n\n\n.env\n\n# only 1 line variable is allowed\nAWS_REGION=ap-northeast-2\nAWS_ACCESS_KEY_ID=********************\nAWS_SECRET_ACCESS_KEY=********************\nSERVER_INSTANCE_COUNT=2\nSSH_IP=121.135.181.56\nSSH_PUBLIC_KEY_PATH=~/.ssh/id_rsa.pub\nSSH_PRIVATE_KEY_PATH=~/.ssh/id_rsa\nMYSQL_USER=dudu\nMYSQL_PASSWORD=secret\nMYSQL_ROOT_PASSWORD=secret\nDATABASE_NAME=cloud\nSITE_TITLE='hyunghki blog'\nADMIN_NAME=admin\nADMIN_PASSWORD=secret\nADMIN_EMAIL=admin@example.com\nUSER_NAME=user\nUSER_PASSWORD=secret\nUSER_EMAIL=user@example.com\n\n그후 make 명령어를 입력하면 자동으로 build가 진행됩니다.\n\n\n\n명령어 실행 결과\n\n\nbuild가 완료되면 완료 메세지의 ip로 접속해줍니다.\n\n\n\n접속 페이지\n\n\nwordpress 접속 페이지가 잘 뜨는 것을 확인할 수 있습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/09.html#결과",
    "href": "posts/02_categories/42_seoul/notes/09.html#결과",
    "title": "cloud-1 코드 설명",
    "section": "결과",
    "text": "결과\n\n\n\n최종 점수\n\n\n\n\n\n최종 평가",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/09.html#outro",
    "href": "posts/02_categories/42_seoul/notes/09.html#outro",
    "title": "cloud-1 코드 설명",
    "section": "outro",
    "text": "outro\n솔직히 일반적으로 사용되는 cloud 구조를 적용한건 아니긴 하지만, 과제에 맞춰서 진행하기 위해 고민하는 과정에서 다양한 구조를 적용해봤는데, 그 과정이 나름 학습에 도움이 된거 같습니다. 이 분야에 공부를 꽤 했고, 그 내용들을 다양하게 고민하며 적용해보고 싶다면 이 프로젝트가 괜찮은 선택지가 될 수도 있어 보입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "cloud-1 코드 설명"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/index.html",
    "href": "posts/02_categories/deep_learning/index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep Learning 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/index.html#details",
    "href": "posts/02_categories/deep_learning/index.html#details",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep Learning 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/index.html#tasks",
    "href": "posts/02_categories/deep_learning/index.html#tasks",
    "title": "Deep Learning",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/index.html#참고-자료",
    "href": "posts/02_categories/deep_learning/index.html#참고-자료",
    "title": "Deep Learning",
    "section": "참고 자료",
    "text": "참고 자료\n  \n\n기타",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/index.html#related-posts",
    "href": "posts/02_categories/deep_learning/index.html#related-posts",
    "title": "Deep Learning",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/00.html#퍼셉트론이란",
    "href": "posts/02_categories/deep_learning/notes/basic/00.html#퍼셉트론이란",
    "title": "퍼셉트론",
    "section": "퍼셉트론이란",
    "text": "퍼셉트론이란\n다수의 신호를 입력으로 받아 하나의 신호를 출력하는 것\n\\[\ny = \\begin{cases} 0 & (w_1x_1 + w_2x_2 \\leq \\theta) \\\\ 1 & (w_1x_1 + w_2x_2 &gt; \\theta) \\end{cases}\n\\]",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/00.html#논리-회로",
    "href": "posts/02_categories/deep_learning/notes/basic/00.html#논리-회로",
    "title": "퍼셉트론",
    "section": "논리 회로",
    "text": "논리 회로\n파라미터 \\((w_1, w_2, θ)\\)의 값을 조정하여 AND, OR, NAND 게이트를 구현할 수 있다.\n머신 러닝의 목적은, 기계가 알아서 파라미터의 값을 적절히 조정하는 것이다.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/00.html#퍼셉트론-구현",
    "href": "posts/02_categories/deep_learning/notes/basic/00.html#퍼셉트론-구현",
    "title": "퍼셉트론",
    "section": "퍼셉트론 구현",
    "text": "퍼셉트론 구현\n\nAND 게이트\n\ndef AND(x1, x2):\n    w1, w2, theta = 0.5, 0.5, 0.7 # parameter\n    return (x1*w1 + x2*w2 &gt; theta)\n\n\nprint(AND(0, 0))\nprint(AND(1, 0))\nprint(AND(0, 1))\nprint(AND(1, 1))\n\nFalse\nFalse\nFalse\nTrue\n\n\n여기서 θ를 \\(-b\\)로 치환하고 식을 다시 정리하면 다음과 같다.\n\\[\ny = \\begin{cases} 0 & (b + w_1x_1 + w_2x_2 ≤ 0) \\\\ 1 & (b + w_1x_1 + w_2x_2 &gt; 0) \\end{cases}\n\\]\n이때 \\(w_1\\)과 \\(w_2\\)(가중치)는 각각의 입력신호가 결과에 주는 영향력을 조절하고, \\(b\\)(편향)은 뉴런이 얼마나 쉽게 활성화되는지를 조정한다. (가중치 합이 -b를 초과할 때만 뉴런이 활성화된다.)\n이제 재구성한 식과, numpy를 이용하여 NAND와 OR 게이트를 구현해보자.\n\n\nNAND 게이트\n\nimport numpy as np\n\ndef NAND(x1, x2):\n  x = np.array([x1, x2])\n  w = np.array([-0.5, -0.5])\n  b = 0.7\n  return (b + np.sum(x * w) &gt; 0)\n\n\ndef OR(x1, x2):\n  x = np.array([x1, x2])\n  w = np.array([0.5, 0.5])\n  b = -0.2\n  return (b + np.sum(x * w) &gt; 0)\n\n세 게이트의 차이는 오직 파라미터의 값이다.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/00.html#퍼셉트론의-한계",
    "href": "posts/02_categories/deep_learning/notes/basic/00.html#퍼셉트론의-한계",
    "title": "퍼셉트론",
    "section": "퍼셉트론의 한계",
    "text": "퍼셉트론의 한계\nAND, NAND, OR 게이트는 만들 수 있지만, XOR 게이트는 만들 수 없다. 다른 게이트들과 다르게 선형적으로 구분이 안되기 때문이다.\n하지만 AND NAND OR 게이트를 다음과 같이 배치하면 XOR 게이트를 만들 수 있다.\n\n\n\n\n\nflowchart LR\n    x1((x1)) --&gt; OR\n    x2((x2)) --&gt; OR\n    x1 --&gt; NAND\n    x2 --&gt; NAND\n    OR[OR 게이트] --&gt; AND\n    NAND[NAND 게이트] --&gt; AND\n    AND[AND 게이트] --&gt; output((XOR 출력))\n\n\n\n\n\n\n(mermaid로는 이렇게 그리는게 최선이다.)\n이와 같이 여러 퍼셉트론을 연결한 형태를 다층 퍼센트론이라고 한다.\n\nXOR 게이트\n\ndef XOR(x1, x2):\n  s1 = OR(x1, x2)\n  s2 = NAND(x1, x2)\n  return AND(s1, s2)\n\n\nprint(XOR(0, 0))\nprint(XOR(0, 1))\nprint(XOR(1, 0))\nprint(XOR(1, 1))\n\nFalse\nTrue\nTrue\nFalse",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "퍼셉트론"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/01.html#신경망이란",
    "href": "posts/02_categories/deep_learning/notes/basic/01.html#신경망이란",
    "title": "신경망",
    "section": "신경망이란",
    "text": "신경망이란\n\n퍼셉트론에서 가중치를 자동으로 학습하는 방법이다.\n입력층, 은닉층, 출력층으로 구성된다.\n\n앞에서 살펴본 퍼셉트론 함수를 다시 살펴보자.\n\\[\ny = \\begin{cases} 0 & (b + w_1x_1 + w_2x_2 ≤ 0) \\\\ 1 & (b + w_1x_1 + w_2x_2 &gt; 0) \\end{cases}\n\\]\n이때, \\(y = h(b + w_1x_1 + w_2x_2)\\)로 표현하면 다음과 같이 표현할 수 있다.\n\\[\nh(x) = \\begin{cases} 0 & (x ≤ 0) \\\\ 1 & (x &gt; 0) \\end{cases}\n\\]\n이때 \\(h(x)\\)는 활성화 함수(activation function)라고 한다.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "신경망"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/01.html#활성화-함수",
    "href": "posts/02_categories/deep_learning/notes/basic/01.html#활성화-함수",
    "title": "신경망",
    "section": "활성화 함수",
    "text": "활성화 함수\n\n계단 함수(step function): 앞서 살펴본 함수\n\n\nimport numpy as np\n\ndef step_function(x):\n    return np.array(x &gt; 0, dtype=int)\n\nprint(step_function(np.array([-1.0, 1.0, 2.0])))\n\n[0 1 1]\n\n\n\nimport matplotlib.pyplot as plt\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = step_function(x)\nplt.plot(x, y)\nplt.ylim(-0.1, 1.1)\nplt.show()\n\n\n\n\n\n\n\n\n\n시그모이드 함수: \\(h(x) = \\frac{1}{1 + \\exp(-x)}\\)\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = sigmoid(x)\nplt.plot(x, y)\nplt.ylim(-0.1, 1.1)\nplt.show()\n\n\n\n\n\n\n\n\n\n계단 함수와의 차이점\n\n시그모이드가 더 부드러움\n\n계단 함수와의 공통점\n\n입력이 작을 때는 0에 가깝고, 입력이 커지면 1에 가까워짐\n입력이 아무리 작거나 커도 출력은 0에서 1 사이\n비선형 함수 (선형 함수는 은닉층 업싱도 똑같이 구현할 수 있기 때문에 신경망에서 활성화 함수는 반드시 비선형 함수여야 함)\n\nReLU 함수: \\(h(x) = \\begin{cases} x & (x &gt; 0) \\\\ 0 & (x ≤ 0) \\end{cases}\\)\n\n\ndef relu(x):\n    return np.maximum(0, x)\n\n\nSoftMax 함수: \\(y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^{n} \\exp(a_i)}\\)\n\n\ndef softmax(a):\n    exp_a = np.exp(a)\n    sum_exp_a = np.sum(exp_a)\n    y = exp_a / sum_exp_a\n    return y\n\nsoftmax 함수는 값이 기하급수적으로 증가하기 때문에 쉽게 overflow가 발생할 수 있음.\n따라서 다음과 같이 개선이 필요함\n\ndef softmax(a):\n    c = np.max(a)\n    exp_a = np.exp(a - c)\n    sum_exp_a = np.sum(exp_a)\n    y = exp_a / sum_exp_a\n    return y\n\nsofrmax 함수 출력의 총합은 1이고, 개별 출력은 0에서 1 사이이다.\n따라서 softmax 함수의 출력을 확률로 해석할 수 있다.\n여기서 softmax 함수는 입력 값의 대소관계가 유지된다는 성질이 있기 때문에 학습이 아닌, 추론 단계에서는 보통 생략한다.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "신경망"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/01.html#층-신경망-구성",
    "href": "posts/02_categories/deep_learning/notes/basic/01.html#층-신경망-구성",
    "title": "신경망",
    "section": "3층 신경망 구성",
    "text": "3층 신경망 구성\n입력층에서 1층으로 신호 전달\n\nX = np.array([1.0, 0.5])\nW1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\nB1 = np.array([0.1, 0.2, 0.3])\n\nA1 = np.dot(X, W1) + B1\nZ1 = sigmoid(A1)\n\nprint(A1)\nprint(Z1)\n\n[0.3 0.7 1.1]\n[0.57444252 0.66818777 0.75026011]\n\n\n1층에서 2층으로 신호 전달\n\nW2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\nB2 = np.array([0.1, 0.2])\n\nA2 = np.dot(Z1, W2) + B2\nZ2 = sigmoid(A2)\n\nprint(A2)\nprint(Z2)\n\n[0.51615984 1.21402696]\n[0.62624937 0.7710107 ]\n\n\n2층에서 출력층으로 신호 전달\n\ndef identity_function(x):\n    return x\n\nW3 = np.array([[0.1, 0.3], [0.2, 0.4]])\nB3 = np.array([0.1, 0.2])\n\nA3 = np.dot(Z2, W3) + B3\nY = identity_function(A3)\n\n출력층의 활성화 함수는 보통 풀고자 하는 문제의 성질에 맞게 정함\n\n회귀: 항등 함수\n2클래스 분류: 시그모이드 함수\n다중 클래스 분류: 소프트맥스 함수",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "신경망"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/04.html#매개변수-갱신",
    "href": "posts/02_categories/deep_learning/notes/basic/04.html#매개변수-갱신",
    "title": "학습 관련 기술들",
    "section": "매개변수 갱신",
    "text": "매개변수 갱신\n\n확률적 경사하강법은 매개변수를 찾는 과정이 비효율적이다.\n\n비등방성 함수에서 탐색 경로가 비효율적임\n\n\n\n모멘텀\n\n\\(v = αv - η \\frac{dL}{dW}\\)\n\\(W = W + v\\)\n\n\nclass Momentum:\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n\n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():\n                self.v[key] = np.zeros_like(val)\n        for key in params.keys():\n            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n            params[key] += self.v[key]\n\n\n\nAdaGrad\n\n각각의 매개변수에 대해 학습률을 점점 낮추는 방법\n\\(h = h + \\frac{dL}{dW} ⊙ \\frac{dL}{dW}\\)\n\\(W = W - η\\frac{1}{\\sqrt{h}}\\frac{dL}{dW}\\)\n\n\nclass AdaGrad:\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n\n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n        for key, in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n\n\n하지만 시간이 지나면 결국 기울기가 0으로 되버림\n\n이것을 개선한 기법: RMSProp\n\n\n\n\nAdam",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "학습 관련 기술들"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/04.html#가중치의-초깃값",
    "href": "posts/02_categories/deep_learning/notes/basic/04.html#가중치의-초깃값",
    "title": "학습 관련 기술들",
    "section": "가중치의 초깃값",
    "text": "가중치의 초깃값\n\n초깃값은 무작위로 설정되어야 한다.\n\n\nXavier 초깃값\n\n표준편차가 \\(\\frac{1}{\\sqrt{n}}\\)인 초깃값\nsigmoid, tanh에서 사용됨.\n\n\n\nHe 초깃값\n\n표준편차가 \\(\\frac{2}{\\sqrt{n}}\\)인 초깃값\nReLU에 특화됨",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "학습 관련 기술들"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/04.html#배치-정규화",
    "href": "posts/02_categories/deep_learning/notes/basic/04.html#배치-정규화",
    "title": "학습 관련 기술들",
    "section": "배치 정규화",
    "text": "배치 정규화\n\n각 층의 활성화를 적당히 퍼뜨리도록 강제 하는 것\n학습속도 개선, 초깃값 의존도 감소, 과대적합 억제의 장점이 있음\n활성화 함수 앞이나 뒤에서 standardization scaling을 진행\n이후 \\(y_i = \\gamma \\hat{x}_i + β\\)의 수식으로, 두 파라미터를 적합한 값으로 학습해 나감",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "학습 관련 기술들"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/04.html#과대적합-방지",
    "href": "posts/02_categories/deep_learning/notes/basic/04.html#과대적합-방지",
    "title": "학습 관련 기술들",
    "section": "과대적합 방지",
    "text": "과대적합 방지\n\n가중치 감소\n\n손실함수에 l2(\\(\\frac{1}{2}λ W^2\\)) l1 norm을 더함\n\n\n\n드롭아웃\n\n신경망 모델이 복잡해지면 가중치 감소만으로 대응하기 어려움\n훈련 때 은닉층의 뉴런을 무작위로 골라 삭제한다.\n시험 때 각 뉴련의 출력에 훈련 때 삭제 안 한 비율을 곱한다.\n\n\nclass Dropout:\n    def __init__(self, dropout_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.mask = None\n\n    def forward(self, x, train_flg=True):\n        if train_flg:\n            self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio\n            return x * self.mask\n        return x * (1.0 - self.dropout_ratio)\n\n    def backward(self, dout):\n        return dout * self.mask",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "학습 관련 기술들"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/02.html#손실함수",
    "href": "posts/02_categories/deep_learning/notes/basic/02.html#손실함수",
    "title": "신경망 학습",
    "section": "손실함수",
    "text": "손실함수\n\n오차 제곱합\n\n\\(E = \\frac{1}{2} \\sum_{i=1}^{n} (y_i - t_i)^2\\)\n\\(y_i\\): 예측값\n\\(t_i\\): 정답값\n\\(\\frac{1}{2}\\)는 미분을 쉽게 하기 위해서 곱해주는 상수\n\n\nimport numpy as np\n\ndef sum_squared_error(y, t):\n    return 0.5 * np.sum((y - t) ** 2)\n\n\n\n교차 엔트로피\n\n\\(E = -\\sum_{i=1}^{n} t_i \\log(y_i)\\)\n일반적으로 정답값인 \\(t_i\\)는 0 또는 1이기 때문에(one hot encoding), \\(t_i = 1\\)인 경우에만 계산된다.\n\\(y_i\\)가 1에 가까울수록 손실이 작아진다.\n\n\ndef cross_entropy_error(y, t):\n    delta = 1e-7  # log(0) 방지\n    return -np.sum(t * np.log(y + delta))",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "신경망 학습"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/02.html#미니배치-학습",
    "href": "posts/02_categories/deep_learning/notes/basic/02.html#미니배치-학습",
    "title": "신경망 학습",
    "section": "미니배치 학습",
    "text": "미니배치 학습\n\n모든 데이터를 한 번에 학습하는 것이 아니라, 일부 데이터만을 사용하여 학습하는 방법\n\n\nimport numpy as np\nfrom dl_dataset.mnist import load_mnist\n\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\n\ntrain_size = x_train.shape[0]\nbatch_size = 10\nbatch_mask = np.random.choice(train_size, batch_size)\nx_batch = x_train[batch_mask]\nt_batch = t_train[batch_mask]\n\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n    batch_size = y.shape[0]\n    return -np.sum(t * np.log(y + 1e-7)) / batch_size",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "신경망 학습"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/02.html#경사법",
    "href": "posts/02_categories/deep_learning/notes/basic/02.html#경사법",
    "title": "신경망 학습",
    "section": "경사법",
    "text": "경사법\n\n기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향\n기울어진 방향이 반드시 최솟값은 아니지만, 그 방향으로 가야 함수의 값을 줄일 수 있다.\n\n\nimport numpy as np\ndef numerical_gradient(f, x):\n    h = 1e-4\n    grad = np.zeros_like(x)\n\n    for idx in range(x.size):\n        tmp_val = x[idx]\n        x[idx] = tmp_val + h\n        fxh1 = f(x)\n\n        x[idx] = tmp_val - h\n        fxh2 = f(x)\n\n        grad[idx] = (fxh1 - fxh2) / (2 * h)\n        x[idx] = tmp_val\n\n    return grad\n\n\ndef gradient_descent(f, init_x, lr=0.01, step_num=100):\n    x = init_x.copy()\n\n    for _ in range(step_num):\n        grad = numerical_gradient(f, x)\n        x -= lr * grad\n    return x\n\n\nfrom dl_common.functions import softmax, cross_entropy_error\nfrom dl_common.gradient import numerical_gradient\n\nclass simpleNet:\n    def __init__(self):\n        self.W = np.random.randn(2, 3)\n\n    def predict(self, x):\n        return np.dot(x, self.W)\n\n    def loss(self, x, t):\n        z = self.predict(x)\n        y = softmax(z)\n        return cross_entropy_error(y, t)\n\n\nnet = simpleNet()\nx = np.array([0.6, 0.9])\np = net.predict(x)\nnp.argmax(p)\n\nnp.int64(1)\n\n\n\nt = np.array([0, 0, 1])\nnet.loss(x, t)\n\nnp.float64(2.860086041618459)\n\n\n\nf = lambda w: net.loss(x, t)\n\ndW = numerical_gradient(f, net.W)\nprint(dW)\n\n[[ 0.22560159  0.34003918 -0.56564077]\n [ 0.33840239  0.51005877 -0.84846116]]",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "신경망 학습"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/02.html#학습-알고리즘-구현",
    "href": "posts/02_categories/deep_learning/notes/basic/02.html#학습-알고리즘-구현",
    "title": "신경망 학습",
    "section": "학습 알고리즘 구현",
    "text": "학습 알고리즘 구현\n\nfrom dl_common.functions import *\nfrom dl_common.gradient import numerical_gradient\n\nclass TwoLayerNet:\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n\n    def predict(self, x):\n        W1, b1 = self.params['W1'], self.params['b1']\n        W2, b2 = self.params['W2'], self.params['b2']\n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        return y\n\n    def loss(self, x, t):\n        y = self.predict(x)\n        return cross_entropy_error(y, t)\n\n    def accuracy(self, x, t):\n        y = self.predict(x)\n        if t.ndim != 1:  # one-hot encoding\n            t = np.argmax(t, axis=1)\n        return np.sum(np.argmax(y, axis=1) == t) / float(x.shape[0])\n\n    def numerical_gradient(self, x, t):\n        loss_w = lambda w: self.loss(x, t)\n        grads = {}\n        grads['W1'] = numerical_gradient(loss_w, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_w, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_w, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_w, self.params['b2'])\n        return grads\n\n\nfrom dl_dataset.mnist import load_mnist\n\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\niters_num = 10000\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\niter_per_epoch = max(train_size / batch_size, 1)\n\nfor i in range(iters_num):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    grads = network.numerical_gradient(x_batch, t_batch)\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grads[key]\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n\n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "신경망 학습"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/time_series/00.html#모델",
    "href": "posts/02_categories/deep_learning/notes/time_series/00.html#모델",
    "title": "overview",
    "section": "모델",
    "text": "모델\n\n단일 단계 모델: 한 대상에 대한 한 시간 단계 예측\n다중 단계 모델: 한 대상에 대한 여러 시간 단계 예측을 생성\n다중 출력 모델: 여러 대상에 대한 여러 시간 단계 예측을 생성",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Time Series",
      "overview"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/time_series/00.html#example",
    "href": "posts/02_categories/deep_learning/notes/time_series/00.html#example",
    "title": "overview",
    "section": "Example",
    "text": "Example\n\nimport pandas as pd\ndf = pd.read_csv(\"data/traffic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\ndate_time\ntemp\nrain_1h\nsnow_1h\nclouds_all\ntraffic_volume\n\n\n\n\n0\n2016-09-29 17:00:00\n291.75\n0.0\n0\n0\n5551.0\n\n\n1\n2016-09-29 18:00:00\n290.36\n0.0\n0\n0\n4132.0\n\n\n2\n2016-09-29 19:00:00\n287.86\n0.0\n0\n0\n3435.0\n\n\n3\n2016-09-29 20:00:00\n285.91\n0.0\n0\n0\n2765.0\n\n\n4\n2016-09-29 21:00:00\n284.31\n0.0\n0\n0\n2443.0\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nax.plot(df['traffic_volume'])\nax.set_xlabel('Time')\nax.set_ylabel('Traffic Volume')\n\nplt.xticks(np.arange(7, 400, 24), ['Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.xlim(0, 400)\nfig.autofmt_xdate()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n일별 계절성: 하루의 시작과 긑에 교통량이 적음\n주간 계절성: 주말에 교통량이 적음\nSARIMAX는 이러한 계절성이 여러개 있는 시계열 데이터를 모델링하는데 적합하지 않음\n\n\nfig, ax = plt.subplots()\n\nax.plot(df['temp'])\nax.set_xlabel('Time')\nax.set_ylabel('Temperature (K)')\n\nplt.xticks([2239, 10999], [2017, 2018])\n\nfig.autofmt_xdate()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n기온 역시 연간 계절성을 가지고,\n\n\nfig, ax = plt.subplots()\n\nax.plot(df['temp'])\nax.set_xlabel('Time')\nax.set_ylabel('Temperature (K)')\n\nplt.xticks(np.arange(7, 400, 24), ['Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\nplt.xlim(0, 400)\nfig.autofmt_xdate()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n일별 계절성도 가지고 있다.\n\n\ndf.describe().transpose()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ntemp\n17551.0\n281.416203\n12.688262\n243.39\n272.22\n282.41\n291.89\n310.07\n\n\nrain_1h\n17551.0\n0.025523\n0.259794\n0.00\n0.00\n0.00\n0.00\n10.60\n\n\nsnow_1h\n17551.0\n0.000000\n0.000000\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\nclouds_all\n17551.0\n42.034129\n39.065960\n0.00\n1.00\n40.00\n90.00\n100.00\n\n\ntraffic_volume\n17551.0\n3321.484588\n1969.223949\n113.00\n1298.00\n3518.00\n4943.00\n7280.00\n\n\n\n\n\n\n\n\n정보가 없는 강수량, 강설량은 제거\n\n\nfrom datetime import datetime\n\ndf = df.drop(['rain_1h', 'snow_1h'], axis=1)\ndf['date_time'] = pd.to_datetime(df['date_time']).map(datetime.timestamp)\ndf\n\n\n\n\n\n\n\n\ndate_time\ntemp\nclouds_all\ntraffic_volume\n\n\n\n\n0\n1.475136e+09\n291.75\n0\n5551.0\n\n\n1\n1.475140e+09\n290.36\n0\n4132.0\n\n\n2\n1.475143e+09\n287.86\n0\n3435.0\n\n\n3\n1.475147e+09\n285.91\n0\n2765.0\n\n\n4\n1.475150e+09\n284.31\n0\n2443.0\n\n\n...\n...\n...\n...\n...\n\n\n17546\n1.538302e+09\n283.45\n75\n3543.0\n\n\n17547\n1.538305e+09\n282.76\n90\n2781.0\n\n\n17548\n1.538309e+09\n282.73\n90\n2159.0\n\n\n17549\n1.538312e+09\n282.09\n90\n1450.0\n\n\n17550\n1.538316e+09\n282.12\n90\n954.0\n\n\n\n\n17551 rows × 4 columns\n\n\n\n\n시간을 초 단위로 변환.\n\n\nday = 24 * 60 * 60\ndf['day_sin'] = np.sin(df['date_time'] * (2 * np.pi / day))\ndf['day_cos'] = np.cos(df['date_time'] * (2 * np.pi / day))\ndf = df.drop(['date_time'], axis=1)\ndf\n\n\n\n\n\n\n\n\ntemp\nclouds_all\ntraffic_volume\nday_sin\nday_cos\n\n\n\n\n0\n291.75\n0\n5551.0\n8.660254e-01\n-0.500000\n\n\n1\n290.36\n0\n4132.0\n7.071068e-01\n-0.707107\n\n\n2\n287.86\n0\n3435.0\n5.000000e-01\n-0.866025\n\n\n3\n285.91\n0\n2765.0\n2.588190e-01\n-0.965926\n\n\n4\n284.31\n0\n2443.0\n1.485292e-12\n-1.000000\n\n\n...\n...\n...\n...\n...\n...\n\n\n17546\n283.45\n75\n3543.0\n5.000000e-01\n-0.866025\n\n\n17547\n282.76\n90\n2781.0\n2.588190e-01\n-0.965926\n\n\n17548\n282.73\n90\n2159.0\n2.467248e-12\n-1.000000\n\n\n17549\n282.09\n90\n1450.0\n-2.588190e-01\n-0.965926\n\n\n17550\n282.12\n90\n954.0\n-5.000000e-01\n-0.866025\n\n\n\n\n17551 rows × 5 columns\n\n\n\n\n단순 초 단위 변환은 주기적 패턴을 포착하지 못하기 때문에 사인 및 코사인 변환을 사용하여 일별 주기를 인코딩\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\ntrain, test = train_test_split(df, test_size=0.1, shuffle=False)\ntrain, val = train_test_split(train, test_size=0.2, shuffle=False)\n\nscaler = MinMaxScaler()\ntrain_scaled = scaler.fit_transform(train)\nval_scaled = scaler.transform(val)\ntest_scaled = scaler.transform(test)\n\n\ntrain.to_csv(\"data/traffic_train.csv\", index=False)\nval.to_csv(\"data/traffic_val.csv\", index=False)\ntest.to_csv(\"data/traffic_test.csv\", index=False)",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Time Series",
      "overview"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/독서/index.html",
    "href": "posts/03_archives/stored_categories/독서/index.html",
    "title": "독서",
    "section": "",
    "text": "독서 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "독서"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/독서/index.html#details",
    "href": "posts/03_archives/stored_categories/독서/index.html#details",
    "title": "독서",
    "section": "",
    "text": "독서 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "독서"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/독서/index.html#tasks",
    "href": "posts/03_archives/stored_categories/독서/index.html#tasks",
    "title": "독서",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "독서"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/독서/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/독서/index.html#related-posts",
    "title": "독서",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "독서"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/index.html",
    "href": "posts/03_archives/stored_categories/quantum_programming/index.html",
    "title": "Quantum Programming",
    "section": "",
    "text": "Quantum Programming 정리 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/index.html#details",
    "href": "posts/03_archives/stored_categories/quantum_programming/index.html#details",
    "title": "Quantum Programming",
    "section": "",
    "text": "Quantum Programming 정리 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/index.html#tasks",
    "href": "posts/03_archives/stored_categories/quantum_programming/index.html#tasks",
    "title": "Quantum Programming",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/quantum_programming/index.html#참고-자료",
    "title": "Quantum Programming",
    "section": "참고 자료",
    "text": "참고 자료\n\n인프런 양자 프로그래밍 강의",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/quantum_programming/index.html#related-posts",
    "title": "Quantum Programming",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#양자-프로그래밍이란",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#양자-프로그래밍이란",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍이란?",
    "text": "양자 프로그래밍이란?\n양자 프로그래밍은 양자 컴퓨터의 힘을 활용해 알고리즘과 소프트웨어를 개발하는 것으로, 중첩(superposition), 얽힘(entanglement), 양자 병렬성(quantum parallelism)과 같은 양자 역학 원리를 사용합니다. 이는 양자 회로를 설계하고, 양자 게이트를 적용하며, 큰 수를 인수분해하는 쇼어(Shor) 알고리즘이나 데이터베이스 검색을 위한 그로버(Grover) 검색 알고리즘과 같은 양자 알고리즘을 구현하는 작업을 포함합니다.\n양자 프로그래밍은 아직 초기 단계에 있지만 암호학, AI, 최적화, 과학적 시뮬레이션 등에서 잠재적인 응용 가능성을 가지고 있습니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#양자-프로그래밍-언어",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#양자-프로그래밍-언어",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍 언어",
    "text": "양자 프로그래밍 언어\n양자 프로그래밍 언어는 정의상 양자 컴퓨터용 프로그램을 작성하기 위해 설계된 언어입니다. 양자 프로그래밍 언어를 고전 프로그래밍 언어와 구분 짓는 요소는 양자 시스템의 원리(큐비트, 얽힘, 중첩 법칙 등)에 기반해 양자 알고리즘을 평가하는 방식입니다.\n양자 컴퓨팅에 널리 사용되는 프로그래밍 언어로는 Qiskit, Cirq, Q# 등이 있으며, 이들은 고전 컴퓨팅보다 훨씬 빠르게 복잡한 문제를 해결할 수 있는 양자 알고리즘 개발을 가능하게 합니다. 특히 암호학, 최적화, 머신러닝 분야에서 두각을 나타냅니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#양자-프로그래밍-vs-고전-프로그래밍",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#양자-프로그래밍-vs-고전-프로그래밍",
    "title": "Quantum Programming",
    "section": "양자 프로그래밍 vs 고전 프로그래밍",
    "text": "양자 프로그래밍 vs 고전 프로그래밍\n양자 프로그래밍과 고전 프로그래밍 사이에는 근본적인 차이가 있습니다. 각각의 논리, 언어, 응용 분야가 다르며, 이는 양자 컴퓨팅과 고전 컴퓨팅의 차이와 비슷합니다.\n\n고전 프로그래밍\n고전 프로그래밍은 이진 논리에 기반하며, 정보는 비트(0과 1)로 표현되고 계산은 결정론적 단계를 따릅니다. 프로그램은 CPU나 GPU와 같은 고전 하드웨어에서 실행되며, AND, OR, NOT 같은 부울 논리 게이트를 사용해 순차적이거나 병렬적으로 연산을 수행합니다. Python, C++, Java 같은 전통적인 프로그래밍 언어를 사용하며, 주어진 입력에 대해 출력은 항상 예측 가능합니다.\n고전 컴퓨터는 웹 개발부터 과학적 시뮬레이션까지 일상적인 대부분의 응용 프로그램을 처리합니다. 하지만 암호학이나 복잡한 최적화와 같이 대규모 계산이 필요한 문제에서는 한계를 보입니다.\n\n\n양자 프로그래밍\n양자 프로그래밍은 양자 역학 원리에 기반하며, 중첩 상태에 존재하고 얽힐 수 있는 큐비트를 사용해 훨씬 빠른 계산을 수행합니다. 고전 프로그램과 달리 양자 프로그램은 확률적(probabilistic)입니다. 즉, 출력은 큐비트를 반복적으로 측정해 얻어지며, 이 과정에서 큐비트는 확정된 상태로 붕괴합니다.\n양자 프로그래밍은 Qiskit(Python 기반), Quipper(Haskell 기반), Cirq 같은 특수 양자 언어를 필요로 하며, IBM Quantum이나 Google Sycamore 같은 양자 프로세서에서 작동합니다. 양자 회로는 Hadamard, CNOT, Pauli-X 등의 양자 게이트를 사용하며, 암호학, 최적화, 양자 시뮬레이션과 같은 분야에서 전례 없는 능력을 제공합니다. 다만 기술은 아직 개발 중입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#집에서-양자-프로그래밍-가능할까",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#집에서-양자-프로그래밍-가능할까",
    "title": "Quantum Programming",
    "section": "집에서 양자 프로그래밍: 가능할까?",
    "text": "집에서 양자 프로그래밍: 가능할까?\n과거에는 양자 프로그래밍이 복잡성과 양자 컴퓨팅 하드웨어의 접근성 문제로 인해 대부분의 개인에게 불가능해 보였을 수 있습니다. 하지만 BlueQubit의 등장으로 양자 개발은 열정가와 초보자 모두에게 현실이 되었습니다.\nBlueQubit은 누구나 언제 어디서나 양자 컴퓨팅의 힘을 경험할 수 있게 하는 고급스럽고 사용자 친화적인 플랫폼입니다. BlueQubit이 양자 컴퓨팅 입문자에게 최고의 선택인 이유 중 하나는 사용 편의성입니다. 더 나은 사용자 경험을 제공하는 데 초점을 맞춘 이 플랫폼은 기술적 세부 사항에 깊이 들어가지 않아도 양자 컴퓨터의 능력을 활용할 수 있게 합니다.\nCirq와 Qiskit 같은 오픈소스 라이브러리와 매끄럽게 통합되어 사용자는 집에서도 양자 프로그램을 실행할 수 있습니다. 이 기능은 인프라 투자 없이 양자 컴퓨팅의 잠재력을 탐구하고자 하는 개발자와 연구자에게 무한한 가능성을 열어줍니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#양자-컴퓨팅-언어의-유형",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#양자-컴퓨팅-언어의-유형",
    "title": "Quantum Programming",
    "section": "양자 컴퓨팅 언어의 유형",
    "text": "양자 컴퓨팅 언어의 유형\n양자 컴퓨팅 언어는 양자 알고리즘을 프로그래밍하고 실행하는 데 각기 다른 역할을 하며 다양한 형태로 존재합니다. 여기에는 고급 양자 프로그래밍 언어, 저수준 명령어 세트, 소프트웨어 개발 키트가 포함됩니다.\n\n양자 프로그래밍 언어\n양자 프로그래밍 언어는 양자 알고리즘을 표현하고 큐비트, 양자 게이트, 측정을 제어하기 위해 설계되었습니다. 양자 프로그램 작성을 위한 고수준 추상화를 제공합니다. 고전 언어와 달리 중첩, 얽힘, 양자 병렬성과 같은 양자 특유의 연산을 지원합니다.\n예로는 Qiskit(Python 기반), Quipper(Haskell 기반), Silq(고수준 양자 언어), Q#(Microsoft의 양자 언어)가 있습니다. 이 언어들은 연구자와 개발자가 양자 응용 프로그램을 구축하고 고전 코드와 통합해 하이브리드 양자-고전 계산을 가능하게 합니다.\n\n\n양자 명령어 세트\n양자 명령어 세트는 양자 하드웨어를 직접 제어하는 저수준 명령을 정의합니다. 이는 고전 컴퓨팅의 어셈블리 언어와 비슷합니다. Hadamard, CNOT, 위상 게이트 같은 양자 연산을 위한 게이트 수준 명령을 제공하며, 서로 다른 양자 하드웨어 아키텍처에서 효율적인 실행을 보장합니다.\n예로는 OpenQASM(IBM), Quil(Rigetti), Blackbird(Xanadu)가 있습니다. 이들은 양자 알고리즘과 물리적 큐비트 간의 인터페이스 역할을 합니다.\n\n\n양자 소프트웨어 개발 키트\n양자 SDK는 양자 프로그램을 개발하고, 테스트하고, 실행하기 위한 도구, 라이브러리, 시뮬레이터를 제공합니다. 고수준 프로그래밍 언어와 양자 하드웨어 간의 간극을 메웁니다. 대표적인 SDK로는 Qiskit(IBM), Cirq(Google), PennyLane(Xanadu), Braket(AWS)이 있습니다. 이 SDK들은 양자 회로 시뮬레이션, 실제 양자 장치에서 알고리즘 실행, 기존 응용 프로그램에 양자 컴퓨팅 통합을 가능하게 해 연구와 실용적 채택을 가속화합니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#인기-있는-양자-프로그래밍-언어와-라이브러리",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#인기-있는-양자-프로그래밍-언어와-라이브러리",
    "title": "Quantum Programming",
    "section": "인기 있는 양자 프로그래밍 언어와 라이브러리",
    "text": "인기 있는 양자 프로그래밍 언어와 라이브러리\n양자 시스템의 힘을 활용하기 위해 다양한 프로그래밍 언어와 라이브러리가 개발되었습니다. 이들은 양자 회로를 생성, 조작, 실행하도록 특별히 설계되었으며 고전 프로그래밍 언어와는 다릅니다. 다음은 익숙해질 만한 최고의 양자 프로그래밍 언어 목록입니다:\n\nQiskit\nQiskit은 IBM에서 만든 오픈소스 양자 컴퓨팅 프레임워크입니다. 양자 회로 설계 및 실행을 위한 사용하기 쉬운 인터페이스와 양자 시스템 시뮬레이션 및 양자 알고리즘 최적화 도구를 제공합니다. 널리 채택된 도구로, 초보자와 숙련된 개발자 모두에게 최고의 양자 프로그래밍 언어 중 하나입니다.\n\n\nCirq\nCirq는 Google Quantum AI에서 개발한 인기 있는 양자 프로그래밍 라이브러리입니다. 개발자가 시뮬레이터와 실제 양자 하드웨어에서 양자 회로를 생성, 편집, 실행할 수 있게 합니다. 사용자 친화적인 인터페이스와 강력한 기능으로 양자 프로그래밍을 탐구하려는 이들에게 최고의 선택입니다.\n\n\nPyQuil\nPyQuil은 Rigetti Computing에서 만든 독창적인 양자 명령어 언어로, 양자 프로그래밍에 독특한 접근 방식을 제공합니다. 양자 알고리즘 생성 과정을 단순화하도록 설계된 PyQuil은 Rigetti의 양자 프로세서 및 시뮬레이터와의 호환성을 유지하며 양자 응용 프로그램 개발을 간소화합니다.\n\n\nQ\nMicrosoft에서 개발한 Q#은 양자 프로그래밍을 위해 특화된 도메인별 언어입니다. Quantum Development Kit(QDK)와 통합되어 개발자가 양자 알고리즘을 고전 및 양자 하드웨어에서 작성, 테스트, 디버깅하기 쉽게 합니다. 고수준 문법과 풍부한 라이브러리로 Q#은 양자 응용 프로그램 생성을 단순화합니다.\n\n\nQasm과 OpenQasm\nQasm(Quantum Assembly Language)과 그 오픈소스 버전인 OpenQasm은 양자 회로를 위한 중급 표현입니다. 이 언어들은 양자 명령을 위한 표준 형식을 제공하여 다양한 플랫폼에서 양자 회로를 설계하고 시뮬레이션하기 쉽게 합니다. 특히 OpenQasm은 모듈성과 확장성을 지원해 복잡한 양자 프로그램을 효율적으로 작성할 수 있게 합니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#마무리",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#마무리",
    "title": "Quantum Programming",
    "section": "마무리",
    "text": "마무리\n양자 프로그래밍은 산업을 변화시킬 엄청난 잠재력을 가진 흥미로운 분야입니다. 쇼어 알고리즘과 그로버 알고리즘 같은 핵심 알고리즘을 이해하고, Qiskit, Cirq, PyQuil, Q#, OpenQasm과 같은 인기 언어와 라이브러리를 사용하면 초보자도 자신 있게 양자 세계에 입문할 수 있습니다.\n양자 컴퓨팅 회사인 BlueQubit은 사용자 친화적인 인터페이스, 강력한 양자 시뮬레이터, 실제 양자 하드웨어 접근성을 제공하여 개발자가 양자 컴퓨팅의 힘을 활용하고 혁신을 이끌어내기에 이상적인 선택입니다. 지금 가입하고 프로그래밍을 시작하세요.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#자주-묻는-질문",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/00.html#자주-묻는-질문",
    "title": "Quantum Programming",
    "section": "자주 묻는 질문",
    "text": "자주 묻는 질문\n\n양자 컴퓨팅을 위한 C 언어란 무엇인가요?\nC 자체는 양자 컴퓨팅에 일반적으로 사용되지 않지만, QCOR(Quantum Computing ORchestration)은 C++의 확장으로 양자 프로그래밍과 고전 컴퓨팅을 통합합니다. 이 언어는 양자 하드웨어와 시뮬레이터와 함께 작동하도록 설계되어 개발자가 하이브리드 양자-고전 알고리즘을 효율적으로 작성할 수 있게 합니다. 그러나 오늘날 대부분의 양자 프로그래밍은 Qiskit(Python), Cirq(Python), Q#(Microsoft의 양자 언어)와 같은 고수준 언어에 의존합니다. 이는 사용 편의성과 양자 특유의 기능을 제공하기 때문입니다.\n\n\nPython은 양자 컴퓨팅에 사용되나요?\n네, Python은 Qiskit, Cirq, PennyLane과 같은 강력한 양자 컴퓨터 프로그래밍 라이브러리 덕분에 양자 컴퓨팅에 널리 사용됩니다. 이 라이브러리들은 직관적인 API, 양자 회로 시뮬레이터, 실제 양자 하드웨어에서 프로그램을 실행할 수 있는 도구를 제공합니다. Python의 유연성과 단순함은 양자 연구에 이상적이며, 양자 알고리즘을 구축, 테스트, 배포하면서 고전 계산과 통합하기 쉽게 합니다. IBM Quantum Experience와 Amazon Braket 같은 많은 양자 컴퓨팅 플랫폼도 Python 기반 프레임워크를 지원합니다.\n\n\n양자 컴퓨팅에 가장 적합한 프로그래밍 언어는 무엇인가요?\n양자 컴퓨팅에 가장 적합한 프로그래밍 언어는 사용 사례와 하드웨어 호환성에 따라 다릅니다. Qiskit(Python 기반)은 사용자 친화적인 인터페이스와 IBM Quantum의 강력한 지원으로 초보자와 연구자에게 널리 사용됩니다. Cirq(역시 Python 기반)는 Google의 양자 하드웨어에 최적화되어 있으며, Q#(Microsoft)는 고전 통합과 함께 양자 알고리즘 개발에 설계되었습니다.\n기타 주목할 만한 양자 컴퓨팅 프로그래밍 언어로는 Silq(고수준 양자 프로그래밍), Quipper(Haskell 기반), OpenQASM(어셈블리 스타일 양자 언어)이 있습니다. Python 기반 프레임워크가 이 분야를 지배하고 있으므로 Qiskit과 Cirq가 가장 인기 있는 선택입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Quantum Programming"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/ros/notes/00.html",
    "href": "posts/03_archives/stored_categories/ros/notes/00.html",
    "title": "Basic",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "ROS",
      "Notes",
      "Basic"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/notes/00.html#금융-행동의-개인차",
    "href": "posts/03_archives/stored_categories/금융/notes/00.html#금융-행동의-개인차",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "1. 금융 행동의 개인차",
    "text": "1. 금융 행동의 개인차\n금융 시장에서 개인의 행동 차이는 단순히 정보의 우위나 지적 능력의 차이가 아닌, 개인의 경험과 가치관에서 비롯된다. 우리는 각자의 경험을 바탕으로 나름의 합리적인 의사결정을 내린다. 따라서 겉보기에 비합리적으로 보이는 행동도 개인의 맥락에서는 충분히 이해될 수 있다. 돈 문제에 있어서 누구나 미친짓을 한다. 거의 모두가 이 게임이 처음이기 때문이다. 하지만 실제로 미친사람은 없다. 누구나 자신만의 경험에 근거해서 합리적으로 보이는 의사결정을 내릴 뿐이다",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/notes/00.html#운과-리스크의-역할",
    "href": "posts/03_archives/stored_categories/금융/notes/00.html#운과-리스크의-역할",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "2. 운과 리스크의 역할",
    "text": "2. 운과 리스크의 역할\n금융 시장에서의 결과는 우리의 행동만으로 결정되지 않는다. 운의 영향력을 인정하고, 리스크를 적절히 관리하는 것이 중요하다. 이를 위해 우리는 다음과 같은 질문들을 스스로에게 던져야 한다\n\n추가적인 수익이 정말 필요한가?\n타인과의 비교가 판단을 흐리고 있지는 않은가?\n’충분함’의 기준은 무엇인가?\n돈보다 우선시해야 할 가치는 무엇인가?\n\n어느 정도가 충분한지 깨닫고 리스크를 멈출줄 알아야 한다",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/notes/00.html#지속가능한-투자의-원칙",
    "href": "posts/03_archives/stored_categories/금융/notes/00.html#지속가능한-투자의-원칙",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "3. 지속가능한 투자의 원칙",
    "text": "3. 지속가능한 투자의 원칙\n일회성 수익보다는 지속가능한 수익이 더 가치있다. 투자에는 두 가지 다른 기술이 필요하다\n\n수익 창출: 리스크 감수, 낙관적 사고, 적극적 태도\n자산 보존: 신중함, 위험 관리, 절제\n\n최고의 수익률은 일회성이어서 반복할 수 없는 경향이 있다. 꽤 괜찮은 수익률을 오랫동안 반복할 수 있는게 훌륭한 투자다. 성공적인 투자자는 대중이 비이성적일 때도 침착함을 유지할 수 있는 사람이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/notes/00.html#돈과-시간의-관계",
    "href": "posts/03_archives/stored_categories/금융/notes/00.html#돈과-시간의-관계",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "4. 돈과 시간의 관계",
    "text": "4. 돈과 시간의 관계\n돈의 진정한 가치는 그것이 우리에게 주는 시간의 자유에 있다. 돈이 주는 가장 큰 배당금은 시간이다 단순히 부자(rich)가 되는 것과 진정한 부(wealthy)를 이루는 것은 다르다. 진정한 부자들은 겉으로 보이는 치장(rich)에 돈을 쓰기 보다는 부를 축적(wealthy)하여 자유를 얻는다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/notes/00.html#금융시장의-불변요소와-가변요소",
    "href": "posts/03_archives/stored_categories/금융/notes/00.html#금융시장의-불변요소와-가변요소",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "5. 금융시장의 불변요소와 가변요소",
    "text": "5. 금융시장의 불변요소와 가변요소\n금융 시장에서 인간의 기본적인 행동 패턴은 크게 변하지 않는다. 탐욕, 공포, 스트레스 상황에서의 반응 등은 시대가 바뀌어도 유사하다. 반면, 시장 트렌드, 산업 구조, 투자 방식 등은 끊임없이 진화한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/notes/00.html#리스크-관리의-중요성",
    "href": "posts/03_archives/stored_categories/금융/notes/00.html#리스크-관리의-중요성",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "6. 리스크 관리의 중요성",
    "text": "6. 리스크 관리의 중요성\n\n파산 위험이 있는 리스크는 절대 감수하지 않는다\n계획이 실패했을 때를 대비한 백업 플랜이 필수적이다\n시장의 변동성은 피해야 할 벌금이 아닌, 수수료로 인식해야 한다",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/notes/00.html#현실적인-목표-설정",
    "href": "posts/03_archives/stored_categories/금융/notes/00.html#현실적인-목표-설정",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "7. 현실적인 목표 설정",
    "text": "7. 현실적인 목표 설정\n\n이상적인 목표와 현실적인 스트레스 상황은 큰 차이가 있다\n과거의 비현실적 목표는 과감히 버려야 한다\n내가 지금과 다른 사람일 때 세웠던 목표는 생명 유지 장치를 달고 시간을 질질 끌 게 아니라 가차 없이 버리는 편이 낫다",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/notes/00.html#시장의-본질-이해",
    "href": "posts/03_archives/stored_categories/금융/notes/00.html#시장의-본질-이해",
    "title": "돈의 심리학 - 모건 하우절",
    "section": "8. 시장의 본질 이해",
    "text": "8. 시장의 본질 이해\n\n극단적 상황은 오래 지속되지 않는다\n투자 성공의 대가를 이해하고 지불할 준비가 필요하다\n시장을 완벽히 통제할 수 있다는 환상을 버려야 한다",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융",
      "Notes",
      "돈의 심리학 - 모건 하우절"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/smart_contract/notes/block_chain_basic/00.html",
    "href": "posts/03_archives/stored_categories/smart_contract/notes/block_chain_basic/00.html",
    "title": "what is a blockchain?",
    "section": "",
    "text": "oracle(회사 아님): a trusted third party that provides data to the blockchain\nchain link: a decentralized oracle network that connects smart contracts to external data sourcesa\nsmart contract: trust minimized agreements, unbrakable promises\nmetamask는 nimonics를 이용해 private key를 생성. 다계정을 만들 때는 nimonics + &lt;index&gt;를 이용해 계정 생성\nprivate key는 transaction을 sign할 때 사용. public key는 transaction을 verify할 때 사용\nverify된 transaction은 miner에 의해 블록에 추가됨\ngas price: Base Fee + Priority Fee\ntransaction fee: 실제로 지불하는 금액. Gas Prics * used gas (&lt; gas limit). transaction fee - burnt fee만큼 즉, priority fee * used gas만큼 miner에게 지급됨\ngas fee: transaction을 처리하는데 필요한 비용. gas fee가 높을수록 빨리 처리됨\n\nBase fee: network congestion에 따라 변동. Base fee * used gas 만큼 소각됨\nMax fee: 사용자가 지불할 수 있는 최대 gas price\nMax Priority: 사용자가 지불할 수 있는 최대 fee + tip\n\nconsensus algorithm: 블록체인 네트워크의 모든 노드가 동의하는 방식 (nakamoto consensus: proof of work + longest chain)\n\nChain selection:\n\nlongest chain: 가장 긴 체인을 선택\n\nsybil resistance: 한 사람이 여러 개의 가짜 계정을 만들고 시스템을 조작하는 Sybil 공격을 방어하는 능력\n\nproof of works: hash를 0으로 만드는 nonce를 찾음. 제일 먼저 찾은 사람이 블록을 추가할 수 있음 (transaction fee + block reward(네트워크에서 새로 발행하는 코인. 갈수록 줄어듦))\nproof of stake\n\n\nL1: base layer of blockchain ecosystem\nL2: application built outside of the L1 and hooks back into the L1\nroll up: L2에서 발생한 transaction을 L1에 기록하는 방식\n\noptimistic roll up: L2에서 transaction을 처리하고 L1에 기록함. L1에 기록되기 전까지는 롤백 가능\nzk roll up: L2에서 transaction을 처리하고 L1에 기록함. L1에 기록되면 롤백 불가능\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Smart Contract",
      "Notes",
      "Block Chain Basic",
      "what is a blockchain?"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/00.html#what-is-airflow",
    "href": "posts/03_archives/stored_categories/air_flow/notes/00.html#what-is-airflow",
    "title": "Getting Started",
    "section": "What is Airflow",
    "text": "What is Airflow\n\nopen source platform to pragramatically author, schedule and monitor workflows\nNot a data processing framework\nNot a Real time streaming solution (only for batch processing)\nNot a data storage system\nand simple linear workflow might overkill",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/00.html#why-airflow",
    "href": "posts/03_archives/stored_categories/air_flow/notes/00.html#why-airflow",
    "title": "Getting Started",
    "section": "Why Airflow",
    "text": "Why Airflow\n\nautomation\nvisibility\nflexibility and scalability\nextensibility",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/00.html#core-components",
    "href": "posts/03_archives/stored_categories/air_flow/notes/00.html#core-components",
    "title": "Getting Started",
    "section": "Core Components",
    "text": "Core Components\n\nWebserver: provides UI\nScheduler: triggers tasks. ensure that task runs in correct time and order\nmeta database: memmory, communication between components\ntrigger: daemon that listens to external events and triggers tasks\nexecuter: traffic controller that decide how tasks are executed (sequential or parallel, local or remote)\nqueue\nworker",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/00.html#core-concepts",
    "href": "posts/03_archives/stored_categories/air_flow/notes/00.html#core-concepts",
    "title": "Getting Started",
    "section": "Core Concepts",
    "text": "Core Concepts\n\nDAG\n\nDirected Acyclic Graph\ncollection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies\nno cycles in dependencies graph\n\n\n\nOperator\n\ndefines a single task in a workflow\ne.g. BashOperator, PythonOperator, EmailOperator, etc.\n\n\n\nTask / Task Instance\n\nspecific instance of an operator\nwhen operator assigned to a DAG, it becomes a task\n\n\n\nWorkflow\n\nentire process defined by DAG\nDAG = workflow",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/00.html#arcitecture",
    "href": "posts/03_archives/stored_categories/air_flow/notes/00.html#arcitecture",
    "title": "Getting Started",
    "section": "Arcitecture",
    "text": "Arcitecture",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Getting Started"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/problem_solve/index.html",
    "href": "posts/03_archives/stored_categories/problem_solve/index.html",
    "title": "Problem Solving",
    "section": "",
    "text": "Problem Solving에 대한 노트 모음입니다.\n이전 포스팅은 이곳에서 확인 가능합니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/problem_solve/index.html#details",
    "href": "posts/03_archives/stored_categories/problem_solve/index.html#details",
    "title": "Problem Solving",
    "section": "",
    "text": "Problem Solving에 대한 노트 모음입니다.\n이전 포스팅은 이곳에서 확인 가능합니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/problem_solve/index.html#tasks",
    "href": "posts/03_archives/stored_categories/problem_solve/index.html#tasks",
    "title": "Problem Solving",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/problem_solve/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/problem_solve/index.html#related-posts",
    "title": "Problem Solving",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Problem Solving"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/3_cluster_maintainance.html",
    "href": "posts/03_archives/stored_categories/k8s/notes/3_cluster_maintainance.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s wait for 5 minutes to mark a node as ‘dead’ in default\nif a node is marked as ‘dead’, the pods on the node will be rescheduled to other nodes\ndrain: remove all the pods from a node and reschedule them to other nodes\ncordon: mark a node as ‘unschedulable’ so that no new pods will be scheduled to the node\nuncordon: mark a node as ‘schedulable’ so that new pods can be scheduled to the node but the original pods will not be rescheduled",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/3_cluster_maintainance.html#fail-tolerance",
    "href": "posts/03_archives/stored_categories/k8s/notes/3_cluster_maintainance.html#fail-tolerance",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s wait for 5 minutes to mark a node as ‘dead’ in default\nif a node is marked as ‘dead’, the pods on the node will be rescheduled to other nodes\ndrain: remove all the pods from a node and reschedule them to other nodes\ncordon: mark a node as ‘unschedulable’ so that no new pods will be scheduled to the node\nuncordon: mark a node as ‘schedulable’ so that new pods can be scheduled to the node but the original pods will not be rescheduled",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/3_cluster_maintainance.html#cluster-upgrade-process",
    "href": "posts/03_archives/stored_categories/k8s/notes/3_cluster_maintainance.html#cluster-upgrade-process",
    "title": "김형훈의 학습 블로그",
    "section": "cluster upgrade process",
    "text": "cluster upgrade process\n - k8s supports up to recent 3 minor versions  ### kubeadm upgrade 1. upgrade kubeadm 2. command: kubeadm upgrade apply 3. upgrade kubelet and kubectl",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "fail tolerance"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/7_design_cluster.html",
    "href": "posts/03_archives/stored_categories/k8s/notes/7_design_cluster.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "api-server: multiple instances, active-active, load balancer\ncontroller-manager: multiple instances, active-standby, leader election",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "HA in master node"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/7_design_cluster.html#ha-in-master-node",
    "href": "posts/03_archives/stored_categories/k8s/notes/7_design_cluster.html#ha-in-master-node",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "api-server: multiple instances, active-active, load balancer\ncontroller-manager: multiple instances, active-standby, leader election",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "HA in master node"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html",
    "href": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "if scheduler is not exist, user can mannually schedule pods to nodes - in pod spec, set nodeName field to the name of the node - if the node is not exist, the pod will be in Pending state - bind request",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#manual-scheduling",
    "href": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#manual-scheduling",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "if scheduler is not exist, user can mannually schedule pods to nodes - in pod spec, set nodeName field to the name of the node - if the node is not exist, the pod will be in Pending state - bind request",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#taints-and-tolerations",
    "href": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#taints-and-tolerations",
    "title": "김형훈의 학습 블로그",
    "section": "taints and tolerations",
    "text": "taints and tolerations\n\ntaints: a taint is a key-value pair that is applied to a node\ntolerations: a toleration is a key-value pair that is applied to a pod  \nnot garantee that the pod will be scheduled to the node",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#node-affinity",
    "href": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#node-affinity",
    "title": "김형훈의 학습 블로그",
    "section": "node affinity",
    "text": "node affinity\n\n\n\nnode affinity",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#resource-limits-requests",
    "href": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#resource-limits-requests",
    "title": "김형훈의 학습 블로그",
    "section": "resource limits, requests",
    "text": "resource limits, requests\n\nresource limits: the maximum amount of resources that a container can use\nresource requests: the amount of resources that a container is guaranteed to have\nresource quotas: the maximum amount of resources that a namespace can use\nlimit range: the minimum and maximum amount of resources that a container can use when it is created",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#static-pod",
    "href": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#static-pod",
    "title": "김형훈의 학습 블로그",
    "section": "static pod",
    "text": "static pod\n\nstatic pod is a pod that is created by the kubelet on a node\nif kube-api is available, the kubelet will create the mirror pod in the api server. that is read-only  or in /etc/kubernetes/manifests",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#multiple-shedulers",
    "href": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#multiple-shedulers",
    "title": "김형훈의 학습 블로그",
    "section": "multiple shedulers",
    "text": "multiple shedulers",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#configuring-sheduler-profile",
    "href": "posts/03_archives/stored_categories/k8s/notes/1_scheduler.html#configuring-sheduler-profile",
    "title": "김형훈의 학습 블로그",
    "section": "configuring sheduler profile",
    "text": "configuring sheduler profile\n: single sheduler, multi profile",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "manual scheduling"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/5_storage.html",
    "href": "posts/03_archives/stored_categories/k8s/notes/5_storage.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "A persistant volume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using a storage class.\nuser can create a persistant volume claim to request a persistant volume with specific storage capacity and access modes.\n1:1 mapping between a persistant volume and a persistant volume claim.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/5_storage.html#persistant-volume",
    "href": "posts/03_archives/stored_categories/k8s/notes/5_storage.html#persistant-volume",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "A persistant volume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using a storage class.\nuser can create a persistant volume claim to request a persistant volume with specific storage capacity and access modes.\n1:1 mapping between a persistant volume and a persistant volume claim.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/5_storage.html#storage-class",
    "href": "posts/03_archives/stored_categories/k8s/notes/5_storage.html#storage-class",
    "title": "김형훈의 학습 블로그",
    "section": "storage class",
    "text": "storage class\n\ndynamically provisioned persistant volumes.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "Persistant volume"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/terraform/index.html",
    "href": "posts/03_archives/stored_categories/terraform/index.html",
    "title": "Terraform",
    "section": "",
    "text": "terraform 정리 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/terraform/index.html#details",
    "href": "posts/03_archives/stored_categories/terraform/index.html#details",
    "title": "Terraform",
    "section": "",
    "text": "terraform 정리 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/terraform/index.html#tasks",
    "href": "posts/03_archives/stored_categories/terraform/index.html#tasks",
    "title": "Terraform",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/terraform/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/terraform/index.html#참고-자료",
    "title": "Terraform",
    "section": "참고 자료",
    "text": "참고 자료\n\nKodeKloud - Terraform cloud",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/terraform/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/terraform/index.html#related-posts",
    "title": "Terraform",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Terraform"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/helm/index.html",
    "href": "posts/03_archives/stored_categories/helm/index.html",
    "title": "Helm",
    "section": "",
    "text": "Helm 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/helm/index.html#details",
    "href": "posts/03_archives/stored_categories/helm/index.html#details",
    "title": "Helm",
    "section": "",
    "text": "Helm 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/helm/index.html#tasks",
    "href": "posts/03_archives/stored_categories/helm/index.html#tasks",
    "title": "Helm",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/helm/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/helm/index.html#참고-자료",
    "title": "Helm",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/helm/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/helm/index.html#related-posts",
    "title": "Helm",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Helm"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/인생/index.html",
    "href": "posts/03_archives/stored_categories/인생/index.html",
    "title": "인생",
    "section": "",
    "text": "인생을 살면서 느낀 점들을 적어놓은 노트 모음입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/인생/index.html#details",
    "href": "posts/03_archives/stored_categories/인생/index.html#details",
    "title": "인생",
    "section": "",
    "text": "인생을 살면서 느낀 점들을 적어놓은 노트 모음입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/인생/index.html#tasks",
    "href": "posts/03_archives/stored_categories/인생/index.html#tasks",
    "title": "인생",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/인생/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/인생/index.html#related-posts",
    "title": "인생",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "인생"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/인생/notes/02.html",
    "href": "posts/03_archives/stored_categories/인생/notes/02.html",
    "title": "나의 단점에 관한 고찰",
    "section": "",
    "text": "나는 단점이 없다고 생각했다. 그래서 자기소개서 같은 곳에 장점, 단점을 적는 칸이 있으면 아래같이 유머 아닌 유머같은 답을 적곤 했다.\n\n\n\n장점\n단점이 없다.\n\n\n단점\n\n\n\n\n최근 드는 생각인데, 내 단점은 사람을 대할 때 꽤나 간사한 면이 있다는 것이다.\n사회적으로나, 능력적으로나, 누구든 나보다 잘나다고 생각되는 사람 앞에서 나는 수줍고 소심한 사람이 된다. 뭐.. 사실 이정도는 누구나 그런 면이 있을 수 있다 생각한다. 하지만 나보다 열등하다고 생각되는 사람 앞에서 나는 꽤 강압적이고 무례한 사람이 된다. 흔히 말하는 ’강약약강’이라는 말이 아마 나를 잘 설명해주는 것 같다.\n사실 어쩌면 이런 강압적인 모습이 나의 본성이 아닐까 하는 생각이 든다. 수줍고 소심한 모습은 아마 사회화된 또 다른 나의 모습이 아닐까. 왜냐하면 나는 나의 부모님에게서 수줍고 소심한 모습을 본 적이 없기 때문이다.\n그렇다면 나는 이 단점들을 극복해야 하나? 극복한다면 어떤 모습이 바람직한 나의 모습일까? 쓸모없는 사회화된 모습을 덜어야 할까? 아니면 추한 본성을 덜어야 할까? 사실 지금의 나도 살아가는데 그렇게 큰 불편함은 없긴 하다. 조금씩 밸런스를 맞춰가며 살아가야지.\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "인생",
      "Notes",
      "나의 단점에 관한 고찰"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/03.html#가우스-조던-소거법",
    "href": "posts/03_archives/stored_categories/선형대수/notes/03.html#가우스-조던-소거법",
    "title": "3-몰라",
    "section": "가우스 조던 소거법",
    "text": "가우스 조던 소거법\n\n선형대수의 목표는 \\(Ax = b\\)에서 x를 찾는 것이다.\n\n\\[\\begin{aligned}\nx + 2y \\quad  &= 4 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n이 수식을 다시 살펴보자. 위의 수식은 아래와 같이 적용할 수 있다.\n\\[\\begin{aligned}\n2x + 4y \\quad  &= 8 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n위의 열립방정식을 풀면 \\(y = 1\\)이라는 결과를 얻는다. 다시 \\(y=1\\)을 대입해서 \\(x=2\\)라는 값을 구할 수 있다.\n이제 이를 matrix와 vector로 풀어보자.\n\\[\n\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n9\n\\end{bmatrix}\n\\]\n이를 확장행렬로 표현하면 다음과 같다\n\\[\n[A|b] = \\begin{bmatrix}\n1 & 2 & | & 4 \\\\\n2 & 5 & | & 9\n\\end{bmatrix}\n\\]\n이제 가우스 조던 소거법을 적용해보자\n적용 순서는 다음과 같다.\n\n양 변에 0이 아닌 상수배를 해준다.\n상수배를 한 행을 다른행에 더하거나 뺀다.\n행끼리 자리 바꾼다.\n\n이에 맞춰서 위의 식을 풀이하면,\n\n두 번째 행에서 첫 번째 행의 2배를 빼면\n\n\\[\n\\begin{bmatrix}\n1 & 2 & | & 4 \\\\\n0 & 1 & | & 1\n\\end{bmatrix}\n\\]\n\n첫 번째 행에서 두 번째 행의 2배를 빼면\n\n\\[\n\\begin{bmatrix}\n1 & 0 & | & 2 \\\\\n0 & 1 & | & 1\n\\end{bmatrix}\n\\]\n따라서 \\(x = 2\\), \\(y = 1\\)이라는 해를 얻을 수 있다.\n즉 가우스조던 소거법은 왼쪽을 항등행렬로 만들고, 그 오른쪽에 있는 값이 답이되는 소거법이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/03.html#역행렬-구하기",
    "href": "posts/03_archives/stored_categories/선형대수/notes/03.html#역행렬-구하기",
    "title": "3-몰라",
    "section": "역행렬 구하기",
    "text": "역행렬 구하기\n역행렬을 구할 수 있다면 x의 값을 쉽게 구할 수 있다. (\\(x = A^{-1}b\\))\n가우스 조던 소거법을 이용해 역행렬을 구해보자.\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\nc & d & | & 0 & 1\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\n0 & \\frac{ad-bc}{a} & | & -\\frac{c}{a} & 1\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & b & | & 1 & 0 \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\na & 0 & | & \\frac{ad}{ad-bc} & \\frac{-ab}{ad-bc} \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1 & 0 & | & \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\\n0 & 1 & | & -\\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\]\n\\[\n∴ A^{-1} = \\frac{1}{ad-bc}\n\\begin{bmatrix}\nd & -b \\\\\n-c & a\n\\end{bmatrix}\n\\]\n\ninvertible\n역행렬이 존재할 경우 invertible하다고 한다.\n\nnon singular matrix\ndet(A) ≠ 0: ad - bc(determinant) = 0인 경우 역행렬이 존재하지 않는다.\nA가 full rank이다\nN(A) = 0",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/03.html#determinant",
    "href": "posts/03_archives/stored_categories/선형대수/notes/03.html#determinant",
    "title": "3-몰라",
    "section": "determinant",
    "text": "determinant\n정사각행렬의 element로 scalar 값을 만드는 함\n\n3 x 3 행렬의 det\n\\[\nA=\n\\begin{bmatrix}\na & b & c\\\\\nd & e & f \\\\\ng & h & i\n\\end{bmatrix}\n\\]\n\\(det(A) = a(ei - fh) - b(di-fg)+c(dh-eg)\\)\nLaplace expansion or cofactor expansion\n\n\nproperties\n\ndet(A) = 0 이면 A is singular\nA가 rank-deficient 이면 det(A) = 0\ndiagonal or triangular matrix, det(A) = 대각요소의 곱\n항등행렬의 det=1\ndet(cA) = \\(c^ndet(A)\\) (A = nxn)\n\\(det(A^T) = det(A)\\)\ndet(AB) = det(A)det(B)\n\\(\\color{red}{det(A^{-1}) = \\frac{1}{det(A)}}\\)\n\\(\\color{red}{det(A) = λ_1λ_2,...,λ_n}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/03.html#trace",
    "href": "posts/03_archives/stored_categories/선형대수/notes/03.html#trace",
    "title": "3-몰라",
    "section": "Trace",
    "text": "Trace\n정사각 행렬에 대해서만 정의되는 것, diagonal 전부 더함\n\\(tr(A) = \\sum_{i=1}^{n}a_{ii}\\)\n\ntr(A + B) = tr(A) + tr(B)\ntr(cA) = ctr(A)\n\\(tr(A^T) = tr(A)\\)\ntr(AB) = tr(BA)\n\\(tr(a^Tb) = tr(ba^T)\\)\ntr(ABCD) = tr(BCDA) = tr(CDAB) = tr(DABC) (cyclic property)\n\\(tr(A) = \\sum_{i=1}^{n}\\lambda_i\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/03.html#최소자승법",
    "href": "posts/03_archives/stored_categories/선형대수/notes/03.html#최소자승법",
    "title": "3-몰라",
    "section": "최소자승법",
    "text": "최소자승법",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "3-몰라"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/01.html#vector",
    "href": "posts/03_archives/stored_categories/선형대수/notes/01.html#vector",
    "title": "2-기초(1)",
    "section": "Vector",
    "text": "Vector\nvector는 크기와 방향을 가지고 있다.\n\nExample\n\\[\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix}\\]\n\n\n\n\n\n\n\n\n\n\n크기: \\(\\sqrt{9 + 4} = \\sqrt{13}\\)\n방향: \\(tan^{-1}(\\frac{2}{3})\\)\n\n크기와 방향이 같으면 같은 벡터이다.\n\n\n덧셈\n벡터의 덧셈을 기하학적으로 알아보자\n\\[\n\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n위의 수식을 좌표평면에 나타나면 다음과 같다.\n\n\n\n\n\n\n\n\n\n끝점을 다 더한 좌표와 시작 점을 연결한 벡터인 초록색 화살표가 두 벡터의 합이 된다.\n\n\nScalar 배\nvector에 scalar, 즉 숫자 하나를 곱하면 무슨 일이 생길까?\n\\[\n2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n2\n\\end{bmatrix}\n\\] \\[\n-2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4 \\\\\n-2\n\\end{bmatrix}\n\\]\n마찬가지로 좌표평면으로 나타내는건 귀찮아서 생략하겠다.\n\n\n\n\n\n\nScalar 배를 한 벡터끼리 더하면 모든 2차원 좌표를 표현할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/01.html#전치-transpose",
    "href": "posts/03_archives/stored_categories/선형대수/notes/01.html#전치-transpose",
    "title": "2-기초(1)",
    "section": "전치 (Transpose)",
    "text": "전치 (Transpose)\n행렬 \\(A\\)의 요소 \\(a_{ij}\\)는 A의 Transpose인 \\(A^T\\)의 \\(a_{ji}\\)가 된다. 즉, 행렬 \\(A\\)를 전치하면 diagnal(대각선 요소)를 제외한 모든 요소가 대각선을 기준으로 서로 뒤바뀐다.\n\nSymmetrix matrix: \\(A = A^T\\)인 행렬, 즉 대각선을 기준으로 값이 전부 같은 행렬 Hermitian matrix: \\((A^*)^T = A^H(conjugate transpose) = A\\)를 만족하는 행렬\n\nVector의 경우에는 Column Vector의 경우, Transpose시 Row Vector로, Row Vector의 경우도 반대로 작용한다.\n\nProperties\n\n\\((A^T)^T = A\\)\n\\((A+B)^T = A^T + B^T\\)\n\\(\\color{red}{(AB)^T = B^TA^T}\\)\n\\((A^TA)^T\\)와 \\((AA^T)^T\\)의 결과는 항상 자기 자신이 된다. → Symmetrix matrix\n\\(C(A)^T = CA^T\\)\n\\(det(A^T) = det(A)\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/01.html#inner-product-projection",
    "href": "posts/03_archives/stored_categories/선형대수/notes/01.html#inner-product-projection",
    "title": "2-기초(1)",
    "section": "Inner Product & Projection",
    "text": "Inner Product & Projection\n\\[\n\\underset{a}{\\begin{bmatrix}\n1 \\\\\n3\n\\end{bmatrix}} *\n\\underset{b}{\\begin{bmatrix}\n5 \\\\\n1\n\\end{bmatrix}} = 1 * 5 + 3 * 1 = 8 = a^Tb = b^Ta\n\\]\n갑자기 등장한 \\(a^Tb\\)가 의미하는건 아래와 같다.\n\\(a^Tb = ||a||*||b||cosθ\\)\n\n||a||는 a 벡터의 크기를 의미한다.\n\n위의 식을 그림으로 표현해보자\n\n\n\n\n\n\n\n\n\n내적은 초록색 화살표와 파란색 화살표의 곱으로 표현할 수 있다.\n이는 a 벡터가 b 벡터의 방향에 대해 얼마나 투영되었는지를 나타낸다.\n두 벡터의 방향이 일치할 때 내적의 값이 가장 크고, 수직일 때 0 (안 닮음을 의미), 반대 방향일 때 가장 작은 값이 된다.\n\n단위 벡터(크기가 1인 벡터) 계산\n위의 식으로 부터 다음의 추론 과정을 통해 단위 벡터를 계산할 수 있다.\n\\(a^Ta = ||a||^2\\)\n∴ \\(||a|| = \\sqrt{a^Ta}\\)\n∴ 단위 벡터는 \\(\\frac{a}{||a||}\\) = \\(\\frac{a}{\\sqrt{a^Ta}}\\)\n\n\n정사형 벡터의 좌표 계산\n벡터의 좌표는 방향과 크기의 곱으로 표현할 수 있다.\n\\(a^Tb = ||a||*||b||cosθ\\)\n정사형 벡터의 크기는 \\(\\frac{a^Tb}{||b||} = \\frac{a^Tb}{\\sqrt{b^Tb}}\\)\n장사형 벡터의 방향은 b의 단위 벡터와 같다.\n즉, 정사형 벡터의 좌표는 \\(\\frac{a^Tb}{\\sqrt{b^Tb}} * \\frac{b}{\\sqrt{b^Tb}} = \\frac{a^Tb}{b^Tb}b\\)\n\\(a^T\\frac{b}{\\sqrt{b^Tb}}*\\frac{b}{\\sqrt{b^Tb}}\\)로도 구할 수 있다.\n\na와 수직으로 연결되는 정사형 벡터 \\(\\hat{x}\\)\n\\((a-b\\hat{x})^Tb\\hat{x} = 0\\)\n\\(a^Tb - b^Tbb\\hat{x} = 0\\)\n\\(\\hat{x} = \\frac{a^Tb}{b^Tb}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/01.html#norm",
    "href": "posts/03_archives/stored_categories/선형대수/notes/01.html#norm",
    "title": "2-기초(1)",
    "section": "Norm",
    "text": "Norm\n크기를 나타내는 것(0 포함, 양 음수 scalar)\n\n2-Norm (\\(l_2\\)-norm)\n벡터의 물리적인 길이.\n\\[\na = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n\\]\n\\(||a||_2 = \\sqrt{1^2+2^2+3^2} = (|1|^{\\color{red}{2}}+|2|^{\\color{red}{2}}+|3|^{\\color{red}{2}})^{\\color{red}{\\frac{1}{2}}}\\)\n2 제곱에, \\(\\frac{1}{2}\\)여서 2-norm이다.\n\n두 벡터 사이의 거리는 두 벡터의 차이의 2-norm이다.\n\n\n\n1-Norm (\\(l_1\\)-norm)\n1 제곱에 \\(\\frac{1}{1}\\)을 계산해주면 된다.\n\\(||a||_1 = (|1|^1+|2|^1+|3|^1)^{\\frac{1}{1}}\\)\n\n\np-Norm (\\(l_p\\)-norm)\n\\(||a||_p = (|x_1|^p+|x_2|^p+|x_3|^p+...)^{\\frac{1}{p}} = (\\underset{t}{\\Sigma} |x_t|^p)^{\\frac{1}{p}} \\quad (p ≥ 1)\\)\n\n\ninfinity-Norm\n\\(||a||_∞ = \\underset{t}{max}|x_t|\\)\n1-norm, 2-norm, infinity-norm의 값이 1이 되는 모든 벡터들을 좌표평면에 나타내면 다음과 같다.\n\n\n\n\n\n\n\n\n\n같은 벡터일 때, 1-norm ≥ 2-norm ≥ ∞-norm 순으로 크다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(1)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/10.html#metrix-vector-product",
    "href": "posts/03_archives/stored_categories/선형대수/notes/10.html#metrix-vector-product",
    "title": "Null space and Column space",
    "section": "Metrix vector product",
    "text": "Metrix vector product\n\nSee \\(A\\)’s column space as a set of vectors. → \\(A\\)’s column space is the set of all linear combinations of the columns of \\(A\\).\nSee \\(B\\)’s row space as a set of vectors.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/10.html#null-space",
    "href": "posts/03_archives/stored_categories/선형대수/notes/10.html#null-space",
    "title": "Null space and Column space",
    "section": "Null space",
    "text": "Null space\n\n\\(N = \\{x \\in \\mathbb{R}^n | Ax = 0\\}\\)\n\n\\(N\\) is Null space of \\(A\\).\n\n\n\nN(A) = N(rref(A))\nif N(A) = {0}, then column vector of \\(A\\) is linearly independent. → column vector of \\(A\\) is not a basis for C(A) → pivot variable의 합이 free variable을 만든다. (redundant column)\ndim(N(A)) = nullity of A = # of free variables in rref(A)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/10.html#column-space",
    "href": "posts/03_archives/stored_categories/선형대수/notes/10.html#column-space",
    "title": "Null space and Column space",
    "section": "Column Space",
    "text": "Column Space\n\nC(A) = span(columns of A)\nrref(A)의 pivot variable의 column vector가 C(A)의 basis이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/10.html#column-space의-평면의-방정식",
    "href": "posts/03_archives/stored_categories/선형대수/notes/10.html#column-space의-평면의-방정식",
    "title": "Null space and Column space",
    "section": "Column Space의 평면의 방정식",
    "text": "Column Space의 평면의 방정식\n\n\\({\\rightVectorBar{b} | A\\rightVectorBar{x} = \\rightVectorBar{b} ∧ \\rightVectorBar{x} ∈ R^n}\\)\n위를 만족하는 기약행렬을 만들어, 해가 존재하도록 방정식을 구성하면 평면의 방정식을 구할 수 있다.\n혹은 column space의 basis를 구하고, 이를 이용해 평면의 방정식을 구할 수 있다.\n\\(R^n\\)을 span하는 basis의 vector는 n개이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "Null space and Column space"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/04.html#what-is-vector",
    "href": "posts/03_archives/stored_categories/선형대수/notes/04.html#what-is-vector",
    "title": "벡터와 공간",
    "section": "what is vector",
    "text": "what is vector\nvector는 크기(magnitude)와 방향(direction)을 가지고 있고, 2, 3, 4 차원 너머를 수학적으로 표현할 수 있다.\n\nvector의 수학적 표현\nvector는 ordered list인 tuple 형태로 표현할 수 있다.\n\\[\n\\vec{v} =\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]\ndomain과 dimension에 따라 vector는 다음과 같이 표현할 수 있다.\n\\[\n\\vec{v} ∈ R^2\n\\]\n\n1차원: \\(R^1\\)\n2차원: \\(R^2\\)\n3차원: \\(R^3\\)\nn차원: \\(R^n\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/04.html#vector의-합",
    "href": "posts/03_archives/stored_categories/선형대수/notes/04.html#vector의-합",
    "title": "벡터와 공간",
    "section": "vector의 합",
    "text": "vector의 합\nvector의 합은 각 성분별로 더한 결과를 반환한다.\n\n기하학적 의미\n\\[\n\\begin{bmatrix}\n3 \\\\\n2\n\\end{bmatrix} +\n\\begin{bmatrix}\n-2 \\\\\n1\n\\end{bmatrix}\n\\]\n위의 수식을 좌표평면에 나타나면 다음과 같다.\n\n\n\n\n\n\n\n\n\n끝점을 다 더한 좌표와 시작 점을 연결한 벡터인 초록색 화살표가 두 벡터의 합이 된다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/04.html#vector의-scalar-곱",
    "href": "posts/03_archives/stored_categories/선형대수/notes/04.html#vector의-scalar-곱",
    "title": "벡터와 공간",
    "section": "vector의 scalar 곱",
    "text": "vector의 scalar 곱\nvector에 scalar, 즉 숫자 하나를 곱하면 무슨 일이 생길까?\n\\[\n2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n4 \\\\\n2\n\\end{bmatrix}\n\\] \\[\n-2 * \\begin{bmatrix}\n2 \\\\\n1\n\\end{bmatrix} =\n\\begin{bmatrix}\n-4 \\\\\n-2\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/04.html#vector의-차",
    "href": "posts/03_archives/stored_categories/선형대수/notes/04.html#vector의-차",
    "title": "벡터와 공간",
    "section": "vector의 차",
    "text": "vector의 차\nvector의 차는 각 성분별로 뺀 결과를 반환한다.\n기하학적으로는 두 벡터의 끝점을 연결한 벡터가 된다.\n\\(\\vec{x} - \\vec{y}\\)는 y에서 x를 연결한 벡터가 된다.\n\\(\\vec{y} - \\vec{x}\\)는 x에서 y를 연결한 벡터가 된다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/04.html#단위-벡터",
    "href": "posts/03_archives/stored_categories/선형대수/notes/04.html#단위-벡터",
    "title": "벡터와 공간",
    "section": "단위 벡터",
    "text": "단위 벡터\n\\[\n\\vec{v} = \\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]\n위의 벡터를 단위 벡터의 합으로 만들면 다음과 같다.\n\\[\n\\hat{i} = \\begin{bmatrix}\n1 \\\\\n0\n\\end{bmatrix},\n\\hat{j} = \\begin{bmatrix}\n0 \\\\\n1\n\\end{bmatrix}\n\\]\n\\[\n\\vec{v} = 3\\hat{i} + 4\\hat{j}\n\\]\n\n\n\n\n\n\nScalar 배를 한 기저 벡터끼리 더하면 모든 2차원 좌표를 표현할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "벡터와 공간"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/07.html#subspaces",
    "href": "posts/03_archives/stored_categories/선형대수/notes/07.html#subspaces",
    "title": "Subspaces and the basis",
    "section": "Subspaces",
    "text": "Subspaces\n\n\\(S\\) is a subset of \\(V\\).\n\nS ⊆ V\nS is a vector space\n\ninclude zero vector\nclosed under addition\nclosed under scalar multiplication",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "Subspaces and the basis"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/07.html#basis",
    "href": "posts/03_archives/stored_categories/선형대수/notes/07.html#basis",
    "title": "Subspaces and the basis",
    "section": "Basis",
    "text": "Basis\n\nminimum set of vectors that spans the subset\n\\(S\\) is a basis of \\(V\\) ⟺\n\nelements of \\(S\\) are linearly independent\n\\(S\\) spans \\(V\\)\n\n특정 부분집합의 basis의 linear combination으로 표현되는 모든 벡터는 유일하다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "Subspaces and the basis"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/09.html#reduced-row-echelon-form",
    "href": "posts/03_archives/stored_categories/선형대수/notes/09.html#reduced-row-echelon-form",
    "title": "가감법으로 연립방정식을 풀기 위한 행렬",
    "section": "Reduced Row Echelon Form",
    "text": "Reduced Row Echelon Form\n각 행의 선행항을 1로 만들고, 그 열의 다른 항을 0으로 만드는 방법을 행렬로 표현한 것이다.\n이때 선행항의 변수를 pivot variable이라고 하고, 다른 변수들은 free variable이라고 한다.\n관행적으로 pivot entry는 우하향으로 이동하고, zeroed out 행은 맨 아래쪽에 위치한다.\n기약행렬을 이용해 연립방정식으로 풀 수 없는 식을 행렬의 선형 결합으로 표현할 수 있다.\n\n\n혹은 식이 유효하지 않다면, 해가 없는 경우이며, 이것은 평행한 조건이 존재할 경우 발생한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "가감법으로 연립방정식을 풀기 위한 행렬"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/02.html#행렬의-곱을-바라보는-관점",
    "href": "posts/03_archives/stored_categories/선형대수/notes/02.html#행렬의-곱을-바라보는-관점",
    "title": "2-기초(2)",
    "section": "행렬의 곱을 바라보는 관점",
    "text": "행렬의 곱을 바라보는 관점\n\n내적으로 바라보기\n\n\\[\nA = \\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix}\n\\quad (a_x = \\text{column vector})\n\\]\n\\[\nAB = \\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & b_3\n\\end{bmatrix} =\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & a_1^Tb_3 \\\\\na_2^Tb_1 & a_2^Tb_2 & a_2^Tb_3 \\\\\na_3^Tb_1 & a_3^Tb_2 & a_3^Tb_3\n\\end{bmatrix}\n\\]\n\nrank-1 matrix의 합\n\n\\[\nAB = \\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\nb_3^T\n\\end{bmatrix} =\na_1^Tb_1 + a_2^Tb_2 + a_3^Tb_3\n\\]\n\nColumn space로 바라보기\n\n\\[\nAx = \\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3\n\\end{bmatrix} = a_1x_1 + a_2x_2 + a_3x_3\n\\]\n\nRow space로 바라보기\n\n\\[\nx^TA = \\begin{bmatrix}\nx_1 & x_2 & x_3\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\na_3^T\n\\end{bmatrix} = x_1a_1^T + x_2a_2^T + x_3a_3^T\n\\]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/02.html#span과-column-space",
    "href": "posts/03_archives/stored_categories/선형대수/notes/02.html#span과-column-space",
    "title": "2-기초(2)",
    "section": "span과 column space",
    "text": "span과 column space\n\ncolumn space: column vector들이 span하는 영역\nspan: linear combination으로 만들어지는 모든 벡터들의 집합\nlinear combination: vector들을 scalar 배 하고 더한 것\nlinear independent: span하는 vector들이 서로 독립적인 경우\n수학적 정의: \\(a_1v_1 + a_2v_2 + \\cdots + a_nv_n = 0\\) 일 때 \\(a_1 = a_2 = \\cdots = a_n = 0\\) 인 경우\nbasis: 어떤 공간을 이루는 필수적인 구성요소 (linear independent, span)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/02.html#항등행렬",
    "href": "posts/03_archives/stored_categories/선형대수/notes/02.html#항등행렬",
    "title": "2-기초(2)",
    "section": "항등행렬",
    "text": "항등행렬\n\\(AI = IA = A\\)를 만족하는 행렬 \\(I\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/02.html#역행렬",
    "href": "posts/03_archives/stored_categories/선형대수/notes/02.html#역행렬",
    "title": "2-기초(2)",
    "section": "역행렬",
    "text": "역행렬\n\\(Ax = b\\)를 만족하는 \\(x\\)를 찾는 것은 \\(A^{-1}Ax = A^{-1}b\\)를 만족하는 \\(x\\)를 찾는 것과 같다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/02.html#대각-행렬",
    "href": "posts/03_archives/stored_categories/선형대수/notes/02.html#대각-행렬",
    "title": "2-기초(2)",
    "section": "대각 행렬",
    "text": "대각 행렬\ndiagonal을 제외한 모든 요소가 0인 행렬 (square, rectangular 모두 가능)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/02.html#orthogonal-행렬",
    "href": "posts/03_archives/stored_categories/선형대수/notes/02.html#orthogonal-행렬",
    "title": "2-기초(2)",
    "section": "Orthogonal 행렬",
    "text": "Orthogonal 행렬\n행렬의 모든 column들이 orthonormal vector인 경우\n\\(Q^{-1} = Q^T\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/02.html#행렬의-rank",
    "href": "posts/03_archives/stored_categories/선형대수/notes/02.html#행렬의-rank",
    "title": "2-기초(2)",
    "section": "행렬의 rank",
    "text": "행렬의 rank\nrank: 행렬이 가지는 independent한 column의 개수 → column space의 차원\nrank(A) = rank(A^T)\n\nfull-column rank: 해가 없거나 한 개 존재\nfull-row rank: 해가 무한하다\nfull rank: 해가 한 개 있다.\nrank-deficient: b가 column space에 속하지 않는 경우 해가 없고, 그렇지 않으면 해가 무한하다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/02.html#null-space",
    "href": "posts/03_archives/stored_categories/선형대수/notes/02.html#null-space",
    "title": "2-기초(2)",
    "section": "Null space",
    "text": "Null space\n\\(Ax = 0\\)을 만족하는 모든 \\(x\\)의 집합\nA가 m x n 행렬이라면, dim(N(A)) = n - rank(A)\nnull space와 row space는 orthogonal하다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "2-기초(2)"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/kaggle/index.html",
    "href": "posts/03_archives/stored_categories/kaggle/index.html",
    "title": "Kaggle",
    "section": "",
    "text": "kaggle 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/kaggle/index.html#details",
    "href": "posts/03_archives/stored_categories/kaggle/index.html#details",
    "title": "Kaggle",
    "section": "",
    "text": "kaggle 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/kaggle/index.html#tasks",
    "href": "posts/03_archives/stored_categories/kaggle/index.html#tasks",
    "title": "Kaggle",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/kaggle/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/kaggle/index.html#참고-자료",
    "title": "Kaggle",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/kaggle/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/kaggle/index.html#related-posts",
    "title": "Kaggle",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Kaggle"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/vault/index.html",
    "href": "posts/03_archives/stored_categories/vault/index.html",
    "title": "vault",
    "section": "",
    "text": "vault 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "vault"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/vault/index.html#details",
    "href": "posts/03_archives/stored_categories/vault/index.html#details",
    "title": "vault",
    "section": "",
    "text": "vault 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "vault"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/vault/index.html#tasks",
    "href": "posts/03_archives/stored_categories/vault/index.html#tasks",
    "title": "vault",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "vault"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/vault/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/vault/index.html#참고-자료",
    "title": "vault",
    "section": "참고 자료",
    "text": "참고 자료\n\nvault Udemy 강의",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "vault"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/vault/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/vault/index.html#related-posts",
    "title": "vault",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "vault"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/hadoop/index.html",
    "href": "posts/03_archives/stored_categories/hadoop/index.html",
    "title": "Hadoop",
    "section": "",
    "text": "Hadoop 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/hadoop/index.html#details",
    "href": "posts/03_archives/stored_categories/hadoop/index.html#details",
    "title": "Hadoop",
    "section": "",
    "text": "Hadoop 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/hadoop/index.html#tasks",
    "href": "posts/03_archives/stored_categories/hadoop/index.html#tasks",
    "title": "Hadoop",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/hadoop/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/hadoop/index.html#참고-자료",
    "title": "Hadoop",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/hadoop/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/hadoop/index.html#related-posts",
    "title": "Hadoop",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Hadoop"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/index.html",
    "href": "posts/03_archives/stored_categories/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "블로그 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/index.html#details",
    "href": "posts/03_archives/stored_categories/blog/index.html#details",
    "title": "Blog",
    "section": "",
    "text": "블로그 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/index.html#tasks",
    "href": "posts/03_archives/stored_categories/blog/index.html#tasks",
    "title": "Blog",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    PARA 구조에 맞게 블로그 구조 변경\n                \n                \n            \n\n            \n            \n                \n                    \n                    게시글에 관련 게시글, 관련 directory 추가\n                \n                별로 마음에 들진 않지만 일단 완성\n            \n\n            \n            \n                \n                    \n                    Hugo 적용\n                \n                \n            \n\n            \n            \n                \n                    \n                    google analytics 적용\n                \n                \n            \n\n            \n            \n                \n                    \n                    about me 페이지 작성\n                \n                \n            \n\n            \n            \n                \n                    \n                    link 미리보기 기능 추가\n                \n                \n            \n\n            \n            \n                \n                    \n                    task 리스트 캘린더 추가\n                \n                \n            \n\n            \n            \n                \n                    \n                    종합 task 캘린더 추가",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/blog/index.html#참고-자료",
    "title": "Blog",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/blog/index.html#related-posts",
    "title": "Blog",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/notes/0.html#overview",
    "href": "posts/03_archives/stored_categories/blog/notes/0.html#overview",
    "title": "PARA Blog 제작",
    "section": "Overview",
    "text": "Overview\n유튜브에서 ‘제2의 두뇌’ 관련 영상을 보고 이 구조로 제 학습 블로그에 적용하면 좋겠다는 생각이 들었습니다. Quarto로 만들어진 제 블로그에 이 구조를 적용하는 것이 생각보다 쉽지 않긴 했지만, 나름 해볼만 했습니다.\n사실 이 글을 작성하는 시점에는 이미 블로그 리뉴얼이 어느 정도 완료된 상태입니다. 코드가 최적화되지 않아 따로 제작 과정을 상세히 공유하지는 않으려 합니다만, 제 GitHub 레포에서 전체 코드를 확인하실 수 있습니다. 이전 블로그는 여기에서 확인할 수 있습니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog",
      "Notes",
      "PARA Blog 제작"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/notes/0.html#이후-목표",
    "href": "posts/03_archives/stored_categories/blog/notes/0.html#이후-목표",
    "title": "PARA Blog 제작",
    "section": "이후 목표",
    "text": "이후 목표\n블로그를 완성하고 보니 Quarto의 필요성에 대해 다시 한번 생각해보게 되었습니다. 데이터 분석을 공부하는 입장에서 Quarto는 분명 대체 불가능한 장점들이 있지만, 웹사이트 구조를 구축하는 데에는 일정 부분 한계가 있어 보입니다.\nDocument를 읽어보던 중 Quarto와 Hugo를 통합하는 방법이 있다는 것을 알게 되었습니다. 이를 통해 Hugo로 블로그의 기본 구조를 만들고, R과 Python 코드 실행 환경으로 Quarto를 활용하는 방안을 고려하고 있습니다.\n이번이 jekyll, framer 블로그에 이어서 세번째로 만드는 블로그입니다. 저는 웹 개발보다는 다른 분야에 집중하고 싶기 때문에, 다음 리뉴얼을 마지막으로 블로그 구조 개선을 마무리해보려 합니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog",
      "Notes",
      "PARA Blog 제작"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/notes/00.html#서론",
    "href": "posts/03_archives/completed_project/toeic_speaking/notes/00.html#서론",
    "title": "토익 스피킹 후기",
    "section": "서론",
    "text": "서론\n\n\n\n결과\n\n\n당연히 IM 등급이 나올 줄 알았는데 의외로 점수가 후하게 나온 것 같다.\n말을 7번 정도 절었고, 다른 사람들이 yes라고 대답할 때, 나는 no라고 대답했고, 마지막 문제는 30초 정도 아무 말도 안해서 점수에 대한 큰 기대는 안하고 있었다.\n이렇게 쉽게 점수가 나오는 줄 알았으면 조금 더 열심히 공부해볼걸 하는 생각이 든다.\n물론 내가 대학원을 가지 않는다면, 적어도 근 시일 내에 스피킹 시험을 다시 보는 일은 없겠지만.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비",
      "Notes",
      "토익 스피킹 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/notes/00.html#공부",
    "href": "posts/03_archives/completed_project/toeic_speaking/notes/00.html#공부",
    "title": "토익 스피킹 후기",
    "section": "공부",
    "text": "공부\n유튜브에 있는 제이크 토익 스피킹 채널의 모의고사 9개 정도 풀어봤다.\n해설은 4편 정도 보다가 거기서 설명하는대로 안 할것 같아서 그냥 내 방식대로 템플릿 만드는데 더 시간을 썼다.\n원래 모의고사 20개 정도는 풀어보려고 했는데, 시험 날짜를 너무 일찍 잡아버려서 그냥 이대로 봐버렸다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비",
      "Notes",
      "토익 스피킹 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/notes/00.html#피드백",
    "href": "posts/03_archives/completed_project/toeic_speaking/notes/00.html#피드백",
    "title": "토익 스피킹 후기",
    "section": "피드백",
    "text": "피드백\n\n\n\n생각보다 친절한 피드백\n\n\n이런 기능까지 제공해줄 줄이야..\nopic 시험은 추가 결제를 해야 피드백을 줬던걸로 기억하는데.. 정성이 기가막히다.\n\n말을 만들어 내야 할 때 또렷하지 못한 발음과 부적절한 억양 또는 강세: 템플릿이 아닌 문장은 말을 뭉뚱그려서 한다는 점을 지적한것 같다. 내가 그랬나?\n문법 오류: 수능 영어 이후로 영어 문법을 공부해 본 적이 없다. 애초에 100% 완벽한 문법을 구사할 수 있으리라곤 생각조차 안했다.\n한정된 어휘: 이 부분은 템플릿 티가 많이 났다는 지적이 아닐까 생각한다.\n발음, 억양 및 강세: 전체적인 톤과 발음은 괜찮다고 평가해주는 것 같다. 진짜로 내 발음이 괜찮다기 보단 그냥 평가를 좀 후하게 해준거 같다.\n\n피드백을 전반적으로 보면 템플릿 티가 날 때 감점이 조금 있는 듯 하다.\n유튜브에 있는 유명한 템플릿에 지나치게 의존하는 것 보다는 본인만의 템플릿을 만든다던가, 아니면 애드리브로 본인만의 문장을 구사한다면 좋은 점수를 받을 수 있지 않을까?",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비",
      "Notes",
      "토익 스피킹 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/notes/00.html#결론",
    "href": "posts/03_archives/completed_project/toeic_speaking/notes/00.html#결론",
    "title": "토익 스피킹 후기",
    "section": "결론",
    "text": "결론\n이번 시험 결과에서 IH가 안나오면 대학원을 바로 준비해야지 생각했는데.. 이것 참 럴수 럴수 이럴수가한 상황이 돼버렸다.\n진로에 대한 고민이 참 많아지는 시기인것 같다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비",
      "Notes",
      "토익 스피킹 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#data-and-information",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#data-and-information",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "Data and information",
    "text": "Data and information\n\nData: raw facts. recorded facts\nInformation: meaningful context\nKnowledge: information + 가치",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#what-is-information-system",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#what-is-information-system",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "What is information system?",
    "text": "What is information system?\n\nSystem: a set of components that interact to achieve some purpose or goal\nInformation System: composed of hardware, software, data, procedures, people",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#system-analysis-and-design",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#system-analysis-and-design",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "System Analysis and Design",
    "text": "System Analysis and Design\n\nSystem analysis and design: process of creating and maintaining information systems\nclassic methodology: SDLC\n\n\nSDLC (System Development Life Cycle)\n\n\n\nSDLC\n\n\n\nSystem definitions: 예산 편상, 위험 분석, …\nRequirements analysis\nComponent design\nImplementation\nSystem maintenance\n\n\ndatabase development process\n\nRequirements analysis\ninput: the project plan\noutput: a set of approved requirements -&gt; data model (ER model로 conceptual design)\nsource: Use cases, Business rules\nComponent Design: Relational Database Design (상세 설계)\nImplementation",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#er-model",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#er-model",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "ER model",
    "text": "ER model\n\nEntities\n\nEntity class\nEntity instance\n\nAttributes: Data type, Properties(default, constraints)\nIdentifiers\n\nunique\nNonunique: identifies a set of instances\n\nRelationships\n\nbinary relationship\n\nMaximum cardinality: 1:1(A has a B), 1:N(A has a set of B), M:N\nMinimum cardinality: 0, 1\n\nternary relationship",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#entit-relationship-diagram",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/07.html#entit-relationship-diagram",
    "title": "Data Modeling and the Entity-Relationship Model",
    "section": "Entit-Relationship Diagram",
    "text": "Entit-Relationship Diagram\n\nEntity classes: rectangle\nRelationships: diamond\nmaximum cardinality: inside the diamond\nminimum cardinality: oval or hash mark next to diamond\nstrong entity: 독자적으로 존재 가능. 강한개체 관계는 점선\nNon-ID-dependent: identifier에 다른 entity의 identifier가 포함되어 있지 않음. 점선으로 표기(non-identifying relationship)\nweak entity: 약, 강 관계는 실선. IS: rounded square, traditional: 2 layer square\nID-dependent: identifier에 다른 entity의 identifier가 포함되어 있음. 실선으로 표기(identifying relationship)\nassociative entity: relationship이 entity로 변환된 것.\nMany-to-many relationship을 2개의 1:N으로 변환\nsuper type, sub type: 상속관계. sub type is a super type\n\nexclusive: Discriminator attribute가 필요함\ninclusive\n\nrecursive relationship\nBusiness rule: build-in constraints, trigger, stored procedure, application code로 구현 가능\ndata model validation: form, report를 이용한 prototyping",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Data Modeling and the Entity-Relationship Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#the-importance-of-dbs-today",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#the-importance-of-dbs-today",
    "title": "An Overview of Database",
    "section": "The Importance of DBs Today",
    "text": "The Importance of DBs Today\n\nDepend upon database: Internet, Web 2.0, IOT",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#why-and-how-databases-are-used",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#why-and-how-databases-are-used",
    "title": "An Overview of Database",
    "section": "Why and How Databases are Used?",
    "text": "Why and How Databases are Used?\n\nThe purpose of a database is to keep track of thing\ndb store information that is more complicated than a simple spread sheet",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#problems-with-lists-spread-sheet",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#problems-with-lists-spread-sheet",
    "title": "An Overview of Database",
    "section": "Problems with Lists (spread sheet)",
    "text": "Problems with Lists (spread sheet)\n\nRedundancy\n\n\n\n\n필요없는 column들이 중복됨\n\n\n\nMultiple Themes\n\n\n그 결과로, list에 나타날 때만 존재하는 informartion이 생김\n\n\nList Modification Issues\n\n\n\n\ndeletion problems, update problems, insertion problems",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#relational-databases",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#relational-databases",
    "title": "An Overview of Database",
    "section": "Relational Databases",
    "text": "Relational Databases\n\nRelationa Model is methodology used as a solution for database design\nA relational database stores information in tables\n\nEach informational topic is stored in its own table\n\nEach theme in the list can be stored in a table\n\nTable = file = relation\ncolumn = fields = attribute\nrow = record = tuple",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#sql-structured-query-language",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#sql-structured-query-language",
    "title": "An Overview of Database",
    "section": "SQL (Structured Query Language)",
    "text": "SQL (Structured Query Language)\n\ninternational standard for creating, processing, querying databases and their tables\ndb applications use SQL to retrieve, format, report, insert, delete, modify data for users\ncan combine table by join operation\n\nSELECT  CUSTOMER.CustomerLastName, \n        CUSTOMER.CustomerFirstName, \n        CUSTOMER.Phone,\n        COURSE.CourseDate, \n        ENROLLMENT.AmountPaid,\n        COURSE.Course, \n        COURSE.Fee\nFROM    CUSTOMER, ENROLLMENT, COURSE\nWHERE   CUSTOMER.CustomerNumber = ENROLLMENT.CustomerNumber -- join condition\n        AND  COURSE.CourseNumber = ENROLLMENT.CourseNumber; -- join condition",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#database-system-dbs",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#database-system-dbs",
    "title": "An Overview of Database",
    "section": "Database System (DBS)",
    "text": "Database System (DBS)\n\n\n\nThe four components of database system\n\n\n\nUser: Employ database application to keep track of things\nUse forms to read, enter, query data\nproduce reports\nDatabase Application: web/mobile database applications, Forms, Reports\nDBMS: used to create, process, administer the database\nDatabase: self-describing collection of related tables\nuser data, metadata, index and other overhead data, application metadata(form, reports) are stored in db\nmetadata = about the structure of the database. &lt;-&gt; user data\n\n\nFunction of DBMS\n\nDB administration\n\nControl concurrency\nProvide security\nPerform backup and recovery\n\n\n\n\nReferential Integrity Constraints",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#personal-vs-enterprise-class-database-systems",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#personal-vs-enterprise-class-database-systems",
    "title": "An Overview of Database",
    "section": "Personal vs Enterprise-class Database Systems",
    "text": "Personal vs Enterprise-class Database Systems\n\nPersonal: Access\nEnterprise-class(Organizational): Microsoft SQL server",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#nosql-databases",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#nosql-databases",
    "title": "An Overview of Database",
    "section": "NoSQL databases",
    "text": "NoSQL databases\n\nNoSQL database = non-relational database",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#cloud-databases",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/01-2.html#cloud-databases",
    "title": "An Overview of Database",
    "section": "Cloud databases",
    "text": "Cloud databases\nMain frame -&gt; Client/server -&gt; Cloud",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "An Overview of Database"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#tier-layers-of-database-system",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#tier-layers-of-database-system",
    "title": "ASP.NET",
    "section": "3-Tier Layers of Database System",
    "text": "3-Tier Layers of Database System\n\npresentation layer: user interface\napplication layer: web server(IIS)\ndata layer: database server",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#api-interface-standards-for-db-access",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#api-interface-standards-for-db-access",
    "title": "ASP.NET",
    "section": "API Interface Standards for DB Access",
    "text": "API Interface Standards for DB Access\nDBMS에 접근하기 위한 표준 API\n\nODBC Open Database Connectivity\nDBMS-independent API\nJDBC: Java Database Connectivity\n\n&lt;a target=\"_blank\"&gt;",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#asp-active-server-pages",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#asp-active-server-pages",
    "title": "ASP.NET",
    "section": "ASP (Active Server Pages)",
    "text": "ASP (Active Server Pages)\nserver side scripting(VBScript) language\nCGI: &lt;% %&gt;는 server에서 실행되는 코드",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#asp-데이터베이스-연동",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#asp-데이터베이스-연동",
    "title": "ASP.NET",
    "section": "ASP 데이터베이스 연동",
    "text": "ASP 데이터베이스 연동\n&lt;%\n  Dim conn, connCmd, rs\n  Set connCmd = \"DSN=dsn_name; Database=dbname; UID=user;PWD=password\"\n  Set conn = Server.CreateObject(\"ADODB.Connection\")\n  Set rs = Server.CreateObject(\"ADODB.Recordset\")\n  conn.Open connCmd\n  rs.Open \"SELECT * FROM table_name\", conn\n%&gt;\n\n&lt;%\n  rs.getRows()\n\n  conn.Execute SQL\n%&gt;",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#오류-메세지-한글-설정",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/13.html#오류-메세지-한글-설정",
    "title": "ASP.NET",
    "section": "오류 메세지 한글 설정",
    "text": "오류 메세지 한글 설정\n&lt;meta charset=\"UTF-8\"&gt;\n&lt;%\n  Session.CodePage = 949\n  Response.CharSet = \"euc-kr\"\n  Response.AddHeader \"Pragma\",\"no-cache\"\n  Response.AddHeader \"cache-control\", \"no-staff\"\n  Response.Expires = -1\n%&gt;\n\nform tag 한글 깨짐 문제\n&lt;%\nSession.CodePage=\"65001\"\nResponse.CharSet=\"UTF-8\"\n%&gt;",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "ASP.NET"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/09.html",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/09.html",
    "title": "Database Design",
    "section": "",
    "text": "MS access is prototyping tool for mock-ups",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/09.html#purpose-of-a-database-design",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/09.html#purpose-of-a-database-design",
    "title": "Database Design",
    "section": "Purpose of a Database Design",
    "text": "Purpose of a Database Design\nset of database specifications that can be implemented as a database in a DBMS\n\nconceptual design: non-DBMS specific\nlogical design: DBMS specific\nphysical design: DBMS specific but not implemented directly by humans",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/09.html#logical-designrelational-design",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/09.html#logical-designrelational-design",
    "title": "Database Design",
    "section": "Logical Design(Relational Design)",
    "text": "Logical Design(Relational Design)\n\nCreate a table(relation) for each entity\n\nspecify primary key\nspecify properties for each column\n\ndata type\nconstraints\ndefault value\nnull status\n\nverify normalization: data structure의 complexity를 증가시킬 수도 있다 → denormalization: 조인 불필요, 조회 시 성능 향상 → datastructure complexity vs modification problems\n\nCreate relationships by placing foreign keys:\n\nStrong entity relationships\nID-dependent / non-ID-dependent weak entity relationships\nSubtypes\nRecursive",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/09.html#representing-relationships",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/09.html#representing-relationships",
    "title": "Database Design",
    "section": "Representing Relationships",
    "text": "Representing Relationships\nid-dependent의 경우 부모의 primary key로 composite key 생성\nMaximum cardinality의 유형에 따라 관계 표현 방법이 달라짐\n\n1:1: foreign key를 어디에 두어도 상관 없음\nCREATE UNIQUE INDEX idx_1_1 ON table(foriegn_key);\n1:N: many(child) 쪽에 foreign key를 두는 것이 일반적\n1 side is called parent, many side is called child\nM:N\nData Modeling에서만 쓰임. database design에서는 intersection table을 사용하여 표현. intersection table은 두 entity의 primary key를 포함하는 composite key를 가짐\n만약 두 primary key 외의 attribute를 가진다면, association entity로 표현\nSupertype / Subtype: Supertype의 primary key를 Subtype의 primary key로 사용\nRecursive Relationship: 방향 이거 다시 보자\nN:M의 경우 virtual table을 생성하여 표현\n\n설문조사는\ndescriptive statistics\n남녀 비율, 경험 비율 등등도 포함되어야 한다.\n가중 평균으로 보여준다\n도서관 예약 시스템\n\n퇴설 처리 미흡\n좌석 이용 정보 파악\n앱 알림\n\n좌석 배치도 감이 안온다. 잔여시간도 안뜬다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Design"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#standard-normal-distribution",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#standard-normal-distribution",
    "title": "연속형 확률분포",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\nμ = 0, σ = 1인 정규분포",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#chi-square-distribution",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#chi-square-distribution",
    "title": "연속형 확률분포",
    "section": "Chi-square Distribution",
    "text": "Chi-square Distribution\nα = ν/2, θ = 2인 감마분포\n자유도 ν에 따라 모양이 변함: 커질수록 정규분포에 가까워짐\n표기: \\(X \\sim χ^2(ν)\\)\n\\(E(x) = ν\\)\n\\(Var(x) = 2ν\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#exponential-distribution",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/5-연속형-확률분포.html#exponential-distribution",
    "title": "연속형 확률분포",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nα = 1, \\(λ = \\frac{1}{\\theta}\\)인 감마분포\nPoisson 분포에서 사건 발생 사이의 시간을 나타낼 수 있음\n표기: \\(X \\sim Exp(λ)\\)\n\\(f(x) = λe^{-λx}, x \\geq 0\\)\n\\(E(x) = \\frac{1}{λ}\\)\n\\(Var(x) = \\frac{1}{λ^2}\\)\n\\(P(X &gt; x) = e^{-λx}\\)\n\\(P(X &gt; x + y | X &gt; x) = P(X &gt; y) = e^{-λy}\\)\n포아송분포에서의 \\(\\frac{1}{λ}\\)와 동일\n비기억 특성을 가짐\n독립적으로 동일한 지수분포의 합은 감마분포 \\(Γ(n, \\frac{1}{\\lambda})\\)를 따름",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "연속형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html",
    "title": "이산형 확률분포",
    "section": "",
    "text": "확률분포 정의 단계",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n각 시행의 결과는 성공(A) 또는 실패(B)\n성공 확률은 p, 실패 확률은 1-p\n각 시행은 서로 독립적 → 모집단의 크기가 충분히 크고, 표본의 크기가 충분히 작다면, 비복원 추출에서도 유효\n∴ S = {A,B}, f(1) = P(X=1) = p, f(0) = P(X=0) = 1-p\n\n\n\n베르누이 시행 예시",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(B(1,p)\\)\n\\(f(x) = p^x(1-p)^{1-x}, x = 0, 1\\)\n\\(E(x) = p\\)\n\\(Var(x) = p(1-p)\\)\n\\(m(t) = 1 - p + pe^t\\)\np = 0.5일 때, 분산은 0.25로 가장 큰 값을 가짐",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-1",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-1",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\nn번의 독립적인 베르누이 시행을 했을 때 성공 횟수 X\n서로 독립인 n개의 베르누이 분포의 합과 같다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-1",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-1",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim B(n,p)\\)\n\\(f(x) = {_n}C_x\\) \\(p^x(1-p)^{n-x}, x = 0, 1, 2, ..., n\\)\n\\(E(x) = np\\)\n\\(Var(x) = np(1-p)\\)\n\\(m(t) = (1-p + pe^t)^n\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-2",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-2",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n성공 확률 p인 베르누이 시행을 반복하여 처음 성공할 때까지의 시행 횟수 X\n지수분포와 유사하다\n기하분포는 비기억 속성을 가진다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-2",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-2",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim G(p)\\)\n\\(f(x) = (1-p)^{x-1}p, x = 1, 2, 3, ...\\)\n\\(E(x) = \\frac{1}{p}\\)\n\\(Var(x) = \\frac{1-p}{p^2}\\)\n\\(m(t) = \\frac{pe^t}{1-qe^t}, (qe^t&lt;1), (q=1-p)\\)\n\\(P(X &gt; x + y | X &gt; x) = P(X &gt; y) = (1-p)^y\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-3",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-3",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n모집단의 크기에 비해 샘플의 크기가 작지 않은 경우, 비 복원 추출시 각각의 선택이 베르누이 시행이라 할 수 없다.\n\\(\\frac{r}{N} = p\\)로 일정할 때, N을 증가시키면, \\(HG(n, N, r)\\)은 \\(B(n, p)\\)로 수렴한다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-3",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-3",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim HG(n, N, r)\\)\n\\(f(x) = \\frac{\\binom{r}{x}\\binom{N-r}{n-x}}{\\binom{N}{n}}, x = 0, 1, 2, ..., n\\)\n\\(E(x) = \\frac{nr}{N}\\)\n\\(Var(x) = \\frac{nr(N-r)(N-n)}{N^2(N-1)}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-4",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#시행-4",
    "title": "이산형 확률분포",
    "section": "시행",
    "text": "시행\n임의의 기간동안 어떤 사건이 간헐적으로 발생할 때, 사건이 발생하는 횟수 X\n임의의 기간을 n 등분하여 각 등분에서 사건이 발생할 확률이 p라고 할 때, 발생횟수 기댓값 λ를 고정시킨 채로 n을 무한히 증가시킴\nn이 매우 크고 p가 매우 작을 때 이항분포를 포아송분포로 근사할 수 있다\n포아송 분포 + 포아송 분포 = 포아송 분포: \\(P(λ) + P(λ) = P(2λ)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-4",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/4-이산형 확률분포.html#분포-4",
    "title": "이산형 확률분포",
    "section": "분포",
    "text": "분포\n표기: \\(X \\sim P(\\lambda)\\)\n\\(f(x) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, x = 0, 1, 2, ...\\)\n\\(E(x) = \\lambda\\)\n\\(Var(x) = \\lambda\\)\n\\(m(t) = e^{\\lambda(e^t-1)}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "이산형 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수",
    "title": "확률변수와 확률분포",
    "section": "확률변수",
    "text": "확률변수\nsample space의 원소를 상호 배반인 event들로 분할하여 실수 값으로 대응시키는 함수\n\n이산확률변수: 확률변수가 취할 수 있는 값이 유한개 또는 무한개이지만 셀 수 있는 경우\n연속확률변수: 확률변수가 취할 수 있는 값이 실수의 구간이고 셀 수 없는 경우\n\n이산 표본공간 -&gt; 이산 확률변수\n연속 표본공간 -&gt; 연속 확률변수\n연속 표본공간 -&gt; 이산 확률변수",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률-분포",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률-분포",
    "title": "확률변수와 확률분포",
    "section": "확률 분포",
    "text": "확률 분포\n\n표본공간 S에 정의된 확률변수 X의 모든 함수값들이 발생할 확률. 모집단의 확률구조를 나타냄\n확률 실험 -&gt; 표본공간 -&gt; 확률변수 -&gt; 확률분포\n\n\n이산확률분포\n확률 질량 함수(pmf): P(X=x) = f(x) =&gt; X가 x일 확률\n- 기하분포: 성공확률 p인 베르누이 시행을 독립적으로 반복했을 때 첫 번째 성공이 나타날 때까지의 시행횟수\n\n\n연속확률분포\n\n확률 밀도 함수(pdf): \\(\\int{f(x)}dx = 1\\)\n\\(\\int_a^b {f(x)}dx = P(a ≤ x ≤ b)\\) =&gt; x가 a와 b사이에 있을 확률\nP(X=x) = 0 (연속형 데이터여서 특정값을 가질 확률은 0)\nf(x) ≠ P(X=x)\nf(x)는 1보다 큰 값을 가질 수 있음\n누적분포함수(cdf): \\(F(x) = P(X ≤ x)\\) =&gt; \\(\\int_{-∞}^x{f(y)}dy\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#결합-확률분포",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#결합-확률분포",
    "title": "확률변수와 확률분포",
    "section": "결합 확률분포",
    "text": "결합 확률분포\n\npmf: \\(P(X=x, Y=y) = f(x, y)\\)\npdf: \\(P(a ≤ X ≤ b, c ≤ Y ≤ d) = \\int_{a}^{b}\\int_{c}^{d}{f(x, y)}dydx\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#주변-확률분포",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#주변-확률분포",
    "title": "확률변수와 확률분포",
    "section": "주변 확률분포",
    "text": "주변 확률분포\n\npmf: \\(f_X(x) = \\sum_y{f(x,y)}\\)\npdf: \\(f_X(x) = \\int_{-∞}^{∞}{f(x, y)}dy\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#조건부-확률분포",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#조건부-확률분포",
    "title": "확률변수와 확률분포",
    "section": "조건부 확률분포",
    "text": "조건부 확률분포\n\n\\(f(x|y)\\) = \\(\\frac{joint}{marginal}\\) = \\(\\frac{f(x, y)}{f_Y(y)}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#독립-확률변수",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#독립-확률변수",
    "title": "확률변수와 확률분포",
    "section": "독립 확률변수",
    "text": "독립 확률변수\n\n모든 \\(x, y\\)에 대해 \\(f(x, y) = f_X(x)f_Y(y)\\)\n\n\n\n\\(f(x, y) = g(x) * h(y)\\)\n\nx, y 의 구간이 서로 간섭받지 않는다.\nX,Y는 독립이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수의-변환",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/2-확률변수와-분포.html#확률변수의-변환",
    "title": "확률변수와 확률분포",
    "section": "확률변수의 변환",
    "text": "확률변수의 변환\n\ncdf를 이용한 변환\ncdf를 미분해서 pdf\n\n\n역함수가 존재할 경우\n\\(g(y) = f(u^{-1}(y)) * |\\frac{du^{-1}}{dy}|\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수와 확률분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#중심-극한-정리",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#중심-극한-정리",
    "title": "중심 극한 정리",
    "section": "중심 극한 정리",
    "text": "중심 극한 정리\n평군이 μ이고, 분산이 \\(σ^2\\)인 모집단으로부터 추출한 확률표본 \\(X_1, X_2, ..., X_n\\)의 표본평균 \\(\\bar{X}\\)의 분포\n\n모집단의 분포와 상관 없이 \\(E(\\bar{X}) = μ\\), \\(Var(\\bar{X}) = \\frac{σ^2}{n}\\)\n정규 모집단일 경우 \\(\\bar{X}\\)가 정규분포를 따름\n정규 모집단이 아닐 경우\n\\(n \\geq 30\\) 이면 중심극한정리에 의해 \\(\\bar{X}\\)는 정규분포에 근사됨. (모집단의 skewed에 따라 더 큰 n이 필요할 수 있음)\n∴ \\(\\bar{X} \\sim N(μ, \\frac{σ^2}{n}), \\frac{\\bar{X} - μ}{σ/\\sqrt{n}} \\sim N(0, 1^2)\\)\n모집단의 분포가 이산, 연속 분포일 때 모두 적용 가능하다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "중심 극한 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#이항분포의-정규근사",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/8-central-limit-theorem.html#이항분포의-정규근사",
    "title": "중심 극한 정리",
    "section": "이항분포의 정규근사",
    "text": "이항분포의 정규근사",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "중심 극한 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#overview",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#overview",
    "title": "Signal Detection Theory",
    "section": "Overview",
    "text": "Overview\n\n인간의 정보 처리 과정 중 perception에 관련된 것\nperception 단계에서 자극 뿐 아니라 노이즈도 같이 들어옴\n여러가지 신호 중 무엇이 중요한지 판단하는 것\nsiganal 탐지 과정을 정량적 모델로 분석하고 성능 평가가 목표\n인공지능 분야에서 중요성이 대두되고 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#example",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#example",
    "title": "Signal Detection Theory",
    "section": "Example",
    "text": "Example\n\nQuality control inspector\n빵이나 과자가 찌그러졌는지 검사, 반도체 품질 검사. 요즘에는 기계가 대부분 담당\nDetection of a flashing warning light (or cctv)\n거수자 탐지\nAirport security guard\nDetecting peculiar patterns in medical imaging (x-ray)\n종양, 암세포 탐지\nMobile phone rings (sound)\nphantoms vibration\nMorning alarm is active or not (visual)\n\n주변의 제품, 서비스 문제 파악, 해결 디자인 제시, 검증",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-theory",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-theory",
    "title": "Signal Detection Theory",
    "section": "Signal Detection Theory",
    "text": "Signal Detection Theory\n\nTrials\n\nSignal case(signal + noise): target이 존재\nNoise case(noise only): target이 없음\n\nResponse\n\nYes\nNo\n\n\n\n\nHit rate: P(Hit) = Number of Hits / Number of Signal Trials\nFalse alarm rate: P(FA) = Number of False Alarms / Number of Noise Trials\nMiss rate: P(Miss) = 1 - P(Hit)\nCorrect rejection rate: P(CR) = 1 - P(FA)\n\n\nWhat does it mean to detect?\n\nsignal is digital (exist / not exist)\nAbsolute threshold is exist\n\n\n\nAssumptions\n\n관찰자가 관찰할 수 있는 signal은 숫자나 변수로 표현할 수 있어야함\nsignal이 random variation이 있다\n피험자가 signal이 있는지 없는지 단순하게 표시할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#distribution-of-signal-and-noise",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#distribution-of-signal-and-noise",
    "title": "Signal Detection Theory",
    "section": "Distribution of signal and noise",
    "text": "Distribution of signal and noise\n\n\nsensitivity index (d')\n\n값이 작으면 분간 힘듦\n값이 크면 분간 쉬움\nsignal의 성격에 따라 결정됨\n\nresponse bias (β)\n\ncriterion에 따라 yes라고 대답하는 비중과 no라고 대답하는 비중\n평가자에 따라 결정됨\n\nd′이 0, β가 50%면 그냥 랜덤으로 대답한 것과 같음\n\n\nd′ 계산\n\nP(M), P(CR) 계산\n표준 정규분포를 그림\nM과 CR의 z값을 찾음\nd′ = (0 - z(M)) + (z(CR) - 0)\n\n\n\nβ 계산\n\nd′과 관계 없이 조절\n\n\\(β = \\frac{P(X/(S+N))}{P(X/N)}\\)\n\\(\\ln β = d′λ_{center}\\)\nβ ~ 1: neutral\n\n\n\n\\(λ_{center}\\) 계산\n\\(λ_{center} = -\\frac{1}{2}(Z(FA)+Z(H))\\)\n\n\\(λ_{center}\\) = 0: ideal observer\n\\(λ_{center}\\) &lt; 0: liberal. yes라고 대답하는 비중이 늘어, hit rate가 높아지지만 false alarm rate도 높아짐\nex) 용의자를 찾는 경찰, 암세포 탐지\n\\(λ_{center}\\) &gt; 0: conservative. no라고 대답하는 비중이 늘어, correct rejection rate가 높아지지만 miss rate도 높아짐\nex) 억울한 죄인을 만들지 않으려는 범원 판결",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#optimal-response-criterion",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#optimal-response-criterion",
    "title": "Signal Detection Theory",
    "section": "Optimal Response Criterion",
    "text": "Optimal Response Criterion\nsignal이 더 많은 환경, noise가 더 많은 환경이 있음. 즉, probability가 다를 수 있음\n또, Effects of payoffs가 있음\n\nsignal이 많은 환경 -&gt; criterion을 낮추는게 좋음. \\(β_{opt} &lt; 1\\)\nnoise가 많은 환경 -&gt; criterion을 높이는게 좋음. \\(β_{opt} &gt; 1\\)\n\\(β_{opt} = \\frac{P(N)}{P(S)} * \\frac{V(CR) + C(FA)}{V(H) + C(M)}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#sluggish-β",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#sluggish-β",
    "title": "Signal Detection Theory",
    "section": "Sluggish β",
    "text": "Sluggish β\n\n\n\nprobability 혹은 payoffs의 변화에 따라 bias가 optimal이랑 다르게 나옴\n\n\n\n\\(β_{opt}\\)가 낮은 경우, ideal보다 덜 conservative함.\n\\(β_{opt}\\)가 높은 경우, ideal보다 덜 risky함.\n확률에 의해 b가 조정될 때 더 많이 발생함.\n\n확률에 대한 계산이 잘못되는 경우\n평가자가 반복되는 반응에 bored해지는 경우",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#roc-curve",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#roc-curve",
    "title": "Signal Detection Theory",
    "section": "ROC Curve",
    "text": "ROC Curve\n\n\n\nd′이 높아질 수록 false alarm 비중이 낮아지고, hit 비중이 높아짐",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-performance",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/5_signal_detction.html#signal-detection-performance",
    "title": "Signal Detection Theory",
    "section": "Signal Detection Performance",
    "text": "Signal Detection Performance\n\nResponse Bias (β)\n\n잘 맞추면 보상을 준다\nfalse signals to raise signal rate\nFalse Alarm에서도 incentive를 준다.\n\n\n\nSensitivity (d′)\n\ngive feedback\nsignal을 조금 더 오래 보여줌\nsignal을 강조\nsignal을 움직이게\n휴식 시간을 충분히 줌\nsignal이 어떠넌지 잘 보여줌\n온갖 감각으로 signal을 보여줌",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Signal Detection Theory"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/8_control.html#basic-control-task-and-device",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/8_control.html#basic-control-task-and-device",
    "title": "Control",
    "section": "Basic Control Task and Device",
    "text": "Basic Control Task and Device\n\n\n\n상태를 체크할 때는 toggle switch, 눌렀다 떼는 건 push button\n레버도 상태를 체크할 때 사용. 그 중 큰 힘이 필요한 경우. continuous setting에서는 slider가 쓰임\nselector switch는 lever랑은 다르게 discrete한 상태가 있음 (선풍기 버튼)\n조이스틱은 보통 가속도(2D, 멀리 밀면 빨리 가는 애)를 제어하거나 속도(1D, 버튼 조이스틱)를 제어하는데 사용. 마우스는 위치를 제어.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Control"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/8_control.html#principles-to-design-of-control-device",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/8_control.html#principles-to-design-of-control-device",
    "title": "Control",
    "section": "15 principles to design of control device",
    "text": "15 principles to design of control device\n\nAttention principles\n\nProximity compatibility\n\n컨트롤 하고자 하는 대상과 컨트롤이 가까워야 한다.\n비상 스위치는 가까이 있어야 한다.\n\n\n\nAvoid resource competition\n\n같은 physical or cognitive resource를 사용하는 control은 피해야한다.\nex) 레버로 속도, 방향 모두 제어\n\n\n\n\nPerceptual principles\n\nMake accessible\n\nphysical accessibility: 손이 닿아야 한다.\ncognitive accessibility: 뭐 하는 control인지 이해하기 쉬워야 한다.\nex) 미는 손잡이는 flat하게 만든다.\n다양한 환경을 고려해야한다. (빛이나 소음이 많은 환경)\n\n\n\nMake discriminable\n\nvisual differentiation: 각각의 컨트롤 장비를 구분할 수 있게\nlogical grouping: 비슷한 기능을 하는 것끼리 묶어놓기\n\n\n\nExploit(활용) redundancy gain\n\n두개의 독립적인 정보를 제공하면 성능이 좋아진다.\n한 가지 정보가 없어도 다른 정보로 대체할 수 있다.\n\n\n\nAvoid absolute judgement limits\n\nworking memory limit(7)를 넘기지 말라.\ncontinuous vs with detents: 연속적인 조절에서 anchor point를 만들어주면 좋다.\n\n\n\n\nMemory Principles\n\nKnowledge in the world\n보편적으로 아는 표현을 사용\n\n\nBe consistent\n\n다른 상황에서도 예상 가능하고 일정한 방법으로 control이 가능해야한다.\n\n\n\nmake discriminable vs be consistent\n\n\n\n\nMental model principles\n\nLocation Compatibility\n\nSpatial Compatibility / physical similarity\n\n\n\nMovement Compatibility\n\n\n\n\nPopulation Stereotypes\n\nrotary controls: 시계방향으로 돌리면 커진다\nUp is on\nIncrease is right, Forward is faster\n\n\n\nResponse selection principles\n\nAvoid accidental activation\n\n사고로 눌리는 것을 방지해야한다.\n\n\n\nHick-Hyman Law\n\\(RT = a + b \\log_2(n+1)\\)\nN is the number of choices\n종류가 많아져도 그냥 몇개만 고민함\n\n\nDecision complexity advantage\n\n일반적으로 복잡한 선택을 적게 하는게 간단한 선택을 여러번 하는것보다 효율적이다\n\n\n\nFitt’s Law\n\n\nIndex of Difficulty: \\(ID = \\log_2(\\frac{2A}{W})\\)\nMovement Time: \\(MT = a + bID\\)\n\navoid accidental vs Fitt’s Law - target width가 구석에 있으면 width가 무한대가 된다.\n\n\nprovide feedback\ntouch screen은 haptic feedback이 없어서 불편함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Control"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#attention의-정의",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#attention의-정의",
    "title": "Attention",
    "section": "Attention의 정의",
    "text": "Attention의 정의\n\nAttention acts as a means of focusing limited mental resources on the information and cognitive processes that are most salient at a given moment\nFocusing most salient at a given moment:\n\n주의는 Search light로 비유됨\n한 영역에 집중하면 다른 부분은 배제.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#attention의-네-가지-주요-측면",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#attention의-네-가지-주요-측면",
    "title": "Attention",
    "section": "Attention의 네 가지 주요 측면",
    "text": "Attention의 네 가지 주요 측면\n\nSelective, Focused, Divided, Sustained Attention은 독립적이지 않으며, 상호작용하여 주의 과정 형성.\n\n\nFocused Attention (집중적 주의)\n\n특정 과업에 집중하고, 외부 방해 요인을 배제하는 능력.\n방해 요소(Distraction)를 최소화하여 현재 작업에 주의 집중.\n예시:\n\n냉장고에서 음식을 꺼내려는 도중 질문을 받으면 집중력이 분산되어 원래 작업을 잊어버릴 수 있음.\n\n\n\n\nDivided Attention (분할 주의)\n\n여러 작업을 동시에 수행하며 주의를 분배.\nSelective Attention과의 차이:\n\nSelective Attention: 특정 자극을 선택적으로 받아들임.\nDivided Attention: 여러 작업 간 우선순위를 메기고 주의 자원 분배.\n\n예시:\n\n운전 중 대화하며 라디오 듣기.\n각 작업에 필요한 시간과 노력을 어떻게 배분할지를 결정.\n\n\n\n\nSustained Attention (지속적 주의)\n\n주의가 높거나 낮거나보다는, 장시간 동안 주의를 유지하는 능력.\n높은 주의 레벨 필요 시:\n\n정보를 놓치는 경우가 발생할 가능성이 높음.\n여러 정보와 자극을 동시에 처리.\n예: 주식 거래에서 여러 종목을 모니터링.\n\n낮은 주의 레벨 시:\n\n자극 부족으로 주의 산만 발생.\n예: CCTV 감시 업무.\n\n\n\n\nSelective Attention (선택적 주의)\n\n여러 감각 자극 중 중요한 정보를 선택적으로 처리.\n시각, 청각, 촉각 등 다양한 감각 경로를 통해 들어오는 자극에서 의미 있는 정보 선별.\n예시:\n\n운전 중:\n\n표지판, 신호등, 앞차의 움직임 → 중요한 정보.\n옆 보행자의 얼굴이나 주변 불필요한 자극 → 중요하지 않은 정보.\n\n\n\n\n\nMental Workload (정신적 작업 부하)\n\n동시에 수행할 수 있는 과업이 몇 개인지 혹은 이 과업이 수행하기에 attention scale을 넘어가는 것인지 분석 용도\n측정 방법:\n\n주관적 설문:\n\n작업자가 느끼는 주관적 부담을 평가.\n\n생체 반응 분석:\n\n심박수, 뇌파 등 생리적 데이터를 활용.\n\n부과 과업(parallel tasking):\n\n추가 과업을 부여하여 작업 부하 평가.\n예시:\n\n운전 중 숫자 거꾸로 세기.\n특정 숫자를 기억하고 응답(예: N-back 테스트).\n\n\n\n워크로드 증가의 결과 특정 작업의 실패 확률 증가한다.\n\n\n\nAttention의 결정 요인\n\n의지\n\n개인의 목표와 필요에 따라 주의 집중.\n예: 차선 변경 시 후방 차량 확인.\n\ncaptured by salience and grouping\n\n공간, 강도, 색상, 크기, 음조 등 외부 요인.\n강렬한 자극이 주의를 끌 가능성 높음.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#selective-attention",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#selective-attention",
    "title": "Attention",
    "section": "Selective Attention",
    "text": "Selective Attention\n\n특정한 자극(예: 시각적 또는 청각적 정보)에 주의를 집중하며, 다른 자극을 배제하는 과정.\n중요도에 따라 특정 정보를 선택적으로 처리하며 불필요한 정보는 억제.\nattention이 sensory memory로 부터 들어온 정보의 filter나 gateway나 bottle neck으로 작용한다고 봄\n\n\n작동 원리\n\nTop-down Processing (Mental model):\n\n개인의 경험과 목표에 기반하여 주의 집중 전략을 개발.\n예: 초보 운전때 앞만 보고 가다가 숙련이 되면 사이드미러 같은 주변도 보게 됨.\n\nBottom-up Processing (자극 기반 처리):\n\n강렬하거나 눈에 띄는 자극에 주의가 끌림.\n예: 갑작스러운 소리나 반짝이는 신호등.\n결정 요인\n\nSalience Source: 자극의 강도(밝기, 소리 크기 등).\n\nInformation Access Trade-offs: 특정 정보를 처리함으로써 얻는 이득.\n\n\n\n\nBottleneck Model\nEarly Selection Theory\n- sensory memory까지는 잘 오지만, Attention filter에 선택이 된게 처리가 되고 나머지는 처리가 안 된다.\n- 감각 단계에서 물리적 특성(의미가 아닌)을 기준으로 정보 필터링.\n- 폐기된 정보는 행동에 미치는 영향 없음.\n- 한계: 칵테일 파티 현상(의미 정보 처리 설명 불가).\n\nLate Selection Theory\n- 모든 정보가 cognition / working memory까지 전달 후 선택.\n- 식별되지 않은 정보는 작업 기억의 제한된 용량으로 인해 빠르게 잊혀짐.\n- 선택되지 않은 정보도 행동에 영향을 미침.\n- 광고 실험 - 인지하지 못하는 정보(빠르게 잊어버려서)에 의해서도 행동의 변화가 있을 것이다.\n\n\nTask\nGeneral orientation and scene scanning\n- 그림을 보거나 웹 브라우징\n감독 제어 (Supervisory Control)\n- 자동화된 시스템에서 이상 징후를 탐지.\n- 주로 AOI(Area of Interests)를 스캐닝 함.\nAOI는 여러개가 있음. 시간, 중요도, 과업의 컨텍스트에 따라서 다르게 설정됨\nspecific task-related information이 있는 물리적 위치\nAOI를 몇개를 만들고, 이들에 대한 시선의 이동을 어떻게 만들것인가가 중요한 issue - 예시:\n자율주행 차량 또는 산업 기계 감독.\n제어 패널에서 비정상적인 지표 확인 (예: 전력 공급 문제, 자원 부족).\n탐지 (Noticing)\n- 예상치 못한 사건이나 환경 변화 감지.\n- 예시:\n- CCTV로 비정상적인 활동 탐지.\n- 주요 시스템 성능의 갑작스러운 변화 인식.\n탐색 (Searching)\n- 방해 요소 속에서 특정 목표를 찾는 활동.\n- 예시:\n- 공항에서 수하물의 X-ray 검색.\n읽기 (Reading)\n- 책이나 디스플레이에서 정보를 읽고 이해.\n- 예시:\n- 계기판의 게이지 읽기.\n확인 (Confirming)\n- 작업이나 과정의 결과를 확인.\n- 예시:\n- 비행기 바퀴가 잘 내려왔는지 확인\n선택적 주의 과업 실패는 중요 정보를 놓치거나 잘못 해석하는 경우 발생할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#seev-model",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#seev-model",
    "title": "Attention",
    "section": "SEEV Model",
    "text": "SEEV Model\nvisual attention에 영향을 주는 요소들을 설명\n\nBottom-up factors\n\nSalience: cue의 특징\nEffort: AOI로 이동하는데 드는 비용\n선형적으로 증가하는건 아니고, 그룹핑 할 수 있음.\n(Within foveal vision) 중심시에서 초점 변화. 멀리있는거에서 가까이 있는거 보는거 &lt; Eye movement &lt; Head movement &lt; Body\n중요한 정보는 cost가 작은 쪽에 배치를 해야함.\n\n\n\nTop-down factors\n\nExpectancy: 일어날 것 같은거에 주의를 더 많이 집중. mental model에 의해 예측 능력이 생길 수 있음.\nValue: 이것에 집중했을 때 얻는 이득, 보지 않았을 때 지불하는 비용\n\n\n\nGuidline\n\n중요한 AOI는 salience가 높아야함\n사용 빈도가 높은 AOI 사이의 거리는 가까워야함\n순차적인 디스플레이도 서로 가깝게 배치해야함.\n\n\n\nChange Blindness\n\n발생 원인\n\n멘탈 워크로드가 높은 경우\n눈에 띄는 변화(Slient change)는 발견하기 쉬움\n중심시에서 멀리 떨어진 곳에서 변화가 발생하면 탐지하기 어렵다.\n시야 밖에서 일어나는 변화는 인지하기 어려움(화면이 깜빡이면서 변하면 animation 효과가 안나타남)\n예상치 못한 변화는 탐지하기 어려움.(top-down processing)\n특정 위치를 응시(fixation)하고 있어도 집중(attention)이 부족하면 변화를 인지하지 못함.\n\n\n\n\nSearch Task의 유형\n\nSerial Search\n\n하나씩 순차적으로 탐색, 탐색 시간이 항목 수에 비례.\n\n예: 긴 텍스트 리스트에서 특정 단어 찾기. 같은 그림 2개 찾기\n\nParallel Search\n\n눈에 띄는 단서(pop-out effect)를 이용해 한 번에 탐색.\n\n5 search items is the same for 50 search items\n\npreattentive process로 유발됨\n\nParallel Search를 유도하는 방법\n\n색상, 크기, 대비(contrast), 회전\n\nmotion\n\nfeature를 adding하는건 찾기 쉬운데 missing하는건 찾기 어려움\n\nO안에서 Q 찾기 vs Q안에서 O 찾기\n\n깜빡이는 곳에서 안깜빡이는거 찾기 vs 안깜빡이는거에서 깜빡이는거 찾기",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#divided-attention",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/6_attention.html#divided-attention",
    "title": "Attention",
    "section": "Divided Attention",
    "text": "Divided Attention\n\n개념\n\n일반적으로 단순 작업보다는 멀티 태스킹이 많이 일어남.\n\n단순 작업: 라면 끓일 때 진짜 라면만 순서대로 끓임. (멀티테스킹 x)\n\n복잡 작업: 라면을 끓이면서 설거지도 하고, 반찬도 만들고, 카톡도 하고, … (멀티테스킹 o)\n\n여러 작업을 동시에 수행하면서 주의를 분배.\nAttention을 한계가 있는 자원으로 바라봄\n\n\n\nResource Model\n\nCentral Resource Theory:\n주의 자원을 단일 통으로 간주. 예: 교차로 진입 시 운전에만 집중, 라디오 듣기같은 다른 작업은 집중을 못함.\nMultiple Resource Theory:\n주의 자원이 감각기관의 특성, 과업의 특성에 따라 별개로 존재\n예: 시각(도로), 청각(라디오) 자원을 분리 사용.\n작업 간 유사성이 높을수록 분배 어려움.\n예: 운전 중 영화 감상(둘 다 시각 자원 사용).\n\n\n\n주의 자원과 훈련의 문제\n주의 자원은 고정인가, 훈련으로 확장 가능한가?\n\n리소스 차이는 명확히 증명되지 않았음.\n전략을 통한 과업 배분 및 우선순위 설정 → 성능 향상.\n반복적 학습과 자동화 → 개별 과업 및 주의 배분이 자연스럽게 효율화.\n\n\n\nAttention as capacity\n\n어떤 정보를 얼마나 주의 깊게 받아들일지 결정\n어떤 정보를 선택할 것인지는 disposition(형태), intentions, arousal, evaluation에 의존\n받아들이는 정보의 특성(visual, auditory), 반응(manual, vocal)에 따라 리소스 풀을 나눌 수 있다.\ntasks interfere to the degree that they tap into the same pool of resources\n\n\n\nUnitary Resource Model (단일 자원 모델)\n\n\n주의(attention)를 제한된 자원으로 봄\n과업 수행에 필요한 자원의 양이 가용 자원을 초과하면 성능 저하\n\n\n\nMultiple Resource Model (다중 자원 모델)\n\n서로 다른 유형의 과업은 다른 자원을 사용\n하지만 한쪽의 workload가 높으면 다른쪽에 영향을 미칠 수 있음.\n비주얼 테스트 두 개를 수행하는 것이 비주얼-청각 테스트보다 더 어려움\n한 객체의 두 가지 특징에 주의를 기울이는 것이 두 객체의 한 가지 특징에 주의를 기울이는 것보다 쉬움\nEx) 특성을 여러 막대로 보여주는것보다 육각형으로 보여주는게 더 보기 쉬움\n\n\n\nPerceptual Modalities\n\nAuditory,Visual, and Tactile Perceptual modalities에 사용하는 resource가 전부 다름\nVisual은 Focal과 Ambient가 서로 다른 자원을 사용함\nCross-modality가 15%정도 더 효과가 있음\ntactile은 auditory랑 비슷함.\n\n\n\ncoding\n\nspatial, verbal\nauditory verbal verbal and visual spatial manual is efficient\nverbal은 단 너무 길면 좋지 않다.\n모든 채널에서 다 쓸 수는 없다. (tactile 같은 경우에는 verbal 코딩이 없음)\n\n\n\nAttentional Allocation during Time-sharing: Skill or Ability?\n\nIf skill:\nAttentional allocation should be trainable\nSkills developed in one task transfer to unrelated tasks.\nIf ability:\nNo evidence supports the existence of a universal “multitasking ability.”\nPeople excel at specific tasks due to familiarity and automation, not inherent multitasking talent.\n\n\nPractical Implications\n\nOperator training:\nTraining must develop automaticity in single-task skills to reduce resource demand\nTraining of attentional allocation and time-sharing will help dual-task performance\nOperator selection:\ntime-sharing ability가 좋은 사람을 선택하는 것보다는 single-task performance가 좋고 자동화가 잘 사람을 선택하는 것이 더 좋음\n\n\n\n\nSystem design이나 multi-task performance를 측정할 때 좋은 것\n\nTask analysis나 multiple resource model를 사용하는 것이 좋다.\nTask의 어떤 면이 효율적으로 time-shared 될 것인가?\nTask의 어떤 면이 interference를 일으킬 것인가?\ninterference를 최소화하기 위해 어떻게 디자인 해야하나?\nex) driving할 때 손과 발을 따로 사용하게 하기\nTime-sharing efficiency, task performance, and mental workload를 고려해야한다\n(멘탈 워크로드 측정은 아직도 쉽지 않다.)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Attention"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/1_reaserch_method.html#reaserch-meathods",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/1_reaserch_method.html#reaserch-meathods",
    "title": "Research Method in Human Factors",
    "section": "reaserch meathods",
    "text": "reaserch meathods\n\ndescriptive Research\n관찰을 통해 데이터 묘사\n\n무엇을 측정할지\n어떻게 숫자로 표현할지\n\n\n대부분 평균, 표준편차를 대푯값으로 사용\n변수들 간의 관계를 파악하기 위해 상관분석, 회귀분석을 사용\n\n\ntypes of descriptive research\n\nobservational research\n\n\n관찰 연구를 계획할 때, 측정할 변수, 각 변수를 기록할 방법, 관찰이 이루어지는 조건, 관찰 기간 등을 식별\n\n\nsurvey research\n\n\n설문조사를 통해 데이터 수집\n\n\nincident and accident analysis\n\n\n사고나 오류를 분석하여 원인을 찾음\n사고나 오류를 줄이기 위한 대책을 마련\n\n\n\n\nexperimanetal Research\n하나 이상의 독립변수에 의도적인 변화를 주고, 그 변화가 하나 이상의 종속변수에 미치는 인과관계를 측정\n이때 다른 변수들은 통제한다\n\n예시\n\n휴대전화를 사용하는 것이 운전에 미치는 영향\n인센티브를 미리 주고 잘못 할 때마다 차감하는 것과, 잘할 때마다 인센티브를 주는 것의 차이\n\n\n\ntype of variables\n\nindependent(predictor, stratification) variable\ndependent(descriptive, criterion) variable\ncontrol variable: 이 값은 고정시키고 실험을 진행한다. 일반화하기 어렵게 한다.\nrandom variable (sigma): 통제할 수 없는 변수. 일반화하기 용이하다.\nconfounding variable: 수식에는 포함되지 않지만 주의해야하는 변수.\n\n\ndecide variables\noperational definition: 변수를 관찰 가능하고 측정 가능한 형태로 정의\n\nindependent variable\n\nRange: realistic / select a range taht will show the effect / pilot experiment\n\ndependent variable\n\nreliability: consistent. solution: increase the number of observations\nvalidity: measure what was intended\n\n\n\n\n\n\nwhy use experimental research?\n\nhumans are variable\n\n\nintra individual variability\ninter individual variability\n\n\nways to handle variability\n\n\nuse statistical techniques\ncontrol variability as much as possible\n\n\n\ntypes of experimental design\n\nsingle variable experiment\n\n\ntwo levels\nmulti levels\n\n\nfactorial design: 두개 이상의 독립변수를 조합하여 실험군을 만든다.\n\n\n변수 간 interaction effect을 확인할 수 있다.\nmore difficult to analyze\n2 x 2, 3 x 3, 2 x 2 x 2 등으로 설계한다.\nbetween-subject, within-subject를 모두 사용하는 mixed designs를 사용할 수 있음.\n\n\nbetween-subjects design: 각각의 실험군에 다른 사람들을 넣는다.\n\n\ngeneralibility 높다, intra person variability를 제거할 수 있다.\n\n\nwithin-subjects design: 같은 사람들을 다른 실험군에 넣는다.\n\n\ncost-effective, less variability, inter person variability를 제거할 수 있다.\n\n\n\n\nevaluation research\n시스템이나 제품이 목적을 충족하는지 평가\n\nusability testing: 사용자가 제품을 실제로 사용하면서 발생하는 문제점 파악\n\n태스크 완료 시간, 오류율, 사용자 만족도 등을 측정\n\ncost-benefit analysis: 제품 또는 시스템 도입의 경제성 평가\n\n직접 비용(하드웨어, 소프트웨어 구입비, training cost 등)\n예상되는 이익(생산성 향상, 오류 감소 등)을 비교 분석",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Research Method in Human Factors"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/1_reaserch_method.html#research-design",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/1_reaserch_method.html#research-design",
    "title": "Research Method in Human Factors",
    "section": "research design",
    "text": "research design\n\nqualitative research\n\n보통 마케팅에서 진행.\n\n\n\nquantitative research\n\nexperiments\ncorrelational observation\nsurveys and questionnaires\n\nsample: 랜덤하게 샘플링하는게 중요\nrating\nbias: 질문의 순서, 질문의 내용, 질문의 방향\n\narchival research\n\n\nfield study\n\nuncontrolled\nresults may be more generalizable to real-world situations\nhigher cost\ndifficult to replicate\ndifficult to control extraneous variables\n\n\n\nlab experiment\n\ncontrolled\nprecise replication\nlower cost\nmore flexibility\nreal-world generalizability may be limited",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Research Method in Human Factors"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/0_intro.html#what-is-human-factors",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/0_intro.html#what-is-human-factors",
    "title": "Introduction to Human Factors",
    "section": "what is human factors",
    "text": "what is human factors\n\nhuman factors = Ergonomics\na human-centered design philosophy &lt;-&gt; technology-centered design",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/0_intro.html#component-of-human-factors",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/0_intro.html#component-of-human-factors",
    "title": "Introduction to Human Factors",
    "section": "component of human factors",
    "text": "component of human factors\n\nhuman: physical, cognitive, group\ntask: physical + cognitive + group\nenvirnment: working environment, systems",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/0_intro.html#goal-of-human-factors",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/0_intro.html#goal-of-human-factors",
    "title": "Introduction to Human Factors",
    "section": "Goal of human factors",
    "text": "Goal of human factors\n\nReduce errors\nIncrease productivity\nEnhance safety\nEnhance comfort",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Introduction to Human Factors"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "every service sends metrics to CloudWatch\nnamespace is a container for metrics\nmetric is a variable to monitor\ndimension is a name/value pair that is attributed to a metric\nup to 30 dimensions per metric\ntimestamp is the time of the data point\nstream data to destination near real-time\n\n\n\n\n\nlog data is stored indefinitely\nlog group is a container for logs\nlog stream is a sequence of log events\nlog event is a record of some activity\nSDK, Elastic Beanstalk, ECS, Lambda, CloudTrail, VPC Flow Logs, Route 53, API Gateway, CloudWatch Unified Agent can send logs to CloudWatch Logs\nlog subscription: send logs to Lambda, Kinesis, ElasticSearch, S3\n\n\n\n\n\ncollect more system-level metrics \n\n\n\n\n\nalarm is a notification that is sent when a metric is in breach of the threshold\nstate: OK, ALARM, INSUFFICIENT_DATA\ntarget: stop, terminate, reboot, recover, start, or snapshot an instance / trigger an Auto Scaling action / send a notification to an SNS topic\nsingle metric alarm, composite alarm, anomaly detection alarm\n\n\n\n\n\ncron jobs\nevent is a change in state\nrule is a description of an event pattern\ntarget is a resource that is invoked when a rule is triggered\nevent bus is a container for events\nevent pattern is a JSON object that describes a set of events to match",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#matrics",
    "href": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#matrics",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "every service sends metrics to CloudWatch\nnamespace is a container for metrics\nmetric is a variable to monitor\ndimension is a name/value pair that is attributed to a metric\nup to 30 dimensions per metric\ntimestamp is the time of the data point\nstream data to destination near real-time",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#logs",
    "href": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#logs",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "log data is stored indefinitely\nlog group is a container for logs\nlog stream is a sequence of log events\nlog event is a record of some activity\nSDK, Elastic Beanstalk, ECS, Lambda, CloudTrail, VPC Flow Logs, Route 53, API Gateway, CloudWatch Unified Agent can send logs to CloudWatch Logs\nlog subscription: send logs to Lambda, Kinesis, ElasticSearch, S3",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#cloudwatch-agent",
    "href": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#cloudwatch-agent",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "collect more system-level metrics",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#cloudwatch-alarms",
    "href": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#cloudwatch-alarms",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "alarm is a notification that is sent when a metric is in breach of the threshold\nstate: OK, ALARM, INSUFFICIENT_DATA\ntarget: stop, terminate, reboot, recover, start, or snapshot an instance / trigger an Auto Scaling action / send a notification to an SNS topic\nsingle metric alarm, composite alarm, anomaly detection alarm",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#cloudwatch-events-eventbridge",
    "href": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#cloudwatch-events-eventbridge",
    "title": "Amazon CloudWatch",
    "section": "",
    "text": "cron jobs\nevent is a change in state\nrule is a description of an event pattern\ntarget is a resource that is invoked when a rule is triggered\nevent bus is a container for events\nevent pattern is a JSON object that describes a set of events to match",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#insights-events",
    "href": "posts/03_archives/completed_project/aws_saa/notes/15_monitoring.html#insights-events",
    "title": "Amazon CloudWatch",
    "section": "Insights Events",
    "text": "Insights Events\n\ninsights events provide insights into the performance and availability of your AWS Account",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon CloudWatch"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/00_region.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/00_region.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "regions:\ncluster of data center\navailability zone (AZ)\n\n\nusually 3, min 3, max 6\none or more discrete data center\nseparate from each others, so that isolated from disasters\n\n\nData center\n\n\nrack\nhost\ninstance\n\n\n\naws edge locations / points of presence\n\n400+ points of presence(400+ edge locations, 10+ regional cathes) in 90+ cities across 40+ contries\n\n\n\n\n\n\n\nglobal\n\n\nIAM\nDNS services\nCDN\nWAF\n\n\nregion\n\n\nec2\nlambda\nrekognition\n\n\n\n\n\ncompliance with data governance and legal requirements\nproximity to customers\navailable services within a region\npricing",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "aws global infrastructure"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/00_region.html#aws-global-infrastructure",
    "href": "posts/03_archives/completed_project/aws_saa/notes/00_region.html#aws-global-infrastructure",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "regions:\ncluster of data center\navailability zone (AZ)\n\n\nusually 3, min 3, max 6\none or more discrete data center\nseparate from each others, so that isolated from disasters\n\n\nData center\n\n\nrack\nhost\ninstance\n\n\n\naws edge locations / points of presence\n\n400+ points of presence(400+ edge locations, 10+ regional cathes) in 90+ cities across 40+ contries\n\n\n\n\n\n\n\nglobal\n\n\nIAM\nDNS services\nCDN\nWAF\n\n\nregion\n\n\nec2\nlambda\nrekognition\n\n\n\n\n\ncompliance with data governance and legal requirements\nproximity to customers\navailable services within a region\npricing",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "aws global infrastructure"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": ": identity access management (Global service)\n\n\n\ngroup by users (not group itself)\n\n\n\nroot account created by default each users can have multi groups",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html#define-iam",
    "href": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html#define-iam",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": ": identity access management (Global service)\n\n\n\ngroup by users (not group itself)\n\n\n\nroot account created by default each users can have multi groups",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html#iampolicies",
    "href": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html#iampolicies",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Policies",
    "text": "IAM:Policies\n\nUsers or Groups can be assigned JSON documents called policies\npolicies define permissions of the users(inline) or groups\nAWS apply the least privilege principle\n\n\nJSON exe\n{\n    {\n        \"Version\": \"2012-10-17\",\n        // optional: \"id\": \"...\",\n        \"Statement\": [\n            {\n                // optional: \"Sid\": \"...\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"s3:ListBucket\",\n                \"Resource\": \"arn:aws:s3:::example-bucket\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"s3:GetObject\",\n                    \"s3:PutObject\"\n                ],\n                \"Resource\": \"arn:aws:s3:::example-bucket/*\"\n            }\n        ]\n    }\n}\n\nEffect: Allow/Deny\nPrinciple: who can perform the action (account, user, role)\nAction: list of actions that are allowed or denied\nResource: list of resources that are allowed or denied\nCondition: when the policy is in effect (optional)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html#iamroles-for-services",
    "href": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html#iamroles-for-services",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Roles for Services",
    "text": "IAM:Roles for Services\n\nRoles are used to delegate permissions to entities that you trust\n\n\ntrusted entities\n\nAWS account\n\nAWS services\n\nEC2, Lambda, CodeBuild, CodePipeline, etc.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html#iamsecurity-tools",
    "href": "posts/03_archives/completed_project/aws_saa/notes/01_IAM.html#iamsecurity-tools",
    "title": "김형훈의 학습 블로그",
    "section": "IAM:Security Tools",
    "text": "IAM:Security Tools\n\nIAM Credentials Report (account level):\nlist of all users and their various credentials\nIAM Access Advisor (user level):\nhow long each service has been active and when it was last used",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Define IAM"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html",
    "title": "Amazon RDS",
    "section": "",
    "text": "Amazon RDS is a managed relational database service that provides a highly available, scalable, and secure database.\nAmazon RDS supports multiple database engines:\n\nAmazon Aurora\nMySQL\nMariaDB\nPostgreSQL\nOracle\nMicrosoft SQL Server\n\nAmazon RDS provides the following features:\n\nAutomated backups\nMulti-AZ deployments\nRead replicas\nMonitoring\nSecurity\nScalability\nHigh availability ### auto scaling\n\nmust set maximum storage threshold\n\n\nfree storage is less then 10% of allocated storage\nlow-storage lasts at least 5 minutes\n6 hours have passed since last modification\n\n\n\n\nup to 15 read replicas\nwithin AZ, cross AZ, cross region (dont pay for data transfer across AZ, not across region)\nread replicas can be promoted to a standalone database\n\n\n\n\nused for disaster recovery(not used for scaling)\nsynchronous replication\nfailover to standby in case of primary failure\nno manual intervention ### from single AZ to multi AZ\nzero downtime\njust modify for the database instance\n\n\n\n\n\n\nMySQL and PostgreSQL compatible\ncloud optimized (5 times faster than MySQL, 3 times faster than PostgreSQL)\nstorage auto scaling\n15 read replicas (cross region)\nfailover instantaneously (less than 30 seconds through master node)\ncost more but effective ### High Availability and Read Scalability\n6 copies of data across 3 AZs (4 copies is needed for write, 3 copies is needed for read)\nself-healing storage\nstorage is striped across 100s of volumes\n\n\n\n\n1 primary region\n5 read-only secondary regions\nreplication lag is less than 1 second\nup to 16 read replicas per secondary region\nfailover to secondary region\ncross-region replication takes less than 1 second\n\n\n\n\n\n\n\n\nAutomated backup (can disable, 5 minutes backup window)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\ns3 restore\n\n\n\n\n\nAutomated backup (cannot disable, point-in-time recovery)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\nPercona XtraBackup, s3 restore\n\n\n\n\n\n\n\n\n\nin-memory caching service\nRedis or Memcached (no high availability and backup)\nheavy application code change",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html#rds-read-replicas-for-read-scalibity",
    "href": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html#rds-read-replicas-for-read-scalibity",
    "title": "Amazon RDS",
    "section": "",
    "text": "up to 15 read replicas\nwithin AZ, cross AZ, cross region (dont pay for data transfer across AZ, not across region)\nread replicas can be promoted to a standalone database\n\n\n\n\nused for disaster recovery(not used for scaling)\nsynchronous replication\nfailover to standby in case of primary failure\nno manual intervention ### from single AZ to multi AZ\nzero downtime\njust modify for the database instance",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html#amazon-aurora",
    "href": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html#amazon-aurora",
    "title": "Amazon RDS",
    "section": "",
    "text": "MySQL and PostgreSQL compatible\ncloud optimized (5 times faster than MySQL, 3 times faster than PostgreSQL)\nstorage auto scaling\n15 read replicas (cross region)\nfailover instantaneously (less than 30 seconds through master node)\ncost more but effective ### High Availability and Read Scalability\n6 copies of data across 3 AZs (4 copies is needed for write, 3 copies is needed for read)\nself-healing storage\nstorage is striped across 100s of volumes\n\n\n\n\n1 primary region\n5 read-only secondary regions\nreplication lag is less than 1 second\nup to 16 read replicas per secondary region\nfailover to secondary region\ncross-region replication takes less than 1 second",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html#backup",
    "href": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html#backup",
    "title": "Amazon RDS",
    "section": "",
    "text": "Automated backup (can disable, 5 minutes backup window)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\ns3 restore\n\n\n\n\n\nAutomated backup (cannot disable, point-in-time recovery)\nManual snapshot (retention as log as you want)\nsnapshot restore =&gt; new database\nPercona XtraBackup, s3 restore",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html#elasticcache",
    "href": "posts/03_archives/completed_project/aws_saa/notes/05_RDS_aurora_elasticCache.html#elasticcache",
    "title": "Amazon RDS",
    "section": "",
    "text": "in-memory caching service\nRedis or Memcached (no high availability and backup)\nheavy application code change",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon RDS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html",
    "title": "ELB",
    "section": "",
    "text": "HTTP, HTTPS, WebSockets\n\nLayer 7\nfixed hostname in every AZ\nclient IP address preservation in the X-Forwarded-For header\ncan use sticky sessions through cookies #### target group\nEC2 instances\nECS tasks\nLambda functions\nIP addresses (private) #### routing routing to diffrent target or same machine different application based on:\nrouting based on URL\nrouting based on hostname\nrouting based on path\nrouting based on query string\nrouting based on HTTP header\nrouting based on port\n\n\n\n\n\nTCP, TLS, UDP\n\nLayer 4\nfixed IP address per AZ and support assigning Elastic IP address\nhigh throughput and low latency #### target group\nEC2 instances\nIP addresses (private)\nLambda functions\nALB\n\n\n\n\n\nip\nLayer 3\nDeploy, scale, and manage third-party virtual appliances\nexample: firewall, intrusion detection and prevention, deep packet inspection, and security analytics\nTransparent Network Gateway: single endpoint for all traffic\nLoad Balancer Gateway: distribute traffic across multiple virtual appliances\nUse GENEVE tunneling protocol on port 6081 #### target group\nEC2 instances\nIP addresses (private)\n\n\n\n\n\n\ndistribute traffic evenly across all registered instances in all enabled AZs\nenabled by default for ALB and no charge for inter AZ data transfer (can be disabled in target group)\ndisabled by default for NLB, GWLB and charge for inter AZ data transfer\n\n\n\n\n\n\n\nServer Name Indication\nALB and NLB and cloudFront support SNI\n\n\n\n\n\n\nALB and NLB support connection draining (deregestration delay)\n\n\n\n\n\n\n\nTarget tracking scaling policy\nSimple / Step scaling policy\nScheduled scaling policy\nPredictive scaling policy",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#types-of-elb",
    "href": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#types-of-elb",
    "title": "ELB",
    "section": "",
    "text": "HTTP, HTTPS, WebSockets\n\nLayer 7\nfixed hostname in every AZ\nclient IP address preservation in the X-Forwarded-For header\ncan use sticky sessions through cookies #### target group\nEC2 instances\nECS tasks\nLambda functions\nIP addresses (private) #### routing routing to diffrent target or same machine different application based on:\nrouting based on URL\nrouting based on hostname\nrouting based on path\nrouting based on query string\nrouting based on HTTP header\nrouting based on port\n\n\n\n\n\nTCP, TLS, UDP\n\nLayer 4\nfixed IP address per AZ and support assigning Elastic IP address\nhigh throughput and low latency #### target group\nEC2 instances\nIP addresses (private)\nLambda functions\nALB\n\n\n\n\n\nip\nLayer 3\nDeploy, scale, and manage third-party virtual appliances\nexample: firewall, intrusion detection and prevention, deep packet inspection, and security analytics\nTransparent Network Gateway: single endpoint for all traffic\nLoad Balancer Gateway: distribute traffic across multiple virtual appliances\nUse GENEVE tunneling protocol on port 6081 #### target group\nEC2 instances\nIP addresses (private)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#cross-zone-load-balancing",
    "href": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#cross-zone-load-balancing",
    "title": "ELB",
    "section": "",
    "text": "distribute traffic evenly across all registered instances in all enabled AZs\nenabled by default for ALB and no charge for inter AZ data transfer (can be disabled in target group)\ndisabled by default for NLB, GWLB and charge for inter AZ data transfer",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#ssltls",
    "href": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#ssltls",
    "title": "ELB",
    "section": "",
    "text": "Server Name Indication\nALB and NLB and cloudFront support SNI",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#connection-draining",
    "href": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#connection-draining",
    "title": "ELB",
    "section": "",
    "text": "ALB and NLB support connection draining (deregestration delay)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#asg",
    "href": "posts/03_archives/completed_project/aws_saa/notes/04_elb_asg.html#asg",
    "title": "ELB",
    "section": "",
    "text": "Target tracking scaling policy\nSimple / Step scaling policy\nScheduled scaling policy\nPredictive scaling policy",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "ELB"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html",
    "title": "VPC",
    "section": "",
    "text": ": AWS managed NAT instance, use specific AZ, elastic IP only for another subnet\n\n\n\n: stateless, allow/deny traffic in/out of subnet default, it allows all traffic \n\n\n\n\n\n\nVPC\n\n\n\n\n\n: private connection between VPC and AWS services - Gateway endpoint: S3, DynamoDB. taget of route table - Interface endpoint: API Gateway, CloudWatch, KMS, SSM, S3, DynamoDB, etc.\n\n\n\n: VPC flow logs, capture information about IP traffic going to and from network interfaces in your VPC\n\n\n\n: connect on-premises network to AWS VPC \n\n\n\n: dedicated network connection between on-premises and AWS\n\n\n\n: IPv6 only, allow outbound traffic to the internet",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#nat-gateway",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#nat-gateway",
    "title": "VPC",
    "section": "",
    "text": ": AWS managed NAT instance, use specific AZ, elastic IP only for another subnet",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#nacls",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#nacls",
    "title": "VPC",
    "section": "",
    "text": ": stateless, allow/deny traffic in/out of subnet default, it allows all traffic",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#vpc-peering",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#vpc-peering",
    "title": "VPC",
    "section": "",
    "text": "VPC",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#vpc-endpoint",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#vpc-endpoint",
    "title": "VPC",
    "section": "",
    "text": ": private connection between VPC and AWS services - Gateway endpoint: S3, DynamoDB. taget of route table - Interface endpoint: API Gateway, CloudWatch, KMS, SSM, S3, DynamoDB, etc.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#vpc-flow-logs",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#vpc-flow-logs",
    "title": "VPC",
    "section": "",
    "text": ": VPC flow logs, capture information about IP traffic going to and from network interfaces in your VPC",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#site-to-site-vpn",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#site-to-site-vpn",
    "title": "VPC",
    "section": "",
    "text": ": connect on-premises network to AWS VPC",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#direct-connectdx",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#direct-connectdx",
    "title": "VPC",
    "section": "",
    "text": ": dedicated network connection between on-premises and AWS",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#egress-only-internet-gateway",
    "href": "posts/03_archives/completed_project/aws_saa/notes/18_VPC.html#egress-only-internet-gateway",
    "title": "VPC",
    "section": "",
    "text": ": IPv6 only, allow outbound traffic to the internet",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "VPC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/16_IAM.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/16_IAM.html",
    "title": "AWS Organization",
    "section": "",
    "text": "global service\ncontrol over multiple AWS accounts\nconsolidated billing\nshared reserved instances and savings plans across accounts ## service control policies (SCPs)\nIAM policy applied to OU or account except management account\n\n\n\n\ncloudwatch agent\n\n\n\n\n\ncloudwatch agent\n\n\n\n\n\nIAM role: cross-account access\nResource-based policy: cross-service access \n\n\n\n\n\nsupported for users and roles(not groups)\nmaximum permissions that an entity can have\nIAM policy + permission boundary = effective permissions \n\n\n\n\n\n\n\n\nAD Connector: on-premises AD, redirect to on-premises AD (proxy)\nSimple AD: standalone AD\nAWS Managed Microsoft AD: managed AD, trust relationship",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/16_IAM.html#iam-role-vs-resource-based-policy",
    "href": "posts/03_archives/completed_project/aws_saa/notes/16_IAM.html#iam-role-vs-resource-based-policy",
    "title": "AWS Organization",
    "section": "",
    "text": "IAM role: cross-account access\nResource-based policy: cross-service access",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/16_IAM.html#iam-permission-boundaries",
    "href": "posts/03_archives/completed_project/aws_saa/notes/16_IAM.html#iam-permission-boundaries",
    "title": "AWS Organization",
    "section": "",
    "text": "supported for users and roles(not groups)\nmaximum permissions that an entity can have\nIAM policy + permission boundary = effective permissions",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/16_IAM.html#aws-directory-service",
    "href": "posts/03_archives/completed_project/aws_saa/notes/16_IAM.html#aws-directory-service",
    "title": "AWS Organization",
    "section": "",
    "text": "AD Connector: on-premises AD, redirect to on-premises AD (proxy)\nSimple AD: standalone AD\nAWS Managed Microsoft AD: managed AD, trust relationship",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Organization"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/08_cloudfront.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/08_cloudfront.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "CDN: Content Delivery Network\nedge location: cache content\nTTL: Time To Live. Cache Invalidation\norigin: source of the file the CDN will distribute\n\nS3 bucket: also used as ingress\nEC2 instance\nELB\nany HTTP server\n\ndistribution: the name given to the CDN which consists of a collection of edge locations  ### price class\nprice class: the number of edge locations used\n\nall: all edge locations\n200: all edge locations except the most expensive\n100: only the least expensive edge locations\n\n\n\n\n\nAWS Global Accelerator: improve the availability and performance of your applications with local or global users\nAnycast IP: route user traffic to the nearest edge location\nstatic IP: anycast IP",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "CloudFront"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/08_cloudfront.html#cloudfront",
    "href": "posts/03_archives/completed_project/aws_saa/notes/08_cloudfront.html#cloudfront",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "CDN: Content Delivery Network\nedge location: cache content\nTTL: Time To Live. Cache Invalidation\norigin: source of the file the CDN will distribute\n\nS3 bucket: also used as ingress\nEC2 instance\nELB\nany HTTP server\n\ndistribution: the name given to the CDN which consists of a collection of edge locations  ### price class\nprice class: the number of edge locations used\n\nall: all edge locations\n200: all edge locations except the most expensive\n100: only the least expensive edge locations\n\n\n\n\n\nAWS Global Accelerator: improve the availability and performance of your applications with local or global users\nAnycast IP: route user traffic to the nearest edge location\nstatic IP: anycast IP",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "CloudFront"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "managed service to create and control encryption keys\nAble to audit key usage with CloudTrail\nattached to region =&gt; can replicate across regions\n\n\n\n\nsymmetric key: same key for encryption and decryption\nasymmetric key: public and private key\n\n\n\n\n\nAWS owned key: managed by AWS\nAWS Managed key: managed by AWS but you have control over the key policy\nCustomer managed key: managed by you, but AWS manages the underlying infrastructure, not free\n\n\n\n\n\nkey policy is attached to the key\nDefault key policy: complete access to the key\ncustom key policy: define who can use the key and roles and who can administer the key",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#kmskey-management-service",
    "href": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#kmskey-management-service",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "managed service to create and control encryption keys\nAble to audit key usage with CloudTrail\nattached to region =&gt; can replicate across regions\n\n\n\n\nsymmetric key: same key for encryption and decryption\nasymmetric key: public and private key\n\n\n\n\n\nAWS owned key: managed by AWS\nAWS Managed key: managed by AWS but you have control over the key policy\nCustomer managed key: managed by you, but AWS manages the underlying infrastructure, not free\n\n\n\n\n\nkey policy is attached to the key\nDefault key policy: complete access to the key\ncustom key policy: define who can use the key and roles and who can administer the key",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#aws-wafweb-application-firewall",
    "href": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#aws-wafweb-application-firewall",
    "title": "김형훈의 학습 블로그",
    "section": "AWS WAF(Web Application Firewall)",
    "text": "AWS WAF(Web Application Firewall)\n\ndeploy on\n\nCloudFront (global)\nApplication Load Balancer (regional)\nAPI Gateway (regional)\nAppSync GraphQL API (regional)\ncognito (regional)\n\n\n\nfeatures\n\nprotect from SQL injection, cross-site scripting, and other web attacks\nIP blacklisting and whitelisting\nfilter HTTP headers / body / URI\nlimit the size of requests\ngeo-blocking\nrate limiting (DDoS protection)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#aws-shield",
    "href": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#aws-shield",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Shield",
    "text": "AWS Shield\n\nDDoS protection service\nStandard and Advanced plan",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#aws-firewall-manager",
    "href": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#aws-firewall-manager",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Firewall Manager",
    "text": "AWS Firewall Manager\n\ncentral management service to configure and manage WAF rules across accounts and applications",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#amazon-guardduty",
    "href": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#amazon-guardduty",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon GuardDuty",
    "text": "Amazon GuardDuty\n\nthreat detection service\ngood for detect crypto currency mining",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#amazon-inspector",
    "href": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#amazon-inspector",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Inspector",
    "text": "Amazon Inspector\n\nsecurity assessment service\ncontinuous assessment of applications for vulnerabilities and deviations from best practices",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#amazon-macie",
    "href": "posts/03_archives/completed_project/aws_saa/notes/17_AWS_secure.html#amazon-macie",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Macie",
    "text": "Amazon Macie\n\ndata security and data privacy service\ndetect and protect sensitive data",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "KMS(Key Management Service)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/12_database.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/12_database.html",
    "title": "database choice in aws",
    "section": "",
    "text": "RDBMS(RDS, aurora): SQL, OLTP\nNoSQL: DynamoDB(JSON), ElasticCache(key / value), Neptune(graphs), DocumentDB(MongoDB), Keyspaces(Cassandra)\nObject storage: S3(for big), Glacier(for backup, archive)\nData warehouse: SQL analytics, Redshift(OLAP), athena, EMR\nSearch: openSearch(JSON)\nGraphs: Amazon Neptune\nLedger: QLDB\nTime series: Timestream\n\n\n\n\nMongoDB compatible\nFully managed\nhighly available with replication across 3 AZs\nAutomatically scales up to 10GB storage, millions of requests per seconds workloads\n\n\n\n\n\nGraph database\nFully managed\nHighl available with replication across 3 AZs, up to 15 read replicas\nSupports up to billions of relations\n\n\n\n\n\nCassandra compatible\nFully managed\nAutomatically scale tables based on traffic\ntables replicated across 3 times across multiple AZs\nondemand, provisioned\n\n\n\n\n\nLedger database\nFully managed\nimmutable, transparent, cryptographically verifiable transaction log\nhigh performance, low latency\nserverless, pay as you go\nno decentralized consensus, no blockchain\n\n\n\n\n\nTime series database\nFully managed\nstore and analyze trillions of events per day",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "database choice in aws"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/12_database.html#database-types",
    "href": "posts/03_archives/completed_project/aws_saa/notes/12_database.html#database-types",
    "title": "database choice in aws",
    "section": "",
    "text": "RDBMS(RDS, aurora): SQL, OLTP\nNoSQL: DynamoDB(JSON), ElasticCache(key / value), Neptune(graphs), DocumentDB(MongoDB), Keyspaces(Cassandra)\nObject storage: S3(for big), Glacier(for backup, archive)\nData warehouse: SQL analytics, Redshift(OLAP), athena, EMR\nSearch: openSearch(JSON)\nGraphs: Amazon Neptune\nLedger: QLDB\nTime series: Timestream\n\n\n\n\nMongoDB compatible\nFully managed\nhighly available with replication across 3 AZs\nAutomatically scales up to 10GB storage, millions of requests per seconds workloads\n\n\n\n\n\nGraph database\nFully managed\nHighl available with replication across 3 AZs, up to 15 read replicas\nSupports up to billions of relations\n\n\n\n\n\nCassandra compatible\nFully managed\nAutomatically scale tables based on traffic\ntables replicated across 3 times across multiple AZs\nondemand, provisioned\n\n\n\n\n\nLedger database\nFully managed\nimmutable, transparent, cryptographically verifiable transaction log\nhigh performance, low latency\nserverless, pay as you go\nno decentralized consensus, no blockchain\n\n\n\n\n\nTime series database\nFully managed\nstore and analyze trillions of events per day",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "database choice in aws"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/index.html",
    "href": "posts/03_archives/completed_project/bs_3_2/index.html",
    "title": "학부 3학년 2학기",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2025-09-01\n        종료일: 2025-12-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/index.html#details",
    "href": "posts/03_archives/completed_project/bs_3_2/index.html#details",
    "title": "학부 3학년 2학기",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 3학년 2학기 개념 정리, 과제, 할 일 등을 총 정리한 노트 모음입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/index.html#tasks",
    "href": "posts/03_archives/completed_project/bs_3_2/index.html#tasks",
    "title": "학부 3학년 2학기",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/bs_3_2/index.html#참고-자료",
    "title": "학부 3학년 2학기",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/index.html#related-posts",
    "href": "posts/03_archives/completed_project/bs_3_2/index.html#related-posts",
    "title": "학부 3학년 2학기",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/06.html#채찍-효과",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/06.html#채찍-효과",
    "title": "채찍 효과",
    "section": "채찍 효과",
    "text": "채찍 효과\n\n완충재고: 불확실성에 대처하기 위한 완충 장치\n\n공급 사실 경계마다 축적되어 있음\n얘 때문에 수요 변동이 증폭됨\n\n재고, 생산, 창고, 운송과 관련된 과도한 비용 발생",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "채찍 효과"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/06.html#원인에-대한-연구",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/06.html#원인에-대한-연구",
    "title": "채찍 효과",
    "section": "원인에 대한 연구",
    "text": "원인에 대한 연구\n\nSterman(1989)\n\n의사결정자의 비합리성 때문에 발생\n교육으로 해결\n\nLee and Billington(1992)\n\n공급사슬 구조적 문제\n공급사슬의 정보처리로 해결",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "채찍 효과"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/06.html#원인",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/06.html#원인",
    "title": "채찍 효과",
    "section": "원인",
    "text": "원인\n\n수요 예측 기법의 특성\n\n수식으로 써보면 공급의 분산이 상위 공급자로 갈 수록 점점 커짐\n\n수식: AR(1) 모형 가정\np window를 사용하는 Order-up-to quantity 가정\n\n\n배급 게임: 공급이 부족할거같으면 과하게 주문해서 증폭\n배치 주문: 한번에 많이 주문. 그걸 여러 소매점에서 또 한 번에 주문.\n가격 변동: 가격 하락을 기대하고 주문을 연기하다가, 하락 시 폭주할 가능성",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "채찍 효과"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/06.html#해결책",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/06.html#해결책",
    "title": "채찍 효과",
    "section": "해결책",
    "text": "해결책\n\nPOS(point of Sale) 정보 공유\nVMI(Vendor Managed Inventory): 공급자가 retailer의 재고를 관리. 수요 정보의 전달 과정을 단축하여 변동성이 줄어든다.\n할당 정책 개선: 공급 부족시 주문량에 비례해서 제품을 할다하는 정책 지양으로 배급 게임 완화\n주문 처리 비용 및 고정 운송 비용 절감(?)\n\nEDI(Electronic Data Interchange)\nMixed truckload를 활용한 LTL(Less than Truck Load) 운송\n3rd Party Logistics 활용\n\n주문 집중시 납기일 분산\n상시 저가 전략",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "채찍 효과"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/02.html#수요관리-프로세스",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/02.html#수요관리-프로세스",
    "title": "수요 관리",
    "section": "수요관리 프로세스",
    "text": "수요관리 프로세스\n\n수요관리\n\n고객 요구사항과 주어진 공급사슬 능력 간의 조화를 이루기 위한 공급사슬관리 프로세스\n예상 가능한 수요에 대해 대체 + 예상치 못한 수요에 대한 반응성 향상\n\n수요예측의 정확도 향상, 변동성 완화, 운영 유연성 향상\n\n판매, 영업, 마케팅 부서, 제품 기획 부서 모든 고려가 필요\nS&OP (Sales and Operations Planning)\n\n수요와 공급의 균형을 맞추기 위한 통합적 계획 수립\n월단위로 책임자들이 만나서 계획 조정 및 수립\n\n\n\n\n\n수요 관리 프로세스(가운데는 신경 x)\n\n\n\n수요 계획\n\n주기적으로 재계획\n\n수요 공유\n\n회사 내 혹은 공급사슬 내 기업 간 공유\n적시성이 중요\n\nInfluencing Demand\n\n수요를 늘리거나 줄이기 위한 활동\nPDCA 사이클로 진행\n\nManaging and Prioritizing Demand\n\n예측 수요보다는 주문된 수요를 주로 관리\n주요 업무: 주문 우선순위 결정, 주문 충족\nplanning time fence 설정: 계획 변경 제한\n\n\n\n\n\nPlanning Time Fence",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "수요 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/02.html#수요예측-기법",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/02.html#수요예측-기법",
    "title": "수요 관리",
    "section": "수요예측 기법",
    "text": "수요예측 기법\n\n\nα -&gt; 1: 최근 자료에 비중을 둠. α -&gt; 0: 기존 예측을 따름",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "수요 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/10.html",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/10.html",
    "title": "블록체인기술의 SCM",
    "section": "",
    "text": "못 만든거 나도 안다. 시간이 너무 없어서 적당히 타협이 필요했다.\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "블록체인기술의 SCM"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/07.html#개요",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/07.html#개요",
    "title": "공급사슬 의사결정의 조정과 계약",
    "section": "개요",
    "text": "개요\n\n공급사슬은 하나의 네트워크. 한 기업의 사건이 거래관계로 연결된 기업들에 연쇄적으로 영향을 미침.\n공급사슬 네트워크의 연관관계를 고려한 운영 최적화 필요.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "공급사슬 의사결정의 조정과 계약"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/07.html#배경",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/07.html#배경",
    "title": "공급사슬 의사결정의 조정과 계약",
    "section": "배경",
    "text": "배경\n\n아웃소싱의 증가: 다양한 기업들이 생산과정에 공동 참여\nODM(Original Design Manufacturer) 제조방식 증가: 생산자와 주문자가 제품의 제조 및 설계부터 협력\n전체 최적화의 조건\n\n공급사슬 전체의 이익이 최적화 되는가?\n최대화된 이익이 분배 가능한가?",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "공급사슬 의사결정의 조정과 계약"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/07.html#공급사슬계약의-종류",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/07.html#공급사슬계약의-종류",
    "title": "공급사슬 의사결정의 조정과 계약",
    "section": "공급사슬계약의 종류",
    "text": "공급사슬계약의 종류\n\n주요 요소\n\n가격\n주문량\n주문 인도기간\n제품 또는 원재료의 품질\n미판매 제품의 처리 (반품, 일부변제 등)\n\n\n\n1. 도매가(wholesale) 계약\n\n단가만 결정\n수량에 관계없이 일정한 단위가에 공급\n미판매나 품절에 의한 손실은 모두 판매자가 부담\n공급가와 판매가의 차액에 의한 판매 이익도 판매자만 가져감\n\n\n\n2. 환매(buyback) 계약\n\n미판매 제품을 공급자가 다시 b의 가격에 재구매(환불)\n판매자가 미판매에 덜 신경 쓰고 더욱 많은 양을 구매하도록 유도\n예) 서점 - 출판사, 의류 소매업 - 본사\n성립 조건: 공급자는 판매자로부터 반품을 받을 수 있는 운송망 필요\n공급자가 판매자의 판매수량을 정확히 알아야 함\n\n\n\n3. 매출공유(revenue-sharing) 계약\n\n판매자는 판매가 이루어질 때마다 매출의 일정 비율을 공급자와 공유\n판매자가 초기에 많은 비용을 지불하지 않고, 매출이 이루어진 다음에 공급자에게 비용을 일부 지불\n이를 통해 판매자의 주문량 증가 유도\nNetfilx, Spotify 등\n환매계약과 마찬가지로 매출정보를 공급자가 정확히 알아야 함\n\n\n\n4. 수량유연(quantity-flexibility) 계약\n\n판매자의 미판매 수량중 일부를 공급가로 환불\n공급자는 환불을 통해 판매자의 미판매 위험을 공유\n\n\n\n5. 수량할인(quantity-discount) 계약\n\n판매자의 주문에 따라 공급가 조정\n공급자는 할인을 통해 판매자와 미판매 위험을 공유\n\n\n\n6. 리베이트(rebate) 계약\n\n공급자가 판매자에게 판매 수량에 따라 노력에 보상\n목표량 달성을 유도하여 판매자의 주문량이 늘 수 있도록 유도",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "공급사슬 의사결정의 조정과 계약"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/07.html#공급사슬계약과-조정",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/07.html#공급사슬계약과-조정",
    "title": "공급사슬 의사결정의 조정과 계약",
    "section": "공급사슬계약과 조정",
    "text": "공급사슬계약과 조정\n\n환매 계약 및 매출공유 계약은 전체최적화 및 유연한 이익분배가 가능한 것으로 알려짐\n도매가와 환매가를 모두 조정하여 전체 최적 달성 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "공급사슬 의사결정의 조정과 계약"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/08.html#공급사슬-네트워크-설계와-관련된-의사결정",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/08.html#공급사슬-네트워크-설계와-관련된-의사결정",
    "title": "공급사슬 네트워크 설계",
    "section": "공급사슬 네트워크 설계와 관련된 의사결정",
    "text": "공급사슬 네트워크 설계와 관련된 의사결정\n\n시설의 역할: 각 시설의 프로세스와 역할\n\n어떤 시설(제조, 저장, 운송 등)을 두고 각 시설이 어떤 프로세스를 수행하는지 결정\n수요 변화에 대한 유연성에 영향을 준다(specialize vs flexible facilities)\n\n시설의 위치\n\n신속성 및 비용에 대해 장기적으로 중요한 영향 (local vs global)\n\n용량 할당\n\n수요에 비해 낮은 용량은 타 시설로부터 조달을 요구하여 추가 운송비용 및 배송시간 증가로 인한 반응성 저하\n과도한 용량은 비용 증가의 원인\n적절한 균형 필요\n1, 2에 비해 나중에 조정 용이\n\n시장 및 공급 할당: 각 시설을 어떤 시장, 공급원에 할당할지 결정\n\n생산, 재고, 운송 비용 등에 영향\n\n\n\nExample - ZARA\n\n전략: Fast Fashion: Low price + Fast Production + Trendy Design\n\n\n\n\n\nVertical Integration\n\n\n\n정보가 downstream에서 upstream으로 흘러감(retailer -&gt; designer -&gt; manufacturer)\n수직적 통합을 통해 자유자재로 소통, 리드 타임 감축\n기존 적대적 관계 -&gt; 공동 파트너 & 자회사 편입",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "공급사슬 네트워크 설계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/08.html#의사결정에-영향을-미치는-요인",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/08.html#의사결정에-영향을-미치는-요인",
    "title": "공급사슬 네트워크 설계",
    "section": "의사결정에 영향을 미치는 요인",
    "text": "의사결정에 영향을 미치는 요인\n\n전략적 요인\n기술적 요인\n\n생산 기술의 특성이 공급망 설계에 영향\n대규모 설비투자가 필요하고 규모의 경제효과를 기대할 수 있는 경우 소수의 높은 생산능력을 지닌 입지 선정(반도체)\n고정비용이 낮은 경우 운송비용을 줄이기 위한 입지 선정\n\n거시 경제적 및 정치적 요인\n기반 시설 요인\n\n노동력, 기반 시설(공항, 항만, …)의 접근성\n\n경쟁 요인",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "공급사슬 네트워크 설계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/08.html#공급사슬-설계-프레이워크",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/08.html#공급사슬-설계-프레이워크",
    "title": "공급사슬 네트워크 설계",
    "section": "공급사슬 설계 프레이워크",
    "text": "공급사슬 설계 프레이워크\n\n\nDefine SC strategy\n\n내부 자원 제약 고려 경쟁 전략 정의\n\nRegional Facility Configuration\n\n수요 예측에 따른 위험 분석 후 러프하게 결정\n네트워크 모형 기법 활용\n\nSelect a set of potential sites\n\n지역별 입지 후보 선정\nInfra 가용성 고려\n\nhard infra: 교통 서비스 등\nsoft infra: 인력 가용성, 지역 사회 수용성 등\n\n\nLocation Choices",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "공급사슬 네트워크 설계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/08.html#물류네트워크-설계",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/08.html#물류네트워크-설계",
    "title": "공급사슬 네트워크 설계",
    "section": "물류네트워크 설계",
    "text": "물류네트워크 설계\n\n물류네트워크: 원료 공급자, 제조업체, 물류센터 뿐만 아니라 원재료, 완제품의 생산에 이르는 물류 흐름까지도 고려\n고객 요구 및 서비스에 효과적으로 대응함과 동시에 비용측면(시설비용, 재고비용, 운송비용 등)에서도 가능한 한 효율적으로 운영될 수 있도록 설계\n물류센터 수가 증가함에 따라서\n\n고객으로의 평균 배송시간 단축 -&gt; 고객 서비스 수준 향상\n각 물류센터가 담당하는 배송지역 감소 -&gt; 외향수송비용 감소\n물류센터로의 운송 시 규모의 경제 효과 감소 -&gt; 내향수송비용 증가\n각 물류센터에서 필요로 하는 안전재고 수준 향상 -&gt; 시설 및 재고비용 증가\n\n\n\n성능 평가 척도\n\n고객 서비스 측면 (매출 측면)\n\n반응시간\n제품의 다양성 및 가용성\n고객 경험\n주문 가시성\n반품처리 등을 포함한 사후관리\n\n비용 측면\n\n시설비용, 재고비용, 수송비용\n\n\n\n\n\n반응 시간\n\n\n\n\n\n비용\n\n\n\n\n\n목표 반응시간에 맞춰서 물류 비용을 계산\n\n\n\n\n유형\n\n재고 보유 주체: 공급사슬의 어느 단계에서 보유한 재고를 최종고객 수요에 대응할 것인가?\n상품 수령 방식: 상품을 고객에 배송할 것인지 고객이 직접 수령할 것인지?\n하나 이상의 유형을 조합하여 사용\n\n\n\n\n생산자 재고보유, 생산자 직송\n\n\n\n수요 정보 통합 및 재고 중앙집중 관리\n\n\n\n\n생산자 재고보유, 생산자 직송(배송병합)\n\n\n\n소매업체에서 취급하는 공급업체가 너무 많으면 곤란\n\n\n\n\n유통(소매)업체 재고보유, 소화물 수송업자 배송\n\n\n\n\n\n유통(소매)업체 재고보유, 유통업체 배송\n\n\n\n\n\n유통(소매)업체 재고보유, 소비자 방문수령\n\n\n\n\n\n유통(소매)업체 재고보유, 소비자 방문구매\n\n\n\n유형별 특징 비교",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "공급사슬 네트워크 설계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#품질과-신뢰성",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#품질과-신뢰성",
    "title": "신뢰도 공학",
    "section": "품질과 신뢰성",
    "text": "품질과 신뢰성\n\n신뢰성: 시간적 측면에서 제품을 오랫동안 고장 없이 사용할 수 있는 것을 의미\n\n관리도와의 차이점: 관리도는 특정 시점에서의 변동을 다룸(정적인 시간)\n정해진 사용 조건 하에서 요구되는 기능을 주어진 기간동안 수행하는 능력\n\n신뢰도: 시스템, 기기, 부품 등이 시간성 안정성을 나타내는 정도 (신뢰성의 정도)\n신뢰성과 품질경영: 과거 품질 관리는 공정중심의 관리와 사후 관리에 치중되었으나, 신뢰도 공학은 사전예방 시스템으로 고장 발생 확률을 미리 예방하기 위해, 사전 시스템 공학적 품질경영을 추구\n\n소비자가 사용하다가 고장이 나는 경우도 고려",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#신뢰도-공학의-특징",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#신뢰도-공학의-특징",
    "title": "신뢰도 공학",
    "section": "신뢰도 공학의 특징",
    "text": "신뢰도 공학의 특징\n\n시스템 전 수명 주기에 걸친 체계적인 신뢰성 활동 요구\n장기간 축적된 다양한 고유 기술들이 연관되어 있다.\n신뢰성 데이터는 매우 고가\n\n10년 수명 보장 제품을 만들려면 원칙적으로 10년 동안 테스트 해야 한다.\n가속수명시험을 통계적으로 분석하여 10년 후를 예측. 하지만 과정이 복잡하고 비용이 크다.\n\n\n\n\n\n품질 경영 vs 신뢰성\n\n\n\n\n\n통계적 관점에서의 차이",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#신뢰성-척도",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#신뢰성-척도",
    "title": "신뢰도 공학",
    "section": "신뢰성 척도",
    "text": "신뢰성 척도\n\nf(t): 특정 시점 t에서 시스템이 고장 날 확률의 밀도. 지수분포, 와이블 분포, 정규분포 등\n신뢰도 함수 R(t): 제품이 t 시점까지 고장이 나지 않을 확률\n\n\\(R(t) = P(T &gt; t) = ∫[∞, t] f(x) dx\\)\n\n불신뢰도 함수 F(t): 제품이 t 시점 안에 고장이 날 확률\n\nF(t) = P(T ≤ t) = 1 - R(t)\nF′(t) = f(t)\n\n고장률 함수 λ(t): 시점 t까지 생존한 제품이 아주 짧은 다음 시간 내에 고장 날 순간적인 비율\n\n\\(λ(t) = \\frac{f(t)}{R(t)} = \\frac{f(t)}{1 - F(t)}\\)\n\\(R(t) = e^{-∫[0, t] λ(x) dx}\\)\n\\(f(t) = λ(t) e^{-∫[0, t] λ(x) dx}\\)\n\n\n\n\n\nf(t)와 λ(t)의 차이점",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#욕조-곡선",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#욕조-곡선",
    "title": "신뢰도 공학",
    "section": "욕조 곡선",
    "text": "욕조 곡선\n\n\n일반적으로 제조품의 고장률은 예방보전을 하지 않으면 욕조곡선을 그린다고 알려짐\n수명 T가 Weibull 분포를 따른다고 가정\n\n\n초기고장기간\n\n사용 개시 후에 비교적 빠른 시간에 설계, 제조상의 결함 혹은 사용환경의 부적합으로 발생하는 고장\n와이블 분포(α &lt; 1)로 모델링\n불량품들이 초기에 빠르게 걸러지면서 고장률이 급격히 감소\n\n\n\n우발고장기간\n\n안전계수(\\(\\frac{파괴강도(최대로 견딜 수 있는 힘)}{허용응력(실제하중)}\\)) 여유있게 설정하면 고장률이 낮고 비교적 일정한 기간으로 유지된다.\n와이블 분포(α = 1)로 모델링\n\n\n\n마모고장기간\n\n피로, 마모, 노화현상 등으로 고장률이 높아지는 기간\n\n파국고장: 갑자기 기능을 잃어버림\n열화고장: 점진적으로 부분적으로 기능을 상실함\n\n적절한 예방보전으로 수명을 연장할 수 있음\n정규분포 혹은 와이블 분포(α &gt; 1)로 모델링",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#경험적-신뢰성-척도",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#경험적-신뢰성-척도",
    "title": "신뢰도 공학",
    "section": "경험적 신뢰성 척도",
    "text": "경험적 신뢰성 척도\n\n실제 분포를 모를 때, 관측치만으로 신뢰성 척도를 추정하는 비모수적 방법\n\n\n\n\n시료가 많을 때\n\n\n\n\n\n시료가 적을 때\n\n\n\n평균 순위법의 분모가 n+1인 이유는 안전장치를 둬서 샘플이 다 고장나도 아직 수명이 다 끝나지 않았다는 것을 고려\n두 방식이 큰 차이는 없다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#지수분포",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#지수분포",
    "title": "신뢰도 공학",
    "section": "지수분포",
    "text": "지수분포\n\n\\(f(t) = λe^(-λt), t ≥ 0\\)\n\\(R(t) = e^(-λt)\\)\n\\(F(t) = 1 - e^(-λt)\\)\n\\(λ(t) = λ\\) (t가 없다: 시간에 관계없이 고장률이 일정)\n평균수명(MTTF): \\(E(t) = 1/λ\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#정규분포",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#정규분포",
    "title": "신뢰도 공학",
    "section": "정규분포",
    "text": "정규분포\n\n기계부품의 수명 분포에 활용\n\\(f(t) = \\frac{1}{σ}Φ(\\frac{t - μ}{σ})\\)\n\\(R(t) = 1 - Φ(\\frac{t - μ}{σ})\\)\n평균수명(MTTF): \\(E(t) ~ μ\\)(0부터 계산하니까 근사값. 하지만 거의 비슷함)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#보전",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#보전",
    "title": "신뢰도 공학",
    "section": "보전",
    "text": "보전\n\n고장나거나 결함이 있는 시스템을 수리를 통해 사용 가능한 상태로 유지하기 위한 조치 및 활동\n사후 보전(BM): 고장날 때 까지 사용하고 고장나면 수리하는 것(고장보전, 대응보전)\n예방 보전(PM): 고장을 예방하기 위해 고장 여부와 관계없이 주기적으로 점검, 수리, 교체하는 것(계획보전, 정기보전)\n예측 보전(PdM): 실시간 데이터와 AI/통계 모델로 고장 징후를 미리 예측하고 고장이 임박했을 때 수리하는 방법",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#보전도",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#보전도",
    "title": "신뢰도 공학",
    "section": "보전도",
    "text": "보전도\n\nM(t): 시스템이 고장났을 때, t 시간안에 수리가 끝날 확률을 의미\nMean Time To Repair (MTTR): 평균 수리 시간\nMTTR이 1/μ일 때, 수리 시간이 지수분포를 따른다면\n\n\\(M(t) = 1 - e^{-μt}\\)\n수리율: μ",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#가용도",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#가용도",
    "title": "신뢰도 공학",
    "section": "가용도",
    "text": "가용도\n\nMTBF(Mean Time Between Failures): 평균 고장 간격 시간(MTTF + MTTR)\n가용성 = MTTF / MTBF\nMTTR을 줄이는게 가용성 향상에 효율적",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#신뢰도-입증-시험",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/05.html#신뢰도-입증-시험",
    "title": "신뢰도 공학",
    "section": "신뢰도 입증 시험",
    "text": "신뢰도 입증 시험\n\n샘플이 목표 신뢰도 함수를 달성하는 지 검정\n전수시험: 모든 대상의 수명을 관찰하는 시험\n중단시험: 모두 수명을 다하기 전에 중단하는 시험\n\n정시중단시험: 정해진 시간까지만 시험하고 중단(Type 1 censoring)\n정수중단시험: 정해진 개수까지 고장나면 중단(Type 2 censoring)\n\n\n\n지수 분포의 신뢰도 함수 추정\n\n\n\n전수 시험, 충분한 n\n\n\n\n모수는 최우추정법으로 추정\n\n\n\n\n중단 시험(선형계수 추정)\n\n\n\n\\(R(t) = e^{-λt} -&gt; \\frac{1}{R(t)} = e^{λt} -&gt; ln(\\frac{1}{R(t)}) = λt\\)\nmedian 순위법으로 R(t) 추정. 최우추정법으로 λ 추정\n\n\n\n최우추정치의 신뢰구간\n\n\n\nr &gt; 0\n\n\n\nr = 고장난 샘플 수\n개수가 fix가 아니여서 정시중단 시험에 1개를 넣넣하게 넣어줌\nr = 0일 때: 정수중단시험은 계산 불가, 정시중단시험의 평균 추정치는 ∞, 신뢰구간은 \\(-\\frac{T}{lnα} ≤ θ ≤ ∞\\)\n\n수명이 지수분포를 따를 때, r은 포아송 분포를 따름\n\n\\(P(r &lt;= c) = ∑[0, c] e^{-m}m^r / r!\\)\n\n평균수명이 θ일 때, 총 시험시간(T) = \\(n t_0\\) 고장수의 평균 m = T / θ\n무고장 확률 \\(α = P(r = 0) = e^{-T/θ}\\) (보통 0.05)로 설정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "신뢰도 공학"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#프로세스-품질관리",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#프로세스-품질관리",
    "title": "샘플링 검사법",
    "section": "프로세스 품질관리",
    "text": "프로세스 품질관리\n\n샘플링 검사법: lot 단위로 제품을 검사하여, 그 결과에 따라 lot 전체를 합격 또는 불합격으로 판정하는 방법\n\n\n규준형 샘플링 검사법: 샘플링 검사 후 규격을 만족하지 못하면 로트 전체 불합격 판정\n선별형 샘플링 검사법: 샘플링 검사 후 규격을 만족하지 못한 제품만 선별하여 양품으로 교체\n\n\n전제조건\n\n로트 단위의 생산\n표본 채취의 무작위성\n명백한 품지기준\n품질 특성값의 분포를 알고 있음\n\n\n\n검사의 위험\n\n\n\n검사의 위험\n\n\n\n일부를 가지고 판별하면 통계적 오류의 가능성이 존재함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "샘플링 검사법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#샘플링의-오차",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#샘플링의-오차",
    "title": "샘플링 검사법",
    "section": "샘플링의 오차",
    "text": "샘플링의 오차\n\n정밀도: 시료를 무한히 많이 측정할 대 측정값이 가지는 산포의 크기가 얼마나 좁은 가(측정 결과가 얼마나 일관성 있게 나오는가)\n\n\\(z_{α/2} * \\frac{σ}{\\sqrt{n}}\\)\n\n신뢰도: 신뢰구간(95% 등)\n목표 신뢰도와 정밀도를 위한 n 계산 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "샘플링 검사법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#샘플링-방법의-종류",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#샘플링-방법의-종류",
    "title": "샘플링 검사법",
    "section": "샘플링 방법의 종류",
    "text": "샘플링 방법의 종류\n\n랜덤 샘플링\n층별 샘플링: stratified sampling\n집락 샘플링: 그룹을 랜덤으로 샘플링 해서 선택된 그룹 내에서 전수조사\n\n시간과 비용을 절약할 수 있지만, 선택된 그룹이 전체를 대표하지 못할 위험 존재\n\n2 단계 샘플링\n\n집락 샘플링 처럼 그룹을 랜덤으로 선택(더 많이 선택)\n선택된 그룹 내에서 전수 조사가 아닌 랜덤 샘플링 수행\n\n\n기존 집락 샘플링에서 전수조사를 함으로써 균질된 진랍 내에서만 대표성을 가지는 문제 해결",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "샘플링 검사법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#샘플링-검사법",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#샘플링-검사법",
    "title": "샘플링 검사법",
    "section": "샘플링 검사법",
    "text": "샘플링 검사법\n\n계수형 샘플링 검사\n\n계수 규준형 샘플링 검사\n계수 조정형 샘플링 검사\n계수값 축차 샘플링 검사\n\n계량형 샘플링 검사",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "샘플링 검사법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#계수-규준형-샘플링-검사",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#계수-규준형-샘플링-검사",
    "title": "샘플링 검사법",
    "section": "계수 규준형 샘플링 검사",
    "text": "계수 규준형 샘플링 검사\n\n크기 N인 로트에서 n개를 샘플링하여 불량이 c개 이하면 로트 전체 합격\n\\(p_0\\): 합격품질수준(AQL)\n\\(p_1\\): 한계품질수준(LQL)\n생산자 위험: 불량률이 AQL 이하인 로트가 불합격할 확률\n소비자 위험: 불량률이 LQL 이상인 로트가 합격할 확률\n\n\n\n\n생산자와 소비자가 계약함\n\n\n\nOC 커브: 로트의 실제 불량률이 p라고 할 때, 로트의 합격 확률을 나타내는 곡선\n샘플에서 발견되는 불량의 개수 X는 초기하분포를 따름. \\(L(p) = P(X ≤ c) = Σ (Np C x) * (N - Np C n - x) / (N C n)\\)\nN이 크면 이항분포로 근사. \\(L(p) = P(X ≤ c) = Σ (nCx) * p^x * (1-p)^(n-x)\\)\nn이 충분히 크면 포아송 분포로 근사. \\(L(p) = P(X ≤ c) = Σ (e^(-np) * (np)^x) / x!\\)\n\n\n\n\nOC 곡선\n\n\n\n\n\nn과 c에 따른 OC곡선 변화\n\n\n\nn이 클수록 예방 비용, 검사 비용 증가, 외부 실패비용 감소\nc가 클수록 외부 실패비용 증가, 폐기 비용 감소\nn의 변화에 따른 기울기 변화가 더 가파르다.\np가 줄어들면 당연히 합격 확률이 증가. 생산자 위험 감소\n\n\n계수 규준형 1회\n\n\\(α, β\\)를 만족할 수 있는 n, c 조합 찾기. 하지만 매우 복잡함\n고로 표를 이용함\n\n\n\n\n계수 규준형 1회 샘플링 검사표\n\n\n\n\n\n보조 표\n\n\n\n\n계수 규준형 다회 샘플링 검사\n\n\n마지막 회차 r은 c + 1\nOC 곡선을 그릴 때는 r도 고려해야 함\n\n\n\n계수 조정형 샘플링 검사\n\n생산자의 품질 수준에 따라 차별적인 샘플링 검사 시행\n검사 절차(AQL 지표형 샘플링 검사표 참고)\n\n검사 로트의 구성 및 크기를 정한다.\nAQL을 설정한다.\n검사 수준을 결정한다.\n검사의 엄격도를 정한다. 일반적으로 보통 검사에서 시작.\n샘플링 형식을 정한다.\n정해진 샘플문자, AQL, 샘플링 형식, 엄격도로부터 샘플링 검사 방식을 정한다.\n검사로트로부터 샘플을 채취, 검사하고 합격 불합격을 판정한다.\n\n\n\n\n\n계수 조정형 샘플링 표. 알파벳이 뒤로 갈수록 빡빡한 검사\n\n\n\n로트의 크기가 커질 수록 검사가 빡빡해짐\n특별 검사: 한번 한번의 샘플링이 어려운 경우. 검사 수준이 일반 검사 수준에 비해 낮다.\n\n\n\n\n엄격도 조정 절차\n\n\n\n전환 스코어 Ac: 2가 기준인건 좀 이상하긴 하지만 어쨌든 Ac가 높다는건 샘플링도 많이 했다는 뜻으로 생각해 가산점을 더 많이 주는 것\n계수 조정형에서는 명시적으로 LQL, β를 고려하지 않음. 검사 엄격도로 β가 간접적으로 조절된다고 봄.\n\n\n\n계수값 축차 샘플링 검사\n\n샘플링 개수를 정하지 않고, 로트에서 1개씩 검사(다회 샘플링과의 차이)하여\n\n누적 부적합 / 불량 개수(D_t)가 합격판정 개수(A_t) 이하면 합격, 불합격 판정개수(R_t) 이상이면 불합격\n최대 샘플링 개수에 도달하면 마지막 회차 합격 판정개수에 따라 합불 판정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "샘플링 검사법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#계량-규준형-샘플링-검사",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/04.html#계량-규준형-샘플링-검사",
    "title": "샘플링 검사법",
    "section": "계량 규준형 샘플링 검사",
    "text": "계량 규준형 샘플링 검사\n\n정보의 양이 많기 때문에 계수형에 비해 샘플링 개수가 적어도 정확한 판정 가능\n\n파괴 검사 등 검사 비용이 높을 경우 유리\n\n하지만 계량형이 덜 직관적이다.(표본에 불량이 없어도 로트 불합격 가능)\n\n\n불량률 보증 (로트가 단품의 묶음인 경우)\n\n\n\\(\\bar{X}_U\\): USL - kσ, 샘플링의 평균의 합불 여부를 USL이 아닌 안전마진이 있는 값과 비교. (샘플에 극단적인 값들이 포함될 수 있기 때문)\nσ는 \\(\\bar{R}/d_2\\)로 추정 혹은 알려져 있다고 가정.\n\n\n\n\nn과 k 도출 식\n\n\n\n\n\nOC 곡선\n\n\n\nσ와 무관하게 m을 바꿔가며 p와 L(p) 조합을 찾을 수 있다.\n\n\n\n평균 보증 (로트가 연속체인 경우)\n\n\nn이 의미하는건 1000ml 단위 1병, 2병, …\n\n\n\n\n계산식\n\n\n\n\n\nG_0는 \\(K_α / \\sqrt{n\\)}\n\n\n\n\n\nOC 곡선",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "샘플링 검사법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/00.html#급수",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/00.html#급수",
    "title": "무한 급수",
    "section": "급수",
    "text": "급수\n\n유한급수: 유한한 항의 합\n무한급수: 무한한 항의 합\n\n부분합: n번째 항까지의 합\n\n\n\n부분합을 구하기 쉬운 급수\n\n기하급수\n\n|r| &lt; 1일때 \\(\\frac{a}{1-r}\\)로 수렴\n\np 급수: \\(\\Sigma \\frac{1}{n^p}\\)\n\np &gt; 1이면 수렴, p &lt;= 1이면 발산",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "무한 급수"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/00.html#급수의-수렴에-관한-정리",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/00.html#급수의-수렴에-관한-정리",
    "title": "무한 급수",
    "section": "급수의 수렴에 관한 정리",
    "text": "급수의 수렴에 관한 정리\n\n급수의 합과 scalar 곱은 급수가 수렴할 때만 정의된다.\n부분합을 구하지 않고 수렴 발산을 판별해보자\n\n\n\\(\\Sigma a_n\\)이 수렴하면 \\(\\limit_{n \\to \\infty} a_n = 0\\)이다.\n\n대우 성립 (발산 판정법)\n역은 성립 x (예: 조화급수)\n수렴하면, 극한은 0이고, \\(|a_n| &lt; 1\\)이라는 사실을 알 수 있다.\n\n양항급수 \\(\\Sigma a_n\\)의 부분합 \\(S_n\\)이 모든 n에 대하여 어떤 상수 k보다 작으면 이 급수는 수렴하며, 그 합은 k보다 크지 않다.\n적분 판정법\n\n\\(f(n) = a_n\\)가 [1, \\(\\infty\\))에서 양수이고 감소하는 함수라고 하자. 그러면 \\(\\Sigma_{n=1}^{\\infty} f(n)\\)가 수렴이면 \\(\\int_{1}^{\\infty} f(x) dx\\)는 수렴, 발산이면 발산한다.\n\n꼭 1부터 시작 안해도 된다.\n\n\n비교 판정법\n\n양항급수 \\(\\Sigma a_n\\), \\(\\Sigma b_n\\)에 대하여 모든 n에 대하여 \\(0 \\leq a_n \\leq b_n\\)이라고 하자.\n꼭 0부터 시작 안해도 된다.\n\n\\(\\Sigma b_n\\)이 수렴하면 \\(\\Sigma a_n\\)도 수렴한다.\n\\(\\Sigma a_n\\)이 발산하면 \\(\\Sigma b_n\\)도 발산한다.\n\n\n비판정법\n\n양항급수 \\(\\Sigma a_n\\)에서 \\(\\limit_{n \\to \\infty} \\frac{a_{n+1}}{a_n} = L\\)이라고 하자.\n\nL &lt; 1이면 급수는 수렴한다.\nL &gt; 1이면 급수는 발산한다.\nL = 1이면 판정 불가\n\n\n근판정법\n\n양항급수 \\(\\Sigma a_n\\)에서 \\(\\limit_{n \\to \\infty} \\sqrt[n]{a_n} = L\\)이라고 하자.\n\nL &lt; 1이면 급수는 수렴한다.\nL &gt; 1이면 급수는 발산한다.\nL = 1이면 판정 불가\n\n\n교대급수 판정법\n\n모든 자연수 n에 대하여 \\(a_{n+1} ≤ a_n\\)\n\\(\\limit_{n \\to \\infty} a_n = 0\\)\n\n\n위 두 조건을 만족하면 급수 \\(\\Sigma (-1)^{n+1} a_n\\)는 수렴한다.\n교대급수의 합 S에 대하여, \\(|S - S_n| ≤ a_{n+1}\\)\n절대수렴하는 급수는 수렴한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "무한 급수"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/00.html#문제-풀이-방법",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/00.html#문제-풀이-방법",
    "title": "무한 급수",
    "section": "문제 풀이 방법",
    "text": "문제 풀이 방법\n\n일반항을 구한다.\n\n기하급수면 기하급수 공식을 쓴다.\np 급수면 p 급수 공식을 쓴다.\n그 외 다음 단계로\n\n일반항의 극한을 구한다.\n\n존재하지 않거나 0이 아니면 발산\n0이면 다음 단계로\n\n양항급수이면\n\n해당 급수보다 큰 수렴하는 급수를 찾아, 2번 정리로 푼다.\n풀 수 없으면 감소하는 영역에서 적분 판정법을 쓴다.\n풀 수 없으면 비교 판정법을 쓴다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "무한 급수"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section",
    "title": "기말",
    "section": "12-5",
    "text": "12-5\n\n매개방정식\n대칭 방정식\n직선과 점 사이의 거리: \\(\\frac{||P x v||}{||v||}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-1",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-1",
    "title": "기말",
    "section": "12-6",
    "text": "12-6\n\n평면 방정식 구하는 법\n\n세 점이 주어졌을 때: 벡터 2개 만들어서 외적\n\n두 평면의 사이각: \\(cosθ = \\frac{|n_1 · n_2|}{||n_1|| ||n_2||}\\)\n두 평면의 교선: z=0으로 놓고 연립방정식 풀어서 교점 구하기, 두 법선벡터의 외적 구해서 기울기 구하기\n평면과 점 사이 거리: \\(\\frac{|ap_1 + bp_2 + cp_3 - d|}{\\sqrt{a^2 + b^2 + c^2}}\\)\n평면과 직선의 사잇각: 법선 벡터와 직선의 사이각",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-2",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-2",
    "title": "기말",
    "section": "13-4",
    "text": "13-4\n\n\\(dz = \\frac{\\partial z}{\\partial x} dx + \\frac{\\partial z}{\\partial y} dy\\)\n\nz의 근사오차.\nz의 상대오차: \\(\\frac{dz}{z}\\)\nz의 백분비오차: \\(\\frac{dz}{z} * 100\\)\n\n실제 증가량: \\(Δz\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-3",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-3",
    "title": "기말",
    "section": "13-5",
    "text": "13-5\n\n\\(\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial z}{\\partial y} \\frac{dy}{dt}\\)\n음함수의 편도함수(x + y = 0 꼴): \\(\\frac{dy}{dx} = - \\frac{f_x}{f_y}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-4",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-4",
    "title": "기말",
    "section": "13-6",
    "text": "13-6\n\nf(x, y)의 점 \\((x_0, y_0)\\)에서 단위벡터 \\(u=&lt;a, b&gt;\\) 바향으로의 방향미분계수: \\(D_uf(x_0, y_0) = f_x(x_0, y_0)a + f_y(x_0, y_0)b\\)\n\n\\(= ∇F(x_0, y_0) · u\\)\n\n단위 벡터라는 점에 주의\n\\(∇F(x_0, y_0, z_0) = &lt;x_0, y_0, z_0&gt;\\) 위치에서의 접평면의 기울기",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-5",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-5",
    "title": "기말",
    "section": "13-7",
    "text": "13-7\n\n최대 최소 판별:\n\n후보 선정\n\n임계값: \\(f_x(a, b) = 0, f_y(a, b) = 0\\) 인 (a, b)\n\n\\(f_{xx}(a, b)f_{yy}(a, b) - (f_{xy}(a, b))^2 &gt; 0\\), 0보다 작으면 안장점\n\\(f_{xx}(a, b) &gt; 0\\) 이면 극소, \\(f_{xx}(a, b) &lt; 0\\) 이면 극대\n\n경계값\n임계값, 경계값의 함수값 비교",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-6",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-6",
    "title": "기말",
    "section": "14-1",
    "text": "14-1\n\nR = [0, 2] x [0, 3] 이면 \\(\\int_{0}^{2} \\int_{0}^{3} f(x,y) dy dx\\)\n순서 상관 없음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-7",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-7",
    "title": "기말",
    "section": "14-2",
    "text": "14-2\n\n\\(D = {(x, y) | a ≤ x ≤ b, g_1(x) ≤ y ≤ g_2(x)}\\) 이면 \\(\\int_{a}^{b} \\int_{g_1(x)}^{g_2(x)} f(x,y) dy dx\\)\n\ny = g(x)\n\n\\(D = {(x, y) | c ≤ y ≤ d, h_1(y) ≤ x ≤ h_2(y)}\\) 이면 \\(\\int_{c}^{d} \\int_{h_1(y)}^{h_2(y)} f(x,y) dx dy\\)\n\nx = h(y)\n\n순서 유의\n그냥 직사각형의 넓이는 f(x, y) = 1로 두고 계산",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-8",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-8",
    "title": "기말",
    "section": "14-3",
    "text": "14-3\n\nr 추가해서 적분하면 됨",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-9",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-9",
    "title": "기말",
    "section": "14-4",
    "text": "14-4\n\nA(S) = \\(\\iint_{R} \\sqrt{1 + (f_x(x,y))^2 + (f_y(x,y))^2} dA\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-10",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-10",
    "title": "기말",
    "section": "14-5",
    "text": "14-5\n\n적분 순서: 미지수가 많은거 먼저\nz, y, x 순으로 범위 계산",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-11",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/02.html#section-11",
    "title": "기말",
    "section": "14-6",
    "text": "14-6\n\n원주 좌표계: (r, θ, z)\n구면 좌표계: (ρ, θ, φ)\n\nx = ρ sinφ cosθ, y = ρ sinφ sinθ, z = ρ cosφ\nϕ의 전체 범위는 0부터 π까지\n\n넓이: \\(ρ^2 sin ϕ\\) 추가\nθ 범위 구할 때 x, y 범위 보기",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "기말"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/06.html#모델링-유형",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/06.html#모델링-유형",
    "title": "BPMN",
    "section": "모델링 유형",
    "text": "모델링 유형\n\nProcess\n\nPrivate(internal) Business Process\n\n조직 내부에서 수행되는 비즈니스 프로세스를 하나의 pool에 작성\n\nPublic Process / Abstract Process\n\n다른 프로세스와 의사소통하는 액티비티만 표현. 양자간 메세지 교환으로 표현됨\n\n\nCollaboration: 2개 이상의 Public Process가 상호작용하는 것\nChoreography: pool 사이의 메시지 교환에 초점",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPMN"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/06.html#bpmn의-상충되는-목표",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/06.html#bpmn의-상충되는-목표",
    "title": "BPMN",
    "section": "BPMN의 상충되는 목표",
    "text": "BPMN의 상충되는 목표\n\n비즈니스 분석가가 쉽게 사용 가능해야 한다\n\nBasic BPMN Modeling Elements\n\n복잡한 비즈니스 프로세스를 표현할 수 있어야 한다.\n\nExtended BPMN Modeling Elements",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPMN"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/06.html#basic",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/06.html#basic",
    "title": "BPMN",
    "section": "Basic",
    "text": "Basic\n\nFlow Objects\n\n\n\n\nEvents\n\n\n\nStart Event: 프로세스의 시작\nIntermediate Event: 프로세스 도중 발생하는 이벤트\nEnd Event: 프로세스의 종료\n\n - Task: 단일 단위 작업 - Sub-Process: 여러 작업을 포함하는 복합 작업. + 아이콘 - Collapsed: 내부 상세 내용 숨김 - Expanded: + 누르면 나오는 거 - Collapsed Sub-Choreography - Expanded Sub-Choreography\n - And, Or, Xor - Event-based: event가 연결된 여러 경로 가운데 하나만 배타적으로 활성화 - branching, forking, merging, joining\n\n\n\nGateway Type\n\n\n\nData\n\nData Object: 프로세스에서 사용되는 데이터\nMessage: 두 참여자가 의사소통 시 교환하는 내용을 표현\n\nConnecting Objects\n\n - 활동 간의 순서\n\n\n - 다른 pool 간의 메시지 교환\n - Artifacts와 Flow Objects 간의 연결 - 방향이 있는 경우 화살표 사용\n\nSwimlanes\n\nPool: 조직 또는 참가자 그룹\nLane: Pool 내의 역할 또는 부서\n\nArtifacts\n\nGroup: 관련 활동 그룹화\nAnnotation: 추가 정보 제공\n\n\n\nTask\n\nService task: 자동화된 서비스 호출\nUser task: 사람이 시스템으로 수행하는 작업\nManual task: 자동화되지 않은 수동 작업\nScript task: BPMN에서 스크립트로 정의된 작업",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPMN"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/06.html#extended",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/06.html#extended",
    "title": "BPMN",
    "section": "Extended",
    "text": "Extended\n\n\n\ntype demension\n\n\n\nNon-Interrupting: 서브 프로세스에서 계속 진행되는 것.\nMessage, Timer, Error, Escalation, Compensation, Termination, Cancel\n\n\n\n이 이상은 그냥 pdf 보고 외우셈",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPMN"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpm이란",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpm이란",
    "title": "BPM 개요",
    "section": "BPM이란?",
    "text": "BPM이란?\n\n프로세스 관점에서 기업을 경영하는 것\n소프트웨어 도구를 활용하여 조직의 업무와 프로세스를 끊임없이 최적화하는 구조적인 접근방법 및 관리역량이자 도구의 집합\n즉, 프로세스 경영 + IT 도구의 접목\n경영전략과 IT 기술의 alignment를 위한 방법론\n\n\n\n\nIT의 영향\n\n\n\nBPM의 영역 및 정의\n\n\nBPM의 영역\n\npeople: 공급자, 임직원, 고객\nprocess: PDCA\ntechnology: workflow, integration solution, UI solution",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpm의-특징",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpm의-특징",
    "title": "BPM 개요",
    "section": "BPM의 특징",
    "text": "BPM의 특징\n\n어플리케이션 로직에서 프로세스 로직을 분리하여 명시적으로 관리한다.\n\nagility와 visibility 제공",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpms-bpm-system의-개념",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpms-bpm-system의-개념",
    "title": "BPM 개요",
    "section": "BPMS (BPM System)의 개념",
    "text": "BPMS (BPM System)의 개념\n\n프로세스를 지속적으로 혁신시키고 실행시키는 것을 가능하게 하는 범용 소프트웨어 시스템\n\nBPM suite로 발전: BPM을 위한 모든 솔루션을 통합 제공\n\n\n\n\n\nBPMS Life Cycle\n\n\n\n정보시스템 구축 관점:\n\n진단 / 요구사항 분석\n설계\n구성/구현\n구동/모니터링\n조정 또는 진단/요구사항 재분석",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpm의-도입효과",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpm의-도입효과",
    "title": "BPM 개요",
    "section": "BPM의 도입효과",
    "text": "BPM의 도입효과\n\n프로세스 가시화를 통한 실시간 관리 및 모니터링 능력의 향상\n자동화를 통한 프로세스 운영 효율성 및 생산성 향상\n프로세스의 병렬처리 가능\naccountability 향상\n\n프로세스의 각 단계에서 실행 결과와 진행상태를 모두 기록 -&gt; 감사와 통제에 활용\n\n프로세스 모니터링 및 데이터 분석을 통한 최적화\n고객과 파트너와의 통합\n\n공동 목표를 대상으로 가상기업(VE)화 하여 생산성을 높임\n\n조직의 agility 제고\n\nRTE(Real Time Enterprise) 구현",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpr",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/02.html#bpr",
    "title": "BPM 개요",
    "section": "BPR",
    "text": "BPR\n\nFundamental radical dramatic improvements process redesign\n\n\n추진방향\n\nreduction input\ninnovation process\nvalue up output\n\n\n\n추진 원칙\n\n정보는 최초 발생지에서 1회 입력\n병렬 처리\n정보 생성 부서에서 바로 처리\n업무 단위가 아닌 결과 중심 설계\n\n\n\n방법론\n\n프로세스 선정\n\n대표적 증상 (data redundancy, bottleneck)을 제시하여 개선 대상 process 발굴\n\nAs-Is 이해\n\n현재 프로세스의 목적과 흐름을 고객 관점에서 분석\nwhat to do 정의\n\nTo-Be 설계\n\n프로세스 재설계 원칙을 적용\n새로운 프로세스 모델 설계\n\n변화 관리\n\n현 위치 파악 및 위기 의식 공유\nvision 제시, 구성원 참여 유도\n\n\n\n\nBPR 성공을 위한 요인\n\n경영진의 지원과 참여\n비전 및 명확한 목표 설정\n정보기술의 활용\n변화 관리\n\n\n\n\nBPR vs BPM",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/04.html#표준의-개념-및-필요성",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/04.html#표준의-개념-및-필요성",
    "title": "BPM 표준",
    "section": "표준의 개념 및 필요성",
    "text": "표준의 개념 및 필요성\n\n분류\n\n적용 범위:\n\n국제표준\n지역표준\n국가표준\n\n제정 기구:\n\nDe Jure 표준: 공공기관에 의해 승인된 강제력 있는 표준\nDe Facto 표준: 시장에서 자연스럽게 채택된 표준",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 표준"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/04.html#정보통신-표준",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/04.html#정보통신-표준",
    "title": "BPM 표준",
    "section": "정보통신 표준",
    "text": "정보통신 표준\n\nIT 시스템의 상호 연동에 필요한 합의된 규약(Protocol)의 집합\n시스템 간 상호운용성(interoperability)과 사용의 편리성을 위한 것\n\n\n정보통신 산업의 특징\n\n네트워크적 특징: 유관 분야가 밀접하게 서로 경제적 영향을 주면서 결합되어 있음\n경제학적 특성\n\nNetwork Externality: 소비자군의 규모에 따라 가치가 달라짐\n\nDirect Network Effect\nCross Network Effect\nMetcalfe의 법칙: 지수함수적 가치 증가\n\nLock-in effect: 한번 표준을 채택하면 전환하기 어려움\nTipping effect: 비슷한 시장 점유율을 보이다가 어느 순간 급격히 한쪽으로 쏠림",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 표준"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/04.html#bpm-표준의-필요성-및-유형",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/04.html#bpm-표준의-필요성-및-유형",
    "title": "BPM 표준",
    "section": "BPM 표준의 필요성 및 유형",
    "text": "BPM 표준의 필요성 및 유형\n\n필요성\n\n상이한 BPM 시스템 간 상호작용(interoperability) 촉진\nBPM을 더 잘 이해하고 보급을 촉진할 수 있음\n\n\n\n유형\n\n용어, 모델링 표기법에 대한 표준(BPMN(Business Process Model and Notation))\n프로세스 실행 및 BPM System Interations 관련 표준\n\n자동화 실행 측면의 표준이 필요\nSOA(Service-Oriented Architecture), WS-BPEL(Web Services Business Process Execution Language) 등이 있음\n\nProcess Metrics 관련 표준\n\n효과성 지표: CSF(Critical Success Factor)\n효율성 지표",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM 표준"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#경영환경-변화와-무한-경쟁-속-프로세스-경영",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#경영환경-변화와-무한-경쟁-속-프로세스-경영",
    "title": "프로세스 경영 개요",
    "section": "경영환경 변화와 무한 경쟁 속 프로세스 경영",
    "text": "경영환경 변화와 무한 경쟁 속 프로세스 경영\n\n기술 발전과 경쟁 격화로 기업의 평균 수명은 지속적으로 단축되고 있음\n아래의 주요 변화 요소들에 적응하고 뛰어난 성과를 창출하기 위해 프로세스 경영이 필요\n\n\n주요 요소\n\nGlobalization\n제품 수명 주기 단축 및 비용 압력\n제품과 서비스의 개인화 및 맞춤화\n기업의 인수합병\n\n기존의 업무 프로세스가 많이 바뀌어야 함\n변화에 대한 민첩한 적응력(agility) 요구\n\n가치사슬의 확대\n\n협업의 범위 및 레벨 확장/심화\n\nAI & 자동화 확산\nDX\n사이버 보안 및 개인정보 보호\nESG\nCX: 프로세스 설계의 기준이 고객 경험이 됨",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#프로세스-경영이란",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#프로세스-경영이란",
    "title": "프로세스 경영 개요",
    "section": "프로세스 경영이란?",
    "text": "프로세스 경영이란?\n\n고객가치 창출을 위해 내외부의 업무 프로세스를 최적화하는 경영체계\n내부의 고객과 외부의 고객에게 가치를 전달하는 시작점과 종료점까지의 업무 처리 전 과정을 통합적으로 관리함으로써 기업의 성공을 도모\n기업의 모든 활동을 프로세스를 중심으로 가시화함으로써 경영환경 변화에 민첩하게 대응할 수 있도록 하고, 이를 통해 가치사슬 전반에 걸쳐 비효율을 개선\nIT의 전략적 활용",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#프로세스-중심적-사고방식",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#프로세스-중심적-사고방식",
    "title": "프로세스 경영 개요",
    "section": "프로세스 중심적 사고방식",
    "text": "프로세스 중심적 사고방식\n\n사물을 구조보다는 프로세스로 인식하는 사고 방식\n기업 역시 기업 외부와의 상호작용, 내부의 업무 흐름을 비즈니스 프로세스의 집합으로 인식\n\n\n\n\nMichael Porter’s Value Chain",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#business-process-및-workflow",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#business-process-및-workflow",
    "title": "프로세스 경영 개요",
    "section": "Business process 및 workflow",
    "text": "Business process 및 workflow\n\nBusiness process: 고객 중심\n\n특정 고객또는 시장을 대상으로 필요한 산출물(output)을 생서하기 위해 정의된 구조화되고 측정가능한 활동들의 집합\n고객이나 기업에게 가치를 주는 결과(outcome)으로 이끄는 activities, events, decisions의 모임\nend-to-end 고객 여정\n\n\n\n\n\nBPMN에 따라 작성된 business process\n\n\n\nWorkflow: 작업 중심\n\nbusiness process를 자동화한 것\n\n\n\nBusiness process 특징\n\n대상 고객이 있다.\ncross organizational boundaries이다.\n\n조직 경계와 무관하게 업무가 진행됨",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#프로세스-경영-구축-프레임워크",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#프로세스-경영-구축-프레임워크",
    "title": "프로세스 경영 개요",
    "section": "프로세스 경영 구축 프레임워크",
    "text": "프로세스 경영 구축 프레임워크\n\n\n\n프로세스 경영 구축 방법론 프레임워크\n\n\n\n비전수립\n프로세스 아키텍처\n프로세스 모델링 / 분석\n프로세스 자동화\n프로세스 성과 관리",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#프로세스-경영의-역사",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/01.html#프로세스-경영의-역사",
    "title": "프로세스 경영 개요",
    "section": "프로세스 경영의 역사",
    "text": "프로세스 경영의 역사\n\n공정 기술\n\n1910년 초반 테일러의 과학적 관리\n\n공정을 분석하여 분업화, 전문화 하여 생산성 향상\n\n테일러 이론에 의한 공정기술의 발전\n기존 제조/생산 분야의 공정 기술이 구매, 회계, 배송, 고객 서비스 등 조직 전체 업무 영역으로 확대\n제조업을 넘어 여러 서비스 산업 전반으로 확산\n\n\n\n품질 경영\n\nSQM\nTQM\n6 시그마\n품질 경영: 오늘날의 프로세스 경영\n\n품질 뿐만 아니라 플세스를 활용하는 조직 전반의 포괄적 경영 패러다임으로 확장한 것",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 개요"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#bpm-suite란",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#bpm-suite란",
    "title": "BPM Suite",
    "section": "BPM Suite란",
    "text": "BPM Suite란\n\nBPM Lifecycle 전체 단계를 지원하는 IT 도구 집합\n기존에는 프로세스 경영을 지원하는 IT 도구에서 비저닝, 프로세스 아키텍처, 모델링/분석, 자동화, 성과관리 등을 포괄하는 개념으로 확장됨\n\n\n\n\nBPM 용어 확장 단계",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM Suite"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#bpm-suite-구성-요소",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#bpm-suite-구성-요소",
    "title": "BPM Suite",
    "section": "BPM Suite 구성 요소",
    "text": "BPM Suite 구성 요소\n\n\n\n구성 요소\n\n\n\nBPA(Business Process Analysis)\n\n비즈니스 프로세스를 발견/식별하여 모델링하고, 요구사항을 충족하는지 분석하는 솔루션\n비즈니스 프로세스를 어떻게 모델링할 것인가? 가 핵심\n\nBPE(Business Process Execution)\n\n설계된 모델을 자동화하여 실행하는 솔루션\n\n자동화: 실행 프로세스 정의, 자동화 실행, 모니터링, 실행 결과 분석\n자동화 실행 도구: 프로세스 자동화 실행 서비스를 제공하는 프로세스 엔진을 중심으로 정의, 모니터링, 분석 도구가 포함됨\n\nCamunda Web Modeler: 프로세스 실행에 필요한 각종 요소의 디자인을 위한 도구\nCamunda Zeebe Orchestration Engine: 자동화 실행 서비스를 제공하는 엔진\nCamunda Tasklist: Human process에서 해야하는 일 리스트\nCamunda Operate: 실행중인, 완료된, 일시중지된 프로세스(instance / case)를 모니터링 하는 도구\n\n일시 중지, 취소, 기록 삭제 가능\n\nCamunda Optimize: 프로세스 진행 이력 기반 분석 도구\n\nEAI & ESB(Enterprise Application Integration, Enterprise Service Bus)\n\n이질적 어플리케이션들을 프로세스 기반으로 연계/통합하는 솔루션\n\n\n\n\n\nEAI & ESB\n\n\n\nBRE(Business Rule Engine)\n\n어플리케이션 로직과 비즈니스 룰을 분리하여 별도로 관리\n\nBAM(Business Activity Monitoring)\n\n이벤트를 수집하여 실시간 경영 의사결정을 지원하는 솔루션 - RTE 실현을 위한 도구",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM Suite"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#프로세스-자동화-구축-및-운영-과정",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#프로세스-자동화-구축-및-운영-과정",
    "title": "BPM Suite",
    "section": "프로세스 자동화 구축 및 운영 과정",
    "text": "프로세스 자동화 구축 및 운영 과정\n\nDesign & Deploy\n\nBPE에서 정의하는 실행 가능한 형태(process definition)\nDesign 요소\n\n실행가능한 자동화 프로세스 모델 &lt;-&gt; BPA 개념 모델\nIT Application(ERP 모듈, Form)\n프로세스 데이터 모델(프로세스 변수)\n업무 규칙(Business Rule)\n\ngateway condition 등\n\n업무 담당자 / 역할 지정\n통합 인터페이스(service task)\n\n\nAutomation & Integration\n\nBPM 엔진이 담당, 사람 / 사람, 사람 / 시스템, 시스템 / 시스템 통합 지휘(Orchestration)\n자동화 대상 프로세스 유형:\n\n전통적인 워크플로우 형태 프로세스\n시스템 통합 중심 프로세스: 사람의 개입이 최소화 되는 것\nRPA-Centric Process\nAI-Centric Process\nEvent-driven / Data-driven Process\nEnd-to-End Orchestrated Process: 모든 유형을 BPM 엔진이 통합 관리할 수 있다.\n\n\nMonitor & Control\n\n프로세스가 정의된 대로 수행되도록 관리할 수 있으며, 프로세스의 성과를 일정 수준에 맞추기 위해 필요한 조치 / 통제를 할 수 있음\n\n독촉, 일시 중지, 재개, 성과지표 측정 값이 일정 수준에 도달하도록 통제\n\n진행되는 상황을 실시간으로 모니터링해볼 수 있으며, 수행된 기록을 통해 이력을 추적할 수 있음\n단순 모니터링을 넘어서 성과관리, 위험관리, 전략 활도오가의 연계로 범위 확장 -&gt; BAM 솔루션 등장(Comunda 내장 기본적인게 있긴 함)\n\nAnalysis & Report\n\nOLAP 도구 이용\n\nKPI 형태로 사전 정의된 성과지표를 측정할 수 있음\n\nCamunda Optimize 사용 가능\n\nSimulation & Optimize\n\n시뮬레이션: 매우 효과적인 최적화 방법\n\n가상으로 프로세스를 실행시켜 문제점 도출 및 분석 도구\nEvent logs를 이용하여 현실을 모델링한 시뮬레이션 가능\n\n최적화: 분석을 통해 발견된 문제점을 해결\n\n개선기회 모색을 통해 수립된 개선 방안을 적용하여 해당 프로세스가 최적화 상태로 실행될 수 있도록 함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM Suite"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#ai---rpa---bpm-통합",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#ai---rpa---bpm-통합",
    "title": "BPM Suite",
    "section": "AI - RPA - BPM 통합",
    "text": "AI - RPA - BPM 통합\n\nHyperautomation: 모든 자동화 기술을 하나의 오케스트레이션 플랫폼에서 통합 관리하는 차세대 자동화 패러다임\n구성 요소:\n\nBPM 엔진\nRPA 봇\nAI Agent\nAPI / Connector / Event\nProcess Minig / Optimize\n\n목표:\n\n단순 자동화를 넘어 지능형 / 자가개선형 프로세스 발전\n조직의 End-to-End 디지털 전환 가속화",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM Suite"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#시스템-통합-문제",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#시스템-통합-문제",
    "title": "BPM Suite",
    "section": "시스템 통합 문제",
    "text": "시스템 통합 문제\n\n메인 프레임 시대 -&gt; 유닉스 시스템\n\np2p로는 한계가 있다.\n\nEAI 등장: 이질적 기업 환경을 엔터프라이즈 미들웨어로 통합\n\nEAI가 중앙에 허브역할을 하면서 모든 통신을 EAI를 거치도록 함\n이기종 간에 통신 프로토콜이나 통합 방식을 변경할 수 있는 Adaptor를 제공함\n\n\n\n\n\nEAI\n\n\n\nESB로 진화\n\nEAI에 SOA 사상이 추가\n애플리케이션을 느슨한 구조의 비즈니스 서비스 단위로 연결하고 중개하는 표준 매커니즘\n\n표준 인터페이스가 없다면 SOA adpator or service-enabled 필요\n기능: Routing of information, data transformation, security, …\n\n\n\n\n\n\nESB\n\n\n\nMicro Service\n\nSOA/BPM을 거쳐 발전\n\nMicro service, legacy system 등을 ESB에 전부 통합",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM Suite"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#business-rules",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#business-rules",
    "title": "BPM Suite",
    "section": "Business Rules",
    "text": "Business Rules\n\ngovern business decision-making\n업무 수행 시 적용되는 규정, 정책, 법규, 업무처리 규칙 등을 의미\n각종 조건들과 이에 따른 행동에 대한 것을 총체적으로 의미\n\n비즈니스 룰의 정의를 위해서는 조건(If)과 행동(then)의 rule(row), 이를 표현하기 위한 비즈니스 구조(input, output column) 요소가 필요\nDMN으로 표현\n\n\n\n\n\nbusiness rule vs decision\n\n\n\n일상적인 업무 규칙에서 사내 정책 및 내규, 기업 외적인 법적 규제 등 다양한 유형 존재\n만약 BRE나 BRMS(logic 또는 business rule을 정의, 설치, 수행, 감시, 유지보수 하는 시스템)가 없다면 Business rule이 프로그램 내에서 업무 로직 형태로 구현됨\n\n문제점: business rules are hidden in applications\n프로그래밍 난이도가 매우 높아짐\nBusiness Agility 문제 발생: 수정이 매우 오래걸림\n사람들이 수동으로 rules를 inconsistently 해석함\n\nBRE/BRMS 도입 시 장점\n\nbusiness rule을 어플리케이션으로부터 separate 할 수 있음\n비즈니스 룰의 가시성(visibility) 확보 - transparency\n비즈니스 사용자가 변화관리를 직접 수행 가능 - agility\n모든 rule을 consistency 적용 가능\n\n주요 기능\n\n비즈니스 룰 모델링(코드 대신 Declare 형식)\n룰 일관성 검증\n추론 기능(A -&gt; B, B -&gt; C 이면 A -&gt; C)\nIT 어플리케이션과의 연계 기능\n\n\n\n\n\nDeclare",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM Suite"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#bam",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/08.html#bam",
    "title": "BPM Suite",
    "section": "BAM",
    "text": "BAM\n\n비즈니스 성과지표에 대한 실시간 정보를 제공하는 것\nCEP(Complex Event Processing), Fraud Detection 기술 등이 성과 관리 전문 솔루션 형태로 발전\n실시간 기업(RTE)이 되기 위해서는 이벤트들을 필터링하고 분석하여 유용한 정보를 도출해 내야함. BAM 솔루션이 이를 지원\n\n\n\n\nBAM 실시간 모니터링 개념도\n\n\n\n프로세스 모델링 및 모니터링 모델링\n프로세스의 실행\n웹 기반 dashboard에서의 실시간 모니터링 및 경보\n이벤트 발생에 따른 대응 프로세스의 자동 실행 및 실행 프로세스 추적\n\nreactive -&gt; proactive(선제적) 관리로 발전\n\n\n\n한계:\n\n정형화된 KPI 모니터링에 초점 -&gt; 근본 원인 분석 한계\n실행 로그 기반의 세밀한 경로 분석 부족\n\n진화: Process Intelligence / Process Mining\n\n이벤트 로그 기반으로 실제 수행된 프로세스 흐름 재구성\n병목, 재작업, 비표준 경로 자동 탐지\nAI 기반 분석 및 시뮬레이션으로 개선안 제시",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "BPM Suite"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/00.html#구조",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/00.html#구조",
    "title": "Intro",
    "section": "구조",
    "text": "구조\n\n\n\n\n\nflowchart LR\n    Entity((Entity)) --&gt; Queue\n    Queue[Queue] -&gt; Process(process)\n\n\n\n\n\n\n\nEntity: attribute를 가짐.\nprocess: 추상화된 개념. 여러 resource를 사용할 수 있음.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "시뮬레이션",
      "Intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/00.html#리틀의-법칙",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/00.html#리틀의-법칙",
    "title": "Intro",
    "section": "리틀의 법칙",
    "text": "리틀의 법칙\n\ntally: 사람의 수로 나눈 통계값\n\naverage waiting time, system 체류 시간 등\n\ntime persistant statistic: 시간으로 나눈 통계값\n\naverage queue length 등\n\n\\(λ_q = λ w_q\\)\nNR: Busy or Idle\nMR: Scheduled or not\nN: in system or not\nQ: wait or not\nu(t): if M(t) &gt; 0 ? \\(\\frac{B(t)}{M(t)}\\) else 0\nB(t): busy?\nM(t): scheduled?\nInstantaneius utilization: \\(\\int_{0}^{t} u(t) dt * \\frac{1}{t}\\)\nScheduled utilization: \\(\\frac{\\int_{0}^{t} B(t) dt}{\\int_{0}^{t} M(t) dt}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "시뮬레이션",
      "Intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/00.html#resource",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/00.html#resource",
    "title": "Intro",
    "section": "Resource",
    "text": "Resource\n\nignore: 하던거만 마무리 + 늦어도 제 때 복귀\nwait: 하던거만 마무리 + 늦은 만큼 더 쉬기\npreemt: 하던거 중단 + 제 때 복귀",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "시뮬레이션",
      "Intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/03.html#요구사항",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/03.html#요구사항",
    "title": "반도체 공정 시뮬레이션",
    "section": "요구사항",
    "text": "요구사항",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "시뮬레이션",
      "반도체 공정 시뮬레이션"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/03.html#보고서",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/03.html#보고서",
    "title": "반도체 공정 시뮬레이션",
    "section": "보고서",
    "text": "보고서",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "시뮬레이션",
      "반도체 공정 시뮬레이션"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/03.html#파일-및-캡처-화면",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/03.html#파일-및-캡처-화면",
    "title": "반도체 공정 시뮬레이션",
    "section": "파일 및 캡처 화면",
    "text": "파일 및 캡처 화면\nArena 파일\n\n\n\n전체 코드\n\n\n\n\n\n애니메이션",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "시뮬레이션",
      "반도체 공정 시뮬레이션"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/03.html#여담",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/시뮬레이션/03.html#여담",
    "title": "반도체 공정 시뮬레이션",
    "section": "여담",
    "text": "여담\n\nArena tool 특성 상 요구조건을 모두 만족시키기에 살짝 애매한 부분이 몇 가지 있었다.\n해결할 수는 있지만 뭐.. 어차피 과제 제출용 프로젝트고, 교수님도 굳이 싶다는걸 아시니까 그냥 뒀다.\n\n\nFOUP을 Loading 하고 Unloading 하는 과정에서의 애니메이션이 없다.\nAMR을 선점한 FOUP이 애니매이션 상에서 보이지 않는다.\n대기 상태인 AMR은 근처에 parking 교차로가 없는 경우 애니메이션 상에서 이상한 위치에 멈춰선다.\n교차로에서 2개 이상의 AMR이 동시에 존재할 수 있다.\n\n\n재미있긴 했지만 Arena를 더 깊이 있게 사용해보고싶진 않다. 노가다 요소가 너무 많다.\n다른 tool을 사용해보고 싶긴 한데 유명한게 딱히 있는진 모르겠다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "시뮬레이션",
      "반도체 공정 시뮬레이션"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/cte/00.html#보고서",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/cte/00.html#보고서",
    "title": "Could Quantum Computing be the Next Paradigm Shift Soon?",
    "section": "보고서",
    "text": "보고서",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Could Quantum Computing be the Next Paradigm Shift Soon?"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/cte/00.html#발표자료",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/cte/00.html#발표자료",
    "title": "Could Quantum Computing be the Next Paradigm Shift Soon?",
    "section": "발표자료",
    "text": "발표자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "Cte",
      "Could Quantum Computing be the Next Paradigm Shift Soon?"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/06.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/06.html",
    "title": "비지도 학습 템플릿",
    "section": "",
    "text": "Partitioning methods\n\n\n\nk-means\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() # or RobustScaler\ndf_scaled = scaler.fit_transform(df)\n\n# Elbow method\nI = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i) # sklearn은 기본적으로 k-means++\n    kmeans.fit(df_scaled)\n    I.append(kmeans.inertia_)\nplt.plot(range(1, 11), I, marker='o')\n\n# k 선택 후 군집화\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(df_scaled)\ndf['cluster'] = kmeans.labels_\n\n# 군집 중심값 정보\ncenters = scaler.inverse_transform(kmeans.cluster_centers_)\ncenters_df = pd.DataFrame(centers, columns=df.columns[:-1], index=[f'cluster_{i}' for i in range(centers.shape[0])])\ndisplay(centers_df)\n\n\nKmeans\n\npolinominal 시간 안에 해결 가능\nnoise, outlier에 민감함\n수치형만 처리 가능\n\nk-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함\nk-prototype: 범주형, 수치형 섞인거 처리 가능\nk-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함\n\nPAM: Partitioning Around Medoids\n\nscalability 문제 있음\n\nCLARA: sampling을 통해서 PAM의 scalability 문제를 해결\n\n샘플링 과정에서 biased될 수 있음\n\nCLARANS: medoid 후보를 랜덤하게 선택함\n\n\n\nk-modes, k-prototype, k-medoids는 ADP 환경에서 제공 안함 ADP 환경에서 제공하는 모듈로는 범주형, 수치형 섞인거 처리하는 군집 방법이 없음 (일단 내가 생각하기로는 그렇다)\n\n\nHierarchical methods\n\n\ntop-down: divisive, dia\nbottom-up: agglomerative\n\n\n\nagglomerative\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage. cut_tree\n\nz = linkage(df_scaled, method='ward') # 'single', 'complete', 'average', 'ward' 등등\n\nresult = cut_tree(z, n_clusters=3).flatten()\n\nd = dendogram(z, labels=list(df.index))\nplt.show()\n\n\n\n합쳐지는 거리\n\nthr = pd.DataFrame(d['dcoord'])\nthr # (0, 3) = 합쳐지기 전 왼쪽, 오른쪽 높이, (1, 2) = 합쳐진 후 높이\nthr[thr[2] &gt; 70].sort_values(2)[[2]] # 70 이상의 높이에서 합쳐지는 군집들의 높이\n\n\n거리기반 군집의 단점:\n\n군집의 모양이 구형이 아닐 경우 찾기 어려움\n군집의 갯수 결정하기 어려움\n군집의 밀도가 높아야함\n\n\n\n\n\n\n다양한 모양의 군집을 찾을 수 있음\nnoise, outlier에 강함\n\n\nDBSCAN: 잡음 포인트는 군집에서 제외\n\ncore point를 찾음(eps 이내에 minPts 이상 있는 점)\ncore point를 중심으로 군집을 확장\n\ncore point가 아닌 경우 확장 종료\n\n\n\n고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음\n군집간 계층관계를 인식하기 어렵다\n대신 빠르고, DBSCAN만으로도 충분해서 많이 사용되는 듯\n\n\n\n\nDBSCAN eps 결정\n\nfrom sklearn.neighbors import NearestNeighbors\n\nMIN_SAMPLES = 5\ndf_scaled = scaler.fit_transform(df)\n\nneigh = NearestNeighbors(n_neighbors=MIN_SAMPLES)\nneigh.fit(df_scaled)\ndistances, indices = neigh.kneighbors(df_scaled)\n\nk_distances = np.sort(distances[:, MIN_SAMPLES-1])[::-1]\n\n# k-dist plot 그리기\nplt.figure(figsize=(12, 6))\nplt.plot(k_distances)\nplt.xlabel('Data Points sorted by distance')\nplt.ylabel(f'{MIN_SAMPLES-1}-th Nearest Neighbor Distance') # 자기 자신을 제외한 거리\nplt.title('k-distance Graph')\nplt.grid(True)\nplt.show()\n\n\nmin samples 기준\n\n2 * 차원, log(샘플 수), 4~5개 등등의 기준이 있다.\n이론적으로 증명이 되거나 한건 아니니까 적절히 선택하거나, for문 돌려가면서 최적값 찾기\n\neps 기준\n\nk-dist plot에서 급격히 꺾이는 지점\n\n\n\n\nDBSCAN\n\nfrom sklearn.cluster import DBSCAN\n\n# k-dist plot에서 찾은 값으로 DBSCAN 적용\neps_value = 18  # k-dist plot에서 결정한 값\n\ndb = DBSCAN(eps=eps_value, min_samples=MIN_SAMPLES).fit(df_scaled)\nlabels = db.labels_\n\n\nOPTICS: DBSCAN의 단점을 보완\n\n군집의 밀도가 다를 때도 잘 처리함\n군집의 계층 구조를 인식할 수 있음\nminPts 파라미터가 필요함\n얘로도 이상치 탐지 가능\n\n\n\n\nOPTICS\n\nfrom sklearn.cluster import OPTICS\n\noptics = OPTICS(min_samples=MIN_SAMPLES).fit(df_scaled)\nlabels = optics.labels_\n\n\n\n\n\nsilhuette score: \\(\\frac{\\sum_{i=1}^{n} s(i)}{n}\\)\n\ns(i): \\(\\frac{b(i) - a(i)}{max((a(i), b(i)))}\\)\n\na(i): 군집 내 노드간의 평균 거리\nb(i): 가장 가까운 군집과의 노드 간 평균 거리\n\n1에 가까울 수록 좋음\n\n그 외 sklearn metrics 참고",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "비지도 학습 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/06.html#군집-분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/06.html#군집-분석",
    "title": "비지도 학습 템플릿",
    "section": "",
    "text": "Partitioning methods\n\n\n\nk-means\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() # or RobustScaler\ndf_scaled = scaler.fit_transform(df)\n\n# Elbow method\nI = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i) # sklearn은 기본적으로 k-means++\n    kmeans.fit(df_scaled)\n    I.append(kmeans.inertia_)\nplt.plot(range(1, 11), I, marker='o')\n\n# k 선택 후 군집화\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(df_scaled)\ndf['cluster'] = kmeans.labels_\n\n# 군집 중심값 정보\ncenters = scaler.inverse_transform(kmeans.cluster_centers_)\ncenters_df = pd.DataFrame(centers, columns=df.columns[:-1], index=[f'cluster_{i}' for i in range(centers.shape[0])])\ndisplay(centers_df)\n\n\nKmeans\n\npolinominal 시간 안에 해결 가능\nnoise, outlier에 민감함\n수치형만 처리 가능\n\nk-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함\nk-prototype: 범주형, 수치형 섞인거 처리 가능\nk-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함\n\nPAM: Partitioning Around Medoids\n\nscalability 문제 있음\n\nCLARA: sampling을 통해서 PAM의 scalability 문제를 해결\n\n샘플링 과정에서 biased될 수 있음\n\nCLARANS: medoid 후보를 랜덤하게 선택함\n\n\n\nk-modes, k-prototype, k-medoids는 ADP 환경에서 제공 안함 ADP 환경에서 제공하는 모듈로는 범주형, 수치형 섞인거 처리하는 군집 방법이 없음 (일단 내가 생각하기로는 그렇다)\n\n\nHierarchical methods\n\n\ntop-down: divisive, dia\nbottom-up: agglomerative\n\n\n\nagglomerative\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage. cut_tree\n\nz = linkage(df_scaled, method='ward') # 'single', 'complete', 'average', 'ward' 등등\n\nresult = cut_tree(z, n_clusters=3).flatten()\n\nd = dendogram(z, labels=list(df.index))\nplt.show()\n\n\n\n합쳐지는 거리\n\nthr = pd.DataFrame(d['dcoord'])\nthr # (0, 3) = 합쳐지기 전 왼쪽, 오른쪽 높이, (1, 2) = 합쳐진 후 높이\nthr[thr[2] &gt; 70].sort_values(2)[[2]] # 70 이상의 높이에서 합쳐지는 군집들의 높이\n\n\n거리기반 군집의 단점:\n\n군집의 모양이 구형이 아닐 경우 찾기 어려움\n군집의 갯수 결정하기 어려움\n군집의 밀도가 높아야함\n\n\n\n\n\n\n다양한 모양의 군집을 찾을 수 있음\nnoise, outlier에 강함\n\n\nDBSCAN: 잡음 포인트는 군집에서 제외\n\ncore point를 찾음(eps 이내에 minPts 이상 있는 점)\ncore point를 중심으로 군집을 확장\n\ncore point가 아닌 경우 확장 종료\n\n\n\n고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음\n군집간 계층관계를 인식하기 어렵다\n대신 빠르고, DBSCAN만으로도 충분해서 많이 사용되는 듯\n\n\n\n\nDBSCAN eps 결정\n\nfrom sklearn.neighbors import NearestNeighbors\n\nMIN_SAMPLES = 5\ndf_scaled = scaler.fit_transform(df)\n\nneigh = NearestNeighbors(n_neighbors=MIN_SAMPLES)\nneigh.fit(df_scaled)\ndistances, indices = neigh.kneighbors(df_scaled)\n\nk_distances = np.sort(distances[:, MIN_SAMPLES-1])[::-1]\n\n# k-dist plot 그리기\nplt.figure(figsize=(12, 6))\nplt.plot(k_distances)\nplt.xlabel('Data Points sorted by distance')\nplt.ylabel(f'{MIN_SAMPLES-1}-th Nearest Neighbor Distance') # 자기 자신을 제외한 거리\nplt.title('k-distance Graph')\nplt.grid(True)\nplt.show()\n\n\nmin samples 기준\n\n2 * 차원, log(샘플 수), 4~5개 등등의 기준이 있다.\n이론적으로 증명이 되거나 한건 아니니까 적절히 선택하거나, for문 돌려가면서 최적값 찾기\n\neps 기준\n\nk-dist plot에서 급격히 꺾이는 지점\n\n\n\n\nDBSCAN\n\nfrom sklearn.cluster import DBSCAN\n\n# k-dist plot에서 찾은 값으로 DBSCAN 적용\neps_value = 18  # k-dist plot에서 결정한 값\n\ndb = DBSCAN(eps=eps_value, min_samples=MIN_SAMPLES).fit(df_scaled)\nlabels = db.labels_\n\n\nOPTICS: DBSCAN의 단점을 보완\n\n군집의 밀도가 다를 때도 잘 처리함\n군집의 계층 구조를 인식할 수 있음\nminPts 파라미터가 필요함\n얘로도 이상치 탐지 가능\n\n\n\n\nOPTICS\n\nfrom sklearn.cluster import OPTICS\n\noptics = OPTICS(min_samples=MIN_SAMPLES).fit(df_scaled)\nlabels = optics.labels_\n\n\n\n\n\nsilhuette score: \\(\\frac{\\sum_{i=1}^{n} s(i)}{n}\\)\n\ns(i): \\(\\frac{b(i) - a(i)}{max((a(i), b(i)))}\\)\n\na(i): 군집 내 노드간의 평균 거리\nb(i): 가장 가까운 군집과의 노드 간 평균 거리\n\n1에 가까울 수록 좋음\n\n그 외 sklearn metrics 참고",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "비지도 학습 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/06.html#차원-축소",
    "href": "posts/03_archives/completed_project/adp_실기/notes/06.html#차원-축소",
    "title": "비지도 학습 템플릿",
    "section": "차원 축소",
    "text": "차원 축소\n\nPCA, LSA, t-SNE, UMAP, ICA, MDS, NMF 등등\n\n정말 많은 방법들이 있다.\n\n요인분석에서 확인적 요인분석은 python에서 제공하는 library가 없는걸로 알고 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "비지도 학습 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/06.html#연관-분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/06.html#연관-분석",
    "title": "비지도 학습 템플릿",
    "section": "연관 분석",
    "text": "연관 분석\n\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\ntransactions = {}\nte = TransactionEncoder()\n\n# 전처리\ndata = df['target'].str.split(', ').values\nte_ary = te.fit_transform(data)\ntransactions[name] = pd.DataFrame(te_ary, columns=te.columns_)\n\n# 빈발패턴 생성\nfset = apriori(t, min_support=0.6, use_colnames=True, verbose=False)\nif fset.shape[0] == 0:\n    print(\"빈발패턴이 존재하지 않습니다.\")\nelse:\n    # 연관규칙 생성\n    rule = association_rules(fset, metric=\"confidence\", min_threshold=0.7)\n    rule['len_ant'] = rule['antecedents'].apply(lambda x: len(x))\n    rule['len_con'] = rule['consequents'].apply(lambda x: len(x))\n    display(rule[(rule['len_con'] == 1) & (rule['lift'] &gt;= 1.2)].reset_index(drop=True))\n\n\n지지도: 전체 거래에서 특정 항목 집합이 나타나는 비율\n신뢰도: 특정 항목 집합이 주어졌을 때 다른 항목\n향상도: 두 항목 집합이 독립적인 경우에 비해 함께 나타날 가능성",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "비지도 학습 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/07.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/07.html",
    "title": "시계열 분석",
    "section": "",
    "text": "추세(level)\n계절, 순한: 추세에서 벗어나는 변화의 정도\n잔차(white noise)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "시계열 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/07.html#구성-요소",
    "href": "posts/03_archives/completed_project/adp_실기/notes/07.html#구성-요소",
    "title": "시계열 분석",
    "section": "",
    "text": "추세(level)\n계절, 순한: 추세에서 벗어나는 변화의 정도\n잔차(white noise)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "시계열 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/07.html#eda",
    "href": "posts/03_archives/completed_project/adp_실기/notes/07.html#eda",
    "title": "시계열 분석",
    "section": "EDA",
    "text": "EDA\ndf_isna = df.asfreq('H') # 시간 기준 결측치 탐색. index가 datetime이어야 함\ndf_isna[df_isna.isnull().any(axis=1)]\nfrom statsmodels.tsa.seasonal import STL\n\ndecomposition = STL(df_final['value'], period=24).fit() # period는 계절성 주기\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(10, 8))\nax1.plot(decomposition.observed)\nax1.set_ylabel('Obseved')\n\nax2.plot(decomposition.trend)\nax2.set_ylabel('Trend')\n\nax3.plot(decomposition.seasonal)\nax3.set_ylabel('Seasonal')\n\nax4.plot(decomposition.resid)\nax4.set_ylabel('Residuals')\n\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "시계열 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/07.html#예측-방법",
    "href": "posts/03_archives/completed_project/adp_실기/notes/07.html#예측-방법",
    "title": "시계열 분석",
    "section": "예측 방법",
    "text": "예측 방법\nRolling Forecast (동적 예측): - 각 시점에서 예측을 수행한 후, 실제 값이 관측되면 이를 학습 데이터에 추가하여 다음 시점을 예측 - 실제 운영 환경을 시뮬레이션하는 방법으로, 새로운 정보가 지속적으로 업데이트됨 - 장점: 최신 정보를 반영하여 더 정확한 예측 가능 - 단점: 계산 비용이 높고, 모델 성능 평가 시 과적합 위험\nStatic Forecast (정적 예측): - 초기 학습 데이터로만 모델을 한 번 학습하고, 이후 테스트 기간 전체에 대해 연속적으로 예측 - 고정된 모델로 여러 시점을 예측하므로 모델의 일반화 성능을 더 엄격하게 평가 - 장점: 계산 비용이 낮고, 공정한 모델 평가 가능 - 단점: 시간이 지날수록 예측 정확도가 떨어질 수 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "시계열 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/07.html#단순-기법",
    "href": "posts/03_archives/completed_project/adp_실기/notes/07.html#단순-기법",
    "title": "시계열 분석",
    "section": "단순 기법",
    "text": "단순 기법\n\n단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 \\(\\hat{p_{t+1}} = p_t\\)\n추세분석: 전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. \\(\\hat{p_{t+1}} = p_t + p_t - p_{t-1}\\)\n단순 이동평균법: time window를 계속 이동하면서 평균 구하는거\n\ntime window ↑: 먼 과거까지 보겠다\n\n가중 이동평균법: 가중치를 다르게 부여한 단순이동평균법\n\n\n\n단순 기법 rolling forecast\n\ndef rolling_forecast_simple(train_data, test_data, method='naive'):\n    predictions = []\n    train_extended = train_data.copy()\n    \n    for i in range(len(test_data)):\n        if method == 'naive':\n            # 단순예측법: 마지막 값\n            pred = train_extended.iloc[-1]['data']\n            \n        elif method == 'trend':\n            # 추세분석법\n            pred = train_extended.iloc[-1]['data'] + (train_extended.iloc[-1]['data'] - train_extended.iloc[-2]['data'])\n            \n        elif method == 'ma_4':\n            # 4기간 이동평균\n            pred = np.mean(train_extended.iloc[-4:]['data'])\n            \n        elif method == 'ma_12':\n            # 12기간 이동평균\n            window = min(12, len(train_extended))\n            pred = np.mean(train_extended.iloc[-window:]['data'])\n            \n        elif method == 'seasonal_naive':\n            # 계절 단순예측법 (4분기 전과 동일)\n            if len(train_extended) &gt;= 4:\n                pred = train_extended.iloc[-4]['data']\n            else:\n                pred = train_extended.iloc[-1]['data']\n        \n        predictions.append(pred)\n        \n        # 실제 값을 train에 추가 (rolling)\n        actual_value = test_data.iloc[i]\n        train_extended = pd.concat([train_extended, actual_value.to_frame().T], ignore_index=True)\n    \n    return predictions\n\n# 각 방법별 rolling forecast 수행\nmethods = ['naive', 'trend', 'ma_4', 'ma_12', 'seasonal_naive']\nrolling_results = {}\n\nfor method in methods:\n    predictions = rolling_forecast_simple(train, test, method)\n    rolling_results[method] = predictions\n    \n# 결과를 DataFrame으로 정리\nresults_df = pd.DataFrame(rolling_results)\nresults_df['actual'] = test['data'].values\nresults_df.index = test.index\nprint(results_df)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "시계열 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/07.html#지수-평활법",
    "href": "posts/03_archives/completed_project/adp_실기/notes/07.html#지수-평활법",
    "title": "시계열 분석",
    "section": "지수 평활법",
    "text": "지수 평활법\n\n\nα -&gt; 1: 최근 자료에 비중을 둠. α -&gt; 0: 기존 예측을 따름\n\n\n\n지수 평활법 rolling forecast\n\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\n\ndef rolling_forecast_exponential_smoothing(train_data, test_data, a=0.5):\n    predictions = []\n    train_extended = train_data.copy()\n    \n    for i in range(len(test_data)):\n        try:\n            # 모델 적합\n            model = SimpleExpSmoothing(train_extended['value'])\n            model_fit = model.fit(smoothing_level=a, optimized=False)\n\n            # 예측\n            pred = model_fit.predict(len(train_extended))\n            predictions.append(pred.iloc[0])\n\n        except Exception as e:\n            print(f\"Error at step {i}: {e}\")\n            # 오류 발생시 naive 예측 사용\n            predictions.append(train_extended.iloc[-1]['value'])\n        \n        # 실제 값을 train에 추가 (rolling)\n        actual_value = test_data.iloc[i]\n        train_extended = pd.concat([train_extended, actual_value.to_frame().T], ignore_index=True)\n    \n    return predictions\n\n# 지수 평활법 rolling forecast 실행 예시\nexp_smooth_predictions = rolling_forecast_exponential_smoothing(train, test, a=0.3)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "시계열 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/07.html#sarimax-계열",
    "href": "posts/03_archives/completed_project/adp_실기/notes/07.html#sarimax-계열",
    "title": "시계열 분석",
    "section": "SARIMAX 계열",
    "text": "SARIMAX 계열\n\n\n\nSARIMAX 과정\n\n\n\n정상성 확인\n\n정상시계열은 분산과 평균, 자기 상관이 시간에 따라 변하지 않는 시계열\n분산이 일정하지 않은 경우: 로그 변환\n\n그래프로 확인\n\n평균이 일정하지 않은 경우: 차분\n\n\n\nstationarity test\n\nfrom statsmodels.tsa.stattools import adfuller\n\nad_fuller_result = adfuller(df['value'])\n\nprint('ADF Statistic:', ad_fuller_result[0])\nprint('p-value:', ad_fuller_result[1])\n\n\np-value &lt; 0.05: 귀무가설 기각, 정상시계열\n주로 평균이 일정하지 않은 것을 찾아낼 수 있다.\n\n\n\ndifference\n\ndf = df.diff()\n\n\n만약 차분 후에도 정상성이 만족되지 않는다면, 계절 차분을 고려\n\n\n\nseasonal difference\n\ndf = df.diff(12) # 12개월 주기\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(df['value'].dropna(), lags=30)\nplot_pacf(df['value'].dropna(), lags=30)\nplt.show()\n\n자기 상관이 존재하는 경우\n\nACF, PACF 그래프로 확인\n\n\n\n\n\n모델 선택 기준\n\n\n\n하지만 매우 주관적일 수 있으므로, 직접 여러 모델을 돌려본 후 AIC 기준으로 선택.\n\n그리고 다시 검정 진행\n\n\n\n\nSARIMAX 모델 적합\n\n\nSARIMAX\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom itertools import product\n\np = range(0, 4, 1)\nd = 1 # 차분 횟수\nq = range(0, 4, 1)\nP = range(0, 4, 1)\nD = 0 # 계절 차분 횟수\nQ = range(0, 4, 1)\ns = 4 # 계절 주기\nparameters = product(p, q, P, Q)\norder_list = list(parameters)\n\nresults = []\nfor order in order_list:\n    try:\n        model = SARIMAX(train['value'], \n                        exog,\n                        order=(order[0], d, order[1]), \n                        seasonal_order=(order[2], D, order[3], s),\n                        simple_differencing=False).fit(disp=False)\n    except:\n        continue\n    aic = model.aic\n    results.append([order, aic])\nresults_df = pd.DataFrame(results, columns=['(p, q, P, Q)', 'AIC'])\nresults_df = results_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\ndisplay(results_df)\n\nbest_model = SARIMAX(train['value'], \n                    exog,\n                    order=(2, 1, 2), # [2, 3], 0, 1 이런 식으로도 가능(y_t-2, y_t-3 사용하는 방법)\n                    simple_differencing=False).fit(disp=False)\nbest_model.summary()\n\n결과 확인\n\nbest_model.plot_diagnostics(figsize=(10, 8))\nplt.show()\n\n등분산성, 정규성 만족하는지 육안으로 확인\n\n\n\nljung-box\n\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\nljung_box_result = acorr_ljungbox(best_model.resid, lags=[10], return_df=True)\nprint(ljung_box_result)\n\n\np-value가 0.05보다 크면 잔차가 백색잡음\n\n\n\nSARIMAX rolling forecast\n\ndef rolling_forecast_sarimax(train_data, test_data, order, seasonal_order, exog_train=None, exog_test=None):\n    predictions = []\n    train_extended = train_data.copy()\n    exog_extended = exog_train.copy() if exog_train is not None else None\n    \n    for i in range(len(test_data)):\n        try:\n            # 모델 적합\n            if exog_extended is not None:\n                model = SARIMAX(train_extended['value'], \n                               exog=exog_extended,\n                               order=order, \n                               seasonal_order=seasonal_order,\n                               simple_differencing=False)\n            else:\n                model = SARIMAX(train_extended['value'], \n                               order=order, \n                               seasonal_order=seasonal_order,\n                               simple_differencing=False)\n            model_fit = model.fit(disp=False)\n            # 예측\n            if exog_test is not None:\n                pred = model_fit.forecast(steps=1, exog=exog_test.iloc[i:i+1])\n            else:\n                pred = model_fit.forecast(steps=1)\n            predictions.append(pred.iloc[0])\n        except Exception as e:\n            print(f\"Error at step {i}: {e}\")\n            # 오류 발생시 naive 예측 사용\n            predictions.append(train_extended.iloc[-1]['value'])\n        \n        # 실제 값을 train에 추가 (rolling)\n        actual_value = test_data.iloc[i]\n        train_extended = pd.concat([train_extended, actual_value.to_frame().T], ignore_index=True)\n        \n        if exog_extended is not None and exog_test is not None:\n            exog_extended = pd.concat([exog_extended, exog_test.iloc[i:i+1]], ignore_index=True)\n    \n    return predictions\n\n# SARIMAX rolling forecast 실행 예시\nsarimax_predictions = rolling_forecast_sarimax(\n    train, test, \n    order=(2, 1, 2), \n    seasonal_order=(0, 0, 0, 4)  # 계절성이 없는 경우\n)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "시계열 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/05.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/05.html",
    "title": "분산 분석 템플릿",
    "section": "",
    "text": "검정 방법 선택 기준\n\n\n\n\n\n모수 검정 방법 선택 기준\n\n\n\n관측치 간에 독립이 아닌 경우(시간: 자기 상관이 존재, 공간: 패널, 계층, …), 각 케이스에 맞는 모형을 사용해야 함.\n\n\n\n\n표본이 정규분포를 따르는지 검정.\n따르지 않더라도 중심극한정리에 의해 표본의 크기가 충분히 크면 모수 검정을 사용할 수 있다.\nshapiro wilk 검정: 표본의 크기가 3-5000개인 데이터에 사용. 동일한 값이 많은 경우 성능이 떨어질 수 있음\n\nH0: 데이터가 정규분포를 따른다.\nH1: 데이터가 정규분포를 따르지 않는다.\n\njarque-Bera: 대표본에 사용.\n\nH0: 데이터가 정규분포를 따른다.\nH1: 데이터가 정규분포를 따르지 않는다.\n\nQ-Q plot: x축이 이론적 분위수, y축이 표본 분위수\n\n\n\nnormality test\n\n#| eval: false\n\nfrom scipy.stats import shapiro, jarque_bera, zscore, probplot\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nstat, p = shapiro(data)\n\nprint(f\"Shapiro-Wilk Test: stat={stat:.3f}, p={p:.3f}\")\n\nstat, p = jarque_bera(data)\nprint(f\"Jarque-Bera Test: stat={stat:.3f}, p={p:.3f}\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nzdata = zscore(data)\nfig, ax = plt.subplots(1, 2, figsize=(10, 3))\n\n(osm, odr), (slope, intercept, r) = probplot(zdata, plot=ax[0])\nax[0].set_title(\"Q-Q Plot\")\n\ndata.hist(ax=ax[1], color='skyblue', density=True)\nsns.kdeplot(data, color='red', linewidth=2, label='KDE', ax=ax[1])\nax[1].set_title(\"Histogram\")\n\nplt.show()\n\n\n\n\n\nBarlett 검정: 정규성을 만족하는 경우에만 사용 가능\n\nH0: \\(σ_1^2 = σ_2^2 = ... = σ_k^2\\)\nH1: \\(σ_i ≠ σ_j\\) for some i, j\n\nLevene 검정: 정규성을 만족하지 않는 경우에도 사용 가능\n\nH0: \\(σ_1^2 = σ_2^2 = ... = σ_k^2\\)\nH1: \\(σ_i ≠ σ_j\\) for some i, j\n\n\n\n\nequal variance test\n\n#| eval: false\n\nfrom scipy.stats import bartlett, levene\n\ngroup1 = [1, 2, 3, 4, 5]\ngroup2 = [2, 3, 4, 5, 6]\ngroup3 = [3, 4, 5, 6, 7]\n\nstat, p = bartlett(group1, group2, group3)\nprint(f\"Bartlett's Test: stat={stat:.3f}, p={p:.3f}\")\n\nstat, p = levene(group1, group2, group3)\nprint(f\"Levene's Test: stat={stat:.3f}, p={p:.3f}\")",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "분산 분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/05.html#가정-검정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/05.html#가정-검정",
    "title": "분산 분석 템플릿",
    "section": "",
    "text": "검정 방법 선택 기준\n\n\n\n\n\n모수 검정 방법 선택 기준\n\n\n\n관측치 간에 독립이 아닌 경우(시간: 자기 상관이 존재, 공간: 패널, 계층, …), 각 케이스에 맞는 모형을 사용해야 함.\n\n\n\n\n표본이 정규분포를 따르는지 검정.\n따르지 않더라도 중심극한정리에 의해 표본의 크기가 충분히 크면 모수 검정을 사용할 수 있다.\nshapiro wilk 검정: 표본의 크기가 3-5000개인 데이터에 사용. 동일한 값이 많은 경우 성능이 떨어질 수 있음\n\nH0: 데이터가 정규분포를 따른다.\nH1: 데이터가 정규분포를 따르지 않는다.\n\njarque-Bera: 대표본에 사용.\n\nH0: 데이터가 정규분포를 따른다.\nH1: 데이터가 정규분포를 따르지 않는다.\n\nQ-Q plot: x축이 이론적 분위수, y축이 표본 분위수\n\n\n\nnormality test\n\n#| eval: false\n\nfrom scipy.stats import shapiro, jarque_bera, zscore, probplot\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nstat, p = shapiro(data)\n\nprint(f\"Shapiro-Wilk Test: stat={stat:.3f}, p={p:.3f}\")\n\nstat, p = jarque_bera(data)\nprint(f\"Jarque-Bera Test: stat={stat:.3f}, p={p:.3f}\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nzdata = zscore(data)\nfig, ax = plt.subplots(1, 2, figsize=(10, 3))\n\n(osm, odr), (slope, intercept, r) = probplot(zdata, plot=ax[0])\nax[0].set_title(\"Q-Q Plot\")\n\ndata.hist(ax=ax[1], color='skyblue', density=True)\nsns.kdeplot(data, color='red', linewidth=2, label='KDE', ax=ax[1])\nax[1].set_title(\"Histogram\")\n\nplt.show()\n\n\n\n\n\nBarlett 검정: 정규성을 만족하는 경우에만 사용 가능\n\nH0: \\(σ_1^2 = σ_2^2 = ... = σ_k^2\\)\nH1: \\(σ_i ≠ σ_j\\) for some i, j\n\nLevene 검정: 정규성을 만족하지 않는 경우에도 사용 가능\n\nH0: \\(σ_1^2 = σ_2^2 = ... = σ_k^2\\)\nH1: \\(σ_i ≠ σ_j\\) for some i, j\n\n\n\n\nequal variance test\n\n#| eval: false\n\nfrom scipy.stats import bartlett, levene\n\ngroup1 = [1, 2, 3, 4, 5]\ngroup2 = [2, 3, 4, 5, 6]\ngroup3 = [3, 4, 5, 6, 7]\n\nstat, p = bartlett(group1, group2, group3)\nprint(f\"Bartlett's Test: stat={stat:.3f}, p={p:.3f}\")\n\nstat, p = levene(group1, group2, group3)\nprint(f\"Levene's Test: stat={stat:.3f}, p={p:.3f}\")",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "분산 분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/05.html#분산-분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/05.html#분산-분석",
    "title": "분산 분석 템플릿",
    "section": "분산 분석",
    "text": "분산 분석\n\n\n정규성을 만족 못할 경우, 서열척도 비모수 검정 사용 가능\n코크란 Q 검정: 이항분포를 따르는 반복 측정 자료에 사용\n\n만약 대응표본이 아닐 경우 (실패, 성공) 변수를 만들어서 독립성 검정 사용\n만약 이항분포가 아닐 경우 프리드만 검정 고려\n\n독립성 검정의 cell의 기대도수가 5 미만인 경우, Boschloo’s test 혹은 Fisher’s Exact 사용\n\n만약 2x2을 넘어갈 경우 몬테카를로 시뮬레이션 사용(python으로 구현하기 복잡하다.)\n\n크루스칼, 맨휘트니: 동점이 과하게 많을 경우 permutation test, 순열 분산 분석 사용 가능\n\n하지만 ADP 환경의 scipy에서는 버전이 낮아서 permutation test를 쓸 수 없다.\n\n\n\n\n사후 검정\n\n#| eval: false\n\nfrom statsmodels.sandbox.stats.multicomp import MultiComparison\nfrom scipy.stats import ttest_ind\n\nmc = MultiComparison(data, groups).allpairtest(ttest_ind, method='bonf')\nprint(mc[0])\n\nmc.plot_simultaneous()\nplt.show()\n\n\n적합성 검정\n\n\n카이제곱 적합성 검정\n\n#| eval: false\n\ncount = df_count['count'].value_counts().sort_index()\n\n# 빈도 수가 5 미만인 경우 합침\n\ncount.loc[1] += count.loc[0]\ncount.loc[6] += count.loc[[7, 8]].sum()\ncount.drop([0, 7, 8], inplace=True)\n\n# 모수 추정\nlam = df_count['count'].mean()\npoi = poisson(lam)\n\nn = count.values.sum()\nexp = np.array([(poi.pmf(0) + poi.pmf(1)), \n       *poi.pmf(np.arange(2, 6)), \n       poi.sf(5)]) # 마지막 값은 이상 값으로\nexp *= n\n\nfrom scipy.stats import chisquare\n\n# 추정한 모수 갯수 만큼 자유도 차감\nstat, p = chisquare(count.values, exp, ddof=1)\np\n\n\n연속형\n\n이표본 검정: 콜모고로프-스미르노프 검정 사용\n일표본 검정:\n\n정규분포, 지수분포: 앤더슨-달링 검정 사용\n그 외: 몬테카를로 방법 사용\n\n\n이산형\n\n이표본: 카이제곱 독립성 검정\n일표본: 카이제곱 동질성 검정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "분산 분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/05.html#다변량-분산분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/05.html#다변량-분산분석",
    "title": "분산 분석 템플릿",
    "section": "다변량 분산분석",
    "text": "다변량 분산분석\n\n2-way ANOVA\n\n반복이 없는 경우 교효작용 검정은 불가능\n정규성, 등분산성 검정은 잔차에 대해 수행\n\n모든 집단이 정규성과 등분산성을 만족하면 오차는 N(0, σ²)를 따른다.\n고로 오차의 추정값인 잔차가 정규성과 등분산성을 만족하는지 검정으로 대체 가능\n\n\n\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\n\nmodel = ols(\"성적~C(성별)+C(교육방법)+C(성별):C(교육방법)\", data=data).fit()\natab = anova_lm(model)\natab\n\n\nfrom scipy.stats import shapiro\n\nresiduals = model.resid\n\nshapiro_test = shapiro(residuals)\nprint(f\"Shapiro-Wilk Test on Residuals: Statistic={shapiro_test.statistic}, p-value={shapiro_test.pvalue}\")\n\n\nsns.residplot(x=model.fittedvalues, y=residuals, lowess=True,\n              line_kws={'color': 'red', 'lw': 2})\nplt.show()\n\n\n교효작용이 유의하지 않을 경우 오차항에 pooling을 한다.\n\n\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\n\nmodel = ols(\"성적~C(성별)+C(교육방법)\", data=tmp).fit()\natab = anova_lm(model)\natab\n\n\nmain 효과의 사후검정은 anova와 동일\n\n하지만 interation 효과가 유의할 경우, 주효과는 무의미하다.\n\ninteraction 효과는 시각적으로 보여주는게 좋다.\n\n\ngrouped = tmp.groupby(['성별', '교육방법'])['성적'].mean().reset_index()\npivot = grouped.pivot(index='성별', columns='교육방법', values='성적')\npivot.plot(marker='o')\nplt.ylabel('평균 성적')\nplt.title('성별-교육방법 교호작용 효과')\nplt.legend(title='교육방법')\nplt.tight_layout()\nplt.show()\n\n\n2way 이상은 해석이 어려워서 잘 사용하지 않는다.\n\n\n\n반복측정 분산분석\n\n\nrepeated measures anova\n\nimport pingouin as pg\n\ndf = pd.DataFrame({\n    'subject': [1, 2, 3, 1, 2, 3, 1, 2, 3],\n    'condition': ['T1', 'T1', 'T1', 'T2', 'T2', 'T2', 'T3', 'T3', 'T3'],\n    'score': [80, 78, 85, 82, 80, 86, 88, 83, 90]\n})\n\n# 반복측정 ANOVA 수행 및 구형성 검정, 보정 포함\naov = pg.rm_anova(dv='score', within='condition', subject='subject', data=df, correction=True)\ndisplay(aov)\n\n\nsphericity가 true이면 구형성 만족\np-unc: 구형성 보정 전 p-value\np-GG-corr: Greenhouse-Geisser 보정 후 p-value\n\n구형성 위반 시 사용",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "분산 분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html",
    "title": "확률과 통계",
    "section": "",
    "text": "불확실한 상황 하에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법의 체계\n모집단으로 부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 한다.\n\n\n\n\n통계적 의사결정 과정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html#통계학",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html#통계학",
    "title": "확률과 통계",
    "section": "",
    "text": "불확실한 상황 하에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법의 체계\n모집단으로 부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 한다.\n\n\n\n\n통계적 의사결정 과정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html#확률",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html#확률",
    "title": "확률과 통계",
    "section": "확률",
    "text": "확률\n\n고전적 의미: 표본공간에서 특정 사건이 차지하는 비율\n통계적 의미: 특정 사건이 발생하는 상대도수의 극한\n\n각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html#확률-분포-정의-단계",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html#확률-분포-정의-단계",
    "title": "확률과 통계",
    "section": "확률 분포 정의 단계",
    "text": "확률 분포 정의 단계\n\n\nExperiment(확률실험): 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\nSample space(표본공간): 모든 simple event의 집합\nEvent(사건): 실험에서 발생하는 결과 (부분 집합)\nSimple event(단순사건): 원소가 하나인 사건\n확률 변수: 확률실험의 결과를 수치로 나타낸 변수",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html#확률-분포",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html#확률-분포",
    "title": "확률과 통계",
    "section": "확률 분포",
    "text": "확률 분포\n\n이산 확률 분포\n이산 표본 공간, 연속 표본공간에서 정의 가능포\n\n베르누이 시행: 각 시행은 서로 독립적이고, 실패와 성공 두 가지 결과만 존재.\n\n단 모집단의 크기가 충분히 크고, 표본(시행)의 크기가 충분히 작다면 비복원 추출에서도 유효\n평균: p\n분산: p(1-p)\n\n이항 분포: n번의 독립적인 베르누이 시행을 수행하여 성공 횟수를 측정\n\nX ~ B(n, p), \\(f(x) = \\binom{n}{x} p^x (1-p)^{n-x}\\)\n평균: np\n분산: np(1-p)\nn이 매우 크고, p가 매우 작을 때, 포아송 분포로 근사할 수 있다. (λ = np)\n\n음이항 분포\n\n정의: n번의 독립적인 베르누이 시행을 수행하여 k번 성공하고, r번 실패한 경우 (n = k + r)\n\nr번의 실패가 나오기 전까지, 성공한 횟수 x\n\nX ~ NB(r, p), \\(f(x) = \\binom{x+r-1}{x} p^x (1-p)^r\\)\n평균: \\(\\frac{rp}{1-p}\\)\n분산: \\(\\frac{rp}{(1-p)^2}\\)\n\nr번의 실패가 나오기 전까지, 시행한 횟수 x\n\n4번에서 성공을 실패로 바꿈\n\nk번의 성공이 나오기 전까지, 실패한 횟수 x\n\n1번에서 실패를 성공으로 바꿈\n\nk번의 성공이 나오기 전까지, 시행한 횟수 x\n\n\\(f(x) = \\binom{x-1}{k-1} p^k (1-p)^{x-k}\\)\nk가 1일 때 기하분포와 동일\n\nn번의 시행 횟수에서, k번 성공 또는 r번 실패한 경우: 이항분포\n\n\n기하 분포:\n\n정의:\n\n성공 확률이 p인 베르누이 시행에서 첫 성공까지의 시행 횟수\n\nX ~ G(p), \\(f(x) = (1-p)^{x-1} p, x = 1, 2, 3, ...\\)\n평균: \\(\\frac{1}{p}\\)\n분산: \\(\\frac{1-p}{p^2}\\)\n\n성공 확률이 p인 베르누이 시행에서 첫 성공까지의 실패 횟수\n\nX ~ G(p), \\(f(x) = (1-p)^x p, x = 0, 1, 2, ...\\)\n평균: \\(\\frac{1-p}{p}\\)\n분산: \\(\\frac{1-p}{p^2}\\)\n\n\n비기억 특성: \\(P(X &gt; n+k | X &gt; n) = P(X &gt; k)\\)\n\n초기하 분포: 베르누이 시행이 아닌 시행에서 성공하는 횟수\n\nX ~ H(n, N, k), \\(f(x) = \\frac{\\binom{K}{x} \\binom{N-K}{n-x}}{\\binom{N}{n}}\\)\n평균: \\(\\frac{nK}{N}\\)\n분산: \\(\\frac{nK(N-K)(N-n)}{N^2(N-1)}\\)\n\n포아송 분포: 임의의 기간동안 어떤 사건이 간헐적으로 발생할 때, 동일한 길이의 기간동안 실제 사건이 발생하는 횟수\n\nX ~ Poisson(λ), \\(f(x) = \\frac{e^{-λ} λ^x}{x!}, λ &gt; 0\\)\n평균: λ\n분산: λ\n\n\n\n\n연속 확률 분포\n연속 표본 공간에서 정의 가능\n\n균일 분포\n\n\\(f(x) = \\frac{1}{b-a}, a ≤ x ≤ b\\)\n평균: \\(\\frac{a+b}{2}\\)\n분산: \\(\\frac{(b-a)^2}{12}\\)\n\n정규 분포\n\n\\(X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)\\)\n선형 변환: \\(Y = aX + b \\sim N(aμ + b, a^2σ^2)\\)\n\nt 분포\n\n자유도가 커질수록 표준 정규분포에 근사함.\n\\(\\frac{Z}{\\sqrt{V/n}} \\sim t(n)\\), Z: 표준정규분포, V: 자유도가 n인 카이제곱분포\n\nf 분포\n\n\\(F = \\frac{X_1/ν_1}{X_2/ν_2}\\), \\(X_1 \\sim χ^2(ν_1)\\), \\(X_2 \\sim χ^2(ν_2)\\), X1과 X2는 서로 독립\n\n감마 분포\n\nα: 분포의 형태 결정, θ: 분포의 크기 결정\n평균: αθ\n분산: αθ²\n카이제곱 분포: α = v/2, θ = 2 인 감마분포\n\n\\(Z_i \\sim N(0,1)\\)일 때, \\(Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)\\)\n\\(X_i\\)가 서로 독립이고, 자유도가 \\(ν_i\\)인 카이제곱분포를 따른다면, \\(X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)\\)\n자유도가 커질수록 기댓값을 중심으로 모이고, 대칭에 가까워진다.\n\n지수 분포: α = 1, θ = 1/λ 인 감마분포\n\n\\(X ~ Exp(λ = \\frac{1}{θ})\\), f(x) = \\(λe^{-λx}, x &gt; 0\\)\nθ: 평균 사건 발생 간격, λ: 단위 시간당 사건 발생 횟수\n포아송 분포에서 사건 발생 간격의 분포\n\\(\\sum_{i=1}^{n} X_i \\sim Γ(n, θ)\\), \\(θ = 1/λ\\)\n비기억 특성을 가진다: \\(p(X &gt; s + t | X &gt; s) = p(X &gt; t) = e^{-λt}\\)\n독립적으로 동일한 지수분포를 따르는 확률변수 n개의 합은 \\(α = n, θ = \\frac{1}{λ}\\)인 감마분포를 따른다.\n\n\n\n\n\n다변량 분포\n\n다항 분포: n번의 독립적인 베르누이 시행을 수행하여 k개의 범주로 분류\n\nX ~ M(n, p1, p2, …, pk), \\(f(x_1, x_2, ..., x_k) = \\frac{n!}{x_1! x_2! ... x_k!} p_1^{x_1} p_2^{x_2} ... p_k^{x_k}\\)\n평균: \\([np_1, np_2, ..., np_k]\\)\n분산: \\([np_1(1-p_1), np_2(1-p_2), ..., np_k(1-p_k)]\\)\n공분산: \\(-np_ip_j (i ≠ j)\\)\n독립인 변수의 갯수는 k-1개 (k개의 사건)\n\n\n\n\n샘플링\n\n\n분포의 동질성 검정\n\n연속형\n\n이표본 검정: 콜모고로프-스미르노프 검정 사용\n일표본 검정:\n\n정규분포, 지수분포: 앤더슨-달링 검정 사용\n그 외: 몬테카를로 방법 사용\n\n\n이산형\n\n이표본: 카이제곱 독립성 검정\n일표본: 카이제곱 동질성 검정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html#표본의-분포",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html#표본의-분포",
    "title": "확률과 통계",
    "section": "표본의 분포",
    "text": "표본의 분포\n\n샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.\n통계량: 표본의 특성을 나타내는 값\n추정량: 아래의 조건을 만족하는 통계량\n\n불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.\n효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.\n\n\n\n표본 평균의 분포\n\n모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 \\(σ^2\\)이면, \\(\\bar{X}\\)의 평균은 μ이고, 분산은 \\(σ^2/n\\)인 정규분포를 따른다.\n\n단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리1)\n\n만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.\n\n단 이때는 모집단이 정규분포를 따라야 한다.\n\n\n\n\n표본 분산의 분포\n\n정규 모집단으로 부터 나온 표본의 분산 S에 대하여, \\(\\frac{(n-1)S^2}{σ^2}\\)은 자유도가 n-1인 카이제곱 분포를 따른다.\n\n모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.\n\n두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html#추정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html#추정",
    "title": "확률과 통계",
    "section": "추정",
    "text": "추정\n\n통계적 추론: 모집단에서 추출된 표본의 통계량으로부터 모수를 추론하는 것\n\n추정\n\n점추정\n구간추정\n\n가설 검정\n\n\n\n점 추정\n\n불편성\n\n\\(E(\\hat{\\theta}) = θ\\)\nbias = \\(E(\\hat{\\theta}) - \\theta\\)\n\n보통 sample size가 커질수록 bias는 0에 수렴\n\n\\(\\bar{X}, X_n\\)은 μ의 불편추정량이다.\n\n최소분산\n\n\\(Var(\\bar{X})\\)가 \\(Var(X_n)\\)보다 분산이 작아서 더 좋은 추정량\n\\(MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2\\)\n\n큰 오차에 더 큰 페널티를 주기 위해 제곱\n\n\n\n\n\n\n대표적인 불편추정량\n\n\n\n전부 중심극한의정리를 적용할 수 있다. (비율은 0과 1의 평균이므로)\n모평균, 모비율의 차이는 서로 독립이라는 가정이 필요하다.\n\n\n\n구간 추정\n\nα: 유의수준\n1 - α: 신뢰수준2\n(θ_L, θ_U) = (1 - α) × 100% 신뢰구간\n\n\n(\\(θ_L, θ_U\\)) 이 충분이 높은 가능성으로 미지의 모수 θ를 포함해야 한다\n구간이 충분히 좁아야 한다\n\n표준 정규분포에서 0을 중심으로 대칭일 때 길이가 짧다.\n고로 신뢰구간이 대칭임\n\n\n\n\n표본의 크기 결정\n특정 오차 아래로 하는 표본의 수 구하는 법\n\n그냥 표본오차가 목표 오차보다 작게 하는 값을 구하면 됨.\n모비율을 모를 때는 일단 0.5로 보수적으로 놓고 계산",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html#모분산-추정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html#모분산-추정",
    "title": "확률과 통계",
    "section": "모분산 추정",
    "text": "모분산 추정\n\n카이제곱 분포는 가장 짧은 신뢰구간을 구하기 쉽지 않음\n\n그냥 쉽게 구하기 위해 \\((x^2_{α/2}, x^2_{1-α/2})\\)를 사용\n\n모분산의 신뢰구간: \\((\\frac{(n-1)s^2}{x^2_{(1-\\alpha)/2}(n-1)}, \\frac{(n-1)s^2}{x^2_{\\alpha/2}(n-1)})\\)\n표본의 수가 적을수록, 카이제곱 분포의 신뢰구간은 더 길어진다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/10.html#footnotes",
    "href": "posts/03_archives/completed_project/adp_실기/notes/10.html#footnotes",
    "title": "확률과 통계",
    "section": "각주",
    "text": "각주\n\n\n모집단의 분포와 상관 없이, 표본의 평균은 정규분포에 수렴한다는 정리. 이항분포의 경우, P(X=c) ~ P(c - 0.5 &lt; X &lt; c + 0.5)로 근사 가능하다는 라플라스의 정리를 일반화한 것↩︎\n샘플링을 무한히 반복했을 때, 이들의 신뢰 구간 중 95%의 구간이 실제 모수를 포함한다. 즉, 구간이 확률 변수이다.↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "확률과 통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/06.html#넷-중-높은-값",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/06.html#넷-중-높은-값",
    "title": "최솟값, 최댓값 그리고 혼합 분포",
    "section": "넷 중 높은 값",
    "text": "넷 중 높은 값\n\nimport numpy as np\nfrom empiricaldist import Pmf\n\nn = 10000\na = np.random.randint(1, 7, size=(n, 4))\na.sort(axis=1)\nt = a[:, 1:].sum(axis=1)\npmf_best3 = Pmf.from_seq(t)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "최솟값, 최댓값 그리고 혼합 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/06.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/06.html#연습문제",
    "title": "최솟값, 최댓값 그리고 혼합 분포",
    "section": "연습문제",
    "text": "연습문제\n\n7-1\n\nn = 10000\na = np.random.randint(1, 7, size=(n, 4))\na.sort(axis=1)\nt = a[:, 1:].sum(axis=1)\npmf_best3 = Pmf.from_seq(t)\ncdf_best3 = pmf_best3.make_cdf()\n\n\nfrom empiricaldist import Cdf\nimport matplotlib.pyplot as plt\n\nstandard = [15,14,13,12,10,8]\nstandard_pmf = Pmf.from_seq(standard)\n\ncdf_best3.plot(label='best 3 of 4', color='C1', ls='--')\ncdf_standard = Cdf.from_seq(standard)\n\ncdf_standard.step(label='standard set', color='C7')\nplt.ylabel('CDF');\n\ncdf_max_dist6 = cdf_best3.max_dist(6)\ncdf_min_dist6 = cdf_best3.min_dist(6)\n\n\n\n\n\n\n\n\n\nprint(f\"Best 3 of 4 - 평균: {pmf_best3.mean():.2f}, 표준편차: {pmf_best3.std():.2f}\")\nprint(f\"Standard array - 평균: {standard_pmf.mean():.2f}, 표준편차: {standard_pmf.std():.2f}\")\n\n\nprob_less_than_8 = cdf_best3(7)\nprint(f\"Best 3 of 4에서 8보다 작은 값이 나올 확률: {prob_less_than_8:.4f}\")\n\nprint(f\"6번 굴렸을 때 최소 하나가 8보다 작을 확률: {(1 - cdf_min_dist6(8)):.4f}\")\n\nprob_greater_than_15 = 1 - cdf_best3(15)\nprint(f\"Best 3 of 4에서 15보다 큰 값이 나올 확률: {prob_greater_than_15:.4f}\")\nprint(f\"6번 굴렸을 때 최소 하나가 15보다 클 확률: {(1 - cdf_max_dist6(15)):.4f}\")\n\nBest 3 of 4 - 평균: 12.28, 표준편차: 2.85\nStandard array - 평균: 12.00, 표준편차: 2.38\nBest 3 of 4에서 8보다 작은 값이 나올 확률: 0.0545\n6번 굴렸을 때 최소 하나가 8보다 작을 확률: 0.5247\nBest 3 of 4에서 15보다 큰 값이 나올 확률: 0.1324\n6번 굴렸을 때 최소 하나가 15보다 클 확률: 0.5735\n\n\n\n\n7-2\n\ndef update_dice(pmf, data):\n    hypos = pmf.qs\n    likelihood = 1 / hypos\n    likelihood[data &gt; hypos] = 0\n    pmf *= likelihood\n    pmf.normalize()\n\npmf = Pmf.from_seq([6, 8, 10])\nupdate_dice(pmf, 1)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n6\n0.425532\n\n\n8\n0.319149\n\n\n10\n0.255319\n\n\n\n\n\n\n\n\nimport pandas as pd\n\ndef make_mixture(pmf, pmf_seq):\n    df = pd.DataFrame(pmf_seq).fillna(0).transpose()\n    df *= np.array(pmf)\n    total = df.sum(axis=1)\n    return Pmf(total)\n\npmf_6 = Pmf.from_seq(range(1, 7))  # 6면체 주사위\npmf_8 = Pmf.from_seq(range(1, 9))  # 8면체 주사위\npmf_10 = Pmf.from_seq(range(1, 11)) # 10면체 주사위\n\nmixture = make_mixture(pmf, [pmf_6, pmf_8, pmf_10])\nprob_6_damage = mixture[6] if 6 in mixture.qs else 0\nprint(f\"\\nProbability of 6 points of damage: {prob_6_damage:.4f}\")\n\n\nProbability of 6 points of damage: 0.1363\n\n\n\n\n7-3\n\nmean = 950\nstd = 50\n\nsample = np.random.normal(mean, std, size=365)\npmf = Pmf.from_seq(sample)\ncdf = pmf.make_cdf()\n\n\nmeans = []\nn_values = np.arange(1, 20)\n\nfor n in n_values:\n    cdf_max_dist = cdf.max_dist(n)\n    mean_max = cdf_max_dist.mean()\n    means.append(mean_max)\n\nmeans = np.array(means)\ntarget = 1000\nclosest_idx = np.argmin(np.abs(means - target))\noptimal_n = n_values[closest_idx]\nclosest_mean = means[closest_idx]\n\nprint(f\"\\n결과:\")\nprint(f\"1000g에 가장 가까운 평균을 만드는 n: {optimal_n}\")\nprint(f\"해당 n에서의 평균: {closest_mean:.2f}g\")\nprint(f\"목표값과의 차이: {abs(closest_mean - target):.2f}g\")\n\n\n결과:\n1000g에 가장 가까운 평균을 만드는 n: 4\n해당 n에서의 평균: 1002.83g\n목표값과의 차이: 2.83g",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "최솟값, 최댓값 그리고 혼합 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html",
    "title": "확률",
    "section": "",
    "text": "import pandas as pd\n\ngss = pd.read_csv('https://raw.githubusercontent.com/AllenDowney/ThinkBayes2/master/data/gss_bayes.csv')\nbanker = (gss['indus10'] == 6870)\nprint(f'은행원 수: { banker.sum() }')\nprint(f'은행원 비율: { banker.mean() }')\n\n은행원 수: 728\n은행원 비율: 0.014769730168391155",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#확률",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#확률",
    "title": "확률",
    "section": "확률",
    "text": "확률\n\ndef prob(A):\n    \"\"\"A의 확률 계산\"\"\"\n    return A.mean()\nprob(banker)\n\nnp.float64(0.014769730168391155)\n\n\n\nfemale = (gss['sex'] == 2)\nprob(female)\n\nnp.float64(0.5378575776019476)\n\n\n\nliberal = (gss['polviews'] &lt;= 3)\nprob(liberal)\n\nnp.float64(0.27374721038750255)\n\n\n\ndemocrat = (gss['partyid'] &lt;= 1)\nprob(democrat)\n\nnp.float64(0.3662609048488537)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#and-논리곱",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#and-논리곱",
    "title": "확률",
    "section": "And 논리곱",
    "text": "And 논리곱\n\nprob(banker & democrat)\n\nnp.float64(0.004686548995739501)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#조건부-확률",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#조건부-확률",
    "title": "확률",
    "section": "조건부 확률",
    "text": "조건부 확률\n\nselected = democrat[liberal]\nprob(selected)\n\nnp.float64(0.5206403320240125)\n\n\n진보(liberal) 성향 중 민주당(democrat)의 비율\n\ndef conditional(proposition, given):\n  return prob(proposition[given])\nconditional(liberal, given=female)\n\nnp.float64(0.27581004111500884)\n\n\n\nprob(liberal & female) / prob(female)\n\nnp.float64(0.2758100411150089)\n\n\n\\(P(A|B) = \\frac{P(A and B)}{P(B)}\\)\n\nprint(prob(female) * conditional(liberal, given=female))\nprint(prob(female & liberal))\n\n0.14834652059241224\n0.14834652059241227\n\n\n\\(P(A and B) = P(B)P(A|B)\\)\n\nconditional(female, given=liberal)\n\nnp.float64(0.5419106203216483)\n\n\n조건부 확률은 교환 불가\n\nprint(conditional(female, given=liberal))\nprint(prob(female) * conditional(liberal, given=female) / prob(liberal))\n\n0.5419106203216483\n0.5419106203216482\n\n\n\\(P(A|B) = \\frac{P(A)*P(B|A)}{P(B)}\\)\n\nmale = (gss['sex'] == 1)\nsum(prob(gss['sex'] == i) * conditional(banker, gss['sex'] == i) for i in range(1, 3))\n\nnp.float64(0.014769730168391153)\n\n\n\\(P(A) = P(B_1)P(A|B_1) + P(B_2)P(A|B_2)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#조건과-논리곱",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#조건과-논리곱",
    "title": "확률",
    "section": "조건과 논리곱",
    "text": "조건과 논리곱\n\nconditional(female, given=liberal & democrat)\n\nnp.float64(0.576085409252669)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/00.html#연습문제",
    "title": "확률",
    "section": "연습문제",
    "text": "연습문제\n\n1-1\n\nprob(female), prob(liberal), prob(democrat)\n\n(np.float64(0.5378575776019476),\n np.float64(0.27374721038750255),\n np.float64(0.3662609048488537))\n\n\n\n\n1-2\n\nconditional(liberal, given=democrat), conditional(democrat, given=liberal)\n\n(np.float64(0.3891320002215698), np.float64(0.5206403320240125))\n\n\n\n\n1-3\n\nyoung = (gss['age'] &lt; 30)\nold = (gss['age'] &gt;= 65)\nconservative = (gss['polviews'] &gt;= 5)\nprob(young & liberal), conditional(liberal, given=young), prob(old & conservative), conditional(old, given=conservative)\n\n(np.float64(0.06579427875836884),\n np.float64(0.338517745302714),\n np.float64(0.06701156421180766),\n np.float64(0.19597721609113564))",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "확률"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/01.html#이론",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/01.html#이론",
    "title": "베이즈 정리",
    "section": "이론",
    "text": "이론\n사전확률을 이용해서 사후확률을 구하는 과정을 베이즈 갱신이라고 한다.\n\\(P(H|D) = \\frac{P(D|H)P(H)}{P(D)}\\)\n\n\\(P(H)\\): 가설의 확률. 사전확률\n\\(P(H|D)\\): 데이터가 주어졌을 때 가설의 확률. 사후확률\n\\(P(D|H)\\): 가설하에서 데이터가 나올 확률. 가능도(우도)\n\\(P(D)\\): 데이터의 확률. \\(P(D) = \\sum P(H_i)P(D|H_i)\\)\n\n가설은 상호 배제와 전체 포괄 가정이 필요하다.\n즉, 가설은 서로 배타적이어야 하고, 모든 가능한 가설을 포함해야 한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "베이즈 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/01.html#베이즈-테이블",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/01.html#베이즈-테이블",
    "title": "베이즈 정리",
    "section": "베이즈 테이블",
    "text": "베이즈 테이블\n\n가설과 데이터를 정리한다.\n사전확률을 구한다.\n각 가설 하에서의 데이터 가능도를 구한다.\n베이즈 테이블을 정리한다.\n\n\n쿠키 문제\n\n그릇 1에서 바닐라 쿠키를 집을 확률은 3/4\n그릇 2에서 바닐라 쿠키를 집을 확률은 1/2\n바닐라 쿠키를 집었을 때, 그릇 i에서 집었을 확률은?\n\n\nimport pandas as pd\n\ntable = pd.DataFrame(index=['bowl1', 'bowl2'])\ntable['prior'] = 1/2, 1/2\ntable['likelihood'] = 3/4, 1/2\ntable['unnorm'] = table['prior'] * table['likelihood']\ntable['posterior'] = table['unnorm'] / table['unnorm'].sum()\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\nbowl1\n0.5\n0.75\n0.375\n0.6\n\n\nbowl2\n0.5\n0.50\n0.250\n0.4\n\n\n\n\n\n\n\n\n\n주사위 문제\n\n육면체, 팔면체, 십이면체 주사위가 든 상자가 있다.\n이 중 임의로 하나를 집어 굴렸더니 1이 나왔다.\n이 주사위가 육면체일 확률은?\n\n\ndef update(table):\n  table['unnorm'] = table['prior'] * table['likelihood']\n  prob_data = table['unnorm'].sum()\n  table['posterior'] = table['unnorm'] / prob_data\n  return prob_data\n\nfrom fractions import Fraction\n\ntable2 = pd.DataFrame(index=[6, 8, 12])\ntable2['prior'] = Fraction(1, 3)\ntable2['likelihood'] = Fraction(1, 6), Fraction(1, 8), Fraction(1, 12)\nprob_data = update(table2)\ntable2\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n6\n1/3\n1/6\n1/18\n4/9\n\n\n8\n1/3\n1/8\n1/24\n1/3\n\n\n12\n1/3\n1/12\n1/36\n2/9\n\n\n\n\n\n\n\n\n\n몬티홀 문제\n\n1,2,3 번호가 붙은 세 개의 문 중 하나에 상품이 있다.\n참가자는 문 하나를 선택하고, 사회자는 나머지 두 문 중 상품이 없는 문을 열어 보여준다.\n참가자는 처음 선택한 문을 유지하거나 다른 문으로 바꿀 수 있다.\n당신은 1번 문을 선택했고, 사회자는 문 3을 열었다.\n다시 참가자가 문을 선택했을 때 상품이 있는 문을 선택할 확률은?\n\n\ntable3 = pd.DataFrame(index=[1, 2, 3])\ntable3['prior'] = Fraction(1, 3)\ntable3['likelihood'] = Fraction(1, 2), 1, 0\nprob_data = update(table3)\ntable3\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n1\n1/3\n1/2\n1/6\n1/3\n\n\n2\n1/3\n1\n1/3\n2/3\n\n\n3\n1/3\n0\n0\n0",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "베이즈 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/01.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/01.html#연습문제",
    "title": "베이즈 정리",
    "section": "연습문제",
    "text": "연습문제\n\n2-1\n\ntable = pd.DataFrame(index=['normal', 'weird'])\ntable['prior'] = Fraction(1, 2)\ntable['likelihood'] = Fraction(1, 2), 1\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\nnormal\n1/2\n1/2\n1/4\n1/3\n\n\nweird\n1/2\n1\n1/2\n2/3\n\n\n\n\n\n\n\n\n\n2-2\n\ntable = pd.DataFrame(index=['bb', 'bg', 'gb', 'gg'])\ntable['prior'] = Fraction(1, 4)\ntable['likelihood'] = 0, 1, 1, 1\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\nbb\n1/4\n0\n0\n0\n\n\nbg\n1/4\n1\n1/4\n1/3\n\n\ngb\n1/4\n1\n1/4\n1/3\n\n\ngg\n1/4\n1\n1/4\n1/3\n\n\n\n\n\n\n\n\n\n2-3\n\ntable = pd.DataFrame(index=[1, 2, 3])\ntable['prior'] = Fraction(1, 3)\ntable['likelihood'] = 1, 0, 1\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n1\n1/3\n1\n1/3\n1/2\n\n\n2\n1/3\n0\n0\n0\n\n\n3\n1/3\n1\n1/3\n1/2\n\n\n\n\n\n\n\n\ntable = pd.DataFrame(index=[1, 2, 3])\ntable['prior'] = Fraction(1, 3)\ntable['likelihood'] = 0, 1, 0\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n1\n1/3\n0\n0\n0\n\n\n2\n1/3\n1\n1/3\n1\n\n\n3\n1/3\n0\n0\n0\n\n\n\n\n\n\n\n\n\n2-4\n\ntable = pd.DataFrame(index=['1994_1996', '1996_1994'])\ntable['prior'] = Fraction(1, 2)\ntable['likelihood'] = Fraction(2, 10) * Fraction(2, 10), Fraction(14, 100) * Fraction(1, 10)\nprob_data = update(table)\ntable\n\n\n\n\n\n\n\n\nprior\nlikelihood\nunnorm\nposterior\n\n\n\n\n1994_1996\n1/2\n1/25\n1/50\n20/27\n\n\n1996_1994\n1/2\n7/500\n7/1000\n7/27",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "베이즈 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/18.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/18.html",
    "title": "MCMC",
    "section": "",
    "text": "from scipy.stats import gamma, poisson\nimport numpy as np\nfrom empiricaldist import Pmf\n\nalpha = 1.4\ndist = gamma(alpha)\n\nlams = np.linspace(0, 10, 101)\nprior_pmf = Pmf(dist.pdf(lams), lams)\nprior_pmf.normalize()\n\ndata = 4\nlikelihood = poisson.pmf(data, lams)\nposterior = prior_pmf * likelihood\nposterior.normalize()\n\nnp.float64(0.05015532557804499)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "MCMC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/18.html#사전예측분포",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/18.html#사전예측분포",
    "title": "MCMC",
    "section": "사전예측분포",
    "text": "사전예측분포\n\nsample_prior = dist.rvs(1000)\nsample_prior_pred = poisson.rvs(sample_prior)\npmf_prior_pred = Pmf.from_seq(sample_prior_pred)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "MCMC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/18.html#mcmc",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/18.html#mcmc",
    "title": "MCMC",
    "section": "MCMC",
    "text": "MCMC",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "MCMC"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/13.html#a의-키",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/13.html#a의-키",
    "title": "비교",
    "section": "A의 키",
    "text": "A의 키\n\n미국 성인 남성 중 두명을 임의로 골랐다.\nA가 B보다 큰 것 같을 때, A의 키는 얼마인가\n\n\nimport numpy as np\nfrom scipy.stats import norm\nfrom empiricaldist import Pmf\n\nmean = 178\nstd = 7.7\n\nqs = np.arange(mean-24, mean+24, 0.5)\nps = norm(mean, std).pdf(qs)\nprior = Pmf(ps, qs)\nprior.normalize()\n\nnp.float64(1.9963309462450582)\n\n\n\nimport pandas as pd\n\ndef make_joint(pmf1, pmf2):\n  X, Y = np.meshgrid(pmf1, pmf2)\n  return pd.DataFrame(X * Y, columns=pmf1.qs, index=pmf2.qs)\n\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\njoint = make_joint(prior, prior)\nplt.contour(joint.columns, joint.index, joint, linewidths=2)\nplt.xlabel('A의 키(cm)')\nplt.ylabel('B의 키(cm)')\n\nText(0, 0.5, 'B의 키(cm)')\n\n\n\n\n\n\n\n\n\n\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = np.where((X &gt; Y), 1, 0)\nlikelihood = pd.DataFrame(a, index=x, columns=y)\nposterior = joint * likelihood\n\n\ndef normalize(pdf):\n  prob_data = joint.to_numpy().sum()\n  pdf /= prob_data\n  return prob_data\nnormalize(posterior)\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_B = Pmf(posterior.sum(axis=1))\nmarginal_A.normalize()\nmarginal_B.normalize()\nmarginal_A.plot()\nmarginal_B.plot()\nprior.plot()\n\n\n\n\n\n\n\n\n\nmarginal_A.mean(), marginal_B.mean()\n\n(np.float64(182.38728123421686), np.float64(173.60286000233393))\n\n\n\nA의 키가 170이라면?\n\n\npmf = Pmf(posterior[170])\npmf.normalize()\npmf.plot()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비교"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/13.html#연습-문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/13.html#연습-문제",
    "title": "비교",
    "section": "연습 문제",
    "text": "연습 문제\n\n11-1\n\npmf = Pmf(posterior.loc[180])\npmf.normalize()\npmf.plot()\n\n\n\n\n\n\n\n\n\n\n11-2\n\nmean = 163\nstd = 7.3\nqs = np.arange(mean-24, mean+24, 0.5)\nps = norm(mean, std).pdf(qs)\ngirl_prior = Pmf(ps, qs)\ngirl_prior.normalize()\njoint = make_joint(marginal_A, girl_prior)\n\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = np.where((X - Y &gt;= 15), 1, 0)\nlikelihood = pd.DataFrame(a, index=y, columns=x)\nposterior = joint * likelihood\nnormalize(posterior)\nmarginal_A.plot(label='c 정보 이전')\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_A.normalize()\nmarginal_A.plot(label='c 정보 이후')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n11-3\n\nmean_A = 1600\nstd_A = 100\nqs_A = np.arange(mean_A - 300, mean_A + 300, 10)\nps_A = norm(mean_A, std_A).pdf(qs_A)\nprior_A = Pmf(ps_A, qs_A)\nprior_A.normalize()\n\nmean_B = 1800\nstd_B = 100\nqs_B = np.arange(mean_B - 300, mean_B + 300, 10)\nps_B = norm(mean_B, std_B).pdf(qs_B)\nprior_B = Pmf(ps_B, qs_B)\nprior_B.normalize()\n\njoint = make_joint(prior_A, prior_B)\n\n\ndef logistic_prob(r_a, r_b):\n    return 1 / (1 + 10**((r_b - r_a) / 400))\nx, y = joint.columns, joint.index\nX, Y = np.meshgrid(x, y)\na = 1 / (1 + 10**((Y - X) / 400))\nlikelihood = pd.DataFrame(a, index=y, columns=x)\n\nposterior = joint * likelihood\nnormalize(posterior)\nmarginal_A = Pmf(posterior.sum(axis=0))\nmarginal_A.normalize()\n\nprint(marginal_A.max_prob(), marginal_A.mean())\nmarginal_A.plot()\n\n1640 1636.648345528236",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비교"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/08.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/08.html",
    "title": "의사결정분석",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\ndf1 = pd.read_csv('https://raw.githubusercontent.com/AllenDowney/ThinkBayes2/master/data/showcases.2011.csv', index_col=0, skiprows=[1]).dropna().transpose()\ndf2 = pd.read_csv('https://raw.githubusercontent.com/AllenDowney/ThinkBayes2/master/data/showcases.2012.csv', index_col=0, skiprows=[1]).dropna().transpose()\ndf = pd.concat([df1, df2], ignore_index=True)\ndf\n\n\n\n\n\n\n\n\nShowcase 1\nShowcase 2\nBid 1\nBid 2\nDifference 1\nDifference 2\n\n\n\n\n0\n50969.0\n45429.0\n42000.0\n34000.0\n8969.0\n11429.0\n\n\n1\n21901.0\n34061.0\n14000.0\n59900.0\n7901.0\n-25839.0\n\n\n2\n32815.0\n53186.0\n32000.0\n45000.0\n815.0\n8186.0\n\n\n3\n44432.0\n31428.0\n27000.0\n38000.0\n17432.0\n-6572.0\n\n\n4\n24273.0\n22320.0\n18750.0\n23000.0\n5523.0\n-680.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n308\n25375.0\n31986.0\n36000.0\n32000.0\n-10625.0\n-14.0\n\n\n309\n24949.0\n30696.0\n20500.0\n31000.0\n4449.0\n-304.0\n\n\n310\n23662.0\n22329.0\n26000.0\n20000.0\n-2338.0\n2329.0\n\n\n311\n23704.0\n34325.0\n23800.0\n34029.0\n-96.0\n296.0\n\n\n312\n20898.0\n23876.0\n28000.0\n25000.0\n-7102.0\n-1124.0\n\n\n\n\n313 rows × 6 columns\nfrom scipy.stats import gaussian_kde\nfrom empiricaldist import Pmf\n\ndef kde_from_sample(sample, qs):\n    kde = gaussian_kde(sample)\n    ps = kde(qs)\n    pmf = Pmf(ps, qs)\n    pmf.normalize()\n    return pmf\n\nqs = np.linspace(0, 80000, 81)\nprior1 = kde_from_sample(df['Showcase 1'], qs)\nprior2 = kde_from_sample(df['Showcase 2'], qs)\n\nprior1.plot(label='진열대 1번의 사전확률')\nprior2.plot(label='진열대 2번의 사전확률')\nplt.xlabel('진열대 물건 총 금액')\nplt.ylabel('PMF')\nplt.legend()\nsample_diff1 = df['Bid 1'] - df['Showcase 1']\nsample_diff2 = df['Bid 2'] - df['Showcase 2']\n\nqs = np.linspace(-40000, 20000, 61)\nkde_diff1 = kde_from_sample(sample_diff1, qs)\nkde_diff2 = kde_from_sample(sample_diff2, qs)\n\nkde_diff1.plot(label='1번의 차이')\nkde_diff2.plot(label='2번의 차이')\nplt.xlabel('차이 금액')\nplt.ylabel('PMF')\nplt.legend()\nfrom scipy.stats import norm\n\nstd_diff1 = sample_diff1.std()\nstd_diff2 = sample_diff2.std()\n\nerror_dist1 = norm(0, std_diff1)\nerror_dist2 = norm(0, std_diff2)\n\nguess1 = 23000\nerror1 = guess1 - prior1.qs\nlikelihood1 = error_dist1.pdf(error1)\nposterior1 = prior1 * likelihood1\nposterior1.normalize()\n\nguess2 = 38000\nerror2 = guess2 - prior2.qs\nlikelihood2 = error_dist2.pdf(error2)\nposterior2 = prior2 * likelihood2\nposterior2.normalize()\n\nposterior1.plot(label='1번의 사후분포')\nposterior2.plot(label='2번의 사후분포')\nplt.legend()\ndef compute_prob_win(my_diff, op_diff):\n    if my_diff &gt; 0:\n        return 0\n    p1 = np.mean(op_diff &gt; 0)\n    p2 = np.mean(op_diff &lt; my_diff)\n    return p1 + p2\n\nxs = np.linspace(-30000, 5000, 121)\nys1 = [compute_prob_win(x, sample_diff2) for x in xs]\nys2 = [compute_prob_win(x, sample_diff1) for x in xs]\n\nplt.plot(xs, ys1, label='1번 참가자가 이길 확률')\nplt.plot(xs, ys2, label='2번 참가자가 이길 확률')\nplt.legend()\ndef total_prob_win(bid, posterior, op_diff):\n    total = 0\n    for price, prob in posterior.items():\n        diff = bid - price\n        total += prob * compute_prob_win(diff, op_diff)\n    return total\n\nbids1 = posterior1.qs\nprobs1 = [total_prob_win(bid, posterior1, sample_diff2) for bid in bids1]\nprob1_wins = pd.Series(probs1, index=bids1)\n\nbids2 = posterior2.qs\nprobs2 = [total_prob_win(bid, posterior2, sample_diff1) for bid in bids2]\nprob2_wins = pd.Series(probs2, index=bids2)\n\nprob1_wins.plot(label=f'1번 참가자의 이길 확률 (최적: {prob1_wins.idxmax()})')\nplt.axvline(x=prob1_wins.idxmax(), color='b', linestyle=':')\n\nprob2_wins.plot(label=f'2번 참가자의 이길 확률 (최적: {prob2_wins.idxmax()})')\nplt.axvline(x=prob2_wins.idxmax(), color='r', linestyle=':')\n\nplt.legend()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "의사결정분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/08.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/08.html#연습문제",
    "title": "의사결정분석",
    "section": "연습문제",
    "text": "연습문제\n\n9-6\n열차 도착 시간에 대한 분포(z)\n\nobserved_gap_times = [\n    428.0, 705.0, 407.0, 465.0, 433.0, 425.0, 204.0, 506.0, 143.0, 351.0, \n    450.0, 598.0, 464.0, 749.0, 341.0, 586.0, 754.0, 256.0, 378.0, 435.0, \n    176.0, 405.0, 360.0, 519.0, 648.0, 374.0, 483.0, 537.0, 578.0, 534.0, \n    577.0, 619.0, 538.0, 331.0, 186.0, 629.0, 193.0, 360.0, 660.0, 484.0, \n    512.0, 315.0, 457.0, 404.0, 740.0, 388.0, 357.0, 485.0, 567.0, 160.0, \n    428.0, 387.0, 901.0, 187.0, 622.0, 616.0, 585.0, 474.0, 442.0, 499.0, \n    437.0, 620.0, 351.0, 286.0, 373.0, 232.0, 393.0, 745.0, 636.0, 758.0]\nzs = np.array(observed_gap_times) / 60\nqs = np.linspace(0, 20, 101)\npmf_z = kde_from_sample(zs, qs)\n\nlikelihood = pmf_z.qs\nposterior_z = pmf_z * likelihood\nposterior_z.normalize()\n\npmf_z.plot(label=f'사전분포 (평균: {pmf_z.mean():.2f})')\nplt.axvline(x=pmf_z.mean(), color='b', linestyle=':')\n\nposterior_z.plot(label=f'사후분포 (평균: {posterior_z.mean():.2f})')\nplt.axvline(x=posterior_z.mean(), color='r', linestyle=':')\nplt.legend()\nplt.title('열차 도착 시간 간격')\nplt.xlabel('시간')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n내가 관찰하는 열차 지연 시간은 실제 열차 지연시간보다 더 길다.\n예를들어 5분 간격의 길이와 15분 간격의 길이가 있다면, 더 긴 간격의 중간에 내가 도착할 확률이 더 높기 때문\n그래서 사후분포가 오른쪽으로 이동함. (신기하네)\n\n내가 도착했을 때 이미 지난 시간에 대한 분포(x)\n\ndef make_mixture(pmf, pmf_seq):\n    df = pd.DataFrame(pmf_seq).fillna(0).transpose()\n    df *= np.array(pmf)\n    total = df.sum(axis=1)\n    return Pmf(total)\n\ndef make_elapsed_dist(gap, qs):\n    qs = qs[qs &lt;= gap]\n    n = len(qs)\n    return Pmf(1/n, qs)\n\nqs = posterior_z.qs\npmf_seq = [make_elapsed_dist(gap, qs) for gap in qs]\npmf_x = make_mixture(posterior_z, pmf_seq)\n\n\nz의 각 간격별 지난 시간을 균등분포로 지정\n\n\nfrom scipy.stats import poisson\n\nlam = 2\nnum_passengers = 10\nlikelihood = poisson(lam * pmf_x.qs).pmf(num_passengers)\nposterior_x = pmf_x * likelihood\nposterior_x.normalize()\npmf_x.plot(label='사전분포', color='C1')\nposterior_x.plot(label='사후분포', color='C2')\nplt.legend()\nplt.title('10명이 기다리고 있는것을 관찰한 이전과 이후 분포')\n\nText(0.5, 1.0, '10명이 기다리고 있는것을 관찰한 이전과 이후 분포')\n\n\n\n\n\n\n\n\n\n\n일반적으로 분당 2명의 승객이 기다림. 가능도는 포아송 분포를 따름.\n10명이 기다리고 있는것을 관찰했을 때의 사후분포를 구해준다.\n\n다음 열차 도착까지의 남은 시간에 대한 분포\n\nposterior_y = Pmf.sub_dist(posterior_z, posterior_x)\nnonneg = (posterior_y.qs &gt;= 0)\nposterior_y = Pmf(posterior_y[nonneg])\nposterior_y.normalize()\n\nposterior_x.make_cdf().plot(label='x의 사후분포', color='C2')\nposterior_y.make_cdf().plot(label='y의 사후분포', color='C3')\nposterior_z.make_cdf().plot(label='z의 사후분포', color='C4')\nplt.legend()\n\n\n\n\n\n\n\n\n결정 분석\n\nsample = posterior_z.sample(260)\ndelays = [30, 40, 50]\naugmented_sample = np.append(sample, delays)\n\nqs = np.linspace(0, 60, 101)\naugmented_posterior_z = kde_from_sample(augmented_sample, qs)\naugmented_posterior_z.plot(label='보강된 z의 사후분포', color='C4')\nplt.legend()\n\n\n\n\n\n\n\n\n\n이전 데이터에서는 긴 지연시간에 대한 데이터가 없기 때문에 sampling을 통해 임의로 만들어줌\n\n\nqs = augmented_posterior_z.qs\npmf_seq = [make_elapsed_dist(gap, qs) for gap in qs]\npmf_x = make_mixture(augmented_posterior_z, pmf_seq)\nlam = 2\ndef compute_posterior_y(num_passengers):\n    likelihood = poisson(lam * qs).pmf(num_passengers)\n    posterior_x = pmf_x * likelihood\n    posterior_x.normalize()\n    posterior_y = Pmf.sub_dist(augmented_posterior_z, posterior_x)\n    nonneg = (posterior_y.qs &gt;= 0)\n    posterior_y = Pmf(posterior_y[nonneg])\n    posterior_y.normalize()\n    return posterior_y\n\nnums = np.arange(0, 37, 3)\nposteriors = [compute_posterior_y(num) for num in nums]\nmean_wait = [posterior_y.mean()\n             for posterior_y in posteriors]\nplt.plot(nums, mean_wait)\nplt.title('승객 수에 따른 예상 대기 시간')\nplt.xlabel('승객 수')\nplt.ylabel('예상 다음 도착 시간')\n\nText(0, 0.5, '예상 다음 도착 시간')\n\n\n\n\n\n\n\n\n\n\nprob_late = [1 - posterior_y.make_cdf()(15) \n             for posterior_y in posteriors]\nplt.plot(nums, prob_late)\nplt.xlabel('승객 수')\nplt.ylabel('늦을 확률')\nplt.title('승객 수에 따른 늦을 확률')\n\nText(0.5, 1.0, '승객 수에 따른 늦을 확률')\n\n\n\n\n\n\n\n\n\n\nlam이 만약 알려져 있지 않을 경우, lam에 대해서도 분포를 계산해야함 (쉽지 않네)\n\n\n\n9-7\n\ndef print_cost(printed):\n    if printed &lt; 100:\n        return printed * 5\n    else:\n        return printed * 4.5\n\ndef total_income(printed, orders):\n    sold = min(printed, np.sum(orders))\n    return sold * 10\n\ndef inventory_cost(printed, orders):\n    excess = printed - np.sum(orders)\n    if excess &gt; 0:\n        return excess * 2\n    else:\n        return 0\n\ndef out_of_stock_cost(printed, orders):\n    weeks = len(orders)\n    total_orders = np.cumsum(orders)\n    for i, total in enumerate(total_orders):\n        if total &gt; printed:\n            return (weeks-i) * 50\n    return 0\n\ndef compute_profit(printed, orders):\n    return (total_income(printed, orders) -\n            print_cost(printed)-\n            out_of_stock_cost(printed, orders) -\n            inventory_cost(printed, orders))\n\n\nfrom scipy.stats import gamma\n\nalpha = 9\nqs = np.linspace(0, 25, 101)\nps = gamma.pdf(qs, alpha)\npmf = Pmf(ps, qs)\npmf.normalize()\npmf.mean()\n\nnp.float64(8.998788382371902)\n\n\n\nrates = pmf.choice(1000)\nnp.mean(rates)\n\nnp.float64(8.9375)\n\n\n\norder_array = np.random.poisson(rates, size=(8, 1000)).transpose()\norder_array[:5, :]\n\narray([[ 9,  5, 10,  9,  4, 10,  4,  5],\n       [ 7,  3,  5,  6,  1,  3,  2,  8],\n       [ 4,  3,  9,  5,  7,  6,  8,  3],\n       [ 4,  3,  7,  2,  4,  5,  1,  1],\n       [ 8,  6, 10,  2, 10,  3,  8,  7]])\n\n\n\ndef compute_expected_profits(printed, order_array):\n    profits = [compute_profit(printed, orders)\n               for orders in order_array]\n    return np.mean(profits)\n\n\ncompute_expected_profits(70, order_array)\ncompute_expected_profits(80, order_array)\ncompute_expected_profits(90, order_array)\n\nnp.float64(158.554)\n\n\n\nprinted_array = np.arange(70, 110)\nt = [compute_expected_profits(printed, order_array)\n                    for printed in printed_array]\nexpected_profits = pd.Series(t, printed_array)\n\n\nexpected_profits.plot(label='')",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "의사결정분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/02.html#pmf를-이용한-확률",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/02.html#pmf를-이용한-확률",
    "title": "분포",
    "section": "Pmf를 이용한 확률",
    "text": "Pmf를 이용한 확률\n\n쿠키 문제\n\nfrom empiricaldist import Pmf\nimport pandas as pd\n\nprior = Pmf.from_seq(['Bowl 1', 'Bowl 2'])\nlikelihood_vanilla = [0.75, 0.5]\nposterior = prior * likelihood_vanilla\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nBowl 1\n0.6\n\n\nBowl 2\n0.4\n\n\n\n\n\n\n\n\n만약 같은 그릇에서 한 번 더 쿠키를 꺼냈을 때 바닐라일 경우\n\n\nposterior *= likelihood_vanilla\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nBowl 1\n0.692308\n\n\nBowl 2\n0.307692\n\n\n\n\n\n\n\n\n만약 같은 그릇에서 한 번 더 쿠키를 꺼냈을 때 초코쿠키일 경우\n\n\nlikelihood_chocolate = [0.25, 0.5]\nposterior *= likelihood_chocolate\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nBowl 1\n0.529412\n\n\nBowl 2\n0.470588\n\n\n\n\n\n\n\n\n\n101개의 쿠키 그릇\n\n그릇 i에는 바닐라 쿠키가 i% 있다.\n임의의 그릇을 골라 쿠키를 임의로 꺼냈을 때, 바닐라 쿠키라면 그릇 x에서 쿠키가 나왔을 확률은 얼마일까?\n\n\nimport numpy as np\n\nhypos = np.arange(101)\nprior = Pmf(1, hypos)\nprior.normalize()\nprior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n0\n0.009901\n\n\n1\n0.009901\n\n\n2\n0.009901\n\n\n3\n0.009901\n\n\n4\n0.009901\n\n\n...\n...\n\n\n96\n0.009901\n\n\n97\n0.009901\n\n\n98\n0.009901\n\n\n99\n0.009901\n\n\n100\n0.009901\n\n\n\n\n101 rows × 1 columns\n\n\n\n\nlikelihood_vanilla = hypos / 100\nposterior1 = prior * likelihood_vanilla\nposterior1.normalize()\n\nnp.float64(0.5000000000000001)\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\nprior.plot(label='prior', color='C5')\nposterior1.plot(label='posterior', color='C4')\nplt.legend()\nplt.xlabel('그릇 번호')\nplt.ylabel('PMF')\nplt.title('바닐라 쿠키 하나 뽑은 후')\n\nText(0.5, 1.0, '바닐라 쿠키 하나 뽑은 후')\n\n\n\n\n\n\n\n\n\n\n한 번 더 뽑았을 때 바닐라 쿠키인 경우\n\n\nposterior2 = posterior1 * likelihood_vanilla\nposterior2.normalize()\n\nnp.float64(0.6699999999999999)\n\n\n\nposterior2.plot(label='posterior', color='C4')\nplt.xlabel('그릇 번호')\nplt.ylabel('PMF')\nplt.title('바닐라 쿠키 두 번 뽑은 후')\n\nText(0.5, 1.0, '바닐라 쿠키 두 번 뽑은 후')\n\n\n\n\n\n\n\n\n\n\n한 번 더 뽑았는데 초코 쿠키인 경우\n\n\nlikelihood_chocolate = 1 - hypos / 100\nposterior3 = posterior2 * likelihood_chocolate\nposterior3.normalize()\n\nnp.float64(0.2462686567164179)\n\n\n\nposterior3.plot(label='posterior', color='C4')\nplt.xlabel('그릇 번호')\nplt.ylabel('PMF')\nplt.title('바닐라 쿠키 두 번 뽑고 초코 쿠키 하나 뽑은 후')\n\nText(0.5, 1.0, '바닐라 쿠키 두 번 뽑고 초코 쿠키 하나 뽑은 후')\n\n\n\n\n\n\n\n\n\n\nposterior3.max_prob()\n\nnp.int64(67)\n\n\n\nMAP: 사후확률 분포에서 가장 큰 확률값\n\n\n\n주사위 문제\n\ndef update_dice(pmf, data):\n  hypos = pmf.qs\n  likelihood = 1 / hypos\n  likelihood[data &gt; hypos] = 0\n  pmf *= likelihood\n  pmf.normalize()\n\n\n주사위를 굴려서 1이 나왔다.\n\n\nhypos = [6, 8, 12]\npmf = Pmf(1/3, hypos)\nupdate_dice(pmf, 1)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n6\n0.444444\n\n\n8\n0.333333\n\n\n12\n0.222222\n\n\n\n\n\n\n\n\n주사위를 한번 더 굴렸는데 7이 나왔다.\n\n\nupdate_dice(pmf, 7)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n6\n0.000000\n\n\n8\n0.692308\n\n\n12\n0.307692",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/02.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/02.html#연습문제",
    "title": "분포",
    "section": "연습문제",
    "text": "연습문제\n\n3-1\n\nhypos = [6, 8, 12]\npmf = Pmf(1/3, hypos)\nupdate_dice(pmf, 1)\nupdate_dice(pmf, 3)\nupdate_dice(pmf, 5)\nupdate_dice(pmf, 7)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n6\n0.000000\n\n\n8\n0.835052\n\n\n12\n0.164948\n\n\n\n\n\n\n\n\n\n3-2\n\nhypos = [4, 6, 8, 12, 20]\npmf = Pmf([1/15, 2/15, 3/15, 4/15, 5/15], hypos)\nupdate_dice(pmf, 7)\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n4\n0.000000\n\n\n6\n0.000000\n\n\n8\n0.391304\n\n\n12\n0.347826\n\n\n20\n0.260870\n\n\n\n\n\n\n\n\n\n3-3\n\npmf = Pmf.from_seq(['서랍 1', '서랍 2'])\nlikelihood = [1/2, 1/9]\nposterior = pmf * likelihood\nposterior.normalize()\nposterior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n서랍 1\n0.818182\n\n\n서랍 2\n0.181818",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/14.html#팽귄-데이터",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/14.html#팽귄-데이터",
    "title": "분류",
    "section": "팽귄 데이터",
    "text": "팽귄 데이터\n\nimport pandas as pd\n\ndf = pd.read_csv('https://github.com/allisonhorst/palmerpenguins/raw/main/inst/extdata/penguins_raw.csv')\ndf.shape\n\n(344, 17)\n\n\n\ndef make_cdf_map(df, colname, by='Species2'):\n  cdf_map = {}\n  grouped = df.groupby(by)[colname]\n  for species, group in grouped:\n    cdf_map[species] = Cdf.from_seq(group, name=species)\n  return cdf_map",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "분류"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/04.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/04.html",
    "title": "회귀분석 템플릿",
    "section": "",
    "text": "선형성: 종속변수와 독립변수 간의 관계는 선형이다.\n정규성: 종속변수 잔차들의 분포는 정규분포이다.\n등분산성: 종속변수 잔차들의 분포는 동일한 분산을 갖는다.\n독립성: 모든 잔차값은 서로 독립이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "회귀분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/04.html#가정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/04.html#가정",
    "title": "회귀분석 템플릿",
    "section": "",
    "text": "선형성: 종속변수와 독립변수 간의 관계는 선형이다.\n정규성: 종속변수 잔차들의 분포는 정규분포이다.\n등분산성: 종속변수 잔차들의 분포는 동일한 분산을 갖는다.\n독립성: 모든 잔차값은 서로 독립이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "회귀분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/04.html#가정-검정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/04.html#가정-검정",
    "title": "회귀분석 템플릿",
    "section": "가정 검정",
    "text": "가정 검정\n\n먼저 모델링을 진행한 후, 잔차를 통해 가정을 검정한다.\n\n\n\nlinear regression test\n\nimport statsmodels.api as sm\n\nXc = sm.add_constant(X)\nmodel = sm.OLS(y, Xc).fit()\nresid = model.resid\n\nprint(model.summary())\n\n\n이상치, 영향치 처리\n\n레버러지(변수 내 다른 관측치들이랑 떨어진 정도) * 잔차\n\nCook’s distance\nLeverage\n그 외 DFFITS, DFBETAS 등\n\n\n\nmean = resid.mean()\nstd = resid.std()\noutlier_mask = (resid &lt; mean - 3 * std) | (resid &gt; mean + 3 * std)\n\ninfluence = model.get_influence()\ncooks = influence.cooks_distance[0]\ninfluence = cooks &gt; 4 / (len(resid) - model.df_model - 1)\n\noutlier = resid[outlier_mask | influence].index\n\ny_log = y.drop(outlier)\nXc = sm.add_constant(X.drop(outlier))\nmodel = sm.OLS(y, Xc).fit()\nresid = model.resid\n\nprint(model.summary())\n\n\n\n다중공선성 검정\n\n전 변수 집합 대상\n\n\n\nmulticollinearity test\n\ncor = df.corr()\ncond_num = np.linalg.cond(cor)\nprint(\"Condition Number:\", cond_num)\n\n\n30을 초과하면 다중공선성이 높다고 판단한다.\n선형 종속 가능성을 봄.\nstatsmodels의 ols를 사용해도 볼 수 있음\nscaling이 선행되어야 함.\n초과 시 해석:\n\n독립변수의 전체 차원이 부족한 경우\n표본을 더 모으거나 새로운 변수를 도입\n\n\n\n개별 변수의 계수 추정이 불안정한 경우(표본 오차가 큰 경우)\n\n\n\nVIF test\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef check_vif(X, y):\n    X = sm.add_constant(X)\n    model = sm.OLS(y, X)\n    model.fit()\n    vif_df = pd.DataFrame(columns=['feature', 'VIF'])\n    for i in range(1, len(model.exog_names)):\n        vif_df.loc[i, 'feature'] = model.exog_names[i]\n        vif_df.loc[i, 'VIF'] = variance_inflation_factor(model.exog, i)\n    return vif_df.sort_values(by='VIF', ascending=False)\n\nprint(check_vif(X, y))\n\n\n특정 변수가 다른 변수들의 선형 결합으로 표현될 수 있는 경우\nVIF가 10을 넘을 경우\n덜 중요하다면 제거\n중요하다면 변수에 대한 독립적 정보 보강(세분화, …)\n\n\n두 변수의 상관계수가 높은 경우\n\n\n\ncorrelation heatmap\n\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(cor, annot=True, ax=ax)\n\n\n둘의 관계를 설명하는 제 3의 변수 도입(요인 분석 등)\n둘 중 하나를 제거\n\n\n\n정규성\n\nols의 summary를 통해 확인 가능\n혹은 EDA 과정에서 사용한 정규성 검정 참조\n\n\n\n선형성, 등분산성 검정\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.residplot(x=model.fittedvalues, y=model.resid, lowess=True,\n              line_kws={'color': 'red', 'lw': 2})\nplt.show()\n\n\n선형성:\n\n잔차도가 어떠한 패턴도 보이지 않아야 한다.\n\n등분산성:\n\n잔차도가 일정한 폭을 가져야 한다.\n\n\n\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\n# Breusch-Pagan 검정\nlm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(resid, Xc)\nprint(f'Breusch-Pagan p-value: {lm_pvalue:.3f}')\n\n# 잔차의 정규성 만족시 lm_pvalue, 그 외 f_pvalue 사용\nif lm_pvalue &gt; 0.05:\n    print(\"등분산성 가정을 만족합니다.\")\nelse:\n    print(\"등분산성 가정을 위반합니다.\")\n\n\n\n독립성\n\n독립성은 검정은 연구자 주관에 판단하는 것이 일반적이라고 한다.\n\n더빈-왓슨 검정을 사용할 수도 있지만, 1차 자기상관만 검정 가능하다.\n2에 가까울수록 독립성 만족\nstatsmodels의 ols로 확인 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "회귀분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/04.html#전처리",
    "href": "posts/03_archives/completed_project/adp_실기/notes/04.html#전처리",
    "title": "회귀분석 템플릿",
    "section": "전처리",
    "text": "전처리\n\n범주형 변수 처리:\n\n더미 변수화: 기준이 되는 범주를 하나 정하고, 나머지 범주를 0과 1로 표현\n\n각 범주의 회귀계수는 기준 범주와의 차이를 의미\n\n\n이상치 / 영향점: 관측값 제거\n선형성 위반: 독립변수 변환, GAM\n정규성 / 등분산성 위반: 종속변수 변환, GLM, GAM\n다중공산성 위반: 다중공산성 파트 참고\n\n혹은 변수 선택법을 사용\n\n\n\n가정 만족할 때까지 검정, 전처리 계속 반복\n\n\n변수 변환\n\n\n\n회귀 모델 수정 λ\n\n\n\n\n\n경험적인 적절한 λ\n\n\n\n최적의 λ는 최대 우도 추정법으로 구할 수 있다.\n변수 변환은 예측력은 높일 수 있지만, 해석이 어려워질 수 있다.\n일반적으로 box tidwell 검정을 사용하여 변환을 수행할 수 있지만 파이썬에서는 제공하는 라이브러리가 없다.\n\n아마 양수 변수만 사용 가능한 단점과 다른 방법들이 많아서 그런 것 같다.\n통계적 검정은 아니지만 box cox 변환을 사용하여 최적의 λ를 찾을 수 있다.\n\n\n\n\nbox cox transformation\n\nfrom scipy.stats import boxcox\n\ny_transformed, best_lambda = boxcox(y)\nprint(f\"Best lambda: {best_lambda:.3f}\")\n\n\n\n변수 선택법\n\n전진 선택법\n후진 선택법\n단계적 선택법\n최적조합 선택법: 모든 조합 다 해봄\n기준\n\nR2, Adj R2\nAIC(Akaike Information Criterion): 모델에 변수를 추가할 수록 불이익을 주는 오차 측정법\nBIC(Bayesian Information Criterion): 변수 추가에 더 강한 불이익을 줌\nMallows’ Cp",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "회귀분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/04.html#규제-선형-회귀",
    "href": "posts/03_archives/completed_project/adp_실기/notes/04.html#규제-선형-회귀",
    "title": "회귀분석 템플릿",
    "section": "규제 선형 회귀",
    "text": "규제 선형 회귀\n\n지나치게 많은 독립변수를 갖는 모델에 패널티를 부과하는 방식으로 간명한 모델을 만듦\n독립변수에 대한 scaling이 선행되어야 함 (큰 변수에만 과하게 패널티가 부과될 수 있어서)\n\n일반적으로는 scale을 하든 안하든 r square에 차이가 없다.\n\n\n\n릿지회귀\n\n\\(\\Sigma (y_i - \\hat{y_i})^2 + λ\\Sigma β_j^2\\)\n회귀계수 절댓값을 0에 가깝게 함\n하지만 0으로 만들지는 않음\n작은 데이터셋에서는 선형 회귀보다 점수가 더 좋지만, 데이터가 충분히 많아지면 성능이 비슷해짐.\n회귀계수가 모두 비슷한 크기를 가질 때 라쏘보다 성능이 좋음\n\n라쏘회귀:\n\n\\(\\Sigma (y_i - \\hat{y_i})^2 + λ\\Sigma |β_j|\\)\n회귀계수를 0으로 만들 수 있음\n변수 선택 효과\n릿지보다 해석이 쉬움\n일부 독립계수가 매우 큰 경우 릿지회귀보다 성능이 좋음\n\n엘라스틱넷 회귀\n\n\\(\\Sigma (y_i - \\hat{y_i})^2 + λ_1\\Sigma |β_j| + λ_2\\Sigma β_j^2\\)\n릿지와 라쏘의 장점을 모두 가짐\n변수 선택 효과도 있고, 회귀계수를 0에 가깝게 만듦\n독립변수 간에 상관관계가 있을 때, 그룹으로 선택하는 경향이 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "회귀분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/04.html#일반화-선형-회귀glm",
    "href": "posts/03_archives/completed_project/adp_실기/notes/04.html#일반화-선형-회귀glm",
    "title": "회귀분석 템플릿",
    "section": "일반화 선형 회귀(GLM)",
    "text": "일반화 선형 회귀(GLM)\n\n종속변수가 이항분포를 따르거나 포아송 분포를 따르는 경우\n\n이항분포: 평균이 np, 분산이 np(1-p), 즉 평균과 분산 사이에 관계가 존재하여 등분산성 가정을 만족하기 어렵다.\n포아송 분포: 평균과 분산이 같아서 등분산성 가정을 만족하기 어렵다.\n따라서 위와 같은 경우에 종속변수에 적절한 함수를 적용하여 등분산성 가정을 만족시킨다.\n\n\n\nLogistic 회귀\n\n종속변수가 범주형일 경우\n\\(z = β_0 + β_1 x_1 + β_2 x_2 + ... + β_n x_n\\)\n\\(p = \\frac{1}{1 + e^{-z}}\\)\n오즈: \\(\\frac{p}{1-p}\\) = \\(e^z\\)\n오즈비: 독립변수 k 단위 변화에 따른 오즈(양성 vs 음성)의 변화 비율\n\n\\((e^{β_k})^k\\)\n\nLLR의 p-value가 낮다면, 모델이 통계적으로 유의미함을 의미\n잔차 검정은 안함\n\n\n\n포아송 회귀\n\n종속변수가 count 데이터일 경우\n\n\nimport statsmodels.api as sm\n\nXc = sm.add_constant(X)\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nfitted = model.fit()\nprint(model.summary())\n\n\n\\(x_k\\)가 한 단위 증가할 때, 빈도 수가 \\(exp(β_k)\\)배 증가\n만약 관찰 시간이 다를 경우, offset로 np.log(df[관찰시간])을 넣어줘야 함\nDeviance / DF_resid 가 1보다 크면 과산포, 작으면 과소산포\n\n과산포: 사건발생 확률이 일정하지 않음\n과산포 시 음이항 회귀 사용\n\n\n\n\n음이항 회귀\n\nimport statsmodels.api as sm\n\nXc = sm.add_constant(X)\n\nmodel = sm.GLM(y, Xc, family=sm.families.NegativeBinomial())\nfitted = model.fit()\nprint(model.summary())\n\n\nQuasi-Poisson도 있지만, statsmodels에서는 제공하지 않음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "회귀분석 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html#overview",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html#overview",
    "title": "텍스트 분석",
    "section": "overview",
    "text": "overview\n\nNLP vs 텍스트 분석\n\nNLP(자연어 처리)는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술을 의미\n텍스트 분석은 주로 비정형 텍스트 데이터를 머신러닝, 통계 등의 방법으로 예측 분석이나 유용한 정보를 추출하는 데 중점을 둔다.\n\n\n\n종류\n\n텍스트 분류: 문서가 특정 분류 또는 카테고리에 속하는 것을 예측 (연예 / 정치 / 스포츠 같은 카테고리 분류 혹은 스팸 메일 검출). 지도 학습\n감성 분석: 텍스트에서 주관적 요소를 분석하는 기법. 지도 혹은 비지도.\n텍스트 요약: 텍스트 내에서 주제나 중심 사상을 추출\n텍스트 군집화: 비슷한 유형의 문서를 군집화 하는 것. 비지도 학습",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html#프로세스",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html#프로세스",
    "title": "텍스트 분석",
    "section": "프로세스",
    "text": "프로세스\n\n텍스트 전처리: 대 / 소문자 변경, 특수 문자 제거, 토큰화, 불용어 제거, 어근 추출 등의 정규화 작업\n피처 벡터화 / 추출: 텍스트에서 피처를 추출하고 벡터 값을 할당. BOW와 Word2Vec이 대표적\nML 모델 수립 및 학습 / 예측 / 평가",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html#전처리",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html#전처리",
    "title": "텍스트 분석",
    "section": "전처리",
    "text": "전처리\n\n클렌징: 문자, 기호 등을 사전에 제거\n토큰화\n\n문장 토큰화: 마침표, 개행문자 등을 기준으로 문장을 분리. 각 문장이 가지는 의미가 중요한 경우 사용.\n단어 토큰화: 공백, 콤마, 마침표, 개행문자 등으로 단어를 분리.\n\nn-gram: 단어의 연속된 n개를 묶어서 하나의 단위로 처리하는 방법. 문장이 가지는 의미를 조금이라도 보존할 수 있다.\n\n\n\n\nfrom nltk import sent_tokenize\nimport nltk\nnltk.download('punkt') # 문장을 분리하는 마침표, 개행문자 등의 데이터 셋 다운로드\nnltk.download('punkt_tab')\n\ntext_sample = \"The Matrix is everywhere its all around us, here even in this room. You can see it when you look out your window or when you turn on your television. You can feel it when you go to work, when you go to church, when you pay your taxes.\"\nsentences = sent_tokenize(text_sample)\nprint(sentences)\n\n['The Matrix is everywhere its all around us, here even in this room.', 'You can see it when you look out your window or when you turn on your television.', 'You can feel it when you go to work, when you go to church, when you pay your taxes.']\n\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     /home/cryscham123/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /home/cryscham123/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\n\nfrom nltk import word_tokenize\n\nsentence = \"The Matrix is everywhere its all around us, here even in this room.\"\nwords = word_tokenize(sentence)\nprint(words)\n\n['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n\n\n\ndef tokenize_text(text):\n    sentences = sent_tokenize(text)\n    words = [word_tokenize(sentence) for sentence in sentences]\n\n    return words\n\nword_tokens = tokenize_text(text_sample)\nword_tokens\n\n[['The',\n  'Matrix',\n  'is',\n  'everywhere',\n  'its',\n  'all',\n  'around',\n  'us',\n  ',',\n  'here',\n  'even',\n  'in',\n  'this',\n  'room',\n  '.'],\n ['You',\n  'can',\n  'see',\n  'it',\n  'when',\n  'you',\n  'look',\n  'out',\n  'your',\n  'window',\n  'or',\n  'when',\n  'you',\n  'turn',\n  'on',\n  'your',\n  'television',\n  '.'],\n ['You',\n  'can',\n  'feel',\n  'it',\n  'when',\n  'you',\n  'go',\n  'to',\n  'work',\n  ',',\n  'when',\n  'you',\n  'go',\n  'to',\n  'church',\n  ',',\n  'when',\n  'you',\n  'pay',\n  'your',\n  'taxes',\n  '.']]\n\n\n\nstopword 제거: 분석에 필요하지 않은 단어를 제거하는 작업. 예) 관사, 전치사, 접속사 등\n\n\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')  # stopwords 데이터 셋 다운로드\n\nstopwords.words('english')[:20]\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/cryscham123/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n['a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been']\n\n\n\nsw = stopwords.words('english')\nall_tokens = []\nfor sentence in word_tokens:\n    filtered_words = []\n    for word in sentence:\n        word = word.lower()\n        if word not in sw:\n            filtered_words.append(word)\n    all_tokens.append(filtered_words)\nprint(all_tokens)\n\n[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'look', 'window', 'turn', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', ',', 'pay', 'taxes', '.']]\n\n\n\nstemming, lemmatization: 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것\n\nstemming이 더 단순하고 빠르지만 lemmatization 이 더 저오학함\n\n\n\nfrom nltk.stem import LancasterStemmer\n\nstemmer = LancasterStemmer()\n\nprint(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\nprint(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\nprint(stemmer.stem('happier'), stemmer.stem('happiest'))\nprint(stemmer.stem('fancier'), stemmer.stem('fanciest'))\n\nwork work work\namus amus amus\nhappy happiest\nfant fanciest\n\n\n\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nnltk.download('wordnet')\n\nlemma = WordNetLemmatizer()\n\nprint(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\nprint(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\nprint(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/cryscham123/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\namuse amuse amuse\nhappy happy\nfancy fancy",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html#bow",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/07.html#bow",
    "title": "텍스트 분석",
    "section": "BOW",
    "text": "BOW\n\n문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 빈도 값을 부여해 피처 값을 추출하는 모델\ncount 기반 벡터화: 빈도가 높을수록 중요한 단어로 인식\nTF-IDF(term frequency - inverse document frequency) 기반 벡터화: 빈도가 높을수록 좋으나, 모든 문서에서 전반적으로 나타나는 단어에 대해서는 패털티를 줌\n\n\\(TF_i * log\\frac{N}{DF_i}\\)\n\n\\(TF_i\\): 개별 문서에서의 단어 i 빈도\n\\(DF_i\\): 단어 i를 가지고 있는 문서 개수\nN: 전체 문서 개수\n\n\n희소행렬 문제: 불필요한 0 값이 많아지는 문제\n\nCOO\nCSR\n혹은 희소행렬을 잘 처리하는 알고리즘: 로지스틱 회귀, 선형 svm, 나이브 베이즈 등\n\n\n\nCOO\n\n0이 아닌 데이터만 별도의 array에 저장.\n\n\nimport numpy as np\nfrom scipy import sparse\n\ndense = np.array([[3, 0, 1], [0, 2, 0]])\ndata = np.array([3, 1, 2])\nrow_pos = np.array([0, 0, 1])\ncol_pos = np.array([0, 2, 1])\nsparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\nsparse_coo\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 3 stored elements and shape (2, 3)&gt;\n\n\n\nsparse_coo.toarray()\n\narray([[3, 0, 1],\n       [0, 2, 0]])\n\n\n\n\nCSR\n\nCOO + 시작위치만 기록하는 방법\n\n\nfrom scipy import sparse\n\ndense2 = np.array([[0, 0, 1, 0, 0, 5],\n                   [1, 4, 0, 3, 2, 5],\n                   [0, 6, 0, 3, 0, 0],\n                   [2, 0, 0, 0, 0, 0],\n                   [0, 0, 0, 7, 0, 8],\n                   [1, 0, 0, 0, 0, 0]])\ndata2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\nrow_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\ncol_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\nrow_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n\nsparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\nsparse_csr.toarray()\n\narray([[0, 0, 1, 0, 0, 5],\n       [1, 4, 0, 3, 2, 5],\n       [0, 6, 0, 3, 0, 0],\n       [2, 0, 0, 0, 0, 0],\n       [0, 0, 0, 7, 0, 8],\n       [1, 0, 0, 0, 0, 0]])\n\n\n\nsparse_csr = sparse.csr_matrix(dense2)\nsparse_csr.toarray()\n\narray([[0, 0, 1, 0, 0, 5],\n       [1, 4, 0, 3, 2, 5],\n       [0, 6, 0, 3, 0, 0],\n       [2, 0, 0, 0, 0, 0],\n       [0, 0, 0, 7, 0, 8],\n       [1, 0, 0, 0, 0, 0]])",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#경사하강법",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#경사하강법",
    "title": "회귀",
    "section": "경사하강법",
    "text": "경사하강법\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX = 2 * np.random.rand(100, 1)\ny = 6 + 4 * X + np.random.randn(100, 1)\nplt.scatter(X, y)\n\n\n\n\n\n\n\n\n\ndef get_cost(y, y_pred):\n    N = len(y)\n    cost = np.sum(np.square(y - y_pred)) / N\n    return cost\n\ndef get_weight_updates(w1, w0, X, y, learning_rate=0.01):\n    N = len(y)\n    w1_update = np.zeros_like(w1)\n    w0_update = np.zeros_like(w0)\n    y_pred = np.dot(X, w1.T) + w0\n    diff = y - y_pred\n\n    w1_update = -(2/N) * learning_rate * np.dot(X.T, diff)\n    w0_update = -(2/N) * learning_rate * np.sum(diff)\n\n    return w1_update, w0_update\n\ndef gradient_descent_steps(X, y, iters=10000):\n    w0 = np.zeros((1, 1))\n    w1 = np.zeros((1, 1))\n\n    for _ in range(iters):\n        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)\n        w1 = w1 - w1_update\n        w0 = w0 - w0_update\n\n    return w1, w0\n\n\nw1, w0 = gradient_descent_steps(X, y, iters=1000)\ny_pred = w1[0, 0] * X + w0\nprint(f'w0: {w0[0, 0]:.3f} w1: {w1[0, 0]:.3f}, total cost: {get_cost(y, y_pred):.3f}')\nplt.scatter(X, y)\nplt.plot(X, y_pred)\n\nw0: 6.394 w1: 3.611, total cost: 1.009\n\n\n\n\n\n\n\n\n\n\n일반 경사하강법은 시간이 오래걸려서 잘 안씀",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#미니-배치-확률적-경사-하강법",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#미니-배치-확률적-경사-하강법",
    "title": "회귀",
    "section": "미니 배치 확률적 경사 하강법",
    "text": "미니 배치 확률적 경사 하강법\n\ndef stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):\n    w0 = np.zeros((1, 1))\n    w1 = np.zeros((1, 1))\n\n    for ind in range(iters):\n        stochastic_random_index = np.random.permutation(X.shape[0])\n        sample_X = X[stochastic_random_index[0:batch_size]]\n        sample_y = y[stochastic_random_index[0:batch_size]]\n\n        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)\n        w1 = w1 - w1_update\n        w0 = w0 - w0_update\n\n    return w1, w0\n\n\nw1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000)\ny_pred = w1[0, 0] * X + w0\nprint(f'w0: {w0[0, 0]:.3f} w1: {w1[0, 0]:.3f}, total cost: {get_cost(y, y_pred):.3f}')\nplt.scatter(X, y)\nplt.plot(X, y_pred)\n\nw0: 6.372 w1: 3.619, total cost: 1.010",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#선형-회귀",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#선형-회귀",
    "title": "회귀",
    "section": "선형 회귀",
    "text": "선형 회귀\n\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import load_boston\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nboston = load_boston()\n\ndf = pd.DataFrame(boston.data, columns=boston.feature_names)\ndf['price'] = boston.target\ndf.head()\n\n\nlm_features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRAIO', 'LSTAT', 'RAD']\n\nfig, axs = plt.subplots(figsize=(16, 8), ncols=len(lm_features) // 2, nrows=2)\n\nfor i, feature in enumerate(lm_features):\n    row = i // 4\n    col = i % 4\n\n    sns.regplot(x=feature, y='price', data=df, ax=axs[row][col])\n\nboston 데이터가 윤리적 문제로 사용 불가능하다고 한다.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ny_target = df['price']\nX_data = df.drop(['price'], axis=1, inplace=False)\nlr = LinearRegression()\n\nneg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores = np.sqrt(-1 * neg_mse_scores)\navg_rmse = np.mean(rmse_scores)\n\ncross_val_score는 값이 큰걸 좋게 평가해서 neg를 기준으로 넣어줘야함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#다항-회귀",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#다항-회귀",
    "title": "회귀",
    "section": "다항 회귀",
    "text": "다항 회귀\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\ndef polynominal_func(X):\n    y = 1 + 2 * X[:, 0] + 3 * X[:, 0]**2 + 4 * X[:, 1]**3\n    return y\n\nmodel = Pipeline([('poly', PolynomialFeatures(degree=3)),\n                  ('linear', LinearRegression())])\nX = np.arange(4).reshape(2, 2)\ny = polynominal_func(X)\n\nmodel = model.fit(X, y)\n\nnp.round(model.named_steps['linear'].coef_, 2)\n\narray([0.  , 0.18, 0.18, 0.36, 0.54, 0.72, 0.72, 1.08, 1.62, 2.34])",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#규제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#규제",
    "title": "회귀",
    "section": "규제",
    "text": "규제\n\nL2 규제(Ridge): \\(min(RSS(W) + \\lambda ||W||^2)\\)\nL1 규제(Lasso): \\(min(RSS(W) + \\lambda ||W||_1)\\)\nλ가 크면, 회귀계수의 크기가 작아지고, λ가 0이 되면 일반 선형회귀와 같아짐\nL1 규제는 영향력이 작은 피처의 계수를 0으로 만들어서 피처 선택 효과가 있음. L2는 0으로 만들지는 않음\n\n\n릿지\n\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge(alpha = 10)\nneg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv=5)\nrmse_scores = np.sqrt(-1 * neg_mse_scores)\navg_rmse = np.mean(rmse_scores)\n\n\n\n라쏘 엘라스틱넷\n\nimport pandas as pd\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\ndef get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None):\n    coeff_df = pd.DataFrame()\n    for param in params:\n        if model_name == 'Ridge':\n            model = Ridge(alpha=param)\n        elif model_name == 'Lasso':\n            model = Lasso(alpha=param)\n        elif model_name == 'ElasticNet':\n            model = ElasticNet(alpha=param, l1_ratio=0.7)\n        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring=\"neg_mean_squared_error\", cv=5)\n        rmse_scores = np.sqrt(-1 * neg_mse_scores)\n        avg_rmse = np.mean(rmse_scores)\n        print(f'{param}: {avg_rmse:.3f}')\n\n        model.fit(X_data_n, y_target_n)\n        coeff = pd.Series(data=model.coef_, index=X_data_n.columns)\n        colname = 'alpha:' + str(param)\n        coeff_df[colname] = coeff\n\n    return coeff_df",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#선형-회귀-모델을-위한-데이터-변환",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/05.html#선형-회귀-모델을-위한-데이터-변환",
    "title": "회귀",
    "section": "선형 회귀 모델을 위한 데이터 변환",
    "text": "선형 회귀 모델을 위한 데이터 변환\n\n로그 변환: 언더플로우를 고려해서 logp 보다는 log1p를 사용한다.\n\n\nnp.log1p(data)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html#preprocessing",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html#preprocessing",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('_data/santander/train.csv', encoding='latin-1')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 76020 entries, 0 to 76019\nColumns: 371 entries, ID to TARGET\ndtypes: float64(111), int64(260)\nmemory usage: 215.2 MB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nID\nvar3\nvar15\nimp_ent_var16_ult1\nimp_op_var39_comer_ult1\nimp_op_var39_comer_ult3\nimp_op_var40_comer_ult1\nimp_op_var40_comer_ult3\nimp_op_var40_efect_ult1\nimp_op_var40_efect_ult3\n...\nsaldo_medio_var33_hace2\nsaldo_medio_var33_hace3\nsaldo_medio_var33_ult1\nsaldo_medio_var33_ult3\nsaldo_medio_var44_hace2\nsaldo_medio_var44_hace3\nsaldo_medio_var44_ult1\nsaldo_medio_var44_ult3\nvar38\nTARGET\n\n\n\n\ncount\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n...\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n76020.000000\n7.602000e+04\n76020.000000\n\n\nmean\n75964.050723\n-1523.199277\n33.212865\n86.208265\n72.363067\n119.529632\n3.559130\n6.472698\n0.412946\n0.567352\n...\n7.935824\n1.365146\n12.215580\n8.784074\n31.505324\n1.858575\n76.026165\n56.614351\n1.172358e+05\n0.039569\n\n\nstd\n43781.947379\n39033.462364\n12.956486\n1614.757313\n339.315831\n546.266294\n93.155749\n153.737066\n30.604864\n36.513513\n...\n455.887218\n113.959637\n783.207399\n538.439211\n2013.125393\n147.786584\n4040.337842\n2852.579397\n1.826646e+05\n0.194945\n\n\nmin\n1.000000\n-999999.000000\n5.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n5.163750e+03\n0.000000\n\n\n25%\n38104.750000\n2.000000\n23.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.787061e+04\n0.000000\n\n\n50%\n76043.000000\n2.000000\n28.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.064092e+05\n0.000000\n\n\n75%\n113748.750000\n2.000000\n40.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.187563e+05\n0.000000\n\n\nmax\n151838.000000\n238.000000\n105.000000\n210000.000000\n12888.030000\n21024.810000\n8237.820000\n11073.570000\n6600.000000\n6600.000000\n...\n50003.880000\n20385.720000\n138831.630000\n91778.730000\n438329.220000\n24650.010000\n681462.900000\n397884.300000\n2.203474e+07\n1.000000\n\n\n\n\n8 rows × 371 columns\n\n\n\n\ndf['var3'].replace(-999999, 2, inplace=True)\ndf.drop('ID', axis=1, inplace=True)\n\nX_features = df.iloc[:, :-1]\nlabels = df.iloc[:, -1]\n\n\ntest_df = pd.read_csv('_data/santander/test.csv', encoding='latin-1')\ntest_df['var3'].replace(-999999, 2, inplace=True)\ntest_df.drop('ID', axis=1, inplace=True)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_features, labels, test_size=0.2)\n\n\ntrain, test의 label의 비율이 동일한게 좋은걸까",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html#xgboost",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html#xgboost",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "XGBoost",
    "text": "XGBoost\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3)\n\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb_clf = XGBClassifier(n_estimators=400, \n                    learning_rate=0.05, \n                    early_stopping_rounds=100,\n                    eval_metric=['auc'])\nxgb_clf.fit(X_tr, y_tr, eval_set=evals, verbose=False)\nxgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])\nprint(f'{xgb_roc_score:.3f}')\n\n\n베이지안 최적화\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\ndef objective_func(search_space):\n    xgb_clf = XGBClassifier(n_estimators=100, \n                            early_stopping_rounds=30,\n                            eval_metric='auc',\n                            max_depth=int(search_space['max_depth']),\n                            min_child_weight=int(search_space['min_child_weight']),\n                            colsample_bytree=search_space['colsample_bytree'],\n                            learning_rate=search_space['learning_rate'])\n    roc_auc_list = []\n    kf = KFold(n_splits=3)\n    for tr_index, val_index in kf.split(X_train):\n        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n        X_val, y_val =  X_train.iloc[val_index], y_train.iloc[val_index]\n\n        xgb_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)])\n        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:, 1])\n        roc_auc_list.append(score)\n\n    return -1 * np.mean(roc_auc_list)\n\n\nfrom hyperopt import hp, fmin, tpe, Trials\n\nxgb_search_space = {\n  'max_depth': hp.quniform('max_depth', 5, 15, 1),\n  'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n  'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),\n  'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n}\n\ntrials = Trials()\nbest = fmin(fn=objective_func,\n            space=xgb_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trials)\nprint(best)\n\n\n\n재 학습\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb_clf = XGBClassifier(n_estimators=500, \n                    learning_rate=round(best['learning_rate'], 5),\n                    max_depth=int(best['max_depth']),\n                    min_child_weight=int(best['min_child_weight']),\n                    colsample_bytree=round(best['colsample_bytree'], 5),\n                    early_stopping_rounds=100,\n                    eval_metric=['auc'])\nxgb_clf.fit(X_tr, y_tr, eval_set=evals, verbose=False)\nxgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1])\nprint(f'{xgb_roc_score:.3f}')\n\n\n\nplot importance\n\nfrom xgboost import plot_importance\n\nplot_importance(xgb_clf, max_num_features=20, height=0.4)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html#lightgbm",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html#lightgbm",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "LightGBM",
    "text": "LightGBM\n\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\n\nlgbm_clf = LGBMClassifier(n_estimators=500, early_stopping_rounds=100, eval_metric='auc')\n\neval_set = [(X_tr, y_tr), (X_val, y_val)]\nlgbm_clf.fit(X_tr, y_tr, eval_set=eval_set)\n\nlgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])\nprint(f'{lgbm_roc_score:.3f}')\n\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Info] Number of positive: 1694, number of negative: 40877\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010137 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 13592\n[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 248\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039792 -&gt; initscore=-3.183475\n[LightGBM] [Info] Start training from score -3.183475\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[33]    training's binary_logloss: 0.117693 valid_1's binary_logloss: 0.137269\n[LightGBM] [Warning] Unknown parameter: eval_metric\n0.836\n\n\n\n베이지안 최적화\n\nfrom sklearn.model_selection import KFold\n\ndef objective_func(search_space):\n    lgbm_clf = LGBMClassifier(n_estimators=100, \n                            early_stopping_rounds=30,\n                            eval_metric='auc',\n                            num_leaves=int(search_space['num_leaves']),\n                            max_depth=int(search_space['max_depth']),\n                            min_child_samples=int(search_space['min_child_samples']),\n                            subsample=search_space['subsample'],\n                            learning_rate=search_space['learning_rate'])\n    roc_auc_list = []\n    kf = KFold(n_splits=3)\n    for tr_index, val_index in kf.split(X_train):\n        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n        X_val, y_val =  X_train.iloc[val_index], y_train.iloc[val_index]\n\n        lgbm_clf.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)])\n        score = roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:, 1])\n        roc_auc_list.append(score)\n\n    return -1 * np.mean(roc_auc_list)\n\n\nfrom hyperopt import hp, fmin, tpe, Trials\n\nlgbm_search_space = {\n  'num_leaves': hp.quniform('num_leaves', 32, 64, 1),\n  'max_depth': hp.quniform('max_depth', 100, 160, 1),\n  'min_child_samples': hp.quniform('min_child_samples', 60, 100, 1),\n  'subsample': hp.uniform('subsample', 0.7, 1),\n  'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n}\n\ntrials = Trials()\nbest = fmin(fn=objective_func,\n            space=lgbm_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trials)\nprint(best)\n\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007286 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12947\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.161962\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]                                                      Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.119239 valid_1's binary_logloss: 0.131547\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009173 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 13055\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.210495\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds\n  0%|          | 0/50 [00:01&lt;?, ?trial/s, best loss=?]                                                      Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.11513  valid_1's binary_logloss: 0.139265\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008461 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Total Bins 12996\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Info] Start training from score -3.179828\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      Training until validation scores don't improve for 30 rounds\n  0%|          | 0/50 [00:02&lt;?, ?trial/s, best loss=?]                                                      Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.116828 valid_1's binary_logloss: 0.136952\n  0%|          | 0/50 [00:03&lt;?, ?trial/s, best loss=?]                                                      [LightGBM] [Warning] Unknown parameter: eval_metric\n  0%|          | 0/50 [00:03&lt;?, ?trial/s, best loss=?]  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008509 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12835\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  2%|▏         | 1/50 [00:03&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.134361 valid_1's binary_logloss: 0.134539\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008662 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12988\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  2%|▏         | 1/50 [00:04&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.129831 valid_1's binary_logloss: 0.142347\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008359 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12898\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  2%|▏         | 1/50 [00:05&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.131634 valid_1's binary_logloss: 0.139054\n  2%|▏         | 1/50 [00:06&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  2%|▏         | 1/50 [00:06&lt;02:57,  3.63s/trial, best loss: -0.8341540202815528]  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008437 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12902\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  4%|▍         | 2/50 [00:06&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:\n[88]    training's binary_logloss: 0.113936 valid_1's binary_logloss: 0.131766\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008792 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12988\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  4%|▍         | 2/50 [00:07&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:\n[71]    training's binary_logloss: 0.11326  valid_1's binary_logloss: 0.139317\n  4%|▍         | 2/50 [00:08&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:08&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009763 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12898\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  4%|▍         | 2/50 [00:09&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 Did not meet early stopping. Best iteration is:\n[77]    training's binary_logloss: 0.113657 valid_1's binary_logloss: 0.136864\n  4%|▍         | 2/50 [00:10&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  4%|▍         | 2/50 [00:10&lt;02:30,  3.13s/trial, best loss: -0.8341540202815528]  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12835\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Early stopping, best iteration is:\n[39]    training's binary_logloss: 0.12109  valid_1's binary_logloss: 0.131246\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:10&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008369 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12988\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Early stopping, best iteration is:\n[39]    training's binary_logloss: 0.116743 valid_1's binary_logloss: 0.139211\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Total Bins 12898\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Training until validation scores don't improve for 30 rounds\n  6%|▌         | 3/50 [00:11&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 Early stopping, best iteration is:\n[35]    training's binary_logloss: 0.120149 valid_1's binary_logloss: 0.136702\n  6%|▌         | 3/50 [00:12&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  6%|▌         | 3/50 [00:12&lt;02:38,  3.38s/trial, best loss: -0.8341540202815528]  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008782 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12947\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n  8%|▊         | 4/50 [00:12&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:\n[30]    training's binary_logloss: 0.111064 valid_1's binary_logloss: 0.131895\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010004 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 13055\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n  8%|▊         | 4/50 [00:13&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:\n[27]    training's binary_logloss: 0.108994 valid_1's binary_logloss: 0.139854\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009049 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12996\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:\n[20]    training's binary_logloss: 0.116146 valid_1's binary_logloss: 0.13756\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n  8%|▊         | 4/50 [00:14&lt;02:16,  2.96s/trial, best loss: -0.8346097688713522] 10%|█         | 5/50 [00:14&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008251 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12947\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:\n[25]    training's binary_logloss: 0.120067 valid_1's binary_logloss: 0.131511\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007845 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 13055\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 10%|█         | 5/50 [00:15&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:\n[31]    training's binary_logloss: 0.112434 valid_1's binary_logloss: 0.139423\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009165 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12996\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:\n[28]    training's binary_logloss: 0.115651 valid_1's binary_logloss: 0.136891\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 10%|█         | 5/50 [00:16&lt;02:06,  2.80s/trial, best loss: -0.8346097688713522] 12%|█▏        | 6/50 [00:16&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010475 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12902\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 12%|█▏        | 6/50 [00:17&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.128605 valid_1's binary_logloss: 0.133093\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008203 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12988\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 12%|█▏        | 6/50 [00:18&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.124147 valid_1's binary_logloss: 0.141061\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007711 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12898\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 12%|█▏        | 6/50 [00:19&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.125878 valid_1's binary_logloss: 0.13813\n 12%|█▏        | 6/50 [00:20&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 12%|█▏        | 6/50 [00:20&lt;01:52,  2.55s/trial, best loss: -0.8346097688713522] 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008418 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12947\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Did not meet early stopping. Best iteration is:\n[73]    training's binary_logloss: 0.119266 valid_1's binary_logloss: 0.131216\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:20&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007783 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12998\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:\n[63]    training's binary_logloss: 0.116719 valid_1's binary_logloss: 0.139009\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:21&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008308 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Total Bins 12968\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Training until validation scores don't improve for 30 rounds\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 Early stopping, best iteration is:\n[56]    training's binary_logloss: 0.120087 valid_1's binary_logloss: 0.136444\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 14%|█▍        | 7/50 [00:22&lt;01:58,  2.76s/trial, best loss: -0.8346097688713522] 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009077 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12835\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:22&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:\n[57]    training's binary_logloss: 0.120993 valid_1's binary_logloss: 0.131385\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007612 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12988\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds\n 16%|█▌        | 8/50 [00:23&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:\n[62]    training's binary_logloss: 0.115325 valid_1's binary_logloss: 0.13881\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007536 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12898\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:\n[50]    training's binary_logloss: 0.120231 valid_1's binary_logloss: 0.136346\n 16%|█▌        | 8/50 [00:24&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 16%|█▌        | 8/50 [00:25&lt;01:51,  2.65s/trial, best loss: -0.8354478683012264] 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008124 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12835\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.161962\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:\n[23]    training's binary_logloss: 0.116031 valid_1's binary_logloss: 0.132494\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:25&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007737 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12988\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.210495\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:\n[22]    training's binary_logloss: 0.112419 valid_1's binary_logloss: 0.140329\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007861 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Total Bins 12898\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Info] Start training from score -3.179828\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Training until validation scores don't improve for 30 rounds\n 18%|█▊        | 9/50 [00:26&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 Early stopping, best iteration is:\n[20]    training's binary_logloss: 0.115687 valid_1's binary_logloss: 0.137694\n 18%|█▊        | 9/50 [00:27&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264]                                                                                 [LightGBM] [Warning] Unknown parameter: eval_metric\n 18%|█▊        | 9/50 [00:27&lt;01:46,  2.59s/trial, best loss: -0.8354478683012264] 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007751 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 20%|██        | 10/50 [00:27&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[45]    training's binary_logloss: 0.117033 valid_1's binary_logloss: 0.131893\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007925 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[30]    training's binary_logloss: 0.11876  valid_1's binary_logloss: 0.139543\n 20%|██        | 10/50 [00:28&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008278 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[38]    training's binary_logloss: 0.117423 valid_1's binary_logloss: 0.136738\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 20%|██        | 10/50 [00:29&lt;01:40,  2.52s/trial, best loss: -0.8354478683012264] 22%|██▏       | 11/50 [00:29&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:29&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:29&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008637 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12902\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:\n[78]    training's binary_logloss: 0.115732 valid_1's binary_logloss: 0.13138\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007739 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 22%|██▏       | 11/50 [00:30&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:\n[74]    training's binary_logloss: 0.112624 valid_1's binary_logloss: 0.139339\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008653 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 22%|██▏       | 11/50 [00:31&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:\n[74]    training's binary_logloss: 0.114351 valid_1's binary_logloss: 0.136737\n 22%|██▏       | 11/50 [00:32&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 22%|██▏       | 11/50 [00:32&lt;01:37,  2.49s/trial, best loss: -0.8354478683012264] 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010321 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12947\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 24%|██▍       | 12/50 [00:32&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[46]    training's binary_logloss: 0.11276  valid_1's binary_logloss: 0.13165\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009865 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 13055\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 24%|██▍       | 12/50 [00:33&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[40]    training's binary_logloss: 0.111011 valid_1's binary_logloss: 0.139831\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008053 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12996\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 24%|██▍       | 12/50 [00:34&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[43]    training's binary_logloss: 0.111276 valid_1's binary_logloss: 0.137335\n 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 24%|██▍       | 12/50 [00:35&lt;01:35,  2.51s/trial, best loss: -0.8354478683012264] 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008260 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12844\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:35&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.12681  valid_1's binary_logloss: 0.13222\n 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:36&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008850 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.122208 valid_1's binary_logloss: 0.139981\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:37&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009314 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 26%|██▌       | 13/50 [00:38&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.124131 valid_1's binary_logloss: 0.137316\n 26%|██▌       | 13/50 [00:39&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 26%|██▌       | 13/50 [00:39&lt;01:41,  2.74s/trial, best loss: -0.8354478683012264] 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010560 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12943\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 28%|██▊       | 14/50 [00:39&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[19]    training's binary_logloss: 0.119948 valid_1's binary_logloss: 0.132615\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009233 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[17]    training's binary_logloss: 0.116812 valid_1's binary_logloss: 0.140251\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:40&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009001 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12958\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[19]    training's binary_logloss: 0.117331 valid_1's binary_logloss: 0.137237\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 28%|██▊       | 14/50 [00:41&lt;01:49,  3.04s/trial, best loss: -0.8354478683012264] 30%|███       | 15/50 [00:41&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:41&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:41&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010186 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12943\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[17]    training's binary_logloss: 0.120445 valid_1's binary_logloss: 0.132691\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 30%|███       | 15/50 [00:42&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010950 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[16]    training's binary_logloss: 0.117054 valid_1's binary_logloss: 0.139941\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008302 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12906\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 30%|███       | 15/50 [00:43&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[20]    training's binary_logloss: 0.115401 valid_1's binary_logloss: 0.137413\n 30%|███       | 15/50 [00:44&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 30%|███       | 15/50 [00:44&lt;01:38,  2.83s/trial, best loss: -0.8354478683012264] 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009370 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[18]    training's binary_logloss: 0.11605  valid_1's binary_logloss: 0.133209\n 32%|███▏      | 16/50 [00:44&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008886 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[15]    training's binary_logloss: 0.114923 valid_1's binary_logloss: 0.140959\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:45&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008924 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[14]    training's binary_logloss: 0.117846 valid_1's binary_logloss: 0.13746\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 32%|███▏      | 16/50 [00:46&lt;01:32,  2.72s/trial, best loss: -0.8354478683012264] 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009462 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:46&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[49]    training's binary_logloss: 0.11538  valid_1's binary_logloss: 0.131723\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010342 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 34%|███▍      | 17/50 [00:47&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[31]    training's binary_logloss: 0.117853 valid_1's binary_logloss: 0.139219\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007465 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.120676 valid_1's binary_logloss: 0.136931\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 34%|███▍      | 17/50 [00:48&lt;01:26,  2.63s/trial, best loss: -0.8354478683012264] 36%|███▌      | 18/50 [00:48&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007036 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.119523 valid_1's binary_logloss: 0.131926\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008304 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 36%|███▌      | 18/50 [00:49&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[27]    training's binary_logloss: 0.115902 valid_1's binary_logloss: 0.139583\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010535 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[24]    training's binary_logloss: 0.11906  valid_1's binary_logloss: 0.137256\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 36%|███▌      | 18/50 [00:50&lt;01:20,  2.53s/trial, best loss: -0.8354478683012264] 38%|███▊      | 19/50 [00:50&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014995 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12835\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[28]    training's binary_logloss: 0.117616 valid_1's binary_logloss: 0.132237\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 38%|███▊      | 19/50 [00:51&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007897 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12988\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[23]    training's binary_logloss: 0.115822 valid_1's binary_logloss: 0.140243\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020219 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 12898\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 38%|███▊      | 19/50 [00:52&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[25]    training's binary_logloss: 0.116668 valid_1's binary_logloss: 0.137218\n 38%|███▊      | 19/50 [00:53&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 38%|███▊      | 19/50 [00:53&lt;01:13,  2.38s/trial, best loss: -0.8354478683012264] 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009889 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 13057\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 211\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 40%|████      | 20/50 [00:53&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Did not meet early stopping. Best iteration is:\n[71]    training's binary_logloss: 0.118472 valid_1's binary_logloss: 0.130909\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008639 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 13161\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 40%|████      | 20/50 [00:54&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[60]    training's binary_logloss: 0.116535 valid_1's binary_logloss: 0.138826\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009875 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Total Bins 13044\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Training until validation scores don't improve for 30 rounds\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  Early stopping, best iteration is:\n[58]    training's binary_logloss: 0.118597 valid_1's binary_logloss: 0.136638\n 40%|████      | 20/50 [00:55&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 40%|████      | 20/50 [00:56&lt;01:12,  2.41s/trial, best loss: -0.8354478683012264] 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008395 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13057\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 211\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:\n[96]    training's binary_logloss: 0.116806 valid_1's binary_logloss: 0.131693\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:56&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008654 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13161\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:\n[75]    training's binary_logloss: 0.116396 valid_1's binary_logloss: 0.138474\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:57&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009684 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13044\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  Early stopping, best iteration is:\n[65]    training's binary_logloss: 0.119662 valid_1's binary_logloss: 0.136275\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 42%|████▏     | 21/50 [00:58&lt;01:11,  2.46s/trial, best loss: -0.8361261980967356] 44%|████▍     | 22/50 [00:58&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009310 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13057\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 211\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Early stopping, best iteration is:\n[63]    training's binary_logloss: 0.117775 valid_1's binary_logloss: 0.131201\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [00:59&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009518 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13161\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Early stopping, best iteration is:\n[55]    training's binary_logloss: 0.11576  valid_1's binary_logloss: 0.138797\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [01:00&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009738 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13044\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  Early stopping, best iteration is:\n[48]    training's binary_logloss: 0.119114 valid_1's binary_logloss: 0.136592\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 44%|████▍     | 22/50 [01:01&lt;01:12,  2.58s/trial, best loss: -0.8361261980967356] 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009989 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 12993\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.121674 valid_1's binary_logloss: 0.131107\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [01:01&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009480 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 13086\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.11765  valid_1's binary_logloss: 0.138596\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [01:02&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009169 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Total Bins 12996\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Training until validation scores don't improve for 30 rounds\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  Did not meet early stopping. Best iteration is:\n[95]    training's binary_logloss: 0.119781 valid_1's binary_logloss: 0.136145\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 46%|████▌     | 23/50 [01:03&lt;01:11,  2.65s/trial, best loss: -0.8361261980967356] 48%|████▊     | 24/50 [01:03&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:03&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:03&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008895 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Total Bins 12993\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Training until validation scores don't improve for 30 rounds\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.123945 valid_1's binary_logloss: 0.131312\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:04&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009337 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Total Bins 13086\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Training until validation scores don't improve for 30 rounds\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.119785 valid_1's binary_logloss: 0.138758\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:05&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009450 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Total Bins 12996\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Training until validation scores don't improve for 30 rounds\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.121345 valid_1's binary_logloss: 0.136253\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 48%|████▊     | 24/50 [01:06&lt;01:04,  2.46s/trial, best loss: -0.8362934408440913] 50%|█████     | 25/50 [01:06&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:06&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:06&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010296 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 50%|█████     | 25/50 [01:07&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[99]    training's binary_logloss: 0.115076 valid_1's binary_logloss: 0.131544\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011525 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13086\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 50%|█████     | 25/50 [01:08&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[99]    training's binary_logloss: 0.110704 valid_1's binary_logloss: 0.139171\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009677 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 50%|█████     | 25/50 [01:09&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[74]    training's binary_logloss: 0.117386 valid_1's binary_logloss: 0.137077\n 50%|█████     | 25/50 [01:10&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 50%|█████     | 25/50 [01:10&lt;01:06,  2.65s/trial, best loss: -0.8365225708987197] 52%|█████▏    | 26/50 [01:10&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:10&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:10&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009136 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[40]    training's binary_logloss: 0.118745 valid_1's binary_logloss: 0.13174\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009702 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13086\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 52%|█████▏    | 26/50 [01:11&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[37]    training's binary_logloss: 0.115548 valid_1's binary_logloss: 0.138995\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010037 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 52%|█████▏    | 26/50 [01:12&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[32]    training's binary_logloss: 0.119523 valid_1's binary_logloss: 0.136814\n 52%|█████▏    | 26/50 [01:13&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 52%|█████▏    | 26/50 [01:13&lt;01:13,  3.05s/trial, best loss: -0.8365225708987197] 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009837 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 54%|█████▍    | 27/50 [01:13&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[97]    training's binary_logloss: 0.119337 valid_1's binary_logloss: 0.131417\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010306 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13086\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 54%|█████▍    | 27/50 [01:14&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.114456 valid_1's binary_logloss: 0.139157\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008807 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 54%|█████▍    | 27/50 [01:15&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[97]    training's binary_logloss: 0.11659  valid_1's binary_logloss: 0.136713\n 54%|█████▍    | 27/50 [01:16&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 54%|█████▍    | 27/50 [01:16&lt;01:07,  2.92s/trial, best loss: -0.8365225708987197] 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008596 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943\n 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 56%|█████▌    | 28/50 [01:16&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[22]    training's binary_logloss: 0.121698 valid_1's binary_logloss: 0.132138\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008173 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 56%|█████▌    | 28/50 [01:17&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[25]    training's binary_logloss: 0.11611  valid_1's binary_logloss: 0.139307\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008090 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12958\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[17]    training's binary_logloss: 0.122317 valid_1's binary_logloss: 0.136889\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 56%|█████▌    | 28/50 [01:18&lt;01:06,  3.03s/trial, best loss: -0.8365225708987197] 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009576 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 58%|█████▊    | 29/50 [01:18&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.117385 valid_1's binary_logloss: 0.131388\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 58%|█████▊    | 29/50 [01:19&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008772 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[98]    training's binary_logloss: 0.113488 valid_1's binary_logloss: 0.139105\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:20&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008569 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12958\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.114829 valid_1's binary_logloss: 0.136714\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 58%|█████▊    | 29/50 [01:21&lt;00:57,  2.74s/trial, best loss: -0.8365225708987197] 60%|██████    | 30/50 [01:21&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:21&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:21&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008604 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.136137 valid_1's binary_logloss: 0.135864\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007794 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13059\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 60%|██████    | 30/50 [01:22&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.131758 valid_1's binary_logloss: 0.143909\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008173 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 60%|██████    | 30/50 [01:23&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.133319 valid_1's binary_logloss: 0.140365\n 60%|██████    | 30/50 [01:24&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 60%|██████    | 30/50 [01:24&lt;00:56,  2.84s/trial, best loss: -0.8365225708987197] 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007810 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12902\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 62%|██████▏   | 31/50 [01:24&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[52]    training's binary_logloss: 0.120557 valid_1's binary_logloss: 0.131463\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008104 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[42]    training's binary_logloss: 0.119216 valid_1's binary_logloss: 0.138844\n 62%|██████▏   | 31/50 [01:25&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008458 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[43]    training's binary_logloss: 0.120672 valid_1's binary_logloss: 0.136077\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 62%|██████▏   | 31/50 [01:26&lt;00:52,  2.78s/trial, best loss: -0.8365225708987197] 64%|██████▍   | 32/50 [01:26&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:26&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:26&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008051 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.116782 valid_1's binary_logloss: 0.132291\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008398 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 64%|██████▍   | 32/50 [01:27&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.112525 valid_1's binary_logloss: 0.139834\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008790 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12958\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 64%|██████▍   | 32/50 [01:28&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[26]    training's binary_logloss: 0.116376 valid_1's binary_logloss: 0.13759\n 64%|██████▍   | 32/50 [01:29&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 64%|██████▍   | 32/50 [01:29&lt;00:47,  2.65s/trial, best loss: -0.8365225708987197] 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007899 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13047\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 210\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.121235 valid_1's binary_logloss: 0.131795\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:29&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13161\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 208\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 66%|██████▌   | 33/50 [01:30&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.117131 valid_1's binary_logloss: 0.139355\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009935 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13044\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 66%|██████▌   | 33/50 [01:31&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.11886  valid_1's binary_logloss: 0.136817\n 66%|██████▌   | 33/50 [01:32&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 66%|██████▌   | 33/50 [01:32&lt;00:43,  2.55s/trial, best loss: -0.8365225708987197] 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009479 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13047\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 210\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[87]    training's binary_logloss: 0.119217 valid_1's binary_logloss: 0.131162\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009390 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13130\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 68%|██████▊   | 34/50 [01:32&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[95]    training's binary_logloss: 0.11377  valid_1's binary_logloss: 0.138774\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008385 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13000\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 68%|██████▊   | 34/50 [01:33&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[71]    training's binary_logloss: 0.119355 valid_1's binary_logloss: 0.136516\n 68%|██████▊   | 34/50 [01:34&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 68%|██████▊   | 34/50 [01:34&lt;00:44,  2.76s/trial, best loss: -0.8365225708987197] 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008260 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12993\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[20]    training's binary_logloss: 0.117227 valid_1's binary_logloss: 0.132479\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:34&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007966 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13059\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[22]    training's binary_logloss: 0.110745 valid_1's binary_logloss: 0.140016\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007917 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 70%|███████   | 35/50 [01:35&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[18]    training's binary_logloss: 0.116325 valid_1's binary_logloss: 0.136868\n 70%|███████   | 35/50 [01:36&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 70%|███████   | 35/50 [01:36&lt;00:37,  2.51s/trial, best loss: -0.8365225708987197] 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007671 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12902\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 72%|███████▏  | 36/50 [01:36&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.133676 valid_1's binary_logloss: 0.135443\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007838 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 72%|███████▏  | 36/50 [01:37&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.129111 valid_1's binary_logloss: 0.143846\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008862 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 72%|███████▏  | 36/50 [01:38&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.131021 valid_1's binary_logloss: 0.140157\n 72%|███████▏  | 36/50 [01:39&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 72%|███████▏  | 36/50 [01:39&lt;00:32,  2.33s/trial, best loss: -0.8365225708987197] 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007790 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 74%|███████▍  | 37/50 [01:39&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[65]    training's binary_logloss: 0.118118 valid_1's binary_logloss: 0.131584\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007983 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 74%|███████▍  | 37/50 [01:40&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[65]    training's binary_logloss: 0.113718 valid_1's binary_logloss: 0.139017\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008166 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12906\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 74%|███████▍  | 37/50 [01:41&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[59]    training's binary_logloss: 0.116916 valid_1's binary_logloss: 0.136382\n 74%|███████▍  | 37/50 [01:42&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 74%|███████▍  | 37/50 [01:42&lt;00:34,  2.62s/trial, best loss: -0.8365225708987197] 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008362 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 76%|███████▌  | 38/50 [01:42&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[43]    training's binary_logloss: 0.120211 valid_1's binary_logloss: 0.131444\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011524 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[47]    training's binary_logloss: 0.114602 valid_1's binary_logloss: 0.139106\n 76%|███████▌  | 38/50 [01:43&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008487 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12968\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[40]    training's binary_logloss: 0.118651 valid_1's binary_logloss: 0.136544\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 76%|███████▌  | 38/50 [01:44&lt;00:32,  2.68s/trial, best loss: -0.8365225708987197] 78%|███████▊  | 39/50 [01:44&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:44&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:44&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007906 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12947\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[40]    training's binary_logloss: 0.119116 valid_1's binary_logloss: 0.131438\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010484 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13059\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 78%|███████▊  | 39/50 [01:45&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[37]    training's binary_logloss: 0.116085 valid_1's binary_logloss: 0.138771\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008602 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[34]    training's binary_logloss: 0.11889  valid_1's binary_logloss: 0.136703\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 78%|███████▊  | 39/50 [01:46&lt;00:28,  2.62s/trial, best loss: -0.8365225708987197] 80%|████████  | 40/50 [01:46&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009081 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12947\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.119904 valid_1's binary_logloss: 0.131539\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:47&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011067 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13055\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[28]    training's binary_logloss: 0.115561 valid_1's binary_logloss: 0.139612\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007772 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 80%|████████  | 40/50 [01:48&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[26]    training's binary_logloss: 0.118624 valid_1's binary_logloss: 0.136879\n 80%|████████  | 40/50 [01:49&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 80%|████████  | 40/50 [01:49&lt;00:24,  2.49s/trial, best loss: -0.8365225708987197] 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012833 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[39]    training's binary_logloss: 0.121601 valid_1's binary_logloss: 0.131732\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:49&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012323 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[34]    training's binary_logloss: 0.118977 valid_1's binary_logloss: 0.13905\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008469 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12958\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 82%|████████▏ | 41/50 [01:50&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[34]    training's binary_logloss: 0.120814 valid_1's binary_logloss: 0.136438\n 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 82%|████████▏ | 41/50 [01:51&lt;00:21,  2.40s/trial, best loss: -0.8365225708987197] 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008567 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12947\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 84%|████████▍ | 42/50 [01:51&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.119106 valid_1's binary_logloss: 0.131122\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011181 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12998\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 194\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 84%|████████▍ | 42/50 [01:52&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[90]    training's binary_logloss: 0.116567 valid_1's binary_logloss: 0.138873\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007625 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12968\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 199\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 84%|████████▍ | 42/50 [01:53&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[85]    training's binary_logloss: 0.118801 valid_1's binary_logloss: 0.136111\n 84%|████████▍ | 42/50 [01:54&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 84%|████████▍ | 42/50 [01:54&lt;00:18,  2.35s/trial, best loss: -0.8365225708987197] 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008375 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13047\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 210\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 86%|████████▌ | 43/50 [01:54&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.12624  valid_1's binary_logloss: 0.132688\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008360 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13130\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 205\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 86%|████████▌ | 43/50 [01:55&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.121959 valid_1's binary_logloss: 0.140097\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012750 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13000\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 86%|████████▌ | 43/50 [01:56&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.123583 valid_1's binary_logloss: 0.137169\n 86%|████████▌ | 43/50 [01:57&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 86%|████████▌ | 43/50 [01:57&lt;00:17,  2.52s/trial, best loss: -0.8365225708987197] 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009549 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12902\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 88%|████████▊ | 44/50 [01:57&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[23]    training's binary_logloss: 0.114805 valid_1's binary_logloss: 0.132779\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007723 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[18]    training's binary_logloss: 0.11472  valid_1's binary_logloss: 0.140404\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:58&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[19]    training's binary_logloss: 0.115511 valid_1's binary_logloss: 0.137588\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 88%|████████▊ | 44/50 [01:59&lt;00:16,  2.70s/trial, best loss: -0.8365225708987197] 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008368 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12835\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 90%|█████████ | 45/50 [01:59&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[70]    training's binary_logloss: 0.115612 valid_1's binary_logloss: 0.131625\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007622 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 90%|█████████ | 45/50 [02:00&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[57]    training's binary_logloss: 0.114417 valid_1's binary_logloss: 0.139373\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007511 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 90%|█████████ | 45/50 [02:01&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[62]    training's binary_logloss: 0.114805 valid_1's binary_logloss: 0.136936\n 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 90%|█████████ | 45/50 [02:02&lt;00:12,  2.56s/trial, best loss: -0.8365225708987197] 92%|█████████▏| 46/50 [02:02&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008690 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12943\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[14]    training's binary_logloss: 0.123339 valid_1's binary_logloss: 0.132372\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009379 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 92%|█████████▏| 46/50 [02:03&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[13]    training's binary_logloss: 0.119633 valid_1's binary_logloss: 0.141193\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008788 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12906\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 195\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[13]    training's binary_logloss: 0.12138  valid_1's binary_logloss: 0.137428\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 92%|█████████▏| 46/50 [02:04&lt;00:10,  2.75s/trial, best loss: -0.8365225708987197] 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008990 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12835\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[52]    training's binary_logloss: 0.119292 valid_1's binary_logloss: 0.131268\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [02:04&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007926 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[51]    training's binary_logloss: 0.11486  valid_1's binary_logloss: 0.139012\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007646 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 94%|█████████▍| 47/50 [02:05&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[45]    training's binary_logloss: 0.118593 valid_1's binary_logloss: 0.13685\n 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 94%|█████████▍| 47/50 [02:06&lt;00:07,  2.52s/trial, best loss: -0.8365225708987197] 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009040 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12902\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 197\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 96%|█████████▌| 48/50 [02:06&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.123923 valid_1's binary_logloss: 0.132366\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009230 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12988\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 96%|█████████▌| 48/50 [02:07&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.119581 valid_1's binary_logloss: 0.140569\n 96%|█████████▌| 48/50 [02:08&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:08&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:08&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:08&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010048 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12898\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 192\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 96%|█████████▌| 48/50 [02:09&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  Did not meet early stopping. Best iteration is:\n[100]   training's binary_logloss: 0.121237 valid_1's binary_logloss: 0.137601\n 96%|█████████▌| 48/50 [02:10&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 96%|█████████▌| 48/50 [02:10&lt;00:04,  2.25s/trial, best loss: -0.8365225708987197] 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1647, number of negative: 38897\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010404 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12947\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 203\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.040623 -&gt; initscore=-3.161962\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.161962\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[26]    training's binary_logloss: 0.118459 valid_1's binary_logloss: 0.131829\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:10&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1572, number of negative: 38972\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010339 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 13059\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 200\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038773 -&gt; initscore=-3.210495\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.210495\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[29]    training's binary_logloss: 0.11189  valid_1's binary_logloss: 0.139652\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of positive: 1619, number of negative: 38925\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010263 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Total Bins 12996\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Number of data points in the train set: 40544, number of used features: 202\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] early_stopping_round is set=30, early_stopping_rounds=30 will be ignored. Current value: early_stopping_round=30\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039932 -&gt; initscore=-3.179828\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Info] Start training from score -3.179828\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Training until validation scores don't improve for 30 rounds\n 98%|█████████▊| 49/50 [02:11&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  Early stopping, best iteration is:\n[26]    training's binary_logloss: 0.115607 valid_1's binary_logloss: 0.137612\n 98%|█████████▊| 49/50 [02:12&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]                                                                                  [LightGBM] [Warning] Unknown parameter: eval_metric\n 98%|█████████▊| 49/50 [02:12&lt;00:02,  2.66s/trial, best loss: -0.8365225708987197]100%|██████████| 50/50 [02:12&lt;00:00,  2.55s/trial, best loss: -0.8365225708987197]100%|██████████| 50/50 [02:12&lt;00:00,  2.65s/trial, best loss: -0.8365225708987197]\n{'learning_rate': 0.028291797782733982, 'max_depth': 154.0, 'min_child_samples': 64.0, 'num_leaves': 32.0, 'subsample': 0.9145203867432408}\n\n\n\n\n재학습\n\nlgbm_clf = LGBMClassifier(n_estimators=500, \n                          num_leaves=int(best['num_leaves']),\n                          max_depth=int(best['max_depth']),\n                          min_child_samples=int(best['min_child_samples']),\n                          subsample=round(best['subsample'], 5),\n                          learning_rate=round(best['learning_rate'], 5),\n                          early_stopping_rounds=100, \n                          eval_metric='auc')\n\neval_set = [(X_tr, y_tr), (X_val, y_val)]\nlgbm_clf.fit(X_tr, y_tr, eval_set=eval_set)\n\nlgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1])\nprint(f'{lgbm_roc_score:.3f}')\n\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Info] Number of positive: 1694, number of negative: 40877\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012601 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 13334\n[LightGBM] [Info] Number of data points in the train set: 42571, number of used features: 209\n[LightGBM] [Warning] Unknown parameter: eval_metric\n[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039792 -&gt; initscore=-3.183475\n[LightGBM] [Info] Start training from score -3.183475\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[131]   training's binary_logloss: 0.118645 valid_1's binary_logloss: 0.137022\n[LightGBM] [Warning] Unknown parameter: eval_metric\n0.839",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html#제출",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/03.html#제출",
    "title": "분류 - 산탄데르 고객 만족 예측",
    "section": "제출",
    "text": "제출\n\ntarget = lgbm_clf.predict(test_df)\n\nsubmit = pd.read_csv('_data/santander/sample_submission.csv', encoding='latin-1')\nsubmit['TARGET'] = target\nsubmit.to_csv('_data/santander/submission.csv', encoding='latin-1', index=False)\n\n[LightGBM] [Warning] Unknown parameter: eval_metric",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 산탄데르 고객 만족 예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#voting",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#voting",
    "title": "분류 - 앙상블",
    "section": "voting",
    "text": "voting\n\n서로 다른 알고리즘이 결합. 분류에서는 voting1으로 결정\n\n\nExample\n\nimport pandas as pd\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ncancer = load_breast_cancer()\n\ndf = pd.DataFrame(cancer.data, columns=cancer.feature_names)\ndf\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows × 30 columns\n\n\n\n\nlr_clf = LogisticRegression(solver='liblinear')\nknn_clf = KNeighborsClassifier(n_neighbors=8)\n\nvo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)],\n                          voting='soft')\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\nvo_clf.fit(X_train, y_train)\npred = vo_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n\n0.9385964912280702\n\n\n\nfor classifier in [lr_clf, knn_clf]:\n    classifier.fit(X_train, y_train)\n    pred = classifier.predict(X_test)\n    class_name = classifier.__class__.__name__\n    print(f'{class_name} 정확도: {accuracy_score(y_test, pred):.4f}')\n\nLogisticRegression 정확도: 0.9386\nKNeighborsClassifier 정확도: 0.9123\n\n\n\n반드시 voting이 제일 좋은 모델을 선택하는 것보다 좋은건 아님",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#bagging",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#bagging",
    "title": "분류 - 앙상블",
    "section": "bagging",
    "text": "bagging\n\n같은 유형의 알고리즘의 분류기가 boostrap 해가서 예측. random forest가 대표적. 분류에서는 voting2으로 결정\n\n\nRandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] &gt; 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\n\n\nrf_clf = RandomForestClassifier(max_depth=8)\nrf_clf.fit(X_train, y_train)\npred = rf_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n\n0.9216152019002375",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#boosting",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#boosting",
    "title": "분류 - 앙상블",
    "section": "boosting",
    "text": "boosting\n\nGBM\n\n# from sklearn.ensemble import GradientBoostingClassifier\n# import time\n# \n# X_train, X_test, y_train, y_test = get_human_dataset()\n# start_time = time.time()\n# \n# gb_clf = GradientBoostingClassifier()\n# gb_clf.fit(X_train, y_train)\n# gb_pred = gb_clf.predict(X_test)\n# gb_accuracy = accuracy_score(y_test, gb_pred)\n#\n# end_time = time.time()\n#\n# print(f'{gb_accuracy:.3f}, {end_time - start_time}초')\n\n0.939, 701.6343066692352초\n\n아주 오래 걸림.\n\n\n\nXGBoost\n\n결손값을 자체 처리할 수 있다.\n조기 종료 기능이 있다.\n자체적으로 교차 검증, 성능 평가, 피처 중요도 시각화 기능이 있다.\npython xgboost\n\n\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport numpy as np\n\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n\n\ndtr = xgb.DMatrix(data=X_tr, label=y_tr)\ndval = xgb.DMatrix(data=X_val, label=y_val)\ndtest = xgb.DMatrix(data=X_test, label=y_test)\n\n\nparams = {\n    'max_depth': 3,\n    'eta': 0.05,\n    'objective': 'binary:logistic',\n    'eval_metric': 'logloss'\n}\nnum_rounds = 400\n\n\neval_list = [(dtr, 'train'), (dval, 'eval')]\n\nxgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round=num_rounds, early_stopping_rounds=50, evals=eval_list)\n\n[0] train-logloss:0.62424   eval-logloss:0.59777\n[1] train-logloss:0.58779   eval-logloss:0.56156\n[2] train-logloss:0.55432   eval-logloss:0.53054\n[3] train-logloss:0.52351   eval-logloss:0.50160\n[4] train-logloss:0.49539   eval-logloss:0.47522\n[5] train-logloss:0.46757   eval-logloss:0.44992\n[6] train-logloss:0.44383   eval-logloss:0.42761\n[7] train-logloss:0.42005   eval-logloss:0.40613\n[8] train-logloss:0.39808   eval-logloss:0.38633\n[9] train-logloss:0.37870   eval-logloss:0.36842\n[10]    train-logloss:0.35969   eval-logloss:0.35141\n[11]    train-logloss:0.34290   eval-logloss:0.33598\n[12]    train-logloss:0.32758   eval-logloss:0.32125\n[13]    train-logloss:0.31287   eval-logloss:0.30786\n[14]    train-logloss:0.29892   eval-logloss:0.29789\n[15]    train-logloss:0.28558   eval-logloss:0.28766\n[16]    train-logloss:0.27366   eval-logloss:0.27631\n[17]    train-logloss:0.26184   eval-logloss:0.26744\n[18]    train-logloss:0.25113   eval-logloss:0.25662\n[19]    train-logloss:0.24069   eval-logloss:0.24660\n[20]    train-logloss:0.23095   eval-logloss:0.24040\n[21]    train-logloss:0.22133   eval-logloss:0.23252\n[22]    train-logloss:0.21279   eval-logloss:0.22393\n[23]    train-logloss:0.20420   eval-logloss:0.21701\n[24]    train-logloss:0.19629   eval-logloss:0.20948\n[25]    train-logloss:0.18899   eval-logloss:0.20315\n[26]    train-logloss:0.18165   eval-logloss:0.19747\n[27]    train-logloss:0.17512   eval-logloss:0.19161\n[28]    train-logloss:0.16837   eval-logloss:0.18409\n[29]    train-logloss:0.16213   eval-logloss:0.17747\n[30]    train-logloss:0.15644   eval-logloss:0.17195\n[31]    train-logloss:0.15069   eval-logloss:0.16564\n[32]    train-logloss:0.14545   eval-logloss:0.16122\n[33]    train-logloss:0.14039   eval-logloss:0.15579\n[34]    train-logloss:0.13553   eval-logloss:0.15171\n[35]    train-logloss:0.13076   eval-logloss:0.14631\n[36]    train-logloss:0.12661   eval-logloss:0.14321\n[37]    train-logloss:0.12243   eval-logloss:0.13868\n[38]    train-logloss:0.11868   eval-logloss:0.13596\n[39]    train-logloss:0.11490   eval-logloss:0.13172\n[40]    train-logloss:0.11126   eval-logloss:0.12735\n[41]    train-logloss:0.10781   eval-logloss:0.12351\n[42]    train-logloss:0.10450   eval-logloss:0.11957\n[43]    train-logloss:0.10131   eval-logloss:0.11603\n[44]    train-logloss:0.09846   eval-logloss:0.11396\n[45]    train-logloss:0.09545   eval-logloss:0.11227\n[46]    train-logloss:0.09284   eval-logloss:0.11015\n[47]    train-logloss:0.09020   eval-logloss:0.10715\n[48]    train-logloss:0.08763   eval-logloss:0.10404\n[49]    train-logloss:0.08535   eval-logloss:0.10262\n[50]    train-logloss:0.08302   eval-logloss:0.10000\n[51]    train-logloss:0.08075   eval-logloss:0.09821\n[52]    train-logloss:0.07862   eval-logloss:0.09605\n[53]    train-logloss:0.07660   eval-logloss:0.09381\n[54]    train-logloss:0.07470   eval-logloss:0.09237\n[55]    train-logloss:0.07281   eval-logloss:0.09051\n[56]    train-logloss:0.07107   eval-logloss:0.08923\n[57]    train-logloss:0.06933   eval-logloss:0.08746\n[58]    train-logloss:0.06769   eval-logloss:0.08634\n[59]    train-logloss:0.06609   eval-logloss:0.08479\n[60]    train-logloss:0.06459   eval-logloss:0.08378\n[61]    train-logloss:0.06299   eval-logloss:0.08323\n[62]    train-logloss:0.06160   eval-logloss:0.08241\n[63]    train-logloss:0.06027   eval-logloss:0.08159\n[64]    train-logloss:0.05892   eval-logloss:0.08046\n[65]    train-logloss:0.05770   eval-logloss:0.07975\n[66]    train-logloss:0.05651   eval-logloss:0.07913\n[67]    train-logloss:0.05529   eval-logloss:0.07875\n[68]    train-logloss:0.05421   eval-logloss:0.07815\n[69]    train-logloss:0.05280   eval-logloss:0.07674\n[70]    train-logloss:0.05177   eval-logloss:0.07657\n[71]    train-logloss:0.05074   eval-logloss:0.07522\n[72]    train-logloss:0.04977   eval-logloss:0.07474\n[73]    train-logloss:0.04878   eval-logloss:0.07455\n[74]    train-logloss:0.04788   eval-logloss:0.07450\n[75]    train-logloss:0.04706   eval-logloss:0.07308\n[76]    train-logloss:0.04608   eval-logloss:0.07206\n[77]    train-logloss:0.04528   eval-logloss:0.07178\n[78]    train-logloss:0.04443   eval-logloss:0.07170\n[79]    train-logloss:0.04341   eval-logloss:0.07047\n[80]    train-logloss:0.04251   eval-logloss:0.06994\n[81]    train-logloss:0.04184   eval-logloss:0.06921\n[82]    train-logloss:0.04118   eval-logloss:0.06898\n[83]    train-logloss:0.04037   eval-logloss:0.06854\n[84]    train-logloss:0.03966   eval-logloss:0.06873\n[85]    train-logloss:0.03903   eval-logloss:0.06884\n[86]    train-logloss:0.03844   eval-logloss:0.06833\n[87]    train-logloss:0.03779   eval-logloss:0.06856\n[88]    train-logloss:0.03722   eval-logloss:0.06870\n[89]    train-logloss:0.03648   eval-logloss:0.06859\n[90]    train-logloss:0.03580   eval-logloss:0.06858\n[91]    train-logloss:0.03525   eval-logloss:0.06808\n[92]    train-logloss:0.03472   eval-logloss:0.06694\n[93]    train-logloss:0.03410   eval-logloss:0.06672\n[94]    train-logloss:0.03361   eval-logloss:0.06564\n[95]    train-logloss:0.03307   eval-logloss:0.06545\n[96]    train-logloss:0.03257   eval-logloss:0.06536\n[97]    train-logloss:0.03205   eval-logloss:0.06465\n[98]    train-logloss:0.03150   eval-logloss:0.06441\n[99]    train-logloss:0.03105   eval-logloss:0.06357\n[100]   train-logloss:0.03057   eval-logloss:0.06387\n[101]   train-logloss:0.03006   eval-logloss:0.06370\n[102]   train-logloss:0.02967   eval-logloss:0.06274\n[103]   train-logloss:0.02925   eval-logloss:0.06244\n[104]   train-logloss:0.02878   eval-logloss:0.06232\n[105]   train-logloss:0.02836   eval-logloss:0.06265\n[106]   train-logloss:0.02799   eval-logloss:0.06195\n[107]   train-logloss:0.02764   eval-logloss:0.06176\n[108]   train-logloss:0.02731   eval-logloss:0.06152\n[109]   train-logloss:0.02690   eval-logloss:0.06142\n[110]   train-logloss:0.02657   eval-logloss:0.06076\n[111]   train-logloss:0.02626   eval-logloss:0.06059\n[112]   train-logloss:0.02589   eval-logloss:0.06010\n[113]   train-logloss:0.02553   eval-logloss:0.06003\n[114]   train-logloss:0.02519   eval-logloss:0.05933\n[115]   train-logloss:0.02480   eval-logloss:0.05840\n[116]   train-logloss:0.02451   eval-logloss:0.05855\n[117]   train-logloss:0.02414   eval-logloss:0.05766\n[118]   train-logloss:0.02382   eval-logloss:0.05761\n[119]   train-logloss:0.02348   eval-logloss:0.05676\n[120]   train-logloss:0.02322   eval-logloss:0.05692\n[121]   train-logloss:0.02290   eval-logloss:0.05610\n[122]   train-logloss:0.02265   eval-logloss:0.05592\n[123]   train-logloss:0.02243   eval-logloss:0.05622\n[124]   train-logloss:0.02213   eval-logloss:0.05542\n[125]   train-logloss:0.02189   eval-logloss:0.05473\n[126]   train-logloss:0.02168   eval-logloss:0.05504\n[127]   train-logloss:0.02140   eval-logloss:0.05427\n[128]   train-logloss:0.02118   eval-logloss:0.05401\n[129]   train-logloss:0.02093   eval-logloss:0.05346\n[130]   train-logloss:0.02075   eval-logloss:0.05376\n[131]   train-logloss:0.02049   eval-logloss:0.05298\n[132]   train-logloss:0.02026   eval-logloss:0.05259\n[133]   train-logloss:0.02003   eval-logloss:0.05251\n[134]   train-logloss:0.01981   eval-logloss:0.05233\n[135]   train-logloss:0.01962   eval-logloss:0.05210\n[136]   train-logloss:0.01945   eval-logloss:0.05208\n[137]   train-logloss:0.01925   eval-logloss:0.05146\n[138]   train-logloss:0.01909   eval-logloss:0.05080\n[139]   train-logloss:0.01891   eval-logloss:0.05059\n[140]   train-logloss:0.01877   eval-logloss:0.05075\n[141]   train-logloss:0.01862   eval-logloss:0.05036\n[142]   train-logloss:0.01846   eval-logloss:0.05019\n[143]   train-logloss:0.01826   eval-logloss:0.04959\n[144]   train-logloss:0.01807   eval-logloss:0.04956\n[145]   train-logloss:0.01791   eval-logloss:0.04940\n[146]   train-logloss:0.01776   eval-logloss:0.04897\n[147]   train-logloss:0.01761   eval-logloss:0.04879\n[148]   train-logloss:0.01743   eval-logloss:0.04864\n[149]   train-logloss:0.01730   eval-logloss:0.04864\n[150]   train-logloss:0.01718   eval-logloss:0.04833\n[151]   train-logloss:0.01703   eval-logloss:0.04792\n[152]   train-logloss:0.01690   eval-logloss:0.04819\n[153]   train-logloss:0.01677   eval-logloss:0.04801\n[154]   train-logloss:0.01662   eval-logloss:0.04794\n[155]   train-logloss:0.01646   eval-logloss:0.04764\n[156]   train-logloss:0.01631   eval-logloss:0.04754\n[157]   train-logloss:0.01619   eval-logloss:0.04778\n[158]   train-logloss:0.01609   eval-logloss:0.04780\n[159]   train-logloss:0.01598   eval-logloss:0.04805\n[160]   train-logloss:0.01584   eval-logloss:0.04791\n[161]   train-logloss:0.01575   eval-logloss:0.04757\n[162]   train-logloss:0.01565   eval-logloss:0.04760\n[163]   train-logloss:0.01555   eval-logloss:0.04782\n[164]   train-logloss:0.01547   eval-logloss:0.04758\n[165]   train-logloss:0.01532   eval-logloss:0.04775\n[166]   train-logloss:0.01522   eval-logloss:0.04799\n[167]   train-logloss:0.01514   eval-logloss:0.04821\n[168]   train-logloss:0.01507   eval-logloss:0.04827\n[169]   train-logloss:0.01499   eval-logloss:0.04803\n[170]   train-logloss:0.01485   eval-logloss:0.04819\n[171]   train-logloss:0.01475   eval-logloss:0.04788\n[172]   train-logloss:0.01465   eval-logloss:0.04811\n[173]   train-logloss:0.01456   eval-logloss:0.04763\n[174]   train-logloss:0.01445   eval-logloss:0.04770\n[175]   train-logloss:0.01432   eval-logloss:0.04777\n[176]   train-logloss:0.01425   eval-logloss:0.04780\n[177]   train-logloss:0.01418   eval-logloss:0.04809\n[178]   train-logloss:0.01411   eval-logloss:0.04782\n[179]   train-logloss:0.01405   eval-logloss:0.04787\n[180]   train-logloss:0.01394   eval-logloss:0.04793\n[181]   train-logloss:0.01387   eval-logloss:0.04771\n[182]   train-logloss:0.01377   eval-logloss:0.04778\n[183]   train-logloss:0.01371   eval-logloss:0.04738\n[184]   train-logloss:0.01363   eval-logloss:0.04761\n[185]   train-logloss:0.01353   eval-logloss:0.04763\n[186]   train-logloss:0.01346   eval-logloss:0.04737\n[187]   train-logloss:0.01340   eval-logloss:0.04761\n[188]   train-logloss:0.01335   eval-logloss:0.04761\n[189]   train-logloss:0.01329   eval-logloss:0.04757\n[190]   train-logloss:0.01323   eval-logloss:0.04718\n[191]   train-logloss:0.01318   eval-logloss:0.04718\n[192]   train-logloss:0.01312   eval-logloss:0.04715\n[193]   train-logloss:0.01306   eval-logloss:0.04690\n[194]   train-logloss:0.01300   eval-logloss:0.04713\n[195]   train-logloss:0.01295   eval-logloss:0.04713\n[196]   train-logloss:0.01287   eval-logloss:0.04715\n[197]   train-logloss:0.01282   eval-logloss:0.04691\n[198]   train-logloss:0.01272   eval-logloss:0.04719\n[199]   train-logloss:0.01266   eval-logloss:0.04742\n[200]   train-logloss:0.01261   eval-logloss:0.04718\n[201]   train-logloss:0.01256   eval-logloss:0.04697\n[202]   train-logloss:0.01251   eval-logloss:0.04703\n[203]   train-logloss:0.01243   eval-logloss:0.04704\n[204]   train-logloss:0.01235   eval-logloss:0.04707\n[205]   train-logloss:0.01230   eval-logloss:0.04694\n[206]   train-logloss:0.01225   eval-logloss:0.04699\n[207]   train-logloss:0.01218   eval-logloss:0.04693\n[208]   train-logloss:0.01213   eval-logloss:0.04713\n[209]   train-logloss:0.01208   eval-logloss:0.04689\n[210]   train-logloss:0.01203   eval-logloss:0.04676\n[211]   train-logloss:0.01195   eval-logloss:0.04645\n[212]   train-logloss:0.01190   eval-logloss:0.04666\n[213]   train-logloss:0.01185   eval-logloss:0.04634\n[214]   train-logloss:0.01181   eval-logloss:0.04640\n[215]   train-logloss:0.01176   eval-logloss:0.04627\n[216]   train-logloss:0.01169   eval-logloss:0.04621\n[217]   train-logloss:0.01164   eval-logloss:0.04611\n[218]   train-logloss:0.01160   eval-logloss:0.04588\n[219]   train-logloss:0.01155   eval-logloss:0.04576\n[220]   train-logloss:0.01149   eval-logloss:0.04570\n[221]   train-logloss:0.01145   eval-logloss:0.04594\n[222]   train-logloss:0.01141   eval-logloss:0.04600\n[223]   train-logloss:0.01137   eval-logloss:0.04573\n[224]   train-logloss:0.01133   eval-logloss:0.04552\n[225]   train-logloss:0.01128   eval-logloss:0.04539\n[226]   train-logloss:0.01122   eval-logloss:0.04534\n[227]   train-logloss:0.01118   eval-logloss:0.04554\n[228]   train-logloss:0.01114   eval-logloss:0.04536\n[229]   train-logloss:0.01109   eval-logloss:0.04550\n[230]   train-logloss:0.01105   eval-logloss:0.04529\n[231]   train-logloss:0.01100   eval-logloss:0.04516\n[232]   train-logloss:0.01095   eval-logloss:0.04511\n[233]   train-logloss:0.01091   eval-logloss:0.04516\n[234]   train-logloss:0.01087   eval-logloss:0.04536\n[235]   train-logloss:0.01083   eval-logloss:0.04514\n[236]   train-logloss:0.01079   eval-logloss:0.04536\n[237]   train-logloss:0.01075   eval-logloss:0.04515\n[238]   train-logloss:0.01071   eval-logloss:0.04503\n[239]   train-logloss:0.01066   eval-logloss:0.04498\n[240]   train-logloss:0.01063   eval-logloss:0.04505\n[241]   train-logloss:0.01058   eval-logloss:0.04519\n[242]   train-logloss:0.01053   eval-logloss:0.04511\n[243]   train-logloss:0.01049   eval-logloss:0.04499\n[244]   train-logloss:0.01045   eval-logloss:0.04478\n[245]   train-logloss:0.01041   eval-logloss:0.04500\n[246]   train-logloss:0.01036   eval-logloss:0.04492\n[247]   train-logloss:0.01032   eval-logloss:0.04506\n[248]   train-logloss:0.01028   eval-logloss:0.04485\n[249]   train-logloss:0.01025   eval-logloss:0.04473\n[250]   train-logloss:0.01022   eval-logloss:0.04477\n[251]   train-logloss:0.01016   eval-logloss:0.04469\n[252]   train-logloss:0.01013   eval-logloss:0.04488\n[253]   train-logloss:0.01009   eval-logloss:0.04477\n[254]   train-logloss:0.01006   eval-logloss:0.04456\n[255]   train-logloss:0.01003   eval-logloss:0.04436\n[256]   train-logloss:0.01000   eval-logloss:0.04440\n[257]   train-logloss:0.00995   eval-logloss:0.04433\n[258]   train-logloss:0.00991   eval-logloss:0.04446\n[259]   train-logloss:0.00987   eval-logloss:0.04435\n[260]   train-logloss:0.00984   eval-logloss:0.04415\n[261]   train-logloss:0.00981   eval-logloss:0.04398\n[262]   train-logloss:0.00976   eval-logloss:0.04391\n[263]   train-logloss:0.00973   eval-logloss:0.04380\n[264]   train-logloss:0.00970   eval-logloss:0.04386\n[265]   train-logloss:0.00968   eval-logloss:0.04365\n[266]   train-logloss:0.00964   eval-logloss:0.04378\n[267]   train-logloss:0.00961   eval-logloss:0.04383\n[268]   train-logloss:0.00958   eval-logloss:0.04373\n[269]   train-logloss:0.00955   eval-logloss:0.04356\n[270]   train-logloss:0.00952   eval-logloss:0.04374\n[271]   train-logloss:0.00947   eval-logloss:0.04369\n[272]   train-logloss:0.00944   eval-logloss:0.04358\n[273]   train-logloss:0.00942   eval-logloss:0.04364\n[274]   train-logloss:0.00939   eval-logloss:0.04345\n[275]   train-logloss:0.00936   eval-logloss:0.04362\n[276]   train-logloss:0.00933   eval-logloss:0.04346\n[277]   train-logloss:0.00929   eval-logloss:0.04359\n[278]   train-logloss:0.00927   eval-logloss:0.04338\n[279]   train-logloss:0.00924   eval-logloss:0.04342\n[280]   train-logloss:0.00922   eval-logloss:0.04357\n[281]   train-logloss:0.00917   eval-logloss:0.04352\n[282]   train-logloss:0.00916   eval-logloss:0.04339\n[283]   train-logloss:0.00914   eval-logloss:0.04321\n[284]   train-logloss:0.00911   eval-logloss:0.04305\n[285]   train-logloss:0.00907   eval-logloss:0.04317\n[286]   train-logloss:0.00905   eval-logloss:0.04331\n[287]   train-logloss:0.00903   eval-logloss:0.04311\n[288]   train-logloss:0.00900   eval-logloss:0.04296\n[289]   train-logloss:0.00899   eval-logloss:0.04299\n[290]   train-logloss:0.00896   eval-logloss:0.04317\n[291]   train-logloss:0.00894   eval-logloss:0.04307\n[292]   train-logloss:0.00892   eval-logloss:0.04309\n[293]   train-logloss:0.00890   eval-logloss:0.04327\n[294]   train-logloss:0.00887   eval-logloss:0.04308\n[295]   train-logloss:0.00885   eval-logloss:0.04286\n[296]   train-logloss:0.00883   eval-logloss:0.04301\n[297]   train-logloss:0.00882   eval-logloss:0.04288\n[298]   train-logloss:0.00878   eval-logloss:0.04285\n[299]   train-logloss:0.00876   eval-logloss:0.04266\n[300]   train-logloss:0.00874   eval-logloss:0.04253\n[301]   train-logloss:0.00873   eval-logloss:0.04244\n[302]   train-logloss:0.00871   eval-logloss:0.04230\n[303]   train-logloss:0.00869   eval-logloss:0.04233\n[304]   train-logloss:0.00868   eval-logloss:0.04249\n[305]   train-logloss:0.00867   eval-logloss:0.04236\n[306]   train-logloss:0.00866   eval-logloss:0.04239\n[307]   train-logloss:0.00865   eval-logloss:0.04255\n[308]   train-logloss:0.00863   eval-logloss:0.04243\n[309]   train-logloss:0.00862   eval-logloss:0.04234\n[310]   train-logloss:0.00860   eval-logloss:0.04220\n[311]   train-logloss:0.00857   eval-logloss:0.04237\n[312]   train-logloss:0.00855   eval-logloss:0.04219\n[313]   train-logloss:0.00854   eval-logloss:0.04223\n[314]   train-logloss:0.00853   eval-logloss:0.04237\n[315]   train-logloss:0.00850   eval-logloss:0.04223\n[316]   train-logloss:0.00849   eval-logloss:0.04227\n[317]   train-logloss:0.00848   eval-logloss:0.04215\n[318]   train-logloss:0.00847   eval-logloss:0.04206\n[319]   train-logloss:0.00844   eval-logloss:0.04223\n[320]   train-logloss:0.00843   eval-logloss:0.04211\n[321]   train-logloss:0.00842   eval-logloss:0.04226\n[322]   train-logloss:0.00841   eval-logloss:0.04229\n[323]   train-logloss:0.00840   eval-logloss:0.04245\n[324]   train-logloss:0.00839   eval-logloss:0.04233\n[325]   train-logloss:0.00838   eval-logloss:0.04216\n[326]   train-logloss:0.00836   eval-logloss:0.04199\n[327]   train-logloss:0.00835   eval-logloss:0.04202\n[328]   train-logloss:0.00834   eval-logloss:0.04217\n[329]   train-logloss:0.00831   eval-logloss:0.04220\n[330]   train-logloss:0.00827   eval-logloss:0.04218\n[331]   train-logloss:0.00826   eval-logloss:0.04206\n[332]   train-logloss:0.00825   eval-logloss:0.04188\n[333]   train-logloss:0.00823   eval-logloss:0.04182\n[334]   train-logloss:0.00822   eval-logloss:0.04174\n[335]   train-logloss:0.00820   eval-logloss:0.04168\n[336]   train-logloss:0.00819   eval-logloss:0.04154\n[337]   train-logloss:0.00818   eval-logloss:0.04171\n[338]   train-logloss:0.00814   eval-logloss:0.04169\n[339]   train-logloss:0.00813   eval-logloss:0.04158\n[340]   train-logloss:0.00812   eval-logloss:0.04145\n[341]   train-logloss:0.00811   eval-logloss:0.04129\n[342]   train-logloss:0.00810   eval-logloss:0.04145\n[343]   train-logloss:0.00809   eval-logloss:0.04149\n[344]   train-logloss:0.00808   eval-logloss:0.04138\n[345]   train-logloss:0.00807   eval-logloss:0.04152\n[346]   train-logloss:0.00806   eval-logloss:0.04138\n[347]   train-logloss:0.00805   eval-logloss:0.04145\n[348]   train-logloss:0.00804   eval-logloss:0.04136\n[349]   train-logloss:0.00803   eval-logloss:0.04120\n[350]   train-logloss:0.00802   eval-logloss:0.04124\n[351]   train-logloss:0.00801   eval-logloss:0.04138\n[352]   train-logloss:0.00800   eval-logloss:0.04127\n[353]   train-logloss:0.00799   eval-logloss:0.04114\n[354]   train-logloss:0.00798   eval-logloss:0.04128\n[355]   train-logloss:0.00797   eval-logloss:0.04116\n[356]   train-logloss:0.00796   eval-logloss:0.04103\n[357]   train-logloss:0.00795   eval-logloss:0.04110\n[358]   train-logloss:0.00794   eval-logloss:0.04113\n[359]   train-logloss:0.00793   eval-logloss:0.04108\n[360]   train-logloss:0.00792   eval-logloss:0.04099\n[361]   train-logloss:0.00791   eval-logloss:0.04084\n[362]   train-logloss:0.00790   eval-logloss:0.04090\n[363]   train-logloss:0.00789   eval-logloss:0.04077\n[364]   train-logloss:0.00788   eval-logloss:0.04080\n[365]   train-logloss:0.00787   eval-logloss:0.04095\n[366]   train-logloss:0.00786   eval-logloss:0.04084\n[367]   train-logloss:0.00785   eval-logloss:0.04075\n[368]   train-logloss:0.00784   eval-logloss:0.04060\n[369]   train-logloss:0.00783   eval-logloss:0.04055\n[370]   train-logloss:0.00783   eval-logloss:0.04058\n[371]   train-logloss:0.00782   eval-logloss:0.04072\n[372]   train-logloss:0.00781   eval-logloss:0.04059\n[373]   train-logloss:0.00780   eval-logloss:0.04065\n[374]   train-logloss:0.00779   eval-logloss:0.04057\n[375]   train-logloss:0.00778   eval-logloss:0.04046\n[376]   train-logloss:0.00777   eval-logloss:0.04060\n[377]   train-logloss:0.00776   eval-logloss:0.04063\n[378]   train-logloss:0.00775   eval-logloss:0.04053\n[379]   train-logloss:0.00774   eval-logloss:0.04066\n[380]   train-logloss:0.00773   eval-logloss:0.04053\n[381]   train-logloss:0.00772   eval-logloss:0.04048\n[382]   train-logloss:0.00772   eval-logloss:0.04034\n[383]   train-logloss:0.00771   eval-logloss:0.04037\n[384]   train-logloss:0.00770   eval-logloss:0.04053\n[385]   train-logloss:0.00769   eval-logloss:0.04045\n[386]   train-logloss:0.00768   eval-logloss:0.04034\n[387]   train-logloss:0.00767   eval-logloss:0.04020\n[388]   train-logloss:0.00766   eval-logloss:0.04023\n[389]   train-logloss:0.00765   eval-logloss:0.04037\n[390]   train-logloss:0.00764   eval-logloss:0.04025\n[391]   train-logloss:0.00764   eval-logloss:0.04038\n[392]   train-logloss:0.00763   eval-logloss:0.04033\n[393]   train-logloss:0.00762   eval-logloss:0.04022\n[394]   train-logloss:0.00761   eval-logloss:0.04015\n[395]   train-logloss:0.00760   eval-logloss:0.04018\n[396]   train-logloss:0.00759   eval-logloss:0.04031\n[397]   train-logloss:0.00758   eval-logloss:0.04021\n[398]   train-logloss:0.00758   eval-logloss:0.04007\n[399]   train-logloss:0.00757   eval-logloss:0.04022\n\n\n\npred_probs = xgb_model.predict(dtest)\npreds = [1 if x &gt; 0.5 else 0 for x in pred_probs]\n\n\nsklearn xgboost\n\n\nfrom xgboost import XGBClassifier\n\nevals = [(X_tr, y_tr), (X_val, y_val)]\nxgb = XGBClassifier(n_estimators=400, \n                    learning_rate=0.05, \n                    max_depth=3, \n                    early_stopping_rounds=50,\n                    eval_metric=['logloss'])\nxgb.fit(X_tr, y_tr, eval_set=evals)\npreds = xgb.predict(X_test)\npred_probs = xgb.predict_proba(X_test)[:, 1]\n\n[0] validation_0-logloss:0.62424    validation_1-logloss:0.59777\n[1] validation_0-logloss:0.58779    validation_1-logloss:0.56156\n[2] validation_0-logloss:0.55432    validation_1-logloss:0.53054\n[3] validation_0-logloss:0.52351    validation_1-logloss:0.50160\n[4] validation_0-logloss:0.49539    validation_1-logloss:0.47522\n[5] validation_0-logloss:0.46757    validation_1-logloss:0.44992\n[6] validation_0-logloss:0.44383    validation_1-logloss:0.42761\n[7] validation_0-logloss:0.42005    validation_1-logloss:0.40613\n[8] validation_0-logloss:0.39808    validation_1-logloss:0.38633\n[9] validation_0-logloss:0.37870    validation_1-logloss:0.36842\n[10]    validation_0-logloss:0.35969    validation_1-logloss:0.35141\n[11]    validation_0-logloss:0.34290    validation_1-logloss:0.33598\n[12]    validation_0-logloss:0.32758    validation_1-logloss:0.32125\n[13]    validation_0-logloss:0.31287    validation_1-logloss:0.30786\n[14]    validation_0-logloss:0.29892    validation_1-logloss:0.29789\n[15]    validation_0-logloss:0.28558    validation_1-logloss:0.28766\n[16]    validation_0-logloss:0.27366    validation_1-logloss:0.27631\n[17]    validation_0-logloss:0.26184    validation_1-logloss:0.26744\n[18]    validation_0-logloss:0.25113    validation_1-logloss:0.25662\n[19]    validation_0-logloss:0.24069    validation_1-logloss:0.24660\n[20]    validation_0-logloss:0.23095    validation_1-logloss:0.24040\n[21]    validation_0-logloss:0.22133    validation_1-logloss:0.23252\n[22]    validation_0-logloss:0.21279    validation_1-logloss:0.22393\n[23]    validation_0-logloss:0.20420    validation_1-logloss:0.21701\n[24]    validation_0-logloss:0.19629    validation_1-logloss:0.20948\n[25]    validation_0-logloss:0.18899    validation_1-logloss:0.20315\n[26]    validation_0-logloss:0.18165    validation_1-logloss:0.19747\n[27]    validation_0-logloss:0.17512    validation_1-logloss:0.19161\n[28]    validation_0-logloss:0.16837    validation_1-logloss:0.18409\n[29]    validation_0-logloss:0.16213    validation_1-logloss:0.17747\n[30]    validation_0-logloss:0.15644    validation_1-logloss:0.17195\n[31]    validation_0-logloss:0.15069    validation_1-logloss:0.16564\n[32]    validation_0-logloss:0.14545    validation_1-logloss:0.16122\n[33]    validation_0-logloss:0.14039    validation_1-logloss:0.15579\n[34]    validation_0-logloss:0.13553    validation_1-logloss:0.15171\n[35]    validation_0-logloss:0.13076    validation_1-logloss:0.14631\n[36]    validation_0-logloss:0.12661    validation_1-logloss:0.14321\n[37]    validation_0-logloss:0.12243    validation_1-logloss:0.13868\n[38]    validation_0-logloss:0.11868    validation_1-logloss:0.13596\n[39]    validation_0-logloss:0.11490    validation_1-logloss:0.13172\n[40]    validation_0-logloss:0.11126    validation_1-logloss:0.12735\n[41]    validation_0-logloss:0.10781    validation_1-logloss:0.12351\n[42]    validation_0-logloss:0.10450    validation_1-logloss:0.11957\n[43]    validation_0-logloss:0.10131    validation_1-logloss:0.11603\n[44]    validation_0-logloss:0.09846    validation_1-logloss:0.11396\n[45]    validation_0-logloss:0.09545    validation_1-logloss:0.11227\n[46]    validation_0-logloss:0.09284    validation_1-logloss:0.11015\n[47]    validation_0-logloss:0.09020    validation_1-logloss:0.10715\n[48]    validation_0-logloss:0.08763    validation_1-logloss:0.10404\n[49]    validation_0-logloss:0.08535    validation_1-logloss:0.10262\n[50]    validation_0-logloss:0.08302    validation_1-logloss:0.10000\n[51]    validation_0-logloss:0.08075    validation_1-logloss:0.09821\n[52]    validation_0-logloss:0.07862    validation_1-logloss:0.09605\n[53]    validation_0-logloss:0.07660    validation_1-logloss:0.09381\n[54]    validation_0-logloss:0.07470    validation_1-logloss:0.09237\n[55]    validation_0-logloss:0.07281    validation_1-logloss:0.09051\n[56]    validation_0-logloss:0.07107    validation_1-logloss:0.08923\n[57]    validation_0-logloss:0.06933    validation_1-logloss:0.08746\n[58]    validation_0-logloss:0.06769    validation_1-logloss:0.08634\n[59]    validation_0-logloss:0.06609    validation_1-logloss:0.08479\n[60]    validation_0-logloss:0.06459    validation_1-logloss:0.08378\n[61]    validation_0-logloss:0.06299    validation_1-logloss:0.08323\n[62]    validation_0-logloss:0.06160    validation_1-logloss:0.08241\n[63]    validation_0-logloss:0.06027    validation_1-logloss:0.08159\n[64]    validation_0-logloss:0.05892    validation_1-logloss:0.08046\n[65]    validation_0-logloss:0.05770    validation_1-logloss:0.07975\n[66]    validation_0-logloss:0.05651    validation_1-logloss:0.07913\n[67]    validation_0-logloss:0.05529    validation_1-logloss:0.07875\n[68]    validation_0-logloss:0.05421    validation_1-logloss:0.07815\n[69]    validation_0-logloss:0.05280    validation_1-logloss:0.07674\n[70]    validation_0-logloss:0.05177    validation_1-logloss:0.07657\n[71]    validation_0-logloss:0.05074    validation_1-logloss:0.07522\n[72]    validation_0-logloss:0.04977    validation_1-logloss:0.07474\n[73]    validation_0-logloss:0.04878    validation_1-logloss:0.07455\n[74]    validation_0-logloss:0.04788    validation_1-logloss:0.07450\n[75]    validation_0-logloss:0.04706    validation_1-logloss:0.07308\n[76]    validation_0-logloss:0.04608    validation_1-logloss:0.07206\n[77]    validation_0-logloss:0.04528    validation_1-logloss:0.07178\n[78]    validation_0-logloss:0.04443    validation_1-logloss:0.07170\n[79]    validation_0-logloss:0.04341    validation_1-logloss:0.07047\n[80]    validation_0-logloss:0.04251    validation_1-logloss:0.06994\n[81]    validation_0-logloss:0.04184    validation_1-logloss:0.06921\n[82]    validation_0-logloss:0.04118    validation_1-logloss:0.06898\n[83]    validation_0-logloss:0.04037    validation_1-logloss:0.06854\n[84]    validation_0-logloss:0.03966    validation_1-logloss:0.06873\n[85]    validation_0-logloss:0.03903    validation_1-logloss:0.06884\n[86]    validation_0-logloss:0.03844    validation_1-logloss:0.06833\n[87]    validation_0-logloss:0.03779    validation_1-logloss:0.06856\n[88]    validation_0-logloss:0.03722    validation_1-logloss:0.06870\n[89]    validation_0-logloss:0.03648    validation_1-logloss:0.06859\n[90]    validation_0-logloss:0.03580    validation_1-logloss:0.06858\n[91]    validation_0-logloss:0.03525    validation_1-logloss:0.06808\n[92]    validation_0-logloss:0.03472    validation_1-logloss:0.06694\n[93]    validation_0-logloss:0.03410    validation_1-logloss:0.06672\n[94]    validation_0-logloss:0.03361    validation_1-logloss:0.06564\n[95]    validation_0-logloss:0.03307    validation_1-logloss:0.06545\n[96]    validation_0-logloss:0.03257    validation_1-logloss:0.06536\n[97]    validation_0-logloss:0.03205    validation_1-logloss:0.06465\n[98]    validation_0-logloss:0.03150    validation_1-logloss:0.06441\n[99]    validation_0-logloss:0.03105    validation_1-logloss:0.06357\n[100]   validation_0-logloss:0.03057    validation_1-logloss:0.06387\n[101]   validation_0-logloss:0.03006    validation_1-logloss:0.06370\n[102]   validation_0-logloss:0.02967    validation_1-logloss:0.06274\n[103]   validation_0-logloss:0.02925    validation_1-logloss:0.06244\n[104]   validation_0-logloss:0.02878    validation_1-logloss:0.06232\n[105]   validation_0-logloss:0.02836    validation_1-logloss:0.06265\n[106]   validation_0-logloss:0.02799    validation_1-logloss:0.06195\n[107]   validation_0-logloss:0.02764    validation_1-logloss:0.06176\n[108]   validation_0-logloss:0.02731    validation_1-logloss:0.06152\n[109]   validation_0-logloss:0.02690    validation_1-logloss:0.06142\n[110]   validation_0-logloss:0.02657    validation_1-logloss:0.06076\n[111]   validation_0-logloss:0.02626    validation_1-logloss:0.06059\n[112]   validation_0-logloss:0.02589    validation_1-logloss:0.06010\n[113]   validation_0-logloss:0.02553    validation_1-logloss:0.06003\n[114]   validation_0-logloss:0.02519    validation_1-logloss:0.05933\n[115]   validation_0-logloss:0.02480    validation_1-logloss:0.05840\n[116]   validation_0-logloss:0.02451    validation_1-logloss:0.05855\n[117]   validation_0-logloss:0.02414    validation_1-logloss:0.05766\n[118]   validation_0-logloss:0.02382    validation_1-logloss:0.05761\n[119]   validation_0-logloss:0.02348    validation_1-logloss:0.05676\n[120]   validation_0-logloss:0.02322    validation_1-logloss:0.05692\n[121]   validation_0-logloss:0.02290    validation_1-logloss:0.05610\n[122]   validation_0-logloss:0.02265    validation_1-logloss:0.05592\n[123]   validation_0-logloss:0.02243    validation_1-logloss:0.05622\n[124]   validation_0-logloss:0.02213    validation_1-logloss:0.05542\n[125]   validation_0-logloss:0.02189    validation_1-logloss:0.05473\n[126]   validation_0-logloss:0.02168    validation_1-logloss:0.05504\n[127]   validation_0-logloss:0.02140    validation_1-logloss:0.05427\n[128]   validation_0-logloss:0.02118    validation_1-logloss:0.05401\n[129]   validation_0-logloss:0.02093    validation_1-logloss:0.05346\n[130]   validation_0-logloss:0.02075    validation_1-logloss:0.05376\n[131]   validation_0-logloss:0.02049    validation_1-logloss:0.05298\n[132]   validation_0-logloss:0.02026    validation_1-logloss:0.05259\n[133]   validation_0-logloss:0.02003    validation_1-logloss:0.05251\n[134]   validation_0-logloss:0.01981    validation_1-logloss:0.05233\n[135]   validation_0-logloss:0.01962    validation_1-logloss:0.05210\n[136]   validation_0-logloss:0.01945    validation_1-logloss:0.05208\n[137]   validation_0-logloss:0.01925    validation_1-logloss:0.05146\n[138]   validation_0-logloss:0.01909    validation_1-logloss:0.05080\n[139]   validation_0-logloss:0.01891    validation_1-logloss:0.05059\n[140]   validation_0-logloss:0.01877    validation_1-logloss:0.05075\n[141]   validation_0-logloss:0.01862    validation_1-logloss:0.05036\n[142]   validation_0-logloss:0.01846    validation_1-logloss:0.05019\n[143]   validation_0-logloss:0.01826    validation_1-logloss:0.04959\n[144]   validation_0-logloss:0.01807    validation_1-logloss:0.04956\n[145]   validation_0-logloss:0.01791    validation_1-logloss:0.04940\n[146]   validation_0-logloss:0.01776    validation_1-logloss:0.04897\n[147]   validation_0-logloss:0.01761    validation_1-logloss:0.04879\n[148]   validation_0-logloss:0.01743    validation_1-logloss:0.04864\n[149]   validation_0-logloss:0.01730    validation_1-logloss:0.04864\n[150]   validation_0-logloss:0.01718    validation_1-logloss:0.04833\n[151]   validation_0-logloss:0.01703    validation_1-logloss:0.04792\n[152]   validation_0-logloss:0.01690    validation_1-logloss:0.04819\n[153]   validation_0-logloss:0.01677    validation_1-logloss:0.04801\n[154]   validation_0-logloss:0.01662    validation_1-logloss:0.04794\n[155]   validation_0-logloss:0.01646    validation_1-logloss:0.04764\n[156]   validation_0-logloss:0.01631    validation_1-logloss:0.04754\n[157]   validation_0-logloss:0.01619    validation_1-logloss:0.04778\n[158]   validation_0-logloss:0.01609    validation_1-logloss:0.04780\n[159]   validation_0-logloss:0.01598    validation_1-logloss:0.04805\n[160]   validation_0-logloss:0.01584    validation_1-logloss:0.04791\n[161]   validation_0-logloss:0.01575    validation_1-logloss:0.04757\n[162]   validation_0-logloss:0.01565    validation_1-logloss:0.04760\n[163]   validation_0-logloss:0.01555    validation_1-logloss:0.04782\n[164]   validation_0-logloss:0.01547    validation_1-logloss:0.04758\n[165]   validation_0-logloss:0.01532    validation_1-logloss:0.04775\n[166]   validation_0-logloss:0.01522    validation_1-logloss:0.04799\n[167]   validation_0-logloss:0.01514    validation_1-logloss:0.04821\n[168]   validation_0-logloss:0.01507    validation_1-logloss:0.04827\n[169]   validation_0-logloss:0.01499    validation_1-logloss:0.04803\n[170]   validation_0-logloss:0.01485    validation_1-logloss:0.04819\n[171]   validation_0-logloss:0.01475    validation_1-logloss:0.04788\n[172]   validation_0-logloss:0.01465    validation_1-logloss:0.04811\n[173]   validation_0-logloss:0.01456    validation_1-logloss:0.04763\n[174]   validation_0-logloss:0.01445    validation_1-logloss:0.04770\n[175]   validation_0-logloss:0.01432    validation_1-logloss:0.04777\n[176]   validation_0-logloss:0.01425    validation_1-logloss:0.04780\n[177]   validation_0-logloss:0.01418    validation_1-logloss:0.04809\n[178]   validation_0-logloss:0.01411    validation_1-logloss:0.04782\n[179]   validation_0-logloss:0.01405    validation_1-logloss:0.04787\n[180]   validation_0-logloss:0.01394    validation_1-logloss:0.04793\n[181]   validation_0-logloss:0.01387    validation_1-logloss:0.04771\n[182]   validation_0-logloss:0.01377    validation_1-logloss:0.04778\n[183]   validation_0-logloss:0.01371    validation_1-logloss:0.04738\n[184]   validation_0-logloss:0.01363    validation_1-logloss:0.04761\n[185]   validation_0-logloss:0.01353    validation_1-logloss:0.04763\n[186]   validation_0-logloss:0.01346    validation_1-logloss:0.04737\n[187]   validation_0-logloss:0.01340    validation_1-logloss:0.04761\n[188]   validation_0-logloss:0.01335    validation_1-logloss:0.04761\n[189]   validation_0-logloss:0.01329    validation_1-logloss:0.04757\n[190]   validation_0-logloss:0.01323    validation_1-logloss:0.04718\n[191]   validation_0-logloss:0.01318    validation_1-logloss:0.04718\n[192]   validation_0-logloss:0.01312    validation_1-logloss:0.04715\n[193]   validation_0-logloss:0.01306    validation_1-logloss:0.04690\n[194]   validation_0-logloss:0.01300    validation_1-logloss:0.04713\n[195]   validation_0-logloss:0.01295    validation_1-logloss:0.04713\n[196]   validation_0-logloss:0.01287    validation_1-logloss:0.04715\n[197]   validation_0-logloss:0.01282    validation_1-logloss:0.04691\n[198]   validation_0-logloss:0.01272    validation_1-logloss:0.04719\n[199]   validation_0-logloss:0.01266    validation_1-logloss:0.04742\n[200]   validation_0-logloss:0.01261    validation_1-logloss:0.04718\n[201]   validation_0-logloss:0.01256    validation_1-logloss:0.04697\n[202]   validation_0-logloss:0.01251    validation_1-logloss:0.04703\n[203]   validation_0-logloss:0.01243    validation_1-logloss:0.04704\n[204]   validation_0-logloss:0.01235    validation_1-logloss:0.04707\n[205]   validation_0-logloss:0.01230    validation_1-logloss:0.04694\n[206]   validation_0-logloss:0.01225    validation_1-logloss:0.04699\n[207]   validation_0-logloss:0.01218    validation_1-logloss:0.04693\n[208]   validation_0-logloss:0.01213    validation_1-logloss:0.04713\n[209]   validation_0-logloss:0.01208    validation_1-logloss:0.04689\n[210]   validation_0-logloss:0.01203    validation_1-logloss:0.04676\n[211]   validation_0-logloss:0.01195    validation_1-logloss:0.04645\n[212]   validation_0-logloss:0.01190    validation_1-logloss:0.04666\n[213]   validation_0-logloss:0.01185    validation_1-logloss:0.04634\n[214]   validation_0-logloss:0.01181    validation_1-logloss:0.04640\n[215]   validation_0-logloss:0.01176    validation_1-logloss:0.04627\n[216]   validation_0-logloss:0.01169    validation_1-logloss:0.04621\n[217]   validation_0-logloss:0.01164    validation_1-logloss:0.04611\n[218]   validation_0-logloss:0.01160    validation_1-logloss:0.04588\n[219]   validation_0-logloss:0.01155    validation_1-logloss:0.04576\n[220]   validation_0-logloss:0.01149    validation_1-logloss:0.04570\n[221]   validation_0-logloss:0.01145    validation_1-logloss:0.04594\n[222]   validation_0-logloss:0.01141    validation_1-logloss:0.04600\n[223]   validation_0-logloss:0.01137    validation_1-logloss:0.04573\n[224]   validation_0-logloss:0.01133    validation_1-logloss:0.04552\n[225]   validation_0-logloss:0.01128    validation_1-logloss:0.04539\n[226]   validation_0-logloss:0.01122    validation_1-logloss:0.04534\n[227]   validation_0-logloss:0.01118    validation_1-logloss:0.04554\n[228]   validation_0-logloss:0.01114    validation_1-logloss:0.04536\n[229]   validation_0-logloss:0.01109    validation_1-logloss:0.04550\n[230]   validation_0-logloss:0.01105    validation_1-logloss:0.04529\n[231]   validation_0-logloss:0.01100    validation_1-logloss:0.04516\n[232]   validation_0-logloss:0.01095    validation_1-logloss:0.04511\n[233]   validation_0-logloss:0.01091    validation_1-logloss:0.04516\n[234]   validation_0-logloss:0.01087    validation_1-logloss:0.04536\n[235]   validation_0-logloss:0.01083    validation_1-logloss:0.04514\n[236]   validation_0-logloss:0.01079    validation_1-logloss:0.04536\n[237]   validation_0-logloss:0.01075    validation_1-logloss:0.04515\n[238]   validation_0-logloss:0.01071    validation_1-logloss:0.04503\n[239]   validation_0-logloss:0.01066    validation_1-logloss:0.04498\n[240]   validation_0-logloss:0.01063    validation_1-logloss:0.04505\n[241]   validation_0-logloss:0.01058    validation_1-logloss:0.04519\n[242]   validation_0-logloss:0.01053    validation_1-logloss:0.04511\n[243]   validation_0-logloss:0.01049    validation_1-logloss:0.04499\n[244]   validation_0-logloss:0.01045    validation_1-logloss:0.04478\n[245]   validation_0-logloss:0.01041    validation_1-logloss:0.04500\n[246]   validation_0-logloss:0.01036    validation_1-logloss:0.04492\n[247]   validation_0-logloss:0.01032    validation_1-logloss:0.04506\n[248]   validation_0-logloss:0.01028    validation_1-logloss:0.04485\n[249]   validation_0-logloss:0.01025    validation_1-logloss:0.04473\n[250]   validation_0-logloss:0.01022    validation_1-logloss:0.04477\n[251]   validation_0-logloss:0.01016    validation_1-logloss:0.04469\n[252]   validation_0-logloss:0.01013    validation_1-logloss:0.04488\n[253]   validation_0-logloss:0.01009    validation_1-logloss:0.04477\n[254]   validation_0-logloss:0.01006    validation_1-logloss:0.04456\n[255]   validation_0-logloss:0.01003    validation_1-logloss:0.04436\n[256]   validation_0-logloss:0.01000    validation_1-logloss:0.04440\n[257]   validation_0-logloss:0.00995    validation_1-logloss:0.04433\n[258]   validation_0-logloss:0.00991    validation_1-logloss:0.04446\n[259]   validation_0-logloss:0.00987    validation_1-logloss:0.04435\n[260]   validation_0-logloss:0.00984    validation_1-logloss:0.04415\n[261]   validation_0-logloss:0.00981    validation_1-logloss:0.04398\n[262]   validation_0-logloss:0.00976    validation_1-logloss:0.04391\n[263]   validation_0-logloss:0.00973    validation_1-logloss:0.04380\n[264]   validation_0-logloss:0.00970    validation_1-logloss:0.04386\n[265]   validation_0-logloss:0.00968    validation_1-logloss:0.04365\n[266]   validation_0-logloss:0.00964    validation_1-logloss:0.04378\n[267]   validation_0-logloss:0.00961    validation_1-logloss:0.04383\n[268]   validation_0-logloss:0.00958    validation_1-logloss:0.04373\n[269]   validation_0-logloss:0.00955    validation_1-logloss:0.04356\n[270]   validation_0-logloss:0.00952    validation_1-logloss:0.04374\n[271]   validation_0-logloss:0.00947    validation_1-logloss:0.04369\n[272]   validation_0-logloss:0.00944    validation_1-logloss:0.04358\n[273]   validation_0-logloss:0.00942    validation_1-logloss:0.04364\n[274]   validation_0-logloss:0.00939    validation_1-logloss:0.04345\n[275]   validation_0-logloss:0.00936    validation_1-logloss:0.04362\n[276]   validation_0-logloss:0.00933    validation_1-logloss:0.04346\n[277]   validation_0-logloss:0.00929    validation_1-logloss:0.04359\n[278]   validation_0-logloss:0.00927    validation_1-logloss:0.04338\n[279]   validation_0-logloss:0.00924    validation_1-logloss:0.04342\n[280]   validation_0-logloss:0.00922    validation_1-logloss:0.04357\n[281]   validation_0-logloss:0.00917    validation_1-logloss:0.04352\n[282]   validation_0-logloss:0.00916    validation_1-logloss:0.04339\n[283]   validation_0-logloss:0.00914    validation_1-logloss:0.04321\n[284]   validation_0-logloss:0.00911    validation_1-logloss:0.04305\n[285]   validation_0-logloss:0.00907    validation_1-logloss:0.04317\n[286]   validation_0-logloss:0.00905    validation_1-logloss:0.04331\n[287]   validation_0-logloss:0.00903    validation_1-logloss:0.04311\n[288]   validation_0-logloss:0.00900    validation_1-logloss:0.04296\n[289]   validation_0-logloss:0.00899    validation_1-logloss:0.04299\n[290]   validation_0-logloss:0.00896    validation_1-logloss:0.04317\n[291]   validation_0-logloss:0.00894    validation_1-logloss:0.04307\n[292]   validation_0-logloss:0.00892    validation_1-logloss:0.04309\n[293]   validation_0-logloss:0.00890    validation_1-logloss:0.04327\n[294]   validation_0-logloss:0.00887    validation_1-logloss:0.04308\n[295]   validation_0-logloss:0.00885    validation_1-logloss:0.04286\n[296]   validation_0-logloss:0.00883    validation_1-logloss:0.04301\n[297]   validation_0-logloss:0.00882    validation_1-logloss:0.04288\n[298]   validation_0-logloss:0.00878    validation_1-logloss:0.04285\n[299]   validation_0-logloss:0.00876    validation_1-logloss:0.04266\n[300]   validation_0-logloss:0.00874    validation_1-logloss:0.04253\n[301]   validation_0-logloss:0.00873    validation_1-logloss:0.04244\n[302]   validation_0-logloss:0.00871    validation_1-logloss:0.04230\n[303]   validation_0-logloss:0.00869    validation_1-logloss:0.04233\n[304]   validation_0-logloss:0.00868    validation_1-logloss:0.04249\n[305]   validation_0-logloss:0.00867    validation_1-logloss:0.04236\n[306]   validation_0-logloss:0.00866    validation_1-logloss:0.04239\n[307]   validation_0-logloss:0.00865    validation_1-logloss:0.04255\n[308]   validation_0-logloss:0.00863    validation_1-logloss:0.04243\n[309]   validation_0-logloss:0.00862    validation_1-logloss:0.04234\n[310]   validation_0-logloss:0.00860    validation_1-logloss:0.04220\n[311]   validation_0-logloss:0.00857    validation_1-logloss:0.04237\n[312]   validation_0-logloss:0.00855    validation_1-logloss:0.04219\n[313]   validation_0-logloss:0.00854    validation_1-logloss:0.04223\n[314]   validation_0-logloss:0.00853    validation_1-logloss:0.04237\n[315]   validation_0-logloss:0.00850    validation_1-logloss:0.04223\n[316]   validation_0-logloss:0.00849    validation_1-logloss:0.04227\n[317]   validation_0-logloss:0.00848    validation_1-logloss:0.04215\n[318]   validation_0-logloss:0.00847    validation_1-logloss:0.04206\n[319]   validation_0-logloss:0.00844    validation_1-logloss:0.04223\n[320]   validation_0-logloss:0.00843    validation_1-logloss:0.04211\n[321]   validation_0-logloss:0.00842    validation_1-logloss:0.04226\n[322]   validation_0-logloss:0.00841    validation_1-logloss:0.04229\n[323]   validation_0-logloss:0.00840    validation_1-logloss:0.04245\n[324]   validation_0-logloss:0.00839    validation_1-logloss:0.04233\n[325]   validation_0-logloss:0.00838    validation_1-logloss:0.04216\n[326]   validation_0-logloss:0.00836    validation_1-logloss:0.04199\n[327]   validation_0-logloss:0.00835    validation_1-logloss:0.04202\n[328]   validation_0-logloss:0.00834    validation_1-logloss:0.04217\n[329]   validation_0-logloss:0.00831    validation_1-logloss:0.04220\n[330]   validation_0-logloss:0.00827    validation_1-logloss:0.04218\n[331]   validation_0-logloss:0.00826    validation_1-logloss:0.04206\n[332]   validation_0-logloss:0.00825    validation_1-logloss:0.04188\n[333]   validation_0-logloss:0.00823    validation_1-logloss:0.04182\n[334]   validation_0-logloss:0.00822    validation_1-logloss:0.04174\n[335]   validation_0-logloss:0.00820    validation_1-logloss:0.04168\n[336]   validation_0-logloss:0.00819    validation_1-logloss:0.04154\n[337]   validation_0-logloss:0.00818    validation_1-logloss:0.04171\n[338]   validation_0-logloss:0.00814    validation_1-logloss:0.04169\n[339]   validation_0-logloss:0.00813    validation_1-logloss:0.04158\n[340]   validation_0-logloss:0.00812    validation_1-logloss:0.04145\n[341]   validation_0-logloss:0.00811    validation_1-logloss:0.04129\n[342]   validation_0-logloss:0.00810    validation_1-logloss:0.04145\n[343]   validation_0-logloss:0.00809    validation_1-logloss:0.04149\n[344]   validation_0-logloss:0.00808    validation_1-logloss:0.04138\n[345]   validation_0-logloss:0.00807    validation_1-logloss:0.04152\n[346]   validation_0-logloss:0.00806    validation_1-logloss:0.04138\n[347]   validation_0-logloss:0.00805    validation_1-logloss:0.04145\n[348]   validation_0-logloss:0.00804    validation_1-logloss:0.04136\n[349]   validation_0-logloss:0.00803    validation_1-logloss:0.04120\n[350]   validation_0-logloss:0.00802    validation_1-logloss:0.04124\n[351]   validation_0-logloss:0.00801    validation_1-logloss:0.04138\n[352]   validation_0-logloss:0.00800    validation_1-logloss:0.04127\n[353]   validation_0-logloss:0.00799    validation_1-logloss:0.04114\n[354]   validation_0-logloss:0.00798    validation_1-logloss:0.04128\n[355]   validation_0-logloss:0.00797    validation_1-logloss:0.04116\n[356]   validation_0-logloss:0.00796    validation_1-logloss:0.04103\n[357]   validation_0-logloss:0.00795    validation_1-logloss:0.04110\n[358]   validation_0-logloss:0.00794    validation_1-logloss:0.04113\n[359]   validation_0-logloss:0.00793    validation_1-logloss:0.04108\n[360]   validation_0-logloss:0.00792    validation_1-logloss:0.04099\n[361]   validation_0-logloss:0.00791    validation_1-logloss:0.04084\n[362]   validation_0-logloss:0.00790    validation_1-logloss:0.04090\n[363]   validation_0-logloss:0.00789    validation_1-logloss:0.04077\n[364]   validation_0-logloss:0.00788    validation_1-logloss:0.04080\n[365]   validation_0-logloss:0.00787    validation_1-logloss:0.04095\n[366]   validation_0-logloss:0.00786    validation_1-logloss:0.04084\n[367]   validation_0-logloss:0.00785    validation_1-logloss:0.04075\n[368]   validation_0-logloss:0.00784    validation_1-logloss:0.04060\n[369]   validation_0-logloss:0.00783    validation_1-logloss:0.04055\n[370]   validation_0-logloss:0.00783    validation_1-logloss:0.04058\n[371]   validation_0-logloss:0.00782    validation_1-logloss:0.04072\n[372]   validation_0-logloss:0.00781    validation_1-logloss:0.04059\n[373]   validation_0-logloss:0.00780    validation_1-logloss:0.04065\n[374]   validation_0-logloss:0.00779    validation_1-logloss:0.04057\n[375]   validation_0-logloss:0.00778    validation_1-logloss:0.04046\n[376]   validation_0-logloss:0.00777    validation_1-logloss:0.04060\n[377]   validation_0-logloss:0.00776    validation_1-logloss:0.04063\n[378]   validation_0-logloss:0.00775    validation_1-logloss:0.04053\n[379]   validation_0-logloss:0.00774    validation_1-logloss:0.04066\n[380]   validation_0-logloss:0.00773    validation_1-logloss:0.04053\n[381]   validation_0-logloss:0.00772    validation_1-logloss:0.04048\n[382]   validation_0-logloss:0.00772    validation_1-logloss:0.04034\n[383]   validation_0-logloss:0.00771    validation_1-logloss:0.04037\n[384]   validation_0-logloss:0.00770    validation_1-logloss:0.04053\n[385]   validation_0-logloss:0.00769    validation_1-logloss:0.04045\n[386]   validation_0-logloss:0.00768    validation_1-logloss:0.04034\n[387]   validation_0-logloss:0.00767    validation_1-logloss:0.04020\n[388]   validation_0-logloss:0.00766    validation_1-logloss:0.04023\n[389]   validation_0-logloss:0.00765    validation_1-logloss:0.04037\n[390]   validation_0-logloss:0.00764    validation_1-logloss:0.04025\n[391]   validation_0-logloss:0.00764    validation_1-logloss:0.04038\n[392]   validation_0-logloss:0.00763    validation_1-logloss:0.04033\n[393]   validation_0-logloss:0.00762    validation_1-logloss:0.04022\n[394]   validation_0-logloss:0.00761    validation_1-logloss:0.04015\n[395]   validation_0-logloss:0.00760    validation_1-logloss:0.04018\n[396]   validation_0-logloss:0.00759    validation_1-logloss:0.04031\n[397]   validation_0-logloss:0.00758    validation_1-logloss:0.04021\n[398]   validation_0-logloss:0.00758    validation_1-logloss:0.04007\n[399]   validation_0-logloss:0.00757    validation_1-logloss:0.04022\n\n\n\n\nLightGBM\n\n성능은 xgboost랑 별로 차이가 없음.\n1만건 이하의 데이터 세트에 대해 과적합이 발생할 가능성이 높다.\none hot 인코딩 필요 없음\npython lightgbm\n\n\nfrom lightgbm import LGBMClassifier, early_stopping, plot_importance\nimport matplotlib.pyplot as plt\n\nlgbm = LGBMClassifier(n_estimators=400, learning_rate=0.05)\nevals = [(X_tr, y_tr), (X_val, y_val)]\nlgbm.fit(X_tr, y_tr, \n         callbacks = [early_stopping(stopping_rounds = 50)],\n         eval_metric='logloss', \n         eval_set=evals)\npreds = lgbm.predict(X_test)\npred_proba = lgbm.predict_proba(X_test)[:, 1]\n\nplot_importance(lgbm)\nplt.show()\n\n[LightGBM] [Info] Number of positive: 253, number of negative: 156\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002472 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4092\n[LightGBM] [Info] Number of data points in the train set: 409, number of used features: 30\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.618582 -&gt; initscore=0.483533\n[LightGBM] [Info] Start training from score 0.483533\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[291]   training's binary_logloss: 2.39157e-05  valid_1's binary_logloss: 0.00285194",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#stacking",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#stacking",
    "title": "분류 - 앙상블",
    "section": "stacking",
    "text": "stacking\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2)\n\nknn_clf = KNeighborsClassifier(n_neighbors=4)\nrf_clf = RandomForestClassifier(n_estimators=100)\ndt_clf = DecisionTreeClassifier()\nada_clf = AdaBoostClassifier(n_estimators=100)\n\nlr_final = LogisticRegression()\n\n\nknn_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\nada_clf.fit(X_train, y_train)\n\nknn_pred = knn_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\ndt_pred = dt_clf.predict(X_test)\nada_pred = ada_clf.predict(X_test)\n\npred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\npred = np.transpose(pred)\n\n\nlr_final.fit(pred, y_test)\nfinal = lr_final.predict(pred)\nprint(f'{accuracy_score(y_test, final):.3f}')\n\n0.965\n\n\n\ntest 셋으로 훈련을 하고 있는 부분이 문제 → cv 세트로 해야함\n\n\nCV 세트 기반 stacking\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\ndef get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n    kf = KFold(n_splits=n_folds, shuffle=False)\n    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n        X_tr = X_train_n[train_index]\n        y_tr = y_train_n[train_index]\n        X_te = X_train_n[valid_index]\n\n        model.fit(X_tr, y_tr)\n        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)\n        test_pred[:, folder_counter] = model.predict(X_test_n)\n\n    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)\n\n    return train_fold_pred, test_pred_mean\n\n\nknn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\nrf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)\ndt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\nada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)\n\n\nStack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\nStack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n\nlr_final.fit(Stack_final_X_train, y_train)\nstack_final = lr_final.predict(Stack_final_X_test)\n\nprint(f'{accuracy_score(y_test, stack_final):.3f}')\n\n0.965",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#baysian-optimization",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#baysian-optimization",
    "title": "분류 - 앙상블",
    "section": "Baysian Optimization",
    "text": "Baysian Optimization\n\nGrid search로는 시간이 너무 오래 걸리는 경우\n목표 함수: 하이퍼파라미터 입력 n개에 대한 모델 성능 출력 1개의 모델\nSurrogate model: 목표 함수에 대한 예상 모델. 사전확률 분포에서 최적해 나감.\nacquisition function: 불확실성이 가장 큰 point를 다음 관측 데이터로 결정.\n\n\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n\nsearch_space = {'x': hp.quniform('x', -10, 10, 1),\n                'y': hp.quniform('y', -15, 15, 1)}\ndef objective_func(search_space):\n    x = search_space['x']\n    y = search_space['y']\n\n    return x ** 2 - 20 * y\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=search_space,\n            algo=tpe.suggest,\n            max_evals=20,\n            trials=trial_val)\nbest\n\n  0%|          | 0/20 [00:00&lt;?, ?trial/s, best loss=?]100%|██████████| 20/20 [00:00&lt;00:00, 1720.92trial/s, best loss: -224.0]\n\n\n{'x': -4.0, 'y': 12.0}\n\n\n\nXGBoost 하이퍼파라미터 최적화\n\ndataset = load_breast_cancer()\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1)\n\nxgb_search_space = {\n    'max_depth': hp.quniform('max_depth', 5, 20, 1),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)\n}\n# hp.choice('tree_criterion', ['gini', 'entropy']) 이런식으로도 가능\n\n\nfrom sklearn.model_selection import cross_val_score\n\ndef objective_func(search_space):\n    xgb_clf = XGBClassifier(n_estimators=100, \n                            max_depth=int(search_space['max_depth']),\n                            min_child_weight=int(search_space['min_child_weight']),\n                            learning_rate=search_space['learning_rate'],\n                            colsample_bytree=search_space['colsample_bytree'],\n                            eval_metric='logloss')\n    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n    return {'loss': -1 * np.mean(accuracy), 'status': STATUS_OK}\n\ntrial_val = Trials()\nbest = fmin(fn=objective_func,\n            space=xgb_search_space,\n            algo=tpe.suggest,\n            max_evals=50,\n            trials=trial_val)\nbest\n\n  0%|          | 0/50 [00:00&lt;?, ?trial/s, best loss=?]  2%|▏         | 1/50 [00:00&lt;00:07,  6.51trial/s, best loss: -0.9560241663762055]  4%|▍         | 2/50 [00:00&lt;00:06,  7.84trial/s, best loss: -0.9560241663762055]  6%|▌         | 3/50 [00:00&lt;00:06,  7.20trial/s, best loss: -0.9560241663762055]  8%|▊         | 4/50 [00:00&lt;00:06,  7.19trial/s, best loss: -0.9560241663762055] 10%|█         | 5/50 [00:00&lt;00:06,  6.92trial/s, best loss: -0.9560241663762055] 12%|█▏        | 6/50 [00:00&lt;00:07,  5.84trial/s, best loss: -0.9560241663762055] 14%|█▍        | 7/50 [00:01&lt;00:07,  5.90trial/s, best loss: -0.9560241663762055] 16%|█▌        | 8/50 [00:01&lt;00:06,  6.59trial/s, best loss: -0.9603956082258627] 18%|█▊        | 9/50 [00:01&lt;00:06,  6.20trial/s, best loss: -0.9603956082258627] 20%|██        | 10/50 [00:01&lt;00:06,  6.43trial/s, best loss: -0.9626031137446264] 22%|██▏       | 11/50 [00:01&lt;00:05,  6.50trial/s, best loss: -0.9626031137446264] 24%|██▍       | 12/50 [00:01&lt;00:05,  6.64trial/s, best loss: -0.9626031137446264] 26%|██▌       | 13/50 [00:01&lt;00:05,  7.00trial/s, best loss: -0.9626031137446264] 28%|██▊       | 14/50 [00:02&lt;00:05,  7.00trial/s, best loss: -0.9626031137446264] 30%|███       | 15/50 [00:02&lt;00:04,  7.28trial/s, best loss: -0.9626031137446264] 32%|███▏      | 16/50 [00:02&lt;00:05,  6.69trial/s, best loss: -0.9626031137446264] 34%|███▍      | 17/50 [00:02&lt;00:05,  6.60trial/s, best loss: -0.9626031137446264] 36%|███▌      | 18/50 [00:02&lt;00:05,  6.27trial/s, best loss: -0.9626031137446264] 38%|███▊      | 19/50 [00:03&lt;00:06,  4.83trial/s, best loss: -0.9626031137446264] 40%|████      | 20/50 [00:03&lt;00:07,  3.98trial/s, best loss: -0.9626031137446264] 42%|████▏     | 21/50 [00:03&lt;00:07,  3.99trial/s, best loss: -0.9626031137446264] 44%|████▍     | 22/50 [00:03&lt;00:06,  4.04trial/s, best loss: -0.9626031137446264] 46%|████▌     | 23/50 [00:04&lt;00:06,  4.19trial/s, best loss: -0.9626031137446264] 48%|████▊     | 24/50 [00:04&lt;00:06,  3.93trial/s, best loss: -0.9626031137446264] 50%|█████     | 25/50 [00:04&lt;00:06,  4.04trial/s, best loss: -0.9626031137446264] 52%|█████▏    | 26/50 [00:04&lt;00:06,  3.90trial/s, best loss: -0.9626031137446264] 54%|█████▍    | 27/50 [00:05&lt;00:05,  4.09trial/s, best loss: -0.9626031137446264] 56%|█████▌    | 28/50 [00:05&lt;00:04,  4.52trial/s, best loss: -0.9647960962007668] 58%|█████▊    | 29/50 [00:05&lt;00:04,  4.55trial/s, best loss: -0.9647960962007668] 60%|██████    | 30/50 [00:05&lt;00:04,  4.54trial/s, best loss: -0.9647960962007668] 62%|██████▏   | 31/50 [00:05&lt;00:04,  4.72trial/s, best loss: -0.9647960962007668] 64%|██████▍   | 32/50 [00:06&lt;00:03,  5.13trial/s, best loss: -0.9647960962007668] 66%|██████▌   | 33/50 [00:06&lt;00:03,  5.28trial/s, best loss: -0.9647960962007668] 68%|██████▊   | 34/50 [00:06&lt;00:02,  5.54trial/s, best loss: -0.9647960962007668] 78%|███████▊  | 39/50 [00:06&lt;00:00, 11.51trial/s, best loss: -0.9647960962007668] 82%|████████▏ | 41/50 [00:07&lt;00:01,  8.29trial/s, best loss: -0.9647960962007668] 84%|████████▍ | 42/50 [00:07&lt;00:01,  7.71trial/s, best loss: -0.9647960962007668] 86%|████████▌ | 43/50 [00:07&lt;00:01,  6.58trial/s, best loss: -0.9647960962007668] 88%|████████▊ | 44/50 [00:07&lt;00:00,  6.65trial/s, best loss: -0.9647960962007668] 90%|█████████ | 45/50 [00:07&lt;00:00,  6.76trial/s, best loss: -0.9647960962007668] 92%|█████████▏| 46/50 [00:08&lt;00:00,  5.00trial/s, best loss: -0.9647960962007668] 94%|█████████▍| 47/50 [00:08&lt;00:00,  3.87trial/s, best loss: -0.9647960962007668] 96%|█████████▌| 48/50 [00:08&lt;00:00,  3.48trial/s, best loss: -0.9647960962007668] 98%|█████████▊| 49/50 [00:09&lt;00:00,  3.49trial/s, best loss: -0.9647960962007668]100%|██████████| 50/50 [00:09&lt;00:00,  3.29trial/s, best loss: -0.9647960962007668]100%|██████████| 50/50 [00:09&lt;00:00,  5.25trial/s, best loss: -0.9647960962007668]\n\n\n{'colsample_bytree': 0.8333015582921072,\n 'learning_rate': 0.181846600140329,\n 'max_depth': 11.0,\n 'min_child_weight': 1.0}",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#footnotes",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/02.html#footnotes",
    "title": "분류 - 앙상블",
    "section": "각주",
    "text": "각주\n\n\nhard voting (단순 다수결), soft voting(label을 예측할 확률의 가중 평균으로 분류)으로 나뉨. 일반적으로 soft voting이 사용됨.↩︎\nhard voting (단순 다수결), soft voting(label을 예측할 확률의 가중 평균으로 분류)으로 나뉨. 일반적으로 soft voting이 사용됨.↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 앙상블"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/03.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/03.html",
    "title": "전처리 템플릿",
    "section": "",
    "text": "참고: 이상치, 결측치, 불균형 처리 모두 원칙적으로는 train set에서만 fit 해야함.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "전처리 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/03.html#결측치-처리",
    "href": "posts/03_archives/completed_project/adp_실기/notes/03.html#결측치-처리",
    "title": "전처리 템플릿",
    "section": "결측치 처리",
    "text": "결측치 처리\n\n결측치가 발생하는 원인과 처리 전략\n\n무작위 결측(Missing Completely at Random, MCAR): 결측치가 발생할 확률이 다른 모든 변수와 무관\n\n예: 설문지 작성 중 무작위로 일부 페이지가 누락된 경우\n이 경우, 결측치를 제거하거나 단순 대체해도 편향이 발생하지 않음\n\n조건부 무작위 결측(Missing at Random, MAR): 결측치가 발생할 확률이 관측된 데이터에만 의존\n\n예: 고소득자일수록 소득 공개를 꺼려하는 경우 (교육수준이 높을수록 소득 결측 확률이 높음)\n실무에서 가장 일반적인 가정: 대부분의 실제 데이터에서 결측 패턴은 관측된 변수들로 어느 정도 설명 가능\n관측된 데이터를 활용한 예측 기반 대치법이 효과적\n\n비무작위 결측(Missing Not at Random, MNAR): 결측치가 발생할 확률이 결측된 값 자체와 관련\n\n예: 극도로 낮은 소득자가 소득 공개를 꺼리는 경우\n통계적 방법만으로는 해결이 어려우며, 도메인 지식이나 외부 정보가 필요\n실무에서는 MAR 가정 하에 처리 후, 민감도 분석을 통해 결측 메커니즘과 처리 방법의 적절성을 확인\n\n민감도 분석: 다양한 결측치 처리 방법을 적용하여 결과를 비교\n\n단순 방법(평균/중앙값 대치) vs 고급 방법(KNN, MICE)\n단순 방법과 고급 방법의 결과가 비슷하면 → MCAR 가능성 높음, 단순 방법으로도 충분\n고급 방법이 더 나은 성능을 보이면 → MAR 가능성 높음, 고급 방법 선택\n어떤 방법을 써도 결과가 불안정하면 → MNAR 가능성, 도메인 지식과 추가 정보 필요\n\n\n\n\n결측치 처리 방법\n\n제거: 결측치가 적거나 무작위 결측일 때 사용\n대치(대체):\n\n일반적인 방법\n\n시계열 데이터 o: 이전 값, 이후 값, 선형 보간법\n시계열 데이터 x: 평균, 중앙값, 최빈값\n\n고급 대치법(과적합 발생 가능성 유의)\n\nKNN 대치: 유사한 관측치의 값을 사용하여 결측치를 대체. 결측치가 없는 데이터로 예측\n다변량 대치: 결측치를 다른 변수들의 값으로 예측하여 대체.\n\n\n\n\n\n다변량 대치법\n\nfrom lightgbm import LGBMRegressor, LGBMClassifier\n\ndef multi_impute(df, categorical, max_iter=40):\n    df_imp = df.copy()\n    num_cols = [col for col in df.columns if col not in categorical]\n\n    # 결측치가 적은 column 우선 처리\n    null_counts = df_imp.isnull().sum()\n    null_cols = null_counts[null_counts &gt; 0].sort_values().index.tolist()\n\n    # 초기값 대체\n    for col in categorical:\n        df_imp[col] = df_imp[col].astype('category')\n        mode = df_imp[col].mode(dropna=True)\n        df_imp[col].fillna(mode[0], inplace=True)\n\n    for col in num_cols:\n        mean = df_imp[col].mean()\n        df_imp[col].fillna(mean, inplace=True)\n\n    # 반복 임퓨팅\n    for _ in range(max_iter):\n        prev = df_imp.copy()\n\n        for col in null_cols:\n            idx_missing = df[col].isnull()\n            idx_obs = ~idx_missing\n            predictors = [c for c in df.columns if c != col]\n\n            X_obs = df_imp.loc[idx_obs, predictors]\n            y_obs = df_imp.loc[idx_obs, col]\n            X_mis = df_imp.loc[idx_missing, predictors]\n\n            # column type에 따라 다른 모델 선택\n            # LightGBM으로 randomforest를 사용한 이유: sklearn의 RandomForest는 categorical 변수를 직접 처리하지 못함\n            if col in categorical:\n                model = LGBMClassifier(\n                    boosting_type=\"rf\", n_estimators=5,\n                    bagging_fraction=0.8, bagging_freq=1, feature_fraction=0.8\n                )\n            else:\n                model = LGBMRegressor(\n                    boosting_type=\"rf\", n_estimators=5,\n                    bagging_fraction=0.8, bagging_freq=1, feature_fraction=0.8\n                )\n            model.fit(X_obs, y_obs)\n            y_pred = model.predict(X_mis)\n            df_imp.loc[idx_missing, col] = y_pred\n        if df_imp.equals(prev):\n            break\n    return df_imp\n\n\n현재 결측치 처리 방법 중 가장 성능이 좋은걸로 알려져 있는 missing forest를 모방한 방법.\n\nmissforest 라이브러리는 adp 환경에서 설치 불가\n\n다중 대치를 안 하고 있지만, sklearn 공식 문서에 따르면 대부분 single 대치로도 충분하다고 한다.\n참고로 이 방법은 missforest의 정확한 구현은 아니기 때문에, 시험에서는 다변량 대치법을 사용했다고만 쓰자.\n만약 train set에서만 fit을 시키고 싶다면, 이 방법은 그냥 안 쓰는걸 추천.\n\n그렇게 하려면 class 형태로 바꿔야 하는데, too much인가 싶은 느낌이 슬슬 나기 시작.\n\n\n\nclass MultiImputer:\n    def __init__(self, categorical=[], max_iter=40, n_estimators=5):\n        self.categorical = categorical\n        self.max_iter = max_iter\n        self.n_estimators = n_estimators\n        \n        self.models_ = {}\n        self.initial_fill_ = {}\n        self.null_cols_ = []\n        self.num_cols_ = []\n        \n    def fit(self, X, y=None):\n        df = X.copy()\n        self.num_cols_ = [col for col in df.columns if col not in self.categorical]\n        \n        null_counts = df.isnull().sum()\n        self.null_cols_ = null_counts[null_counts &gt; 0].sort_values().index.tolist()\n        \n        # 초기값 계산 및 저장 (Train의 통계량만 사용!)\n        for col in self.categorical:\n            df[col] = df[col].astype('category')\n            mode = df[col].mode(dropna=True)\n            self.initial_fill_[col] = mode[0]\n            df[col].fillna(self.initial_fill_[col], inplace=True)\n        \n        for col in self.num_cols_:\n            mean = df[col].mean()\n            self.initial_fill_[col] = mean\n            df[col].fillna(mean, inplace=True)\n        \n        for _ in range(self.max_iter):\n            prev = df.copy()\n            \n            for col in self.null_cols_:\n                idx_missing = X[col].isnull()\n                idx_obs = ~idx_missing\n                    \n                predictors = [c for c in df.columns if c != col]\n                X_obs = df.loc[idx_obs, predictors]\n                y_obs = df.loc[idx_obs, col]\n                X_mis = df.loc[idx_missing, predictors]\n                \n                # 모델 선택 및 학습\n                if col in self.categorical:\n                    model = LGBMClassifier(\n                        boosting_type=\"rf\", \n                        n_estimators=self.n_estimators,\n                        bagging_fraction=0.8, \n                        bagging_freq=1,\n                        feature_fraction=0.8,\n                        verbose=-1\n                    )\n                else:\n                    model = LGBMRegressor(\n                        boosting_type=\"rf\", \n                        n_estimators=self.n_estimators,\n                        bagging_fraction=0.8, \n                        bagging_freq=1,\n                        feature_fraction=0.8,\n                        verbose=-1\n                    )\n                \n                model.fit(X_obs, y_obs)\n                \n                y_pred = model.predict(X_mis)\n                df.loc[idx_missing, col] = y_pred\n                \n                self.models_[col] = model\n            \n            if df.equals(prev):\n                break\n        \n        return self\n    \n    def transform(self, X):\n        df_imp = X.copy()\n        \n        for col in self.categorical:\n            df_imp[col] = df_imp[col].astype('category')\n            df_imp[col].fillna(self.initial_fill_[col], inplace=True)\n        \n        for col in self.num_cols_:\n            df_imp[col].fillna(self.initial_fill_[col], inplace=True)\n        \n        for _ in range(self.max_iter):\n            prev = df_imp.copy()\n            \n            for col in self.null_cols_:                   \n                idx_missing = X[col].isnull()\n                \n                predictors = [c for c in df_imp.columns if c != col]\n                X_mis = df_imp.loc[idx_missing, predictors]\n                \n                model = self.models_[col]\n                y_pred = model.predict(X_mis)\n                df_imp.loc[idx_missing, col] = y_pred\n            \n            if df_imp.equals(prev):\n                break\n        \n        return df_imp\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X)\n\n\n혹시몰라서 class 형태로 만든 다변량 대치법\n이걸 언제 다 적고 있을까",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "전처리 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/03.html#이상치-처리",
    "href": "posts/03_archives/completed_project/adp_실기/notes/03.html#이상치-처리",
    "title": "전처리 템플릿",
    "section": "이상치 처리",
    "text": "이상치 처리\n\n이상치 탐지는 EDA 참고\n이상치는 그 자체로 중요한 정보일 수 있다.\n따라서 수집중 오류, 측정 과정에서의 오류 등과 같은 상황이 의심되는 비정상적인 데이터가 아니라면 제거하지 않는 것이 좋다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "전처리 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/03.html#불균형-처리",
    "href": "posts/03_archives/completed_project/adp_실기/notes/03.html#불균형-처리",
    "title": "전처리 템플릿",
    "section": "불균형 처리",
    "text": "불균형 처리\n\n잘 알려져 있는 방법 대충 잘 선택해서 사용.\n딱히 SOTA(가장 좋은 방법)가 있지 않음.\n\n\n성능 비교\n\n각각의 처리법에 대해서 어떤게 제일 좋은지 비교\n\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score\n\ndf[cat_cols] = df[cat_cols].astype('category')\n\ndf1 = df.dropna()\ndf2 = df[num_cols].fillna(df[num_cols].mean())\nfor col in cat_cols:\n    df2[col] = target[col].fillna(target[col].mode()[0])\ndf3 = multi_impute(df, categorical=cat_cols)\n\ncandis = [\n    ('Nothing', df), # 종속변수는 결측치가 없어야 된다.\n    ('Just Delete', df1),\n    ('Simple Impute', df2),\n    ('MultiImputer', df3)\n]\n\nresult = pd.DataFrame()\nfor name, df in candis:\n    rf = LGBMRegressor(boosting_type=\"rf\", \n                       n_estimators=100, \n                       bagging_fraction=0.8, \n                       bagging_freq=1,\n                       feature_fraction=0.8)\n    result[name] = cross_val_score(rf, \n                                   df.drop('target', axis=1), \n                                   df['target'], \n                                   scoring='neg_mean_squared_error')\n                                    # classifier인 경우 'accuracy' 등등\n\nfig, ax = plt.subplots(figsize=(13, 6))\n\nmeans = -result.mean() # regressor인 경우, classifier인 경우 양수\nerrors = result.std()\n\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_yticks(np.arange(means.shape[0]))\nax.set_yticklabels(means.index)\nplt.show()\n\n\nImbalance 처리 방법에 대해서도 적용 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "전처리 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/03.html#encoding",
    "href": "posts/03_archives/completed_project/adp_실기/notes/03.html#encoding",
    "title": "전처리 템플릿",
    "section": "Encoding",
    "text": "Encoding\n\nLGBM, XGBoost, CatBoost 등은 범주형 변수를 직접 처리 가능\n범주형 인코딩의 경우, 문제에서 범주 값이 주어지거나, 실무에서 범주 값이 미리 정해져 있는 경우 굳이 train, test set 나눈 후에 fit할 필요는 없다고 생각한다. (개인적인 의견이다.)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "전처리 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/03.html#scaling",
    "href": "posts/03_archives/completed_project/adp_실기/notes/03.html#scaling",
    "title": "전처리 템플릿",
    "section": "Scaling",
    "text": "Scaling\n\n이상치가 많다면 Robust Scaling, 정규분포를 따른다면 Standard Scaling\n\nStandard Scaling 이후 회귀계수의 단위는 표준편차 단위\nRobust Scaling 이후 회귀계수의 단위는 IQR 단위\n\n일반적으로 같은 scaling 방법을 사용하는게 권장된다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "전처리 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/03.html#feature-selection",
    "href": "posts/03_archives/completed_project/adp_실기/notes/03.html#feature-selection",
    "title": "전처리 템플릿",
    "section": "Feature Selection",
    "text": "Feature Selection\n\nFilter Method\n\nbasic methods\n\n\n하나의 값만 가지는 변수 혹은 분산이 너무 낮은 변수는 제거\n하지만 분산이 낮아도 종속변수와 관계가 있을 수 있다. 일단은 mutual info 까지 확인을 해보는게 좋을 듯.\n\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nsel = VarianceThreshold(threshold=0.01)\n\nselected_cols = df.columns[sel.get_support()]\ndf_selected = df[selected_cols]\n\n\nUnivariate selection methods\n\n\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, chi2\n\nX_new = SelectKBest(chi2, k=2).fit_transform(X, y) # 최상위 2개\n\nX_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y) # 상위 10%\n\n\n카이제곱 검정량이 가장 높은 변수들만 선택.\n연속형 변수에 대해서는 KBinsDiscretizer 작업 필요.\n\n\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, mutual_info_classif, mutual_info_regression\n\nX_new = SelectKBest(mutual_info_classif, k=2).fit_transform(X, y) # 최상위 2개\nX_new = SelectPercentile(mutual_info_regression, percentile=10).fit_transform(X, y) # 상위 10%\n\n\n\nmutual info\n\nfrom sklearn.feature_selection import mutual_info_classif\n\nmi_score = mutual_info_classif(X, y,\n                              discrete_features=[x in cat_cols for x in df.columns])\nmi_result = pd.Series(mi_score, index=X.coulmns).sort_values(ascending=False)\nmi_result\n\n\nEDA 파트 mutual info 참고\n추가: 상관계수, ANOVA F-value 등등 사용 가능\n\n\n\nWrapper Method\n\nforward, backward, 등등\n\n\n\nEmbedded Method\n\nL1, L2, Elasticnet 등등",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "전처리 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/00.html#시계열-소개",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/00.html#시계열-소개",
    "title": "Overview",
    "section": "시계열 소개",
    "text": "시계열 소개\n\n시계열은 단순히 시간에 따라 정렬된 데이터 요소들의 집합이다.\n분해: 시계열을 서로 다른 여러 구성요소로 분리하는 통계적 작업\n\n\n구성 요소\n\n추세(level)\n\n순환?\n\n계절성: 추세에서 벗어나는 변화의 정도\n잔차(white noise)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "Overview"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/00.html#시계열-vs-회귀",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/00.html#시계열-vs-회귀",
    "title": "Overview",
    "section": "시계열 vs 회귀",
    "text": "시계열 vs 회귀\n\n시계열은 순서가 존재\n다른 특징 없이 시계열 정보만 있는 경우도 존재\n\n이동평균 모델\n자기회귀 모델",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "Overview"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/01.html#베이스라인-모델-정의",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/01.html#베이스라인-모델-정의",
    "title": "단순 미래 예측",
    "section": "베이스라인 모델 정의",
    "text": "베이스라인 모델 정의\n\n단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 \\(\\hat{p_{t+1}} = p_t\\)\n추세분석: 전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. \\(\\hat{p_{t+1}} = p_t + p_t - p_{t-1}\\)\n단순 이동평균법: time window를 계속 이동하면서 평균 구하는거\n\ntime window ↑: 먼 과거까지 보겠다\n\n가중 이동평균법: 가중치를 다르게 부여\n\n\nall 평균 (추세)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\ndf = pd.read_csv('_data/jj.csv')\n\ntrain = df[:-4]\ntest = df[-4:]\n\nhistorical_mean = np.mean(train['data'])\nhistorical_mean\n\nnp.float64(4.308499987499999)\n\n\n\ntest['pred_mean'] = historical_mean\n\n\ndef mape(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred) / y_true) * 100\n\nmape_hist_mean = mape(test['data'], test['pred_mean'])\nmape_hist_mean\n\nnp.float64(70.00752579965119)\n\n\n\nsns.lineplot(data=train, x='date', y='data', label='훈련')\nsns.lineplot(data=test, x='date', y='data', label='테스트')\nsns.lineplot(data=test, x='date', y='pred_mean', label='단순 예측')\nplt.xticks(np.arange(0, 85, 8), np.arange(1960, 1981, 2))\n\n([&lt;matplotlib.axis.XTick at 0x74a7e36d46b0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e36d4680&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a82d18ac60&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e3709370&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e3709e80&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e370a330&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e370a9f0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e370b6e0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e370bce0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e373ca40&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e370a990&gt;],\n [Text(0, 0, '1960'),\n  Text(8, 0, '1962'),\n  Text(16, 0, '1964'),\n  Text(24, 0, '1966'),\n  Text(32, 0, '1968'),\n  Text(40, 0, '1970'),\n  Text(48, 0, '1972'),\n  Text(56, 0, '1974'),\n  Text(64, 0, '1976'),\n  Text(72, 0, '1978'),\n  Text(80, 0, '1980')])\n\n\n\n\n\n\n\n\n\n\n\n최근만 평균 (추세)\n\nlast_year_mean = np.mean(train.iloc[-4:]['data'])\ntest['pred_last_yr_mean'] = last_year_mean\nmape_last_year_mean = mape(test['data'], test['pred_last_yr_mean'])\nmape_last_year_mean\n\nnp.float64(15.5963680725103)\n\n\n\nsns.lineplot(data=train, x='date', y='data', label='훈련')\nsns.lineplot(data=test, x='date', y='data', label='테스트')\nsns.lineplot(data=test, x='date', y='pred_last_yr_mean', label='최근 예측')\nplt.xticks(np.arange(0, 85, 8), np.arange(1960, 1981, 2))\n\n([&lt;matplotlib.axis.XTick at 0x74a7e3559370&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e355bb60&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e373fe00&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e3572810&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e14a6120&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e361f170&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e14a6cf0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e14a7650&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e14a7f80&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e14c4950&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e14c5220&gt;],\n [Text(0, 0, '1960'),\n  Text(8, 0, '1962'),\n  Text(16, 0, '1964'),\n  Text(24, 0, '1966'),\n  Text(32, 0, '1968'),\n  Text(40, 0, '1970'),\n  Text(48, 0, '1972'),\n  Text(56, 0, '1974'),\n  Text(64, 0, '1976'),\n  Text(72, 0, '1978'),\n  Text(80, 0, '1980')])\n\n\n\n\n\n\n\n\n\n\n\n단순 예측법\n\nlast = train.iloc[-1]['data']\ntest['pred_last'] = last\nmape_last = mape(test['data'], test['pred_last'])\nmape_last\n\nnp.float64(30.457277908606535)\n\n\n\nsns.lineplot(data=train, x='date', y='data', label='훈련')\nsns.lineplot(data=test, x='date', y='data', label='테스트')\nsns.lineplot(data=test, x='date', y='pred_last', label='단순 예측')\nplt.xticks(np.arange(0, 85, 8), np.arange(1960, 1981, 2))\n\n([&lt;matplotlib.axis.XTick at 0x74a7e14014c0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e1400740&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e14004d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e1429880&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e142a2d0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e142a9f0&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e142b530&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e142be30&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e142af90&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e1250590&gt;,\n  &lt;matplotlib.axis.XTick at 0x74a7e1250ec0&gt;],\n [Text(0, 0, '1960'),\n  Text(8, 0, '1962'),\n  Text(16, 0, '1964'),\n  Text(24, 0, '1966'),\n  Text(32, 0, '1968'),\n  Text(40, 0, '1970'),\n  Text(48, 0, '1972'),\n  Text(56, 0, '1974'),\n  Text(64, 0, '1976'),\n  Text(72, 0, '1978'),\n  Text(80, 0, '1980')])\n\n\n\n\n\n\n\n\n\n\n\n계절적 예측\n\ntest['pred_last_season'] = train.iloc[-4:]['data'].values\nmape_naive_seasonal = mape(test['data'], test['pred_last_season'])\nmape_naive_seasonal\n\nnp.float64(11.561658552433654)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "단순 미래 예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/04.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/04.html",
    "title": "자기귀모형",
    "section": "",
    "text": "자기회귀과정: 예측값이 이전 값에만 선형적으로 의존한다고 가정\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ndf = pd.read_csv('_data/foot.csv')\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\nADF_result = adfuller(df['foot_traffic'])\nADF_result[0], ADF_result[1]\n\n(np.float64(-1.1758885999240747), np.float64(0.683880891789619))\n\n\n\n비 정상성 발견. 차분 진행\n\n\nfoot_diff = np.diff(df['foot_traffic'], n=1)\n\nADF_result = adfuller(foot_diff)\nADF_result[0], ADF_result[1]\n\n(np.float64(-5.268231347422044), np.float64(6.369317654781239e-06))\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(foot_diff, lags=20)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\nma2 = np.array([1, 0, 0])\nar2 = np.array([1, -0.33, -0.50])\nAR2_process = ArmaProcess(ar2, ma2).generate_sample(nsample=1000)\n\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nplot_pacf(AR2_process, lags=20)\nplt.show()\n\n\n\n\n\n\n\n\n\nplot_pacf(foot_diff, lags=20)\nplt.show()\n\n\n\n\n\n\n\n\n\ndf_diff = pd.DataFrame({'foot_traffic_diff': foot_diff})\n\ntrain = df_diff.iloc[:-52]\ntest = df_diff.iloc[-52:]\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "자기귀모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#what-is-확률보행",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#what-is-확률보행",
    "title": "확률보행",
    "section": "what is 확률보행",
    "text": "what is 확률보행\n\n확률보행: 무작위로 상승 또는 하락이 발생할 확률이 동일한 프로세스\n\n\\(y_t = C + y_{t-1} + ϵ_t\\)\nC가 0이 아닌 경우 표류가 있는 확률보행\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nsteps = np.random.standard_normal(1000)\nsteps[0] = 0\nrandom_walk = np.cumsum(steps)\nsns.lineplot(x=np.arange(len(random_walk)), y=random_walk)\nplt.xlabel('시간')\nplt.ylabel('값')\n\nText(0, 0.5, '값')",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#확률보행-식별",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#확률보행-식별",
    "title": "확률보행",
    "section": "확률보행 식별",
    "text": "확률보행 식별\n\n확률보행은 정상적이고 자기상관관계가 없는 시계열로 나타난다.\n\n\n정상성\n\n시간이 지나도 통계적 특성이 변하지 않는 시계열\n\n평균과 분산이 상수이고 자기상관관계가 있으며, 이러한 특성들이 시간에 따라 변하지 않는다.\n\n정상화:\n\n평균: 차분\n분산: 로그 변환, Box-Cox 변환 등\n\n정상성 검정:\n\nADF (Augmented Dickey-Fuller) 테스트\n\n\\(H_0\\): 시계열에 단위근1이 존재하여 비정상적이다.\n\\(H_1\\): 시계열에 단위근이 존재하지 않아 정상적이다.\n\n\n\n\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\n\ndef simulate_process(alpha: float) -&gt; np.array:\n    process = np.empty(401)\n    process[0] = 0\n    for i in range(400):\n        process[i+1] = alpha * process[i] + np.random.standard_normal()\n    return process\n\nstationary = simulate_process(alpha=0.5)\nnon_stationary = simulate_process(alpha=1)\n\nsns.lineplot(x=np.arange(len(stationary)), y=stationary, label='정상성 프로세스')\nsns.lineplot(x=np.arange(len(non_stationary)), y=non_stationary, label='비정상성 프로세스')\n\n\n\n\n\n\n\n\n\ndef mean_var_over_time(process: np.array) -&gt; np.array:\n    means = []\n    vars = []\n    for i in range(len(process)):\n        means.append(np.mean(process[:i]))\n        vars.append(np.var(process[:i]))\n    return means, vars\n\nmeans_stationary, vars_stationary = mean_var_over_time(stationary)\nmeans_non_stationary, vars_non_stationary = mean_var_over_time(non_stationary)\n\nsns.lineplot(x=np.arange(len(means_stationary)), y=means_stationary, label='정상성 평균')\nsns.lineplot(x=np.arange(len(means_non_stationary)), y=means_non_stationary, label='비정상성 평균')\n\n\n\n\n\n\n\n\n\nsns.lineplot(x=np.arange(len(vars_stationary)), y=vars_stationary, label='정상성 분산')\nsns.lineplot(x=np.arange(len(vars_non_stationary)), y=vars_non_stationary, label='비정상성 분산')\n\n\n\n\n\n\n\n\n\nresult1 = adfuller(stationary)\nresult2 = adfuller(non_stationary)\nprint(f'ADF Statistic: 정상성: {result1[0]}, 비정상성: {result2[0]}')\nprint(f'p-value: 정상성: {result1[1]}, 비정상성: {result2[1]}')\n\nADF Statistic: 정상성: -11.581100476911269, 비정상성: -2.2868639886307656\np-value: 정상성: 2.9415819854362634e-21, 비정상성: 0.17623960273475048\n\n\n\n\n자기상관관계\n\n자기 상관관계: 시계열의 선행값과 후행값 아이의 선형관계\n\nx: 지연 (\\(y_t, y_{t-2}\\)의 경우 지연 2)\ny: 계수\n\n추세가 있는 경우: 짧은 지연에서 계수가 높고, 지연이 커질수록 계수가 낮아지는 경향\n계절성이 있는 경우: 주기적인 패턴이 나타남.\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(non_stationary, lags=20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n비정상적 시계열에서 추세가 보인다. 차분을 진행해보자.\n\n\ndiff = np.diff(non_stationary, n=1)\n\nresult = adfuller(diff)\nresult[0], result[1]\n\n(np.float64(-9.004763098084766), np.float64(6.355604725150628e-15))\n\n\n\nplot_acf(diff, lags=20)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#예시",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#예시",
    "title": "확률보행",
    "section": "예시",
    "text": "예시\n\nimport pandas as pd\n\ndf = pd.read_csv('_data/googl.csv')\nsns.lineplot(data=df, x='Date', y='Close')\n\nplt.xlabel('날짜')\nplt.ylabel('종가')\n\nplt.xticks([4, 24, 46, 68, 89, 110, 132, 152, 174, 193, 212, 235],\n           ['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr'],\n           rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nresult = adfuller(df['Close'])\n\nresult[0], result[1]\n\n(np.float64(0.16025048664771302), np.float64(0.9699419435913057))\n\n\n\ndiff = np.diff(df['Close'], n=1)\n\nresult_diff = adfuller(diff)\nresult_diff[0], result_diff[1]\n\n(np.float64(-5.303439704295234), np.float64(5.3865309614545585e-06))\n\n\n\nplot_acf(diff, lags=20)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#확류보행-예측",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#확류보행-예측",
    "title": "확률보행",
    "section": "확류보행 예측",
    "text": "확류보행 예측\n\ndf = pd.DataFrame({'value': random_walk})\ntrain = df.iloc[:800]\ntest = df.iloc[800:]\n\nmean = np.mean(train['value'])\ntest['pred_mean'] = mean\n\nlast_value = train.iloc[-1]['value']\ntest['pred_last'] = last_value\n\n\n표류 기법\n\ndrift = (train.iloc[-1]['value'] - train.iloc[0]['value']) / (len(train) - 1)\ndrift\n\nnp.float64(0.051972323191613747)\n\n\n\nx_vals = np.arange(801, 1001)\npred_drift = drift * x_vals\ntest['pred_drift'] = pred_drift\nsns.lineplot(data=df, x=df.index, y='value')\nsns.lineplot(data=test, x=test.index, y='pred_mean', label='평균 예측')\nsns.lineplot(data=test, x=test.index, y='pred_last', label='마지막 값 예측')\nsns.lineplot(data=test, x=test.index, y='pred_drift', label='표류 예측')\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error\n\nmse_mean = mean_squared_error(test['value'], test['pred_mean'])\nmse_last = mean_squared_error(test['value'], test['pred_last'])\nmse_drift = mean_squared_error(test['value'], test['pred_drift'])\n\nsns.barplot(x=['평균', '마지막 값', '표류'], y=[mse_mean, mse_last, mse_drift])\n\n\n\n\n\n\n\n\n\n\n단순 예측법\n\ndf_shift = df.shift(periods=1)\nmse_one_step = mean_squared_error(test['value'], df_shift['value'].iloc[800:])\nmse_one_step\n\nnp.float64(1.0127334424545265)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#footnotes",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/02.html#footnotes",
    "title": "확률보행",
    "section": "각주",
    "text": "각주\n\n\n\\(y_t = C + αy_{t-1} + ϵ_t\\) 형태의 시계열로, α가 1보다 작은 경우, 과거의 값이 현재 값에 미치는 영향이 작아져 시계열이 정상적이다.↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "확률보행"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/01.html#부호검정이항검정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/01.html#부호검정이항검정",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "부호검정(이항검정)",
    "text": "부호검정(이항검정)\n\n모집단 분포가 정규분포가 아니고, 대칭이 아닐경우 사용\n어떤 분포라도 중앙값이 존재한다.\n표본 중 절반은 중앙값보다 작고, 절반은 크다.\n검정통계량은 \\(\\sum_{i=1}^{n}I(X_i &gt; θ_0)\\)\n영분포는 B(n, 1/2)를 따른다.\n\n\\(P(S≥c_{val} | H_0) = \\sum_{k=c_{val}}^{n}nCk(\\frac{1}{2})^n ≤ α\\)\n\\(c_{val} = Q_{1-α} + 1\\)\n\n만약 이산형 변수여서 \\(θ_0\\)와 같은 값이 m개 이면 n - m개에 대해서만 검정을 진행\n\n\nsign_test &lt;- function(S0, sample, dir='two') {\n    n &lt;- sum(sample != S0)\n    Sobs &lt;- sum(sample &gt; S0)\n    \n    if (dir == 'greater') {\n        pvalue &lt;- 1 - pbinom(Sobs-1, n, 1/2)\n    } else if (dir == 'less') {\n        pvalue &lt;- pbinom(Sobs, n, 1/2)\n    } else {\n        prob_lower &lt;- pbinom(Sobs, n, 1/2)\n        prob_upper &lt;- 1 - pbinom(Sobs-1, n, 1/2)\n        pvalue &lt;- 2 * min(prob_lower, prob_upper)\n    }\n    \n    return(pvalue)\n}\n\nS0 &lt;- 100\niq &lt;- c(98, 121, 110, 89, 109, 108, 102, 92, 131, 114)\nsign_test(S0, iq, 'greater')\n\n[1] 0.171875\n\n\n\nquantile(iq, probs = 0.95)\n\n  95% \n126.5 \n\n\n\nsign_test(127.5, iq, 'greater')\n\n[1] 0.9990234\n\n\n\n부호검정의 영분포 시각화\n\nlibrary(ggplot2)\n\n# 부호검정 영분포 plotting 함수\nplot_sign_test_null &lt;- function(sample, S0, dir = 'two', alpha = 0.05) {\n    n &lt;- sum(sample != S0)\n    Sobs &lt;- sum(sample &gt; S0)\n    \n    # 이항분포 B(n, 0.5)의 가능한 값들\n    x_vals &lt;- 0:n\n    probs &lt;- dbinom(x_vals, n, 0.5)\n    \n    # 데이터프레임 생성\n    df &lt;- data.frame(x = x_vals, prob = probs)\n    \n    # p-value 계산 및 영역 표시를 위한 색상 설정\n    if (dir == 'greater') {\n        critical_region &lt;- x_vals &gt;= Sobs\n        pvalue &lt;- 1 - pbinom(Sobs-1, n, 0.5)\n        title_suffix &lt;- \"(우측검정)\"\n    } else if (dir == 'less') {\n        critical_region &lt;- x_vals &lt;= Sobs\n        pvalue &lt;- pbinom(Sobs, n, 0.5)\n        title_suffix &lt;- \"(좌측검정)\"\n    } else {\n        # 양측검정\n        prob_lower &lt;- pbinom(Sobs, n, 0.5)\n        prob_upper &lt;- 1 - pbinom(Sobs-1, n, 0.5)\n        if (prob_lower &lt;= prob_upper) {\n            critical_region &lt;- x_vals &lt;= Sobs | x_vals &gt;= (n - Sobs)\n        } else {\n            critical_region &lt;- x_vals &gt;= Sobs | x_vals &lt;= (n - Sobs)\n        }\n        pvalue &lt;- 2 * min(prob_lower, prob_upper)\n        title_suffix &lt;- \"(양측검정)\"\n    }\n    \n    df$region &lt;- ifelse(critical_region, \"p-value 영역\", \"채택역\")\n    \n    # 그래프 생성\n    p &lt;- ggplot(df, aes(x = x, y = prob)) +\n        geom_col(aes(fill = region), alpha = 0.7, width = 0.8) +\n        geom_point(data = df[df$x == Sobs, ], \n                   aes(x = x, y = prob), \n                   color = \"red\", size = 4, shape = 19) +\n        scale_fill_manual(values = c(\"p-value 영역\" = \"red\", \"채택역\" = \"lightblue\")) +\n        labs(\n            title = paste0(\"부호검정의 영분포 B(\", n, \", 0.5) \", title_suffix),\n            subtitle = paste0(\"관찰값 S = \", Sobs, \", p-value = \", round(pvalue, 4)),\n            x = \"검정통계량 S (중앙값보다 큰 값의 개수)\",\n            y = \"확률\",\n            fill = \"영역\"\n        ) +\n        theme_minimal() +\n        theme(\n            plot.title = element_text(hjust = 0.5, size = 14),\n            plot.subtitle = element_text(hjust = 0.5, size = 12),\n            legend.position = \"bottom\"\n        ) +\n        scale_x_continuous(breaks = x_vals)\n    \n    return(p)\n}\n\nS0 &lt;- 100\niq &lt;- c(98, 121, 110, 89, 109, 108, 102, 92, 131, 114)\n\nplot_sign_test_null(iq, S0, 'greater')\n\n\n\n\n\n\n\n\n\nplot_sign_test_null(iq, S0, 'two')\n\n\n\n\n\n\n\n\n\nplot_sign_test_null(iq, 127.5, 'less')\n\n\n\n\n\n\n\n\n\nα를 정확하게 통제할 수 없다.\n\n대표본의 경우 이산형 분포가 정규분포로 근사할 수 있기 때문에, 근사해서 검정 진행해서 α 통제\n\n\n\n\n신뢰구간\n\n# 신뢰구간 계산\na = 0.05\nn = length(iq)\nl = qbinom(a / 2, n, 1/2)\nu = n + 1 - l\nsorted_iq = sort(iq)\n(ci = sorted_iq[c(l, u)])\n\n[1]  92 121\n\n\n\n# 실제 신뢰도 계산 (이산분포 특성상 정확히 95%가 아님)\n(coverage = sum(dbinom(l:(u-1), n, 1/2)))\n\n[1] 0.9785156\n\n\n왜 비대칭 분포에서도 유효한가?\n\n중앙값의 불변성: 분포가 비대칭이어도 중앙값은 여전히 모집단을 반으로 나누는 점\n부호의 독립성: 각 관측값이 중앙값보다 클 확률은 항상 0.5 (연속분포 가정)\n순서통계량의 분포: 순서통계량들 사이의 확률적 관계는 분포 형태와 무관\n\n시뮬레이션 검증: 비대칭 분포에서의 신뢰구간\n\n# 부호검정 신뢰구간 함수\nsign_ci &lt;- function(sample, alpha = 0.05) {\n    n &lt;- length(sample)\n    l &lt;- qbinom(alpha / 2, n, 0.5)\n    u &lt;- n + 1 - l\n    sorted_sample &lt;- sort(sample)\n    return(c(lower = sorted_sample[l], upper = sorted_sample[u]))\n}\n\n# 비대칭 분포 시뮬레이션\nset.seed(123)\nn_sim &lt;- 1000\nn_sample &lt;- 20\ncoverage_results &lt;- data.frame(\n    distribution = character(),\n    true_median = numeric(),\n    coverage_rate = numeric(),\n    stringsAsFactors = FALSE\n)\n\n# 1. 정규분포 (대칭)\ntrue_median_normal &lt;- 0\ncoverage_normal &lt;- 0\nfor (i in 1:n_sim) {\n    sample &lt;- rnorm(n_sample, mean = 0, sd = 1)\n    ci &lt;- sign_ci(sample)\n    if (true_median_normal &gt;= ci[1] && true_median_normal &lt;= ci[2]) {\n        coverage_normal &lt;- coverage_normal + 1\n    }\n}\ncoverage_results &lt;- rbind(coverage_results, \n    data.frame(distribution = \"정규분포\", \n               true_median = true_median_normal,\n               coverage_rate = coverage_normal / n_sim))\n\n# 2. 지수분포 (오른쪽 비대칭)\ntrue_median_exp &lt;- log(2)  # 지수분포의 중앙값\ncoverage_exp &lt;- 0\nfor (i in 1:n_sim) {\n    sample &lt;- rexp(n_sample, rate = 1)\n    ci &lt;- sign_ci(sample)\n    if (true_median_exp &gt;= ci[1] && true_median_exp &lt;= ci[2]) {\n        coverage_exp &lt;- coverage_exp + 1\n    }\n}\ncoverage_results &lt;- rbind(coverage_results, \n    data.frame(distribution = \"지수분포\", \n               true_median = true_median_exp,\n               coverage_rate = coverage_exp / n_sim))\n\n# 3. 베타분포 (왼쪽 비대칭)\ntrue_median_beta &lt;- qbeta(0.5, shape1 = 5, shape2 = 2)\ncoverage_beta &lt;- 0\nfor (i in 1:n_sim) {\n    sample &lt;- rbeta(n_sample, shape1 = 5, shape2 = 2)\n    ci &lt;- sign_ci(sample)\n    if (true_median_beta &gt;= ci[1] && true_median_beta &lt;= ci[2]) {\n        coverage_beta &lt;- coverage_beta + 1\n    }\n}\ncoverage_results &lt;- rbind(coverage_results, \n    data.frame(distribution = \"베타분포(5,2)\", \n               true_median = true_median_beta,\n               coverage_rate = coverage_beta / n_sim))\n\nprint(coverage_results)\n\n   distribution true_median coverage_rate\n1      정규분포   0.0000000         0.952\n2      지수분포   0.6931472         0.971\n3 베타분포(5,2)   0.7355500         0.961\n\n\n\n# 각 분포의 형태와 중앙값 시각화\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nset.seed(123)\nx_normal &lt;- rnorm(1000)\nx_exp &lt;- rexp(1000)\nx_beta &lt;- rbeta(1000, 5, 2)\n\np1 &lt;- ggplot() + \n    geom_histogram(aes(x = x_normal), bins = 30, alpha = 0.7, fill = \"blue\") +\n    geom_vline(xintercept = 0, color = \"red\", size = 1) +\n    labs(title = \"정규분포 (대칭)\", x = \"값\", y = \"빈도\") +\n    annotate(\"text\", x = 0.5, y = 80, label = \"중앙값 = 0\", color = \"red\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\np2 &lt;- ggplot() + \n    geom_histogram(aes(x = x_exp), bins = 30, alpha = 0.7, fill = \"green\") +\n    geom_vline(xintercept = log(2), color = \"red\", size = 1) +\n    labs(title = \"지수분포 (오른쪽 비대칭)\", x = \"값\", y = \"빈도\") +\n    annotate(\"text\", x = 2, y = 150, label = paste(\"중앙값 =\", round(log(2), 3)), color = \"red\")\n\np3 &lt;- ggplot() + \n    geom_histogram(aes(x = x_beta), bins = 30, alpha = 0.7, fill = \"orange\") +\n    geom_vline(xintercept = qbeta(0.5, 5, 2), color = \"red\", size = 1) +\n    labs(title = \"베타분포(5,2) (왼쪽 비대칭)\", x = \"값\", y = \"빈도\") +\n    annotate(\"text\", x = 0.5, y = 80, label = paste(\"중앙값 =\", round(qbeta(0.5, 5, 2), 3)), color = \"red\")\n\ngrid.arrange(p1, p2, p3, ncol = 1)\n\n\n\n\n\n\n\n\n결론: 시뮬레이션 결과에서 볼 수 있듯이, 분포가 대칭이든 비대칭이든 부호검정의 신뢰구간 포함률(coverage rate)은 명목 신뢰수준(95%)에 근접합니다. 이는 부호검정 신뢰구간이 분포무관(distribution-free) 성질을 가지기 때문입니다.\n\n\n부호검정 사용 시 주의사항\n\n동일한 값 처리: 중앙값과 정확히 같은 값들은 검정에서 제외\n표본 크기: 작은 표본에서는 정확한 이항분포 사용, 큰 표본에서는 정규분포 근사\n검정 방향: 양측검정 vs 단측검정 선택 중요\n가정: 표본이 연속분포에서 추출되었다고 가정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/01.html#윌콕슨-부호순위검정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/01.html#윌콕슨-부호순위검정",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "윌콕슨 부호순위검정",
    "text": "윌콕슨 부호순위검정\n\n모집단이 대칭인 경우 사용 가능\nrank가 동점인 경우 평균으로 처리\n영분포는 잘 알려져 있지 않은 분포\n평균: \\(\\frac{n(n+1)}{4}\\)\n분산: \\(\\frac{n(n+1)(2n+1)}{24}\\)\n일반적으로 resampling 방식으로 구하고, 수가 많아지면 표준정규분포로 근사해서 계산(Lyapunov의 중심극한정리)\n\n\n중앙값 추정과 신뢰구간\n\n왈시평균: \\(W_{ij} = \\frac{X_i + X_j}{2}\\)\n\\(SR_{+}(θ) = \\#{W_{ij} &gt; θ}\\)\n\n\nx.before &lt;- c(1.83, 0.5, 1.62, 2.48, 1.68, 1.88, 1.55, 3.06, 1.30, 0.39, 0.94, 1.34)\nx.after &lt;- c(0.88, 0.65, 0.59, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29, 0.43, 0.57, 0.92)\nx &lt;- x.before - x.after\nn &lt;- length(x)\nw &lt;- sort(Rfit::walsh(x))\nmedian(w)\n\n[1] 0.4225\n\na = 0.05\nu = n * (n+1) / 2 + 1 - qsignrank(a/2, n)\nl = qsignrank(a/2, n)\nc(l, u)\n\n[1] 14 65\n\n(ci = c(w[l], w[u]))\n\n[1] 0.145 0.685\n\n\n\nwilcox.test(x, alternative=\"greater\", exact=F)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  x\nV = 69, p-value = 0.01033\nalternative hypothesis: true location is greater than 0\n\nwilcox.test(x, conf.int=T)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x\nV = 69, p-value = 0.01611\nalternative hypothesis: true location is not equal to 0\n95 percent confidence interval:\n 0.145 0.685\nsample estimates:\n(pseudo)median \n        0.4225",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/01.html#순열-검정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/01.html#순열-검정",
    "title": "순열검정과 전통적인 비모수통계",
    "section": "순열 검정",
    "text": "순열 검정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "순열검정과 전통적인 비모수통계"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html",
    "title": "EDA 템플릿",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport platform\nimport warnings",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html#load-library",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html#load-library",
    "title": "EDA 템플릿",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport platform\nimport warnings",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html#settings",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html#settings",
    "title": "EDA 템플릿",
    "section": "Settings",
    "text": "Settings\n\nwarnings.filterwarnings('ignore')\n\nif platform.system() == 'Darwin': #맥\n        plt.rc('font', family='AppleGothic') \nelif platform.system() == 'Windows': #윈도우\n        plt.rc('font', family='Malgun Gothic') \nelif platform.system() == 'Linux': #리눅스 (구글 콜랩)\n        #!wget \"https://www.wfonts.com/download/data/2016/06/13/malgun-gothic/malgun.ttf\"\n        #!mv malgun.ttf /usr/share/fonts/truetype/\n        #import matplotlib.font_manager as fm \n        #fm._rebuild() \n        plt.rc('font', family='Malgun Gothic') \nplt.rcParams['axes.unicode_minus'] = False #한글 폰트 사용시 마이너스 폰트 깨짐 해결\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html#load-data",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html#load-data",
    "title": "EDA 템플릿",
    "section": "Load Data",
    "text": "Load Data\n\ndf = pd.read_csv('df.csv')\n\nprint(f\"df shape: {df.shape}\")",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html#missing-values",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html#missing-values",
    "title": "EDA 템플릿",
    "section": "Missing Values",
    "text": "Missing Values\n\nprint(df.isnull().sum())\n\nprint(df[df.isnull().any(axis=1)])\n\n\n결측치 처리는 전처리 참조",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html#단일-column-분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html#단일-column-분석",
    "title": "EDA 템플릿",
    "section": "단일 column 분석",
    "text": "단일 column 분석\n\ndf.head()\n\n\ndf.info()\n\n\n순서형 변수는 연속형으로 처리하던가 범주형으로 처리하던가 알아서 정하면 됨.\n순서형 그 자체로 보고 분석할 수 있는 방법들도 있긴 있음.\n\n\n범주형\n\ndf.describe(include='object')\n\n\nimport matplotlib.pyplot as plt\n\ncat_cols = []\nnum_cols = []\n\nfor col in cat_cols:\n    target_counts = df[col].value_counts().sort_index()\n    target_ratio = df[col].value_counts(normalize=True).sort_index()\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    target_counts.plot.bar(ax=ax[0])\n    ax[0].set_title(f\"{col}'s Count\")\n    ax[0].set_xlabel(f\"{col}'s Level\")\n    ax[0].set_ylabel('count')\n\n    wedges, texts, autotexts = ax[1].pie(target_ratio, autopct='%1.1f%%')\n    ax[1].set_title(f\"{col}'s Distribution\", fontsize=14, pad=20)\n    plt.show()\n\n    summary = pd.concat([target_counts, target_ratio], axis=1)\n    summary.columns = ['counts', 'probs']\n    summary = summary.sort_values('counts', ascending=False)\n    display(summary)\n\n\n\n연속형\ndetailed_stats = []\nfor col in num_cols:\n    stats_dict = {\n        'Mean': df[col].mean(),\n        'Median': df[col].median(),\n        'Min': df[col].min(),\n        'Max': df[col].max(),\n        'Std Dev': df[col].std(),\n        'IQR': df[col].quantile(0.75) - df[col].quantile(0.25),\n        'Skewness': df[col].skew(),\n        'Kurtosis': df[col].kurtosis(),\n        'CV (%)': df[col].std() / df[col].mean() * 100\n    }\n    detailed_stats.append(pd.Series(stats_dict, name=col))\n\ndisplay(pd.concat(detailed_stats, axis=1).round(2).T)\n\n\ndistribution\n\nfrom scipy.stats import probplot\n\nfor col in num_cols:\n    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n\n    # 히스토그램과 KDE\n    df[col].hist(ax=ax[0], color='skyblue', density=True)\n    sns.kdeplot(df[col], color='red', linewidth=2, label='KDE', ax=ax[0])\n    ax[0].set_title(f'{col} Distribution')\n    ax[0].set_xlabel(col)\n    ax[0].set_ylabel('Density')\n\n    mean_val = df[col].mean()\n    median_val = df[col].median()\n    ax[0].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.1f}')\n    ax[0].axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.1f}')\n    ax[0].legend()\n\n    # 박스플롯\n    sns.boxplot(y=df[col], ax=ax[1], color='lightgreen')\n\n    # Q-Q 플롯\n    probplot(df[col], plot=ax[2])\n\n    plt.show()\n\n\n\nnormality\n\nfrom scipy.stats import shapiro, anderson, jarque_bera, normaltest\n\nfor col in num_cols:\n    # 소표본 적합\n    stat, p = shapiro(df[col].dropna())\n    print(f\"Shapiro-Wilk Test: stat={stat:.4f}, p-value={p:.4f}\")\n\n    result = anderson(df[col].dropna())\n    print(f\"Anderson-Darling Test: stat={result.statistic:.4f}, critical values={result.critical_values}, significance levels={result.significance_level}\")\n\n    # 왜도, 첨도 기준 판별. 대표본 적합\n    stat, p = jarque_bera(df[col].dropna())\n    print(f\"Jarque-Bera Test: stat={stat:.4f}, p-value={p:.4f}\")\n\n    # 왜도, 첨도 기준 판별. 범용적\n    stat, p = normaltest(df[col].dropna())\n    print(f\"D'Agostino's K-squared Test: stat={stat:.4f}, p-value={p:.4f}\")\n\n    print()\n\n\n\noutliers - IQR\n\noutliers_info = []\noutliers_mask = pd.DataFrame(False, index=df.index, columns=df.columns)\n\nfor col in num_cols:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    out_idx = (df[col] &lt; lower_bound) | (df[col] &gt; upper_bound)\n    outliers_mask[col] |= out_idx\n    outliers = df[out_idx]\n    \n    outliers_info.append(pd.Series({\n        'Q1': Q1,\n        'Q3': Q3,\n        'IQR': IQR,\n        'Lower Bound': lower_bound,\n        'Upper Bound': upper_bound,\n        'Outlier Count': len(outliers),\n        'Outlier %': len(outliers) / len(df) * 100\n    }, name=col))\noutliers_info = pd.concat(outliers_info, axis=1)\noutliers = df[outliers_mask.any(axis=1)]\noutliers['reason'] = outliers_mask[outliers_mask.any(axis=1)].apply(lambda x: ', '.join(x.index[x]), axis=1)\nnormal = df.loc[(~outliers_mask).all(axis=1)]\n\ndisplay(outliers_info.round(2).T)\ndisplay(outliers)\n\n\n\noutliers - Z-score\n\noutliers_info = []\noutliers_mask = pd.DataFrame(False, index=df.index, columns=df.columns)\n\nfor col in num_cols:\n    mean = df[col].mean()\n    std = df[col].std()\n    z_scores = (df[col] - mean) / std\n    out_idx = (z_scores.abs() &gt; 3)\n    outliers_mask[col] |= out_idx\n    outliers = df[out_idx]\n\n    outliers_info.append(pd.Series({\n        'Mean': mean,\n        'Std Dev': std,\n        'Outlier Count': len(outliers),\n        'Outlier %': len(outliers) / len(df) * 100\n    }, name=col))\noutliers_info = pd.concat(outliers_info, axis=1)\noutliers = df[outliers_mask.any(axis=1)]\noutliers['reason'] = outliers_mask[outliers_mask.any(axis=1)].apply(lambda x: ', '.join(x.index[x]), axis=1)\nnormal = df.loc[(~outliers_mask).all(axis=1)]\n\ndisplay(outliers_info.round(2).T)\ndisplay(outliers)\n\n\n이상치 처리는 전처리 참조\nHUOE(Heterogeneous Univariate Outlier Ensembles) 방법도 있음 (하지만 이를 위한 library는 없는듯)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html#변수간-관계",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html#변수간-관계",
    "title": "EDA 템플릿",
    "section": "변수간 관계",
    "text": "변수간 관계\n\n연속, 연속\n\n산점도로 시각화\nspearman 상관계수 확인\nspearman - pearson 차이로 비선형 관계 확인 가능\n\n0.1 이상 차이가 나는지 확인. (공식 기준 아님)\n\n\n\n\n범주, 연속\n\nfor col in num_cols:\n    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n\n    for target_val in df['target'].unique():\n        subset = df[df['target'] == target_val][col]\n        subset.hist(ax=ax[0], label=target_val, density=True)\n    ax[0].set_title(f'{col} Distribution')\n    ax[0].set_xlabel(col)\n    ax[0].set_ylabel('Density')\n\n    plt.show()\n\n\n이런 식으로 subset 만들어서 단일 분석에서 했던 것과 같이 하면 됨.\n분포 확인 후, ANOVA나 Kruskal-Wallis 등으로 그룹 간 차이 검정 진행\nt 검정을 사용했다면, Cohen’s d 등으로 효과 크기 확인 가능\n자세한건 분산 분석 파트 참조\n\n\n\n범주, 범주\n\n시각화는 마찬가지로 subset 만들어서 단일 분석에서 했던 것과 같이 하면 됨.\nchi-square 검정 등으로 독립성 검정 진행\nCramer’s V 등으로 연관성 정도 확인\n\n두 변수가 level이 2개면 phi 계수로 확인 가능 (Cramer’s V와 동일한 값 나옴)\n\n\n\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats.contingency import association\n\ncontingency_table = pd.crosstab(df['var1'], df['var2'])\nchi2, p, dof, expected = chi2_contingency(contingency_table, correction=False)\ncramers_v = association(contingency_table, method='cramer')\n\nprint(f\"Chi-square Test: chi2={chi2:.4f}, p-value={p:.4f}\")\nprint(f\"Cramer's V: {cramers_v:.4f}\")",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html#독립변수-vs-종속변수",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html#독립변수-vs-종속변수",
    "title": "EDA 템플릿",
    "section": "독립변수 vs 종속변수",
    "text": "독립변수 vs 종속변수\n\n간단한 트리 모델링 이후 feature importance 확인 가능\n0.01이 넘는 변수들만 따로 분석 진행\n\n\nfrom sklearn.feature_selection import mutual_info_regression\n\nmi_score = mutual_info_regression(df[num_cols], df['target']) # df['target']이 범주형이면 mutual_info_classif 사용\nmi_result = pd.Series(mi_score, index=num_cols).sort_values(ascending=False)\nmi_result\n\n\nmutual information로 선형 + 비선형 관계 확인 가능\n구체적으로 어떤 관계인지는 scatter plot으로 확인 (연속 + 연속이면)\n\n\nimport pingouin as pg\n\npcorr = pg.partial_corr(data=df, x='var1', y='var2', covar='var3', method='')\npcorr\n\n\n하나의 변수를 통제했을 때, 두 변수 간의 상관관계를 분석\n내부적으로는 회귀분석을 사용\n도메인 지식으로 통제변수 선정해서 진행\n\n혹은 EDA 과정에서 발견한 관계로 선정 (한 마디로 본인이 알아서 잘 선정하기)\n\n인과분석을 하면 이 과정은 필요 없으나, ADP 환경에서 인과분석 라이브러리는 제공 안하는듯\n\n\nInteration Effect\n\n두 개 이상의 독립변수가 결합하여 종속변수에 미치는 영향\n유효한 상호작용 효과가 있다면 새로운 변수를 만들어서 분석에 포함 (예: new_var = var1 * var2)\n자세한건 분산분석 파트 참조\n\n\n\nPolinorminal Relationships\n\n회귀분석 파트 참조",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/02.html#다변량-분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/02.html#다변량-분석",
    "title": "EDA 템플릿",
    "section": "다변량 분석",
    "text": "다변량 분석\n\n차원 축소\n\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.6)\nplt.xlabel(f'PC1 ({explained_var_ratio[0]:.1%} variance)')\nplt.ylabel(f'PC2 ({explained_var_ratio[1]:.1%} variance)')\nplt.title('PCA Projection by Support Level')\nplt.colorbar(scatter, label='Support Level')\n\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2)\ntsne_result = tsne.fit_transform(df[num_cols])\n\nplt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=df['target'])\n\n\nUMAP이 전역적으로는 구조를 더 잘 보존하고, 속도도 빠르다.\nADP 환경에서 UMAP 라이브러리는 제공 안하는듯\n애초에 2차원으로 정사영해서 plot한 이 자료들로 의미있는 해석을 더 할 수 있는지 의문\n\n\n\nDBSCAN, Isolation Forest 등으로 이상치 탐지 가능\n\nDBSCAN은 비지도 학습 파트 군집분석 참조\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import IsolationForest\n\niso_forest = IsolationForest()\noe = OrdinalEncoder()\n\ndf_encoded = df.copy()\ndf_encoded[cat_cols] = oe.fit_transform(df[cat_cols])\noutlier_labels = iso_forest.fit_predict(df_encoded)\noutliers = df[outlier_labels == -1]\nnormal = df[outlier_labels != -1]\ndisplay(outliers)\n\n\n해당 결과들을 voting 등으로 종합해서 이상치로 판단할 수도 있음.\n자세한건 modeling 파트의 voting 부분 참조\n개인적인 생각: 지워야 하는 이상치는 측정 과정에서의 오류 등인데, 이런 종합적인 방법은 그런 이상치를 찾는데 유용하지는 않은듯 하다.\n\n그래서 이상치 분석 목적이 아니라면 이 방법은 굳이 안써도 되지 않을까\n\n\n\n\noutlier 연속형 분포\n\nfor col in num_cols:\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    # 박스플롯\n    # (이상치 변수 생성 필요)\n    sns.boxplot(x=df['이상치'], y=normal[col], ax=ax[0], data=df)\n    ax[0].set_title(f'{col} Boxplot by Outlier Status')\n\n    sns.kdeplot(normal[col], color='blue', linewidth=2, label='Normal', ax=ax[1])\n    sns.kdeplot(outliers[col], color='orange', linewidth=2, label='Outliers', ax=ax[1])\n    ax[1].set_title(f'{col} Distribution by Outlier Status')\n\n    plt.show()\n\n\n\noutlier 범주형 분포\n\nfor col in cat_cols:\n    normal_counts = normal[col].value_counts().sort_index()\n    outlier_counts = outliers[col].value_counts().sort_index()\n\n    fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n\n    normal_counts.plot.bar(ax=ax[0])\n    ax[0].set_title(f\"{col}'s Count\")\n    ax[0].set_xlabel(f\"{col}'s Level\")\n    ax[0].set_ylabel('count')\n\n    outlier_counts.plot.bar(ax=ax[1], color='orange')\n    ax[1].set_title(f\"{col}'s Count (Outliers)\")\n    ax[1].set_xlabel(f\"{col}'s Level\")\n    ax[1].set_ylabel('count')\n\n    combined_df = pd.DataFrame({'Normal': normal_counts, 'Outliers': outlier_counts}).fillna(0)\n    combined_df.plot.bar(ax=ax[2], stacked=True)\n    ax[2].set_title(f\"{col}'s Count Comparison\")\n    ax[2].set_xlabel(f\"{col}'s Level\")\n    ax[2].set_ylabel('count')\n    plt.show()\n\n\n그 외 profiling 분석이나 맨 휘트니, 카이제곱 독립성 검정 등으로 이상치와 정상치의 차이 분석 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "EDA 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/core/00.html#질적-변수",
    "href": "posts/03_archives/completed_project/adp_실기/notes/core/00.html#질적-변수",
    "title": "EDA",
    "section": "질적 변수",
    "text": "질적 변수\n\n상관분석\n\nfrom scipy.stats import spearmanr, kendalltau\n\ncorr, p = spearmanr(df['var1'], df['var2'])\n\ncorr, p = kendalltau(df['var1'], df['var2'])\n\ndf.corr(method='kendall') # kendal, spearman\n\n\nfrom scipy.stats.contingeny import association\n\nv2 = association(table.values, method=\"tschuprow\") # phi 계수\nv2 = association(table.values, method='cramer') # 크래머 v\nv2\n\n\n상관계수: 공분산을 각 변수의 표준편차로 나눈 것\n스피어만 상관계수: 서열척도 vs 서열척도. 확률분포에 대한 가정 필요 없음.\n켄달의 타우: 서열척도 vs 서열척도.\n\n둘 중 하나가 연속형이여도 스피어만, 켄달의 타우 중 하나를 사용.\n샘플이 적거나, 이상치, 동점이 많은 경우 켄달의 타우를 주로 사용.\n두 변수의 크기는 같아야함.\n\nphi 계수: 명목척도 vs 명목척도\n\n두 변인 모두 level이 2개일 때 사용\n두 변수를 0과 1로 바꾼 후 pearson 상관계수 계산\n\n크래머 v: 명목척도 vs 명목척도.\n\n적어도 하나의 변수가 3개 이상의 level을 가지면 사용\n범위는 0~1. 0.2 이하면 서로 연관성이 약하고, 0.6 이상이면 서로 연관성이 높음.\n\nPoint-biserial correlation: 명목척도 vs 연속형\n\n명목척도의 level이 2개일 때\n\nPolyserial correlation: 명목척도 vs 연속형\n\n명목척도의 level이 3개 이상일 때\n\n명목과 순서의 경우\n\nlevel이 2개: Mann-Whitney U검정\n3개 이상: Kruskal-Wallis H test\n\n\n\n\n시각화\n\nimport pandas as pd\n\ncols = ['your', 'target', 'cols', ...]\nfreq = pd.DataFrame(df[cols].value_counts()) # 도수분포표\nfreq['proportion'] = df[cols].value_counts(normalize=True) # 상대도수분포표\n\n\nfreq['count'].plot.bar(figsize=(15, 10), subplots=True, layout=(3, 3))\nplt.tight_layout()\nplt.show()\n\n\nplt.pie(freq['count'].values, \n        labels=freq.index, \n        autopct='%1.1f%%', \n        colors=sns.color_palette('pastel', n_colors=len(freq)))\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "EDA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/core/00.html#양적-변수",
    "href": "posts/03_archives/completed_project/adp_실기/notes/core/00.html#양적-변수",
    "title": "EDA",
    "section": "양적 변수",
    "text": "양적 변수\n\n기술통계\n\nfrom scipy.stats.mstats import gmean, hmean, tmean\nimport numpy as np\n\nnp.mean(example) # 산술평균\ngmean(example) # 기하평균\nhmean(example) # 조화평균\ntmean(example, (1, 5)) # 절사평균\nnp.sqrt(np.mean(np.array(example) ** 2)) # 평방평균\n\n\n기하평균: 비율의 평균에 주로 사용됨. 한 값이라도 0이면 전체가 0이 됨\n조화평균: 속도, 밀도 등의 평균에 주로 사용됨.\n절사평균: 극단값의 영향을 줄이기 위해 상위, 하위 몇 %를 제외한 평균\n평방평균: 신호, 파동 등에서 자주 사용\n\n\ndf.median()\ndf.mode()[0]\ndf.quantile(q=0.25)\n\n\n상관계수: 피어슨\n\n\n\n시각화\n\n도수분포표\n상대도수분포표\n줄기잎그림\n히스토그램\n상자그림\n산점도",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "EDA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/09.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/09.html",
    "title": "기타",
    "section": "",
    "text": "pulp 이용해서 푼다.\n제약 함수, 결정 변수, 목표 함수만 잘 설정하면 풀 수 있을듯\n\n\n\npulp example\n\nimport pulp\n\nprob = pulp.LpProblem(\"Problem_name\", pulp.LpMinimize) # 문제에 맞게 설정\n\n# 2. 결정 변수 정의 (이름, 하한, 상항, 정수형 여부)\nx_A1 = pulp.LpVariable(\"A_to_1\", 0, None, pulp.LpInteger)\nx_A2 = pulp.LpVariable(\"A_to_2\", 0, None, pulp.LpInteger)\nx_A3 = pulp.LpVariable(\"A_to_3\", 0, None, pulp.LpInteger)\nx_B1 = pulp.LpVariable(\"B_to_1\", 0, None, pulp.LpInteger)\nx_B2 = pulp.LpVariable(\"B_to_2\", 0, None, pulp.LpInteger)\nx_B3 = pulp.LpVariable(\"B_to_3\", 0, None, pulp.LpInteger)\n\n# 3. 목표 함수 정의 (총 운송 비용)\nprob += 2*x_A1 + 4*x_A2 + 5*x_A3 + 3*x_B1 + 1*x_B2 + 6*x_B3, \"obj_name\"\n\n# 4. 제약 조건 정의\nprob += x_A1 + x_A2 + x_A3 &lt;= 100, \"Factory_A_Supply\"\nprob += x_B1 + x_B2 + x_B3 &lt;= 200, \"Factory_B_Supply\"\nprob += x_A1 + x_B1 == 80, \"Warehouse_1_Demand\"\nprob += x_A2 + x_B2 == 90, \"Warehouse_2_Demand\"\nprob += x_A3 + x_B3 == 130, \"Warehouse_3_Demand\"\n\n# 5. 문제 풀이\nprob.solve()\n\n# 6. 결과 확인\nprint(\"Status:\", pulp.LpStatus[prob.status])\nprint(\"\\nOptimal Transportation Plan:\")\nfor v in prob.variables():\n    print(v.name, \"=\", v.varValue)\n\nprint(\"\\nTotal Minimum Cost = \", pulp.value(prob.objective))\n\n\n그 외 자주 안나오는 문제 예상 관련 글이 올라올 경우 추가 예정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "기타"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/09.html#or",
    "href": "posts/03_archives/completed_project/adp_실기/notes/09.html#or",
    "title": "기타",
    "section": "",
    "text": "pulp 이용해서 푼다.\n제약 함수, 결정 변수, 목표 함수만 잘 설정하면 풀 수 있을듯\n\n\n\npulp example\n\nimport pulp\n\nprob = pulp.LpProblem(\"Problem_name\", pulp.LpMinimize) # 문제에 맞게 설정\n\n# 2. 결정 변수 정의 (이름, 하한, 상항, 정수형 여부)\nx_A1 = pulp.LpVariable(\"A_to_1\", 0, None, pulp.LpInteger)\nx_A2 = pulp.LpVariable(\"A_to_2\", 0, None, pulp.LpInteger)\nx_A3 = pulp.LpVariable(\"A_to_3\", 0, None, pulp.LpInteger)\nx_B1 = pulp.LpVariable(\"B_to_1\", 0, None, pulp.LpInteger)\nx_B2 = pulp.LpVariable(\"B_to_2\", 0, None, pulp.LpInteger)\nx_B3 = pulp.LpVariable(\"B_to_3\", 0, None, pulp.LpInteger)\n\n# 3. 목표 함수 정의 (총 운송 비용)\nprob += 2*x_A1 + 4*x_A2 + 5*x_A3 + 3*x_B1 + 1*x_B2 + 6*x_B3, \"obj_name\"\n\n# 4. 제약 조건 정의\nprob += x_A1 + x_A2 + x_A3 &lt;= 100, \"Factory_A_Supply\"\nprob += x_B1 + x_B2 + x_B3 &lt;= 200, \"Factory_B_Supply\"\nprob += x_A1 + x_B1 == 80, \"Warehouse_1_Demand\"\nprob += x_A2 + x_B2 == 90, \"Warehouse_2_Demand\"\nprob += x_A3 + x_B3 == 130, \"Warehouse_3_Demand\"\n\n# 5. 문제 풀이\nprob.solve()\n\n# 6. 결과 확인\nprint(\"Status:\", pulp.LpStatus[prob.status])\nprint(\"\\nOptimal Transportation Plan:\")\nfor v in prob.variables():\n    print(v.name, \"=\", v.varValue)\n\nprint(\"\\nTotal Minimum Cost = \", pulp.value(prob.objective))\n\n\n그 외 자주 안나오는 문제 예상 관련 글이 올라올 경우 추가 예정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "기타"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/others/1.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/others/1.html",
    "title": "자기 소개서",
    "section": "",
    "text": "자기소개 및 가치관 (500자 이내)\n\n저는 데이터 분석과 IT 인프라 설계 분야에 깊은 관심을 가지고 있는 산업공학과 학생입니다. 산업공학을 전공하며 시스템 최적화와 데이터 기반 의사결정에 대한 이론을 배우며 데이터 분석 및 IT 인프라 설계 분야에 관심을 가지게 되었고, 이를 실무에 적용할 수 있는 지식을 학습하고자 42서울 교육기관에서 2년 동안 IT 관련 학습을 진행했습니다. 또한 이 기간 동안 AWS와 ADsP(Advanced Data Analytics Semi-Professional) 자격증을 취득하며 클라우드 컴퓨팅과 데이터 분석에 대한 기초 역량을 쌓았습니다.\n저는 효율적이고 신뢰할 수 있는 시스템을 구축하는 것을 가장 중요한 가치로 삼고 있습니다. 이러한 시스템은 데이터 손실과 보안 위협을 방지할 뿐만 아니라, 장기적인 성장의 토대가 되기 때문입니다. 현재는 데이터와 블록체인 기술을 활용하여 복잡한 문제를 단순화하고, 효율적인 해결책을 찾는 데 큰 관심을 가지고 있습니다. 앞으로도 지속적인 학습과 경험을 통해 해당 분야에서 전문성을 키워가고자 합니다.\n\n졸업 후 IT 및 블록체인 분야에 관련해서 이루고자 하는 꿈과 선정 사유 (500자 이내)\n\n저는 데이터 분석, IT 인프라, 블록체인 기술을 융합하여 현실의 복잡한 문제들을 해결하고 혁신적인 가치를 창출하는 데 기여하고 싶습니다. 전공 수업과 프로젝트를 통해 데이터가 지닌 잠재력을 배워가면서, 동시에 데이터의 신뢰성과 보안이라는 중요한 과제에 대해서도 깊이 고민하게 되었습니다. 특히 42서울에서의 학습 경험을 통해, 안전하고 효율적인 데이터 활용을 위해서는 IT 인프라와 블록체인 기술의 역할이 매우 중요하다는 것을 깨달았습니다. 이러한 경험들을 바탕으로 IT 인프라와 블록체인 기술에 더욱 관심을 가지게 되었고, 관련 기술 서적과 온라인 자료를 통해 꾸준히 학습하며 이해의 폭을 넓혀가고 있습니다. 앞으로도 끊임없이 배우고 성장하여 데이터의 가치를 안전하게 실현할 수 있는 시스템을 만드는 데 기여하고 싶습니다.\n\n목표 달성을 위한 그간의 성과 및 계획 (500자 이내)\n\n저의 주요 성과로는 42서울에서의 프로젝트 경험과 AWS, ADsP 자격증 취득을 들 수 있습니다. 42서울에서 진행한 Solidity 기반 이더리움 스마트 컨트랙트 설계 및 배포 프로젝트를 통해 블록체인의 핵심 원리와 실제 활용 방안을 학습했습니다. 또한 Vagrant, Kubernetes(K8s), ArgoCD, GitLab helm 배포 프로젝트를 수행하며 온프레미스 환경에서의 인프라 설계와 개발 환경 관리 역량을 키웠고, 이를 통해 클라우드와 온프레미스 환경의 IT 인프라 운영에 대한 실질적인 이해도를 높일 수 있었습니다. 향후 계획으로는 학부 과정에 충실히 임하면서 데이터사이언스 대학원 진학을 위한 준비를 체계적으로 진행하고자 합니다. 대학원에서는 빅데이터 처리, 머신러닝, 딥러닝 등 데이터 분석의 핵심 기술을 심도 있게 학습하고자 합니다. 이와 병행하여 온라인 강좌 수강과 실전 프로젝트 수행을 통해 IT 인프라 및 블록체인 분야의 역량을 지속적으로 강화하고, 각종 공모전 참여를 통해 실력을 검증받고자 합니다. 궁극적으로는 이러한 기술들을 융합하여 데이터의 신뢰성과 보안을 보장하고, 효율적인 시스템을 설계하는 전문가로 성장하고 싶습니다.\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "자기 소개서"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/others/3.html#봉사-계획서-2",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/others/3.html#봉사-계획서-2",
    "title": "봉사",
    "section": "봉사 계획서 2",
    "text": "봉사 계획서 2",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/others/3.html#봉사-실습일지",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/others/3.html#봉사-실습일지",
    "title": "봉사",
    "section": "봉사 실습일지",
    "text": "봉사 실습일지",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/others/3.html#봉사-결과보고서",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/others/3.html#봉사-결과보고서",
    "title": "봉사",
    "section": "봉사 결과보고서",
    "text": "봉사 결과보고서",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "봉사"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#setup과-생산주기",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#setup과-생산주기",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "setup과 생산주기",
    "text": "setup과 생산주기\n\nsetup: 기계를 준비하는데 필요한 것\n\n정확히 하나의 제품을 만드는 경우에도 setup이 필요함\n생산하는 양에 관계없이 setup 시간이 일정함\nsequence dependent setup: 순서에 따라 setup 시간이 달라짐\n\n생산주기(production cycle): setup + 생산의 과정을 반복\n\nsetup은 아무것도 못하고 시간을 버림",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#배치-생산과정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#배치-생산과정",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "배치 생산과정",
    "text": "배치 생산과정\n\nbatch1: 부품 집합을 흐름 단위로 사용\n생산 주기: batch size만큼 생산하는 주기\n처리능력: \\(\\frac{batch size}{setup time + (batch size * processing time per unit)}\\)\n\nbatch size가 무한히 커질수록 \\(\\frac{1}{p}\\)로 수렴\nsetuptime이 0이여도 \\(\\frac{1}{p}\\)\n\n\n\n\nbatch는 클 수록 좋은가?\n\nbatch size가 커질수록 처리능력이 증가하지만 재고가 많아짐\n→ 처리능력 제약적 상황에서 bottleneck의 batch size를 늘리고, 수요 제약적 상황에서 non-bottleneck의 batch size를 줄이는게 좋음\n→ \\(\\frac{B}{S + Bp} = R → B = \\frac{SR}{1 - Rp}\\)\nR보다 크면 쓸데없이 제고가 쌓이고, 작으면 capacity가 낮아짐\n\nS가 늘어나면 Batch size를 키우고, 낮아지면 Batch size를 줄여도 됨\np가 늘어나면 Batch size를 키우고, 낮아지면 Batch size를 줄여도 됨",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#경제적-주문량-모형",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#경제적-주문량-모형",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "경제적 주문량 모형",
    "text": "경제적 주문량 모형\n\n외부 공급자에게 부품을 주문하여 생산 및 배송이 이루어지는 경우\n단위시간당 발생하는 비용이 적을수록 좋다\n\n\n\n\n재고량 패턴(재고가 0이 되었을 때 정확히 도착하도록 주문할 수 있다고 가정)\n\n\n\nQ: 한 번에 주문하는 양\nR: 수요(기울기)\n주문 주기: \\(\\frac{Q}{R}\\)\n평균 재고량: \\(\\frac{Q}{2}\\)\n\n\n구매비용(purchase cost / variable cost): 단위 시간 당 구매비용은 Q에 영향을 받지 않음\n단위 재고 비용(h)\n\n단위 시간 당 발생하는 재고 비용: \\(h\\frac{Q}{2}\\)\n\n셋업(주문) 비용 (Fixed cost) (k): 주문량과 무관\n\n단위 시간 당 발생하는 셋업 비용: \\(\\frac{k}{\\frac{Q}{R}}\\)\n\n\n\n목적 함수: \\(C(Q) = \\frac{KR}{Q} + \\frac{hQ}{2}\\)\n경제적 주문량(EOQ): \\(Q^* = \\sqrt{\\frac{2KR}{h}}\\)\n\nK: 주문비용\nR: 수요량\nh: 단위 재고비용\n\nEOQ만큼 주문할 때 단위 시간당 비용\n\n\\(C(Q^*) = \\sqrt{2KhR}\\)\n\n단위당 비용 = \\(\\frac{C(Q^*)}{R} = \\sqrt{\\frac{2Kh}{R}}\\)\n수요가 증가함에 따라 EOQ는 늘어나는데 단위 당 비용은 감소\n\\(\\frac{C(Q)}{C(Q^*)} = \\frac{1}{2}(\\frac{Q^*}{Q} + \\frac{Q}{Q^*})\\)\n\\(\\frac{1}{주문 주기} ≠ 재고 회전율\\)\n\n주문 주기는 Q개가 다 없어지는 시간\n회전율은 Q/2개의 재고가 다 없어지는 시간",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#buffer-or-suffer",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#buffer-or-suffer",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "buffer or suffer",
    "text": "buffer or suffer\n\nbuffer 제고가 없으면 처리능력이 떨어질 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/06.html#footnotes",
    "title": "배치 생산 및 경제적 주문량 모형",
    "section": "각주",
    "text": "각주\n\n\nbatch 1개는 부품 집합 1 단위 의미↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "배치 생산 및 경제적 주문량 모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/02.html#프로세스-흐름-분석",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/02.html#프로세스-흐름-분석",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "프로세스 흐름 분석",
    "text": "프로세스 흐름 분석\n\n간트 차트\n\n\n\n방사선조영실 작업 칸트 차트\n\n\n\n작업시간을 표현\n프로세스 상의 작업 순서와 소요시간, 상호관계를 볼 수 있음\n대기: 수요와 공급의 불일치, 작업들에 존재하는 불확실성으로 생기는 것\n\n\n\n프로세스 평가를 위한 3가지 요소\n\n흐름률(flow rate / throughput): 실제 흐름 단위가 프로세스에 진입, 떠나는 비율\n\n\\(\\frac{흐름단위 수}{단위 시간}\\)\n흐름률이 오르면 생산 능력이 오른다.\n유량: 특정한 시간동안 관찰하는 양\n매출 원가(들어온 가격 기준)를 흐름률로 바라볼 수 있다.\n\n흐름시간(flow time): 하나의 흐름단위가 프로세스상에 머무는 시간\n\n흐름시간이 줄어들면 수요-공급 사이의 시간도 줄어든다.\n\n재고(inventory): 프로세스 상에 존재하는 흐름단위 수\n\n매출 원가 기준\n가공중인 제품 WIP(work-in-process)도 재고에 포함\n저량(stock): 특정 시점에서 관찰하는 양\n\n\n\n\n\n\n\n\n\n\n\n프로세스\n\n\n\n흐름(flow): 작업이 진행되는 것을 tracking\n\n단위: 일반적으로 산출물의 단위로 정의\n\n위의 그림에서 흐름 단위는 상품 1개, 서비스 받은 고객 1명",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/02.html#리틀의-법칙",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/02.html#리틀의-법칙",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "리틀의 법칙",
    "text": "리틀의 법칙\n위의 세개와 수요 공급간의 관계가 있다.\n\n\n\n기울기는 흐름률\n\n\n\nI = R * T (항상 성립)1\n\nI: 평균 재고 (flow time 동안 들어온 input)\nR: 평균 흐름률\nT: 평균 흐름시간",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/02.html#재고-관리-메커니즘",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/02.html#재고-관리-메커니즘",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "재고 관리 메커니즘",
    "text": "재고 관리 메커니즘\n\n재고를 카운트 하는 방법\n\ninput이 여러개일 경우 단순히 흐름단위만으로 재고를 표현하기 어려울 수 있다.\n\n\nIn terms of $s: I. 원가 기준\nIn terms of days-of-supply(DOS, 공급일수): \\(\\frac{I}{R} = T\\)\nIn terms of inventory turns(재고 회전율): \\(\\frac{R}{I} = \\frac{1}{T}\\)\n\n\n\nTurns and DOS at Kohl’s and Walmart\n\n\n\n두 회사의 재무재표\n\n\n\n위의 사례에서 두 기업의 전략을 볼 수 있음.\nKohl’s: 회전률이 낮은 대신 마진을 높임\nWalmart: 마진이 낮은 대신 회전률을 높임\n공급 일수와 회전율은 반비례 관계에 있다.\n\n\n\n재고가 부담이 되는 이유\n\n이자비용\n유지비용\n\n재고가 구식으로 변함\n물리적으로 부식됨\n사라질 수 있음\n저장공간과 추가적인 간접비 유발\n품질의 저하에 따르는 추가적인 비용 존재\n\n제품 당 재고 비용: \\(\\frac{단위 시간 당 재고 유지 비용}{단위 시간 당 재고 회전율}\\)\n\n\n\n재고 유지의 다섯 가지 이유2\n\n재고 유지는 기업 입장에서 부담이 되지만 그럼에도 불구하고 유지하는 이유가 있다.\n\n\n수송중재고(pipeline): 프로세스에 존재하는 재고\n계절재고(seasonal): 공급 능력은 고정되어 있는데 수요는 변동하는 경우(예측 가능한 수요), 미리 만들어둠\n\n계절 재고 vs 주기 재고\n계절 재고 vs 안전 재고\n\n계절 재고: 수요가 예측 가능할 때\n안전 재고: 수요가 예측 불가능할 때\n\n\n주기재고(cycle): 한 번에 많이 사는게 싸다. 규모의 경제 이용한 비용 절감\n완충재고(buffer, decoupling): 프로세스상의 작업 사이의 지속적 공급을 가능하게 해줌. 단 제고가 계속 쌓이지 않게 line balancing(각 프로세스에서 진행하는 일의 양의 밸런스)을 해줘야 함\n안전재고(safety): 불확실성에 대비해 예측된 수요보다 더 많이 재고를 유지함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/02.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/02.html#footnotes",
    "title": "조직을 프로세스 관점에서 바라보기",
    "section": "각주",
    "text": "각주\n\n\n시험문제에 더 복잡하게 낸다고 하시긴 함↩︎\n이것들의 차이와 의미하는 바, 사례를 보고 어떤걸 의미하는지 알아야 한다.↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "조직을 프로세스 관점에서 바라보기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/10.html#예측",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/10.html#예측",
    "title": "예측",
    "section": "예측",
    "text": "예측\n예측: 과거의 데이터를 사용하여 현재 불확실하고 미래에 실현될 결과에 대해 판단하는 과정\n\n\n단기로 갈 수록 디테일한 예측을 하고, 기법이 달라짐.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/10.html#예측-기법",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/10.html#예측-기법",
    "title": "예측",
    "section": "예측 기법",
    "text": "예측 기법\n\n판단적 기법\n\n전문가의 경험과 직관에 의존하여 예측하는 기법\n비정량적 / 주관적 데이터로 정량적인 예측치를 구함\n\n단순예측법: 최근의 자료가 미래에 대한 최선의 추정치 \\(\\hat{p_{t+1}} = p_t\\)\n추세분석:치전기와 현기 사이의 추세를 다음 기의 판매예측에 반영하는 방법. \\(\\hat{p_{t+1}} = p_t + p_t - p_{t-1}\\)\n시장조사법: 설문지, 인터뷰를 바탕으로 신제품의 생산량 결정이나 기존제품의 수요변화 예측\n전문가 의견 종합법: 여러 전문가로 예측치 수집 후 단순평균 or 가중평균\n사례유추법: 비슷한 제품이랑 비교\n델파이 기법: 여러 전문가들로 패널을 구성하고, 반복적인 질문과 결과 피드백을 통하여 합의된 예측치를 도출\n\n\n\n\n\n델파이 기법\n\n\n\n\n시계열 기법\n\n단순 이동평균법: time window를 계속 이동하면서 평균 구하는거\n\ntime window ↑: 먼 과거까지 보겠다\n\n가중 이동평균법: 가중치를 다르게 부여\n지수평활법: 과거의 모든 데이터를 가중 평균\n\n지수평활계수(α): 최근의 값을 더 높은 가중치가 부여되도록 추정\n\\(\\hat{y_{t+1}} = αy_t+ (1-α)\\hat{y_t} = \\hat{y_t} + α(y_t - \\hat{y_t}\\)\n예측치와 관측치 중 어디에 중점을 둘 지에 따라서 α 결정\n오차를 어느정도 반영할지에 따라서 α 결정\nα == 1: 최근 자료에 비중을 둠. α == 0: 기존 예측을 따름\n\n\n\n\n\n\n상관관계 기법\n\n회귀분석\n\n\n\n\n선행 지수법",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/10.html#계절성-수요예측",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/10.html#계절성-수요예측",
    "title": "예측",
    "section": "계절성 수요예측",
    "text": "계절성 수요예측\n\n추세: 시간의 흐름에 따라 일정한 방향성을 가지고 수요가 변화\n계절성\n\n\n\n\n\n단순 변동, 추세, 계절성이 모두 있는 경우\n\n\n\n승법적 모델: (일정수준 + 추세) * 계절성\n\ncycle 별로 평균을 구한다.(추세, 계절성이 제거됨)\n관측치를 cycle의 평균으로 나눈다.\n계절별 평균으로 SI(계절성, 추세 제거됨)를 구한다.\nSI를 cycle 평균 예측치에 곱해서 예측치를 구한다.\n\n합산적 모델: (일정수준 + 추세) + 계절성\n\ncycle 별로 평균을 구한다.(추세, 계절성이 제거됨)\n관측치를 cycle의 평균으로 뺀다.\n계절별 평균으로 SI(계절성, 추세 제거됨)를 구한다.\nSI를 cycle 평균 예측치에 더해서 예측치를 구한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/10.html#예측의-품질-평가",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/10.html#예측의-품질-평가",
    "title": "예측",
    "section": "예측의 품질 평가",
    "text": "예측의 품질 평가\n\n비편향 예측: 평균 예측오차가 0\n\n\nMSE: 편차가 클 수록 불이익\nMAE: 각 편차가 동일하게 나쁜 것으로 간주\nMAPE\n\n\n질문: 예측치가 음수일 수 있나?",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "예측"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/13.html#방법",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/13.html#방법",
    "title": "일정 계획",
    "section": "방법",
    "text": "방법\n\nFCFS\nLCFS\nSPT (Shortest Processing Time)\nLPT (Longest Processing Time)\nEDD (Earliest Due Date)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "일정 계획"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/13.html#평가-척도",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/13.html#평가-척도",
    "title": "일정 계획",
    "section": "평가 척도",
    "text": "평가 척도\n\nmakespan: 모든 작업이 완료되는 시간\n\n전체 소요시간은 달라지지 않음.\nsequential dependent한 set up이 있는 경우는 달라짐\n\nlateness: \\(C_i - d_i\\) (작업이 완료된 시간 - 작업의 마감 시간)\ntardiness: 지연된 시간\nearliness: 일찍 완료된 시간\nflow time: \\(C_i - r_i\\) (작업이 완료된 시간 - 작업이 시작된 시간)\n흐름률: \\(\\frac{작업량}{makespan}\\)\n\n제고와 flow time을 제일 많이 줄이는 것은 SPT rule\n\n정확한 processing time을 알아야 하고\nprocessing time의 예측은 편향될 수 있고\n공정성 문제가 있을 수 있다.\n\nweighted SPT Rule: \\(\\frac{weight}{processing time}\\)이 높은 작업을 우선적으로 처리",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "일정 계획"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#intro",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#intro",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "Intro",
    "text": "Intro\n\n지금까지는 변동성을 고려하지 않았지만 프로세스 성과 평가에 중요한 영향을 미친다.\n변동성이 대기시간에 미치는 영향을 살펴본다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#example",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#example",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "Example",
    "text": "Example\n\n\n변동성\n\n불규칙한 도착 간격\n서비스 시간의 변동성\n영향: 재고, 대기시간, 산출 손실\n\nIU가 100 이하여도 대기가 발생할 수 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#변동성의-원인",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#변동성의-원인",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "변동성의 원인",
    "text": "변동성의 원인\n\n흐름단위의 input (\\(CV_a\\))\n\nrandom arrival\nincoming quality\nproduct mix\n\nprocessing time의 변동성 (\\(CV_p\\))\n\n그냥 내재적인 변동성\n숙련도 (일을 못해서 오래걸림)\n품질 (재작업)\n\n자원의 무작위적 가용성\n\n자원 고장\n작업자 출근 안함\nsetup time\n\n복수의 흐름단위가 무작위적 경로결정\n\n경로의 변동성\n\n\n\n변동성의 측정: \\(\\frac{표준편차}{평균}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#대기시간-예측-단일-자원",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#대기시간-예측-단일-자원",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "대기시간 예측 (단일 자원)",
    "text": "대기시간 예측 (단일 자원)\n\n가정\n\n내재활용률은 100% 미만\n\nif D &gt; C, 대기 - 처리능력 부족 (+ 변동성)\nif D &lt; C, 대기 - 변동성\n\n안정적 도착: 평균 고객 수가 시점에 의존하지 않고, 길이에만 의존함\n\n만약 프로세스가 안정적이지 않다면 더 짧은 시간간격으로 나누어 접근\n\n지수분포를 따르는 도착간격\n\n\\(CV_a = 1\\)\n비기억 특성\n\n\n\n\n변수\n\na: 평균 도착 간격 (줄 기준)\np: 평균 서비스 시간\n\\(CV_a\\): 도착간격의 변동계수\n\\(CV_p\\): 서비스 시간의 변동계수\n\\(T_q\\): 대기 시간\n\\(I_q\\): 대기열의 재고\n\\(I_p\\): 서비스 중 재고\n\n\n\n\n공식\n\ncapacity: \\(\\frac{1}{p}\\)\nflow rate = demand(수요 제약적 상황을 가정하니까): \\(\\frac{1}{a}\\)\nutilization: \\(\\frac{p}{a}\\)\nT: \\(T_q\\) + p\n\\(I_p\\): (1 - u) * 0 + u * 1 = u\nI = \\(I_q\\) + \\(I_p\\) = \\(I_q\\) + utilization\n\\(T_q = p * \\frac{u}{1-u} * \\frac{CV_a^2 + CV_p^2}{2}\\)\n\n도착 간격이 지수분포를 따르지 않는 경우 근사치만을 제공\n\n\\(I_q = \\frac{1}{a} * T_q = \\frac{T_q}{a}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#대기시간-예측-복수-자원",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#대기시간-예측-복수-자원",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "대기시간 예측 (복수 자원)",
    "text": "대기시간 예측 (복수 자원)\n\ncapacity: \\(\\frac{m}{p}\\)\nflow rate: \\(\\frac{p}{am}\\)\n\\(I = I_q + I_p = I_q + mu\\)\n\\(T_q = \\frac{p}{m} * \\frac{u^{\\sqrt{2(m+1)} - 1}}{1-u} * \\frac{CV_a^2 + CV_p^2}{2}\\)\n\n근사치만을 제공\n\nservice level: \\(P(T_q ≤ TWT)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#풀링",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/07.html#풀링",
    "title": "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제",
    "section": "풀링",
    "text": "풀링\n\n대기할 수 있는 방법은 여러가지가 있다\n대기시간을 줄일 수 있는 방법에 사람을 많이 뽑는것 외에 다른 고려 요소\n\n\n\n풀링의 효과\n\n풀링되는 시스템이 서로 완전히 독립\n다양한 input을 처리할 수 있어야 함.\n→ 대기시간, 대기 인원 감소",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 대기시간 문제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/09.html#프로젝트-vs-프로세스",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/09.html#프로젝트-vs-프로세스",
    "title": "프로젝트 관리",
    "section": "프로젝트 vs 프로세스",
    "text": "프로젝트 vs 프로세스\n\n프로젝트: 일회성, 하나의 흐름단위, 제한된 시간 내에 완료해야 됨\n\n프로젝트 관리: 프로젝트를 구성하는 여러 활동들의 배치와 계획\n\n프로세스: 지속적, 여러 흐름 단위",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/09.html#주경로-기법critical-path-method-cpm",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/09.html#주경로-기법critical-path-method-cpm",
    "title": "프로젝트 관리",
    "section": "주경로 기법(Critical Path Method, CPM)",
    "text": "주경로 기법(Critical Path Method, CPM)\n\n프로젝트 완료 시간 계산\n종속성 path 중 가장 긴 path가 주경로. 즉 프로젝트를 완료할 수 있는 가장 짧은 시간.\n\n이 path의 활동이 지연되면 프로젝트 전체가 지연됨\n프로세스 흐름도에서 개별작업의 처리능력이 중요했던것과 달리 프로젝트 종료 시간이 중요.\n\n\n\n주경로 찾기\n\nEST: 각 활동이 가장 빨리 시작할 수 있는 시간. max(선행 activity의 ECT)\nECT: 각 활동이 가장 빨리 끝날 수 있는 시간. EST + cost\nLCT: 각 활동이 가장 늦게 끝날 수 있는 시간. min(후행 activity의 LST)\nLST: 각 활동이 가장 늦게 시작할 수 있는 시간. LCT - cost\nSlack: LST - EST. 0이면 주 활동",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/09.html#불확실성-하에서의-프로젝트-관리",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/09.html#불확실성-하에서의-프로젝트-관리",
    "title": "프로젝트 관리",
    "section": "불확실성 하에서의 프로젝트 관리",
    "text": "불확실성 하에서의 프로젝트 관리\n\nCPM(Critical Path Method): 소요 시간이 정확하게 알려져 있다고 가정\nPERT(Program Evaluation and Review Technique): 소요 시간이 불확실하다고 가정\n\n\n가정\n\n주 경로는 각 활동의 평균 소요시간으로 구함\n주 경로는 바뀌지 않음\n각 활동의 소요시간은 독립적임\n각 활동의 소요시간이 정규분포를 따름\n\n\n\n특징\n\n\\(d_1, d_2, ..., d_n\\)이 확률변수이면 프로젝트 완성시간 (\\(X\\))도 확률변수\n\\(d_1, d_2, ..., d_n\\)이 정규분포를 따른다면 \\(X\\)도 정규분포를 따름",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로젝트 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#example-food-truck",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#example-food-truck",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "example: Food Truck",
    "text": "example: Food Truck\n\n변동(동일한 확률 가정)\n\n수요\n공급할 수 있는 양\n\n수요와 공급이 동시에 발생하지 않는 경우로 인해 평균 흐름률이 실제랑 다름.\n\n변동성이 흐름률에 영향을 미침\nbuffer가 있으면 흐름률 높일 수 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#buffer의-역할",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#buffer의-역할",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "buffer의 역할",
    "text": "buffer의 역할\n\n\n\n변동성으로 인해 capacity가 낮아지는 이유\n\n\n\n\n변동성이 없다면 cycle time은 1/capacity\n변동성이 있다면 cycle time은 늘어남. (시뮬레이션으로 계산)\n버퍼가 있으면 용량이 커질수록 1/capacity로 점점 줄어듦.\ncell layout을 사용하면 cycle time을 제일 많이 줄일 수 있음.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#buffer를-둘-수-없는-상황-병원외상센터",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#buffer를-둘-수-없는-상황-병원외상센터",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "buffer를 둘 수 없는 상황: 병원외상센터",
    "text": "buffer를 둘 수 없는 상황: 병원외상센터\n\n\n대기해야 할 상황이 있으면 다른 병원으로 이동\n\ndiversion 상태, loss(service를 못 받음)\n\n\n\nDiversion 상태 확률\n\nD &lt; C 가정하지 않음\n도착 간격은 지수분포 가정 (processing time 분포는 가정 안함)\n대기하지 않고 바로 이탈한다고 가정\n\\(P_m\\): 내재활용률과 자원의 수에 의해 결정됨\n\\(r = um = \\frac{p}{a}\\), 해야하는 일의 양을 의미\n\n단위: Erlang\n\n\n\n\n\nErlang Loss Table\n\n\n\n들어온 인원(흐름률): \\(\\frac{1}{a}(1 - P_m(r))\\)\n안 들어온 인원: \\(\\frac{1}{a}P_m(r)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#erlang-loss-table",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#erlang-loss-table",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "Erlang Loss Table",
    "text": "Erlang Loss Table\n\n\n\n얼랑 솔실 공식1",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/08.html#footnotes",
    "title": "프로세스 성과에 미치는 변동성의 영향: 산술 손실",
    "section": "각주",
    "text": "각주\n\n\ndiversion 확률, 꽉 차있을 확률, 도착한 환자가 서비스 받을 확률, 다른 병원으로 갈 확률 시험에 나온다.↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "프로세스 성과에 미치는 변동성의 영향: 산술 손실"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#쌍대이론의-본질",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#쌍대이론의-본질",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "쌍대이론의 본질",
    "text": "쌍대이론의 본질\n\n모든 선형계획 문제는 쌍대문제를 가진다:\n\n원문제(Primal): 예를 들어 이익 최대화.\n쌍대문제(Dual): 자원비용 최소화.\n\n\n\n\n\n원 문제와 쌍대 문제의 관계\n\n\n\n원-쌍대 관계의 성질\n\n원문제의 최적해가 존재하면 쌍대문제의 최적해도 존재하며, 두 목적함수값은 같다.\n원문제의 해로부터 쌍대해를 읽을 수 있고, 그 역도 성립한다.\n쌍대해는 자원의 경제적 가치(잠재가격, shadow price)를 의미한다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#원-쌍대-관계와-상보기저해",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#원-쌍대-관계와-상보기저해",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "원-쌍대 관계와 상보기저해",
    "text": "원-쌍대 관계와 상보기저해\n\n상보해(Complementary Solutions)\n\n원문제의 기저해와 쌍대문제의 기저해는 서로 직접적으로 대응한다.\n최적해에서는 원문제와 쌍대문제의 목적함수값이 같다.\n\n\n\n상보여유성\n\n원문제의 기저변수가 0이 아니면, 대응 쌍대변수는 0이고, 그 반대도 성립한다.\n이 속성은 심플렉스 방법의 반복과정에서 두 문제의 해가 어떻게 연동되는지 설명한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#다른-원문제-형태의-쌍대문제",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#다른-원문제-형태의-쌍대문제",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "다른 원문제 형태의 쌍대문제",
    "text": "다른 원문제 형태의 쌍대문제\n\n비표준형(등식제약식, 변수의 음수 허용 등)에서도 쌍대문제는 항상 존재\n\n등식제약식은 쌍대에서 해당 쌍대변수의 부호제약을 제거(음수 허용)한다.\n변수의 음수 허용은 쌍대에서 등식제약식으로 나타난다.\n\n\n\nSOB(Sensible-Odd-Bizarre) 법칙\n\n원문제의 제약식 및 변수의 형태(≤, =, ≥, 비음, 무제약 등)에 따라 쌍대문제의 대응 형태를 쉽게 결정하는 규칙.\n대칭성: 쌍대문제의 쌍대는 원문제이므로, 두 문제의 관계는 완전히 대칭적이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#민감도-분석",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#민감도-분석",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "민감도 분석",
    "text": "민감도 분석",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#민감도-분석-적용-요약-및-주요-내용",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/05.html#민감도-분석-적용-요약-및-주요-내용",
    "title": "쌍대이론과 민감도 분석 (part 6)",
    "section": "6.7 민감도 분석 적용 – 요약 및 주요 내용",
    "text": "6.7 민감도 분석 적용 – 요약 및 주요 내용\n6.7절 “민감도 분석 적용”은 선형계획(Linear Programming) 문제에서 민감도 분석(Sensitivity Analysis)을 실제로 어떻게 적용하는지, 그리고 다양한 매개변수 변화가 최적해에 어떤 영향을 미치는지 구체적으로 설명하는 부분입니다.\n\n주요 내용 요약\n\n민감도 분석의 출발점\n민감도 분석은 보통 자원(b₁, b₂, …, bₘ)의 공급량 변화가 해에 미치는 영향을 분석하는 것으로 시작합니다. 이는 실제 모델에서 자원의 양을 조정할 수 있는 융통성이 크기 때문입니다.\n우변(b) 변화의 영향\n자원(b)의 값이 변하면, 최종 심플렉스 표의 우변만 바뀌고 나머지(행 0의 비기저변수 계수 등)는 변하지 않을 수 있습니다. 이때는 우변만 수정해서 해가 여전히 가능(feasible)한지(기저변수 값이 모두 음이 아닌지) 확인하면 됩니다. 만약 불가능해지면 쌍대심플렉스법 등으로 재최적화가 필요합니다.\n증분 분석\n자원의 값이 변화할 때, 변화분만큼의 영향(증분)을 계산해서 새로운 해와 목적함수 값을 빠르게 구할 수 있습니다.\n허용범위(Allowable Range)\n각 자원(b)의 변화가 해의 가능성과 최적성을 유지할 수 있는 범위를 계산합니다. 이 범위 내에서는 잠재가격(dual price, shadow price)이 유효하게 적용됩니다.\n동시 변화와 100% 규칙\n여러 자원의 값이 동시에 변할 때, 각 변화가 허용범위 내에서 차지하는 비율의 합이 100%를 넘지 않으면 잠재가격을 이용한 해석이 유효합니다.\n목적함수 계수 변화\n비기저변수나 기저변수의 목적함수 계수(c)가 변할 때 해가 어떻게 변하는지, 허용범위를 어떻게 계산하는지 설명합니다.\n새로운 제약식 추가\n모델에 새로운 제약식이 추가되면, 기존 최적해가 여전히 가능해인지 확인하고, 아니라면 심플렉스 표에 새로운 행을 추가해 재최적화를 진행합니다.\n파라메트릭 분석\n하나 또는 여러 매개변수를 연속적으로 변화시키면서 최적해가 어떻게 달라지는지 체계적으로 분석합니다.\n\n\n\n예시: Wyndor Glass Co. 모델\n\nb₂(자원 2의 공급량)가 12에서 24로 증가하면, 기저해가 더 이상 가능하지 않게 되고, 쌍대심플렉스법을 통해 새로운 최적해를 구해야 함을 보여줍니다.\n허용범위 내에서만 자원의 변화에 대해 잠재가격이 유효하며, 이를 벗어나면 해가 바뀌고 잠재가격도 달라집니다.\n여러 자원이 동시에 변할 때 100% 규칙을 적용해, 변화의 합이 100%를 넘지 않으면 기존 해석이 유효함을 설명합니다.\n\n\n\n실무적 의의\n\n실제 기업(예: Pacific Lumber Company)의 대규모 산림관리 최적화 문제에 민감도 분석이 어떻게 적용되어, 불확실성 하에서 더 나은 의사결정과 수익 증대에 기여했는지 사례로 제시합니다.\n\n\n요약:\n6.7절은 선형계획의 해가 자원, 목적함수 계수, 제약식 등 모델의 매개변수 변화에 얼마나 민감한지, 그리고 이런 변화가 있을 때 해를 신속하게 갱신하거나 재최적화하는 절차를 구체적으로 다룹니다. 이를 통해 실제 의사결정에서 불확실성을 관리하고, 최적화 모델의 실용성을 높일 수 있음을 보여줍니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "쌍대이론과 민감도 분석 (part 6)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/14.html#쌍대이론과-민감도-분석",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/14.html#쌍대이론과-민감도-분석",
    "title": "시험 범위",
    "section": "6 - 쌍대이론과 민감도 분석",
    "text": "6 - 쌍대이론과 민감도 분석\n\n쌍대이론의 본질\n원-쌍대 관계들\n다른 원형태들에 적용\n민감도 분석",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/14.html#선형계획을-위한-다른-알고리즘들",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/14.html#선형계획을-위한-다른-알고리즘들",
    "title": "시험 범위",
    "section": "7 - 선형계획을 위한 다른 알고리즘들",
    "text": "7 - 선형계획을 위한 다른 알고리즘들\n\n쌍대 심플렉스 방법\n상한 기법",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/14.html#수송문제와-할당-문제들",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/14.html#수송문제와-할당-문제들",
    "title": "시험 범위",
    "section": "8 - 수송문제와 할당 문제들",
    "text": "8 - 수송문제와 할당 문제들\n\n수송문제의 기초\n수송문제를 위한 능률적인 심플렉스 방법\n할당 문제\n할당문제를 위한 특별한 알고리즘",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/14.html#네트워크-최적화-모형",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/14.html#네트워크-최적화-모형",
    "title": "시험 범위",
    "section": "9 - 네트워크 최적화 모형",
    "text": "9 - 네트워크 최적화 모형\n\n네트워크 용어들\n최단 경로 문제\n최대 흐름 문제\n네트워크 심플렉스 해법",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/11.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/11.html",
    "title": "수송문제와 할당 문제들",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#전제-조건",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#전제-조건",
    "title": "clustering",
    "section": "전제 조건",
    "text": "전제 조건\n\nscalability\n다양한 타입의 속성을 처리해야 함\n\nk-means는 수치형만 처리 가능\n\n인위적인 형상의 군집도 발견할 수 있어야 함\n\nk-means는 non-convex 형태는 잘 못찾음\n\n파라미터 설정에 전문지식을 요하지 않아야함\nnoise와 outliers를 처리해야 함\n데이터가 입력되는 순서에 민감하면 안됨\n차원 수가 높아도 잘 처리할 수 있어야함\n사용자 정의 제약조건도 수용할 수 있어야함\n해석과 사용이 용이해야함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#전처리",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#전처리",
    "title": "clustering",
    "section": "전처리",
    "text": "전처리\n\nscaling 필요\none hot encoding",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#model",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#model",
    "title": "clustering",
    "section": "model",
    "text": "model\n\nDistance-based methods\n\nPartitioning methods\n\nk-means1:\n\npolinominal 시간 안에 해결 가능\nnoise, outlier에 민감함\n수치형만 처리 가능\nnon-convex 형태는 잘 못찾음\n\nk-modes: 범주형 데이터 처리 가능. 빈도수로 유사도 처리함\nk-prototype: 범주형, 수치형 섞인거 처리 가능\nk-medoids: 중심에 위치한 데이터 포인트를 사용해서 outlier 잘 처리함\n\nPAM: Partitioning Around Medoids\n\nscalability 문제 있음\n\nCLARA: sampling을 통해서 PAM의 scalability 문제를 해결\n\n샘플링 과정에서 biased될 수 있음\n\nCLARANS: medoid 후보를 랜덤하게 선택함\n\nk-means++: 초기 centroids를 더 잘 잡음\n\nHierarchical methods\n\ntop-down: divisive, dia\nbottom-up: agglomerative\n\nward’s distance: 군집 간의 거리 계산을 군집 내의 분산을 최소화하는 방식으로 계산\n\nESS: 각 군집의 중심으로 부터의 거리 제곱합\n\n\n\n\nDensity-based methods\n\n다양한 모양의 군집을 찾을 수 있음\nnoise, outlier에 강함\nDBSCAN: 잡음 포인트는 군집에서 제외\n\ncore point를 찾음(eps 이내에 minPts 이상 있는 점)\ncore point를 중심으로 군집을 확장\n\ncore point가 아닌 경우 확장 종료\n\n\n\n고정된 파라미터를 사용하기 때문에 군집간 밀도가 다를 경우 잘 못찾음\n군집간 계층관계를 인식하기 어렵다\n\nOPTICS: DBSCAN의 단점을 보완\n\n군집의 밀도가 다를 때도 잘 처리함\n군집의 계층 구조를 인식할 수 있음\neps, minPts 파라미터가 필요함\n\n\nGrid-based methods: 대표만(각 grid를 대표) 가지고 군집분석 하는거\n\n속도와 메모리 측면에서 효율적\n\nModel-based clustering methods\n\n\n거리기반 군집의 단점:\n\n군집의 모양이 구형이 아닐 경우 찾기 어려움\n군집의 갯수 결정하기 어려움\n군집의 밀도가 높아야함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#평가",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#평가",
    "title": "clustering",
    "section": "평가",
    "text": "평가\n\nsilhuette score: \\(\\frac{\\sum_{i=1}^{n} s(i)}{n}\\)\n\ns(i): \\(\\frac{b(i) - a(i)}{max((a(i), b(i)))}\\)\n\na(i): 군집 내 노드간의 평균 거리\nb(i): 가장 가까운 군집과의 노드 간 평균 거리\n\n1에 가까울 수록 좋음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/06.html#footnotes",
    "title": "clustering",
    "section": "각주",
    "text": "각주\n\n\n장단점 기말고사 언급하심↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/02.html#k-nn",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/02.html#k-nn",
    "title": "분류",
    "section": "K-NN",
    "text": "K-NN\n\n새로운 데이터 포인트에 대해 k개의 가장 가까운 이웃을 찾고, 그 이웃들의 클래스를 투표하여 다수결로 분류한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/02.html#의사결정-트리",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/02.html#의사결정-트리",
    "title": "분류",
    "section": "의사결정 트리",
    "text": "의사결정 트리\n불순도가 가장 낮은(한쪽의 class가 더 많은) leaves를 root에 두고, 그 다음 불순도가 낮은 leaf를 그 아래에 두는 방식으로 트리를 구성한다. leaf 노드의 과반수가 같은 클래스를 가지면 그 클래스를 리턴한다. overfit을 방지하기 위해 pruning을 하거나 max depth를 설정한다.\n\ngood split: 불순도가 낮고, 분할된 각 leave의 비율이 비슷한 경우\n\n\n과정\n\n루트 노드에서 시작전체 데이터셋을 기준으로 시작하여 가장 좋은 분할속성(feature)을 선택\n분할 기준 평가각 속성에 대해 데이터를 분할했을 때의 분할 평가함수 적용\n\ngini index\n\ngini: \\(1 - \\sum_{i=1}^{c} p_i^2\\).\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 0.5\n\nmisclassification error\n\nerror: \\(1 - \\max(p_i)\\)\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 0.5\n\nentropy: 유용하지 못한 정보를 포함하고 있는 정도.\n\nentropy: \\(-\\sum_{i=1}^{c} p_i \\log_2(p_i)\\)\n목표: 최소화\n제일 좋은게 0\n제일 안좋은게 1\n\ninformation gain: 어떤 속성을 기준으로 분할했을 때, 얻을 수 있는 불확실성의 감소량\n\n부모의 엔트로피 - 자식의 엔트로피 가중평균\n목표: 최대화\n단점: 고유한 값을 많이 갖는 변수를 선호하는 경향이 있음\n알고리즘: ID3, C4.5\n\ngain ratio: information gain의 단점을 보완한 방법\n\ngain ratio: \\(\\frac{information\\ gain}{entropy}\\)\n분기의 갯수와 각 분기의 크기를 함께 고려\n목표: 최대화\n단점: entropy가 낮은 것을 선택하는 경향이 있음\n\n→ 평균이 넘는 entorpy만 선택해서 gain ratio를 계산\n\n알고리즘: C4.5\n\n\n최적의 분할 선택 평가된 기준 중 가장 순도가 높은 점수를 갖는 속성을 선택 (greedy)\n재귀적으로 하위 노드 분할. 분할된 하위 데이터에 대해 위의 과정을 반복\n종료 조건 만족 시 정지\n가지치기 수행\n\n\n\nAlgorithm\n\nCART\n\n분할 기준\n\n분류: gini index\n회귀: MSE\n\n모든 분할에서 이진 트리로 분할\n사후 가지치기\n\n비용 복잡도 가지치기: \\(Total SSR + α(leaf size)\\)이 제일 작은 트리 선택\nα는 cross validation으로 결정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/02.html#평가",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/02.html#평가",
    "title": "분류",
    "section": "평가",
    "text": "평가\n\nconfusion matrix\n\nTP, TN, FP, FN\n\naccuracy: \\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\nprecision(정밀도): true로 예측한 것 중 실제 true인 것의 비율\n\n\\(\\frac{TP}{TP + FP}\\)\n\nrecall(민감도, 재현율): 실제 true인 것 중 true로 예측한 것의 비율\n\n\\(\\frac{TP}{TP + FN}\\)\n\nF1 score: \\(2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\\)\nmacro 평균: 그냥 각 클래스의 score를 평균\nweighted 평균: 가중 평균\nmicro 평균: 전체 TP, TN, FP, FN을 합쳐서 계산\nROC curve: y축: TPR(민감도), x축: FPR(1 - 특이도)\n\nAUC: ROC curve 아래 면적\n\n1에 가까울수록 좋은 모델\n0.5는 랜덤 추측과 같음\n\n\n(1,1): 무작위 추측과 동일. 모든 샘플을 무조건 양성으로 예측\n(0,0): 모든 샘플을 무조건 음성으로 예측\n(1,0): 완벽한 모델\n(0,1): 반대로 예측하는 모델",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/02.html#learning-curve",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/02.html#learning-curve",
    "title": "분류",
    "section": "learning curve",
    "text": "learning curve\n\n\nsample 수가 적으면 정확도와 신뢰 구간에 부정적 영향을 미침\ntest set에 대한 오류가 증가하는 구간(cross 되는 구간)은 과적합 구간. 여기서 stop",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "분류"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/04.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/04.html",
    "title": "Support vector machine",
    "section": "",
    "text": "bias variance tradeoff\n\nmargin ↑, bias ↑, variance ↓\nmargin ↓, bias ↓, variance ↑\n\nsuppot vector: 임계값에 가까운 데이터 포인트\nlinear classification fomulation:\n\n\n\n\nif not linear solvable → kernel functions\nsoft margin: 이상치를 포함할 수 있는 마진\n\ncross validation으로 최적의 soft margin을 찾는다.\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Support vector machine"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/01.html#scaling",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/01.html#scaling",
    "title": "데이터 전처리",
    "section": "scaling",
    "text": "scaling\n\nmin-max scaling: \\(x' = \\frac{x - min(x)}{max(x) - min(x)}\\)\n\n0과 1 사이의 값으로 변환\n\nstandardization: \\(x' = \\frac{x - \\mu}{\\sigma}\\), \\(\\mu\\): 평균, \\(\\sigma\\): 표준편차",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/01.html#train-test-resampling",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/01.html#train-test-resampling",
    "title": "데이터 전처리",
    "section": "Train Test Resampling",
    "text": "Train Test Resampling\n\nresampling\n\nresampling: 데이터를 여러 번 나누어 모델을 학습하고 평가하는 방법\ntrain set: 모델을 학습하는 데이터\nvalidation data set: 하이퍼파라미터를 튜닝\ntest set: 모델의 성능 평가\n층화추출법: stratified sampling\n\n\n\nmethods\n\nhold out method: data를 train set과 test set으로 나누는 방법\n\n어떤 데이터가 train, test set에 포함되는지에 따라 결과가 달라질 수 있다.\n→ train set과 test set을 여러 번 나누어 모델을 학습하고 평가하는 방법이 필요하다.\n\ncross validation: 데이터를 여러 번 나누어 모델을 학습하고 평가하는 방법\n\nk-fold cross validation: 데이터를 k개의 fold로 나누고, 각 fold를 test set으로 사용하여 모델을 학습하고 평가한다.\n\nrandom하게 sampling할 수 있고, 안할 수도 있음\n\nleave-one-out cross validation: 데이터의 개수가 n개일 때, n-1개의 데이터를 train set으로 사용하고 1개의 데이터를 test set으로 사용하는 방법\nerror는 각 train set, fold 에서 계산된 error의 평균으로 구한다.\n\nbootstrap: 데이터를 중복을 허용하여 샘플링하는 방법\n\n원본 데이터에서 n개의 데이터를 랜덤하게 선택하여 train set을 만들고, 나머지 데이터를 test set으로 사용한다.\n여러 번 반복하여 모델을 학습하고 평가한다.\n실제 오류 추정치의 편향과 분산 모두에 대한 정확한 측정값을 얻을 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "데이터 전처리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html",
    "title": "classification with trees",
    "section": "",
    "text": "gini 지수가 낮을수록, misclassification error가 낮을수록, entropy가 낮을수록 Information gain이 높을수록, gain ratio가 클수록 순도가 높고 좋다. 최댓값은 0.5. entropy는 최댓값 1.\n과적합 pruning 할 때 misclassification error를 기준으로 한다.\n의사결정 트리는 데이터 마이닝에서 가장 널리 사용되는 분류 기법 중 하나로, 데이터의 패턴을 트리 구조로 표현하여 예측 모델을 구축한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#what-is-data-minig",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#what-is-data-minig",
    "title": "classification with trees",
    "section": "What is Data minig",
    "text": "What is Data minig\n\n데이터 마이닝은 대량의 데이터에서 암시적이고 이전에 알려지지 않았던 잠재적으로 유용한 지식이나 패턴을 추출하는 과정입니다.\n\n\n종류\n\n지도 학습1: 주어진 학습 데이터를 이용하여 목표 속성의 값을 예측하는 모델을 생성하는 과정으로, 입력(속성)과 출력(정답)이 모두 주어진 데이터를 바탕으로 학습\n비지도 학습",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#분류",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#분류",
    "title": "classification with trees",
    "section": "분류",
    "text": "분류\n\n목표\n\n새로운 데이터에 대해서도 정확한 예측이 가능한 일반화된 모델을 만드는 것\n이를 위해 과거 데이터를 학습용과 테스트용으로 나누어 모델의 성능을 검증",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#의사결정-트리의-구조와-원리",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#의사결정-트리의-구조와-원리",
    "title": "classification with trees",
    "section": "의사결정 트리의 구조와 원리",
    "text": "의사결정 트리의 구조와 원리\n\n의사결정 트리는 노드와 가지로 구성된 계층적 구조로, 각 노드는 특성(attribute)을 나타내며 가지는 테스트 결과를 표현.\n잎 노드는 클래스 레이블이나 클래스 분포를 나타냄\n의사결정 트리 구축은 주로 탐욕적 전략(Greedy strategy)을 사용하며, 각 단계에서 가장 좋은 분할 기준을 선택함.\n대표적인 의사결정 트리 알고리즘으로는 CART, ID3, C4.5, SLIQ, SPRINT 등이 있다.\n\n\n노드 불순도 측정 방법\n의사결정 트리에서 최적의 분할을 결정하기 위해 다양한 불순도 측정 방법이 사용됩니다:\n\nGini Index: 노드의 불순도를 측정하는 방법으로, 1-∑[p(j|t)]²로 계산됩니다. 값이 0에 가까울수록 순수한 노드를 의미합니다.\nEntropy(엔트로피): 노드의 동질성을 측정하는 방법으로, -∑p(j|t)log₂p(j|t)로 계산됩니다. 0일 때 완전히 동질적인 노드를 의미합니다.\nInformation Gain(정보 이득): 분할 전후의 엔트로피 차이로, 분할로 인해 얻어지는 불확실성 감소량을 의미합니다. 높은 정보 이득은 해당 속성이 데이터를 잘 나누는 것을 의미합니다.\nGain Ratio(이득 비율): 정보 이득을 분할의 내재 정보량(Split Information)으로 나눈 값으로, 분기가 많은 속성에 대한 편향을 줄이기 위해 고안되었습니다.\n\n\n\n트리 분할 기준\n트리 분할 시 고려해야 할 주요 이슈는 다음과 같습니다: - 데이터 분할 방법 선택 - 속성의 테스트 조건 명시 - 최고의 분할 정의 - 트리 분기 종료 시점 결정\n최적의 분할은 불순도를 최소화하는 방향으로 이루어지며, CART는 Gini 기반 분할을, ID3와 C4.5는 Information Gain 기반 분할을 주로 사용합니다.\n\n\n모델 평가 기준\n의사결정 트리 모델의 평가는 다음과 같은 기준으로 이루어집니다: - 테스트 세트에서의 정확도(%) - 오류율 - 혼동 행렬(Confusion Matrix) - 속도와 확장성 - 노이즈와 결측값 처리 능력",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#결론",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#결론",
    "title": "classification with trees",
    "section": "결론",
    "text": "결론\n의사결정 트리는 직관적이고 이해하기 쉬운 분류 모델을 제공하지만, 과적합(overfitting)이나 데이터 단편화와 같은 문제가 발생할 수 있습니다. 이를 해결하기 위해 C4.5와 같은 알고리즘은 Gain Ratio를 도입하여 분기가 많은 속성에 대한 편향을 줄이는 방법을 제시했습니다.\n의사결정 트리의 성공적인 구축을 위해서는 적절한 불순도 측정 방법 선택, 가지치기(pruning), 그리고 다양한 속성 선택 기준의 이해가 필요합니다. 이러한 방법들을 통해 보다 정확하고 일반화된 모델을 구축할 수 있습니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/13.html#footnotes",
    "title": "classification with trees",
    "section": "각주",
    "text": "각주\n\n\n규칙 기반 시스템 != 연관 규칙 학습↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "classification with trees"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/08.html#회귀",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/08.html#회귀",
    "title": "XGBoost",
    "section": "회귀",
    "text": "회귀\n\nsimilarity score: \\(\\frac{(sum of residuals)^2}{number of residuals + λ}\\)\nλ를 높이면 잔차의 수에 따른 민감도를 완화할 수 있다. overfitting 방지\ngain: \\(Left_{similarity} + Right_{similarity} - Parent_{similarity}\\)\ngain값이 높은 속성을 선택\ngain 값이 특정 γ보다 낮으면 가지치기\nleaf value: \\(\\frac{sum of residuals}{number of residuals + λ}\\)\n예측값 갱신: \\(기존 값 + learning rate(eta) * leaf value\\)로 예측",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "XGBoost"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/08.html#분류",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/08.html#분류",
    "title": "XGBoost",
    "section": "분류",
    "text": "분류\n\nsimilarity score: \\(\\frac{(sum of residuals)^2}{∑_{i=1}^{n}(previous probability_i * (1 - previous probability_i)) + λ}\\)\nleaf value: \\(\\frac{sum of residuals}{∑_{i=1}^{n}(previous probability_i * (1 - previous probability_i)) + λ}\\)\n예측값 갱신: log(odds) = \\(log \\frac{p}{1 - p} + learning rate * leaf value\\)\n\nnew probability = \\(\\frac{e^{log(odds)}}{1 + e^{log(odds)}}\\)\n\n잔차는 점점 0에 수렴하고, probability는 실제 값(1, 0)에 수렴한다.\nmin_child_weight: leaf node의 최소 가중치 합(\\(∑_{i=1}^{n} (previous probability * (1 - previous probility\\))이 이 값보다 작으면 분할하지 않음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "XGBoost"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#여러-모집단의-평균-비교",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#여러-모집단의-평균-비교",
    "title": "ANOVA",
    "section": "여러 모집단의 평균 비교",
    "text": "여러 모집단의 평균 비교\n\n여러 모집단에 대해 t검정을 2개씩 하면 α가 커져서 원하는 유의수준에 대한 가설검정이 어려움.\n\\(t(n_1 + n_2 - 2)^2 = F(1, n_1 + n_2 -2)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#분산분석-anova",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#분산분석-anova",
    "title": "ANOVA",
    "section": "분산분석 (ANOVA)",
    "text": "분산분석 (ANOVA)\n\n요인, 인자 (factor): class\n수준 (level): class 값\n처리 (treatment): 요인과 수준의 조합\n반응치 (response): 관측치",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#완전-확률화-계획법completely-randomized-design",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#완전-확률화-계획법completely-randomized-design",
    "title": "ANOVA",
    "section": "완전 확률화 계획법(Completely Randomized Design)",
    "text": "완전 확률화 계획법(Completely Randomized Design)\n\n일원 분산분석이 공정한 결과를 내기 위한 실험 조건\n처리 i에 해당되는 모집단으로부터 독립인 표본 \\(n_i\\)개를 랜덤으로 샘플링함으로써, k개의 서로 다른 모집단으로부터 독립인 random sample들을 얻는 것과 같음\n반복 수가 같을 필요는 없다\n각 모집단으로부터 랜덤으로 표본을 추출하는 것\n== 실험 대상을 랜덤으로 그룹으로 나눈 후, 각 그룹에 대해 서로 다른 처리를 작용하는 것",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#일원-분산분석-one-way-anova",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#일원-분산분석-one-way-anova",
    "title": "ANOVA",
    "section": "일원 분산분석 (One-Way ANOVA)",
    "text": "일원 분산분석 (One-Way ANOVA)\n\n정규성, 독립성, 등분산성\n\n→ 오차의 독, 정, 불편성, 등분산성\n오차(표본 - 잔차): 관심 없는 다른 모든 요인에 의해 발생하는 오차\n효과: \\(τ_i: μ_i - μ\\)\n\n\\(Y_{ij} = μ + τ_i + ε_{ij}\\)\n\\(Y_{ij} - \\bar{Y} = (\\bar{Y_i} - \\bar{Y}) + (Y_{ij} - \\bar{Y_i})\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#사후-검정-post-hoc-test",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#사후-검정-post-hoc-test",
    "title": "ANOVA",
    "section": "사후 검정 (Post-Hoc Test)",
    "text": "사후 검정 (Post-Hoc Test)\n\nf검정 결과 \\(H_0\\)가 기각되는 경우 어떤 처리 사이에 차이가 있는지 검정해야 함.\n\\(T_0 = \\frac{\\bar{Y}_i - \\bar{Y}_j}{\\sqrt{MS_E}\\sqrt{\\frac{1}{n_i} + \\frac{1}{n_j}}} ~ t(n-k)\\)\nbonferroni 방법: α를 검정 횟수 m으로 나눈 \\(\\frac{\\alpha}{m}\\)으로 사용\n\n이는 매우 보수적인 검정 방법. f 검정시 \\(H_0\\)를 기각해도, 사후 검정에서 \\(H_0\\)를 기각 못할 수도 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#무작위-블록-계획법-randomized-block-design",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#무작위-블록-계획법-randomized-block-design",
    "title": "ANOVA",
    "section": "무작위 블록 계획법 (Randomized Block Design)",
    "text": "무작위 블록 계획법 (Randomized Block Design)\n\n처리 외 독립적으로 영향을 미치는 변수의 변동을 분석하기 위해, 각 요인을 처리 모두에 대해 관찰하는 것\n이때 처리의 순서는 random\n\\(SS_T = SS_A + SS_B + SS_E\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#randomized-block-design에서의-분산분석",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/03.html#randomized-block-design에서의-분산분석",
    "title": "ANOVA",
    "section": "Randomized Block Design에서의 분산분석",
    "text": "Randomized Block Design에서의 분산분석\n\n사후 검정: 자유도가 n - b - k + 1",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "ANOVA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#통계적-검정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#통계적-검정",
    "title": "통계적 가설검정",
    "section": "통계적 검정",
    "text": "통계적 검정\n\n가설 수립\n표본 추출\n통계량 계산\n가설 채택 / 기각",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#가설검정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#가설검정",
    "title": "통계적 가설검정",
    "section": "가설검정",
    "text": "가설검정\n\n귀무가설(\\(H_0\\)): α의 값을 상한으로 지정하고 보수적으로 고려\n대립가설(\\(H_1\\))\n검정 통계량: \\(H_0\\)가 참이라고 가정했을 때, 표본에서 계산된 통계량\n기각역: \\(H_0\\)를 기각할 수 있는 범위\nType 1 error를 보통 보수적으로 지정하기 때문에 Type 2 error는 높아진다. (상충 관계)\n\n둘 다 줄이고 싶다면 n을 늘려야 한다.\n일반적으로 α(Type 1 error)를 고정하고 원하는 β(Type 2 error)를 만족하는 표본의 크기를 결정한다.\n\n상단측 검정\n하단측 검정\n양측 검정\n\n→ 기각역 계산에 주의하자\n\n모비율 차이 검정\n\n\\(\\frac{(\\hat{p}_1 - \\hat{p}_2) - D_0}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}}\\)\n\\(\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2}\\)\n\n\n\nβ 계산\n\\(H_0\\)를 이용해 검정 통계량을 계산하고, 이를 이용해 기각역을 구한 후, \\(H_1\\)을 이용해 계산.\n\n\n대표본 단측 가설검정 표본 크기1\n\n\\(n = \\frac{(z_{1 - \\alpha} + z_{1 - \\beta})^2\\sigma^2}{(\\mu_1 - \\mu_2)^2}\\)\n\n\n\n가설 검정 절차와 신뢰구간의 관계\n\n\\(\\hat{θ} - z_{1-α/2}σ_{\\hat{θ}} ≤ θ_0 ≤ \\hat{θ} + z_{1-α/2}σ_{\\hat{θ}}\\)\n100(1-α)% 신뢰구간은 유의수준 α에서 귀무가설 \\(H_0: θ = θ_0\\)가 채택되는 모든 \\(θ_0\\) 값의 집합.\n\n\n\np-value\n\np-value: \\(H_0\\)를 기각시킬 수 있는 가장 작은 유의수준 α의 값 (즉 확률)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#소표본-가설검정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#소표본-가설검정",
    "title": "통계적 가설검정",
    "section": "소표본 가설검정",
    "text": "소표본 가설검정\n\n쌍체표본: 두 집단이 독립이 아니고 서로 연관되어 있는 경우\n\n각 쌍의 차이를 계산하여 단일 표본으로 변환 후 분석할 수 있습니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#분산-검정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#분산-검정",
    "title": "통계적 가설검정",
    "section": "분산 검정",
    "text": "분산 검정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/02.html#footnotes",
    "title": "통계적 가설검정",
    "section": "각주",
    "text": "각주\n\n\n양측은?↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 가설검정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#통계적-추론",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#통계적-추론",
    "title": "통계적 추정",
    "section": "통계적 추론",
    "text": "통계적 추론\n모집단에서 추출된 표본의 통계량으로부터 모수를 추론하는 것\n\n추정\n\n점추정\n구간추정\n\n가설 검정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#추정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#추정",
    "title": "통계적 추정",
    "section": "추정",
    "text": "추정\n\n불편성\n\n\\(E(\\hat{\\theta}) = θ\\)\nbias = \\(E(\\hat{\\theta}) - \\theta\\)\n\n보통 sample size가 커질수록 bias는 0에 수렴\n\n\\(\\bar{X}, X_n\\)은 μ의 불편추정량이다.\n\n\n\n최소분산\n\n\\(Var(\\bar{X})\\)가 \\(Var(X_n)\\)보다 분산이 작아서 더 좋은 추정량\n\\(MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2\\)\n\n큰 오차에 더 큰 페널티를 주기 위해 제곱\n\n\n\n\n대표적인 불편추정량\n\n전부 중심극한의정리를 적용할 수 있다.\n\n\n모평균\n모비율\n모평균 차이 (독립이라는 가정 필요)\n모비율 차이 (독립이라는 가정 필요)\n\n\n이때, 이들의 평균과 표준편차는 모집단의 분포와 관계없이 일정하다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#구간-추정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#구간-추정",
    "title": "통계적 추정",
    "section": "구간 추정",
    "text": "구간 추정\n\nα: 유의수준\n1 - α: 신뢰수준\n(θ_L, θ_U) = (1 - α) × 100% 신뢰구간\n\n\n(\\(θ_L, θ_U\\)) 이 충분이 높은 가능성으로 미지의 모수 θ를 포함해야 한다\n구간이 충분히 좁아야 한다\n\n표준 정규분포에서 0을 중심으로 대칭일 때 길이가 짧다.\n고로 신뢰구간이 대칭임\n\n\n\n신뢰 구간의 확률적인 의미\n\n샘플링을 무한히 반복했을 때, 이들의 신뢰 구간 중 95%의 구간이 실제 모수를 포함한다. → 구간이 확률 변수이다.\n\n\n\n표본의 크기 결정\n특정 오차 아래로 하는 표본의 수 구하는 법\n\n그냥 표본오차가 목표 오차보다 작게 하는 값을 구하면 됨.\n모비율을 모를 때는 일단 0.5로 보수적으로 놓고 계산",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#소표본-신뢰구간",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#소표본-신뢰구간",
    "title": "통계적 추정",
    "section": "소표본 신뢰구간",
    "text": "소표본 신뢰구간\n\n표본이 작다. → 크면 정규분포\n모집단 정규분포를 따른다. → 비모수 검정\nσ 모름 → 알면 그냥 정규 분포\n\\(σ_1 = σ_2\\)\n\n→ t분포\n\n정규분포에 비해 신뢰구간은 더 길어짐",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#모분산-추정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/01.html#모분산-추정",
    "title": "통계적 추정",
    "section": "모분산 추정",
    "text": "모분산 추정\n\n카이제곱 분포는 가장 짧은 신뢰구간을 구하기 쉽지 않음\n\n그냥 쉽게 구하기 위해 \\((x^2_{α/2}, x^2_{1-α/2})\\)를 사용\n\n모분산의 신뢰구간: \\((\\frac{(n-1)s^2}{x^2_{(1-\\alpha)/2}(n-1)}, \\frac{(n-1)s^2}{x^2_{\\alpha/2}(n-1)})\\)\n표본의 수가 적을수록, 카이제곱 분포의 신뢰구간은 더 길어진다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "통계적 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/dsa/00.html#single-linked-list",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/dsa/00.html#single-linked-list",
    "title": "시험 범위",
    "section": "single linked list",
    "text": "single linked list\n\n안 나오지만 그냥 외우셈",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/dsa/00.html#doubly-linked-list",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/dsa/00.html#doubly-linked-list",
    "title": "시험 범위",
    "section": "doubly linked list",
    "text": "doubly linked list",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/dsa/00.html#circular-linked-list",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/dsa/00.html#circular-linked-list",
    "title": "시험 범위",
    "section": "circular linked list",
    "text": "circular linked list\n\n\n\n\n\n5~6주차\n\nstack\n\nt자 철로 출력\nstack을 이용하는 응용\nstack을 이용하지 않는 중위 -&gt; 후위, 전위 표기\n\n\n7주차\n\n원형 queue",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/tofel_준비/index.html",
    "href": "posts/03_archives/completed_project/tofel_준비/index.html",
    "title": "TOFEL 준비",
    "section": "",
    "text": "FAILED\n    \n    \n        시작일: None\n        종료일: None\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        English",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/tofel_준비/index.html#details",
    "href": "posts/03_archives/completed_project/tofel_준비/index.html#details",
    "title": "TOFEL 준비",
    "section": "Details",
    "text": "Details\nTOFEL을 준비해 봅시다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/tofel_준비/index.html#tasks",
    "href": "posts/03_archives/completed_project/tofel_준비/index.html#tasks",
    "title": "TOFEL 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/tofel_준비/index.html#why-failed",
    "href": "posts/03_archives/completed_project/tofel_준비/index.html#why-failed",
    "title": "TOFEL 준비",
    "section": "Why failed?",
    "text": "Why failed?\n자격증식 영어 공부에 너무 시간을 많이 투자하지 말자.\nopic정도만 시도해보자",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/tofel_준비/index.html#related-posts",
    "href": "posts/03_archives/completed_project/tofel_준비/index.html#related-posts",
    "title": "TOFEL 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "TOFEL 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/03.html",
    "href": "posts/03_archives/completed_project/opic/notes/03.html",
    "title": "일정 정리",
    "section": "",
    "text": "7/16: 오픽노잼 AL 시리즈 마무리\n~7/18: 예상 질문 정리\n\n부단용 오픽 정리\n강지완 오픽 정리\n\n~7/19:\n\n말하기 연습\n\n7/20:\n\n롤플레이 연습\n여우 강사 최나영 오픽 모의고사\n\n7/26: 시험\n\n대학연합 오픽\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "일정 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/04.html#돌발",
    "href": "posts/03_archives/completed_project/opic/notes/04.html#돌발",
    "title": "돌발 script 정리",
    "section": "돌발",
    "text": "돌발\n\n재활용\n\n집에서 하는 재활용 과정\n\nhow do you recycle at home? when and how often do you recycle? describe each step of the process from beginning to end\n\n재활용을 하며 기억에 남는 경험\n\ntell me about a memorable experience you had while recycling. what happened? what made it so memorable? tell me about it in as much detail as possible.\n\n우리나라의 재활용\n\ntell me about recycling in your country. what kind of items do people usually recycle? please describe the recycling system in your country in detail.\n\n\n\n\n약속\n\n약속을 잡는 경향\n\npeople set up appointments for various reasons. what sort of appointments do you usually make, social or otherwise? who do you usually meet with? where do you usually meet them? give me as many details as possible\n\n기억에 남는 약속\n\ntell me about the most memorable appointment you have ever had. what kind of appointment was it? who did you meet? what did you do? did anything unexpected or interesting occur? why was it so memorable?\n\n약속 접기 전 연락 과정\n\nwhen you want to meet up with someone, how do you get in touch with him or her? do you make phone calls, send an e-mail, or do something else? how does the exchange usually go? tell me about the process from beginning to end\n\n\n\n\n은행\n\n우리나라의 은행\n\ndescribe the banks in your country. where are they usually located, and what are they like? what hours are banks typically open there? give me as many details as possible\n\n과거와 현재의 은행 비교\n\nhave there been any changes to the banks in your country since you were a child? how were they in the past? how are they now? please describe the changes in detail\n\n은행에서 겪은 문제\n\nhave you ever experienced any problems at a bank? for instance, sometimes ATMs malfunction. what sort of problem did you face, and how did you deal with it?\n\n\n\n\n휴대폰\n\n\n지형 야외활동\n\n우리나라의 지형\n\ni would like to know about the geographic features of your country. what makes them different from other countries? please describe them in as much detail as possible.\n\n기억에 남는 야회 활동 경험\n\ndescribe the most memorable experience you have had outdoors. what is a beautiful place you have been to? please provide s many details as possible\n\n우리나라 사람들이 즐겨하는 야외 활동\n\nwhat kind of outdoor activities do people in your country do? do they enjoy things like jogging, cycling, or hiking? why do the like to do those activities?\n\n\n\n\n모임, 기념일\n\n\n외식 음식\n\n자주가는 식당\n\ni would like to know about a restaurant you often visit. what kind of dishes does it serve? what do you like about the restaurant? what does it look like?\n\n식당에서 겪은 경험\n\nhave you ever had a special experience at a restaurant? who were you with? what happened? tell me about the experience in detail, and explain what made it memorable.\n\n유명한 한국 요리\n\ntell me about the most famous dish of your country. what are the main ingredients in it? do you know how yo make it? what is special about the dish?\n\n\n\n\n기술\n\n과거와 현재의 기술 비교\n\ntechnology is advancing more rapidly than ever. can you tell me about the way technology has been changing? what changes have occurred since you were child? provide me with as many details as possible.\n\n기술과 관련되어 생긴 문제\n\nhave you ever experienced a problem related to technology? for instance, sometimes a device does not work properly or is difficult to use. describe your technological problem in detail. how did you handle it?\n\n우리나라에서 인기 있는 기술\n\ntell me about the technologies that are popular in your country. which technology do people use the most there? what is it used for? can you tell me why they like to use it?\n\n\n\n\n날씨 계절\n\n우리나라의 계절\n\ni would like to know about the seasons in your country. how many seasons are there? how are they different? what is the weather like in each season?\n\n과거와 현재의 날씨 비교\n\ndo you think that the weather has changed over the past few years? what was it like in the past? how has it changed? please describe the differences in detail.\n\n이상 기후로 인해 겪은 경험\n\nhave you ever had a memorable or unexpected experience because of unusual weather conditions? when was it? what happened? tell me about it in as much detail as possible\n\n\n\n\n휴일",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "돌발 script 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/04.html#roleplaying",
    "href": "posts/03_archives/completed_project/opic/notes/04.html#roleplaying",
    "title": "돌발 script 정리",
    "section": "roleplaying",
    "text": "roleplaying\n\n면접관에게 질문하기\n\ni live in an apartment. please ask me three or four questions about the place i live in.\ni enjoy traveling too. please ask me three or four questions about traveling.\ni live in Canada. ask me three or four questions about the geographic features of my country.\n\n\n\n주어진 상황에서 직접 질문하기\n\ni’m going yo give you a situation to act out. pretend you are overseas on vacation and you need a car to get around, so you have gone to a car rental agency. ask the agent three or four questions about renting a car.\ni am going to give you a situation to act out. imagine that you have gone to a store to buy new furniture. ask the salesperson three or four questions about the furniture you are looking for\n\n\n\n주어진 상황에서 전화로 질문하기\n\ni would like to give you a situation to act out. imagine you are planning a vacation. call a travel agency, and ask three or four questions about potential destinations and itineraries.\n\n\n\n상황 설명하고 대안 제시하기\n\ni have a problem for you to solve. you found out you purchased the wrong tickets at the movie theater. talk to the person at the ticket window about your situation and offer suggestions to solve the problem.\nthere is a problem i’d like you to solve. you’ve ordered some furniture and its just been delivered. however, the furniture thats arrived is not what you ordered. call the furniture store to explain the situation and suggest some alternatives to the problem.\nthere is a problem i need you to solve. pretend that you’ve gone to the library to look for a book, but the one you want has been checked out. explain the situation to a librarian and offer two or theree alternatives to the problem.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "돌발 script 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/00.html#시험-구조",
    "href": "posts/03_archives/completed_project/opic/notes/00.html#시험-구조",
    "title": "오픽 구조 파악",
    "section": "시험 구조",
    "text": "시험 구조\n\n시험 시간은 총 40분. 답변 시간은 자유\n2분 정도 지나면 다음 문제로 넘어갈 수 있는 버튼이 활성화됨.\n후반 문제는 조금 늦게 나옴",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/00.html#질문-유형",
    "href": "posts/03_archives/completed_project/opic/notes/00.html#질문-유형",
    "title": "오픽 구조 파악",
    "section": "질문 유형",
    "text": "질문 유형\n\n자기소개\nbackground survey\n돌발\n\n1번 문제가 자기소개. 점수엔 딱히 영향 없고, 그냥 목소리 푸는 용도\n2/3/4, 5/6/7, 8/9/10, 11/12/13, 14/15 는 background survey 및 돌발 질문\n각각의 묶음은 combo 문제\n\ncombo 별 10개 유형\n\n묘사\n\n일반적으로 이렇다는걸 대답. 현재형으로 대답\n가장 쉽고 배점이 낮기 때문에 적당히 대답\n\n루틴, 단계, 활동\n\n1번과 마찬가지로 현재형으로 대답\n\n처음 / 최근 경험\n\n육하원칙, 과거형\n최초 경험은 시간에 따른 취향 변화도 물어봄\n\n가장 인상적인 경험\n\n구체적인 설명 필요. 문제 경험을 물어보기도 함\n배점이 높으니까 신경써서 대답\n\n에바에게 질문하기\n\n안나옴\n\n주어진 상황에 3-4가지 정보 요청하기\n6번에서의 문제 상황 설명 및 대책 2-3가지 세우기\n이와 같은 본인 경험\n\n7번과 유사한 나의 경험\n\n비교 / 대조\n\n자세한 예를 들어 설명\n마무리는 선호\n\n주제 관련 이슈, 문제점, 걱정거리, 뉴스\n\n예시, 상황, 생각, 의견",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/00.html#답변-구조",
    "href": "posts/03_archives/completed_project/opic/notes/00.html#답변-구조",
    "title": "오픽 구조 파악",
    "section": "답변 구조",
    "text": "답변 구조\n\nmain idea\n\n첫인상, 요지, hook\n\nbody\n\n\n\n\n강조 구간 (2-3 문장 속도감 있게)\n강조 구간\n\nconclusion\n\n역질문 what about you?\n제안하기 why dont we 동사, lets 동사\n급 마무리 thats it for this question\n1 + 3",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/00.html#공부-방법",
    "href": "posts/03_archives/completed_project/opic/notes/00.html#공부-방법",
    "title": "오픽 구조 파악",
    "section": "공부 방법",
    "text": "공부 방법\n주제별로 공부\n묘사 -&gt; 루틴 -&gt; 비교 -&gt; 과거 경험 -&gt; 롤플레이 -&gt; 어드밴스",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "오픽 구조 파악"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/03.html#etl",
    "href": "posts/03_archives/completed_project/adp_필기/notes/03.html#etl",
    "title": "2 - 데이터 처리 프로세스",
    "section": "ETL",
    "text": "ETL\n\n1. ETL 개요\n\nETL(Extract, Transformation, Load): 데이터 이동 및 변환 절차\nbatch ETL, real-time ETL으로 나뉨\n\n\n\n\nETL 작업 단계\n\n\n\nInterface: 다양한 소스로부터 데이터 휙득을 위한 인터페이스(OLEDB, ODBC, FTP)\nStaging: 정기적으로 데이터 원천으로 부터 저장. 아직은 정규화 x\nProfiling: staging table의 데이터 특성을 식별하고, 품질 측정\nCleansing: profiling된 데이터를 보정\nIntegration: 데이터 충돌을 해소하고, 데이터를 통합. 아마 여기서 정규화가 이루어질듯(왜 책에 설명 똑바로 안해놓지)\nExport: 운영보고서 생성, 데이터웨어하우스 / 데이터마트에 적재하기 위한 최적화(denormalization) 진행\n\n\n\n2. ODS 구성\n\n통합된 데이터를 저정하는 중간 저장소\n실시간, 거의 실시간으로 데이터 적재\n\n\n\n3. 데이터 웨어하우스\n\nODS를 통해 정제 / 통합된 데이터를 분석 및 보고서 생성을 위해 저장\n\n특징\n\n주제중심성\n영속성/비휘발성\n통합성\n시계열성\n\n모델링 기법\n\n스타 스키마(조인 스키마)\n\n제 3정규형의 fact 테이블과 제 2정규형의 차원 테이블로 구성\n복잡성이 낮지만, 데이터 무결성이 떨어짐\n\n\n\n\n스노우플레이크 스키마\n\n스타 스키마의 차원 테이블을 제 3정규형으로 정규화한 상태\n데이터 무결성이 높지만, 복잡성이 높음\n\n\n\n\n\n\n\n\n\n제 1 정규형: 반복되는 record나 다치 attribute를 포함하지 않음 제 2 정규형: 부분 종속성(primary key의 일부가 다른 일부를 종속함)이 없음 제 3 정규형: 이행적 종속성(primary key가 아닌 attribute의 종속성)이 없음\n\n\n\n\n\n4. ODS vs DW",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/03.html#cdcchange-data-capture",
    "href": "posts/03_archives/completed_project/adp_필기/notes/03.html#cdcchange-data-capture",
    "title": "2 - 데이터 처리 프로세스",
    "section": "CDC(change data capture)",
    "text": "CDC(change data capture)\n\n1. CDC 개념 및 특징\n\n데이터 변경을 감지하고, 변경된 데이터를 추출하는 기술\n하드웨어 계층부터 어플리케이션 계층까지 다양한 수준에서 적용 가능\n\n\n\n2. CDC 구현 기법\n\nTime Stamp on Rows\nVersion Numbers on Rows: 참조테이블을 같이 사용하는게 일반적이라고 한다.\nStatus on Rows: time stamp, version number 보완 용도로, 사람이 레코드 반영 여부를 직접 판단할 수 있게 적용할 수 있음\nTime/Version/Status on Rows\nTriggers on Tables: message queue로 변경 발생시 subscribe 된 대상에 publish하는 방식. 시스템 관리 복잡도가 높아짐\nEvent Programming: 어플리케이션에 데이터 변경 식별 기능을 추가\nLog Scanner on Database: 데이터 스키마 변경 불필요, 어플리케이션 영향 최소화, 지연시간 최소화\n\n\n\n3. CDC 구현 방식\n\nPush: 데이터 원천에서 변경 식별(agent)\nPull: 대상 시스템에서 원천을 주기적으로 모니터링",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/03.html#eai",
    "href": "posts/03_archives/completed_project/adp_필기/notes/03.html#eai",
    "title": "2 - 데이터 처리 프로세스",
    "section": "EAI",
    "text": "EAI\n\n1. EAI의 개념 및 특징\n\n기업 내 혹은 기업 간 정보시스템을 연계하여 동기화.\nETL은 batch 처리 중심, EAI는 실시간 혹은 근접 실시간 처리 중심\n\n\n\n2. 데이터 연계 방식\n\n\nETL/CDC는 운영 데이터와 분석을 위한 데이터베이스가 구분되지만, EAI는 그냥 통합\n\n\n\n3. EAI 구성요소\n\nAdapter: 시스템 간 데이터 변환\nBroker: 데이터 전송\nBus: 데이터 전송 경로 설정\nTransformer: 데이터 형식 변환\n\n\n\n4. EAI 구현 유형\n\nMediation: Publish/Subscribe 방식\nFederaion: Request/Reply 방식\n\n\n\n5. EAI 활용 효과\n\n협력사, 파트너, 고객과의 상호 협력 프로세스 연계\n그룹 및 지주 회사 계열사들 간 상호 관련 데이터 동기화 등을 위한 데이터 표준화 기반 제공\n\n\n\n6. EAI vs ESB\n\n추가적인 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/03.html#데이터-통합-및-연계-기법",
    "href": "posts/03_archives/completed_project/adp_필기/notes/03.html#데이터-통합-및-연계-기법",
    "title": "2 - 데이터 처리 프로세스",
    "section": "데이터 통합 및 연계 기법",
    "text": "데이터 통합 및 연계 기법\n\n\n빅데이터는 시각화도 하고, NoSQL 같은 환경에서도 사용한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/03.html#대용량의-비정형-데이터-처리-방법",
    "href": "posts/03_archives/completed_project/adp_필기/notes/03.html#대용량의-비정형-데이터-처리-방법",
    "title": "2 - 데이터 처리 프로세스",
    "section": "대용량의 비정형 데이터 처리 방법",
    "text": "대용량의 비정형 데이터 처리 방법\n\n2. 대규모 분산 병렬 처리\n\n하둡:\n\nMapReduce와 HDFS를 기반으로 한 분산 병렬 처리 프레임워크\n비공유 분산 아키텍쳐\n선형적인 성능과 용량 확장\nMapReduce failover\n\n\n\nHadoop ecosystem\n\n\n\n3. 데이터 연동\n대규모 연산을 데이터베이스에서 처리하기 어렵기 때문에, 하둡으로 복사해와서 MapReduce 연산 후, 결과를 다시 데이터베이스에 기록하기 위해 스쿱 사용\n\nSqoop\n\nJDBC를 지원하는 RDBMS, Hbase와 Hadoop 간 데이터 전송(Import, Export)\nSQL 질의로 데이터 추출\nMapReduce 사용\n\n\n\n\n4. 데이터 질의 기술\n\nHive: SQL과 유사한 HiveQL 질의, batch 처리\nSQL on Hadoop: SQL 질의, 실시간 처리\n\napache Drill, Stinger, Shark, Tajo, Impala, HAWQ, Presto",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/05.html#분석-과제-발굴",
    "href": "posts/03_archives/completed_project/adp_필기/notes/05.html#분석-과제-발굴",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 과제 발굴",
    "text": "분석 과제 발굴\n\n풀어야 할 다양한 문제를 데이터 분석 문제로 변환 후, 프로젝트를 수행할 수 있는 과제 정의서 형태로 도출\n\n\n\n\n도출을 위한 접근 방법\n\n\n최적의 의사결정은 두 접근 방식이 상호 보완 관계에 있을 때 가능하다.\n\n\n1. 하향식 접근법\n\n사물을 why 관점에서 보는 방식\n\n\n\n문제 탐색: 문제를 해결함으로써 발생하는 가치에 중점\n\n비즈니스 모델기반\n분석 기회 발굴의 범위 확장\n외부참조 모델 기반\n분석 유즈 케이스\n\n문제 정의: 식별된 비즈니스 문제를 데이터의 문제로 변환\n해결방안 탐색: 분석 역량과, 분석 기법 및 시스템 존재 여부를 고려한다.\n타당성 검토\n\n경제적 타당성: 비용대비 편익 분석 관점의 접근\n데이터 및 기술적 타당성\n\n\n\n\n2. 상향식 접근법\n\n사물을 what 관점에서 보는 방식\n\n\n비지도 학습\n지도 학습\n\n\n프로토타이핑 접근법",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/05.html#분석-기획-방향성-도출",
    "href": "posts/03_archives/completed_project/adp_필기/notes/05.html#분석-기획-방향성-도출",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 기획 방향성 도출",
    "text": "분석 기획 방향성 도출\n\n1. 분석기획의 특징\n\n과제를 발굴, 정의하고 의도했던 결과를 도출할 수 있도록 적절하게 관리할 수 있는 방안을 사전에 계획하는 일련의 작업 (말 그대로 기획)\n\n\n\n3. 목표 시점 별 분석 기획 방안\n\n\n\n4. 분석 기획시 고려사항\n\n가용 데이터\n적절한 활용방안과 유즈케이스\n장애요소들에 대한 사전계획 수립",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/05.html#분석-방법론",
    "href": "posts/03_archives/completed_project/adp_필기/notes/05.html#분석-방법론",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 방법론",
    "text": "분석 방법론\n\n방법론은 절차, 방법, 도구와 기법, 템플릿과 산출물로 구성된다.\n\n\n\n\n방법론 절차의 구성 요소\n\n\n\n폭포수 모델\n프로토타입 모델\n나선형 모델\n\n\n1. KDD 분석 방법론\n\n비즈니스 도메인에 대한 이해, 프로젝트 목표 설정\n데이터셋 선택\n데이터 전처리: 잡음, 이상치, 결측치 처리. 추가로 요구되는 데이터 셋이 필요한 경우, 데이터 선택 프로세스로 돌아감\n데이터 변환: 데이터 차원 축소, 학습용 데이터, 시험용 데이터 분리\n데이터 마이닝\n데이터 마이닝 결과 평가\n\n\n\n2. CRISP-DM 분석 방법론\n\n\n\nCRISP-DM 4레벨 구조\n\n\nGeneric Tasks 예시: 데이터 정제\nSpecialized Tass 예시: 범주형 데이터 정제, 연속형 데이터 정제\n\n\n\nCRISP-DM 6Phase\n\n\n\n업무 이해\n데이터 이해: 데이터셋 선택, 데이터 전처리\n데이터 준비: 데이터 변환\n모델링: 모델 평가\n평가: 모델 적용성 평가\n전개\n\n\n\n3. 빅데이터 분석 방법론\n\n\n\n빅데이터 분석 방법론의 5단계\n\n\n\n분석 기획\n\n비즈니스 이해 및 범위 설정\n프로젝트 정의 및 계획 수립 → SOW\n프로젝트 위험 계획 수립 → 회피, 전이, 완화, 수용\n\n데이터 준비\n\n필요 데이터 정의\n데이터 스토어 설계\n데이터 수집 및 정합성 점검\n\n데이터 분석\n\n분석 데이터 준비\n텍스트 분석\n탐색적 분석\n모델링 → 훈련용, 테스트용 데이터 분리\n모델 평가\n\n시스템 구현\n평가 및 전개",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/05.html#분석-프로젝트-관리-방안",
    "href": "posts/03_archives/completed_project/adp_필기/notes/05.html#분석-프로젝트-관리-방안",
    "title": "3 - 데이터 분석 기획의 이해",
    "section": "분석 프로젝트 관리 방안",
    "text": "분석 프로젝트 관리 방안\n\n1. 분석과제 관리를 위한 5가지 주요 영역\n\nData Size\nData Complexity\nSpeed: 분석 모델의 성능 및 속도를 고려해야한다.\nAnalytic Complexity: 분석 모델의 정확도를 높이면서 해석이 가능하도록 최적 모델을 찾아야 한다.\nAccurancy & Precision: 정확도, 정밀도\n\n\n\n3. 분석 프로젝트 관리방안\n\n범위\n시간\n원가\n품질\n통합\n조달\n자원\n리스크\n의사소통\n이해관계자",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "3 - 데이터 분석 기획의 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/04.html#분산-데이터-저장-기술",
    "href": "posts/03_archives/completed_project/adp_필기/notes/04.html#분산-데이터-저장-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "분산 데이터 저장 기술",
    "text": "분산 데이터 저장 기술\n\n1. 분산 파일 시스템\n\nGFS(Google File System): 구글의 분산 파일 시스템\n\nchunk: 64MB\n트리 구조가 아닌, 해시 테이블 구조로 관리\nPOSIX 인터페이스 지원하지 않음\n단일 마스터 노드가 메모리상에서 메타데이터 관리\n마스터 노드에 대한 로그를 기록하고, 마스터의 상태를 섀도우 마스터 노드에 복제\n하나의 파일에 대한 primary node를 정하고, 다른 노드에 복제본 분산 저장\n낮은 응답 지연시간보다 높은 처리율 중시\nMaster Node, Chunk Node, Client 구성\n\nHDFS(Hadoop Distributed File System): 아파치 하둡의 분산 파일 시스템\n\nGFS의 clone project\nPOSIX 인터페이스 지원하지 않음\nblock: 128MB\nNameNode가 메타데이터 관리\n낮은 응답 지연시간보다 높은 처리율 중시\nNameNode, DataNode, 보조 네임 노드, job tracker, task tracker 구성\n\nLustre: 고성능 컴퓨팅을 위한 분산 파일 시스템\n\nPOSIX 인터페이스 지원\nchunk가 아닌 striping 방식 데이터 저장\nClient Filesystem, Metadata Server, 객체 저장 서버로 구성\n\n\n\n\n2. 데이터베이스 클러스터\n\n\n\n\n\n\n\n무공유 디스크\n\n각 노드가 완전히 분리된 데이터를 가짐\nOracle RAC를 제외한 대부분의 클러스터가 채택\n노드 확장에 제한이 없음\n\n공유 디스크\n\nSAN과 같은 네트워크로 모든 노드가 디스크 공유\n노드 확장시 디스크 병목현상 고려 필요\n\n\n\n\n\n\nOrace RAC 데이터베이스 서버: 확장성보다는 고가용성이 중요한 서비스에 적합\nIBM DB2 ICE(integrated cluster environment)\n마이크로소프트 SQL Server: 전역 스키마가 없어서 모든 노드에 질의를 해야함. active-stanby 구성\nMySQL:\n\n클러스터에 참여하는 노드는 최대 255, 그 중 데이터 노드는 최대 48개까지 가능\n운영중에 노드를 추가 삭제 불가\n\n\n\n\n3. NoSQL\n\nGoogle BigTable:\n\n공유 디스크 방식\nRow Key 순으로 정렬 되어 있고, Row 내부적으로는 Column Key 순으로 정렬\nColumn Key, Value, Timestamp로 구성\nChubby를 이용해 마스터 노드 관리\n\nHBase\nAmazon SimpleDB\n\nschema가 없고, Domain(table), Item(record), Attribute(column), Value으로 구성\n\n마이크로소프트 SSDS: Container(table), Entity(record), Property(column)로 구성",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/04.html#분산-컴퓨팅-기술",
    "href": "posts/03_archives/completed_project/adp_필기/notes/04.html#분산-컴퓨팅-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "분산 컴퓨팅 기술",
    "text": "분산 컴퓨팅 기술\n\n1. MapReduce\n\n대용량 데이터를 분산 처리할 수 있는 모델\n보통 64MB를 기준으로 데이터 분할\n하나의 블록당 하나의 Map Task, 사용자가 지정한 갯수만큼의 Reduce Task 생성\nCount 작업에 적합하고, Sort 작업에는 적합하지 않음\n\n\nGoogle MapReduce\nHadoop MapReduce\n\n절차: 1. Split 1. Map 1. Combine 1. Partition 1. Shuffle 1. Sort 1. Reduce\n\n\n2. 병렬 쿼리 시스템\n\nGoogle Sawzall: MapReduce에 대한 이해가 없어도 쉽게 사용 가능\nApache Pig\nApache Hive\n\n\n\n3. SQL on Hadoop",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/04.html#클라우드-인프라-기술",
    "href": "posts/03_archives/completed_project/adp_필기/notes/04.html#클라우드-인프라-기술",
    "title": "2 - 데이터 처리 기술",
    "section": "클라우드 인프라 기술",
    "text": "클라우드 인프라 기술\n\n2. CPU 가상화\n\n하이퍼바이저: 하드웨어 리소스를 가상화하여 여러 개의 가상 머신을 생성하는 소프트웨어\n\nbare-metal hypervisor: 하드웨어와 host 운영체제 사이에 hypervisor가 존재  \nhosted hypervisor: host 운영체제와 guest 운영체제 사이에 hypervisor가 존재\n\nContainer\n\n\n\n3. 메모리 가상화\n\nVMKernnel: hypervisor 내에 Show Page Table을 두고, 각 VM의 Guest OS의 Page Table을 관리\nMemory Ballooning: Guest OS의 메모리를 빼앗아서 다른 VM에 할당\nTransparent Page Sharing: 같은 내용의 메모리 페이지는 VM들이 공유\nMemory Overcommitment: VM에 할당된 메모리보다 더 많은 메모리를 할당할 수 있음\n\n\n\n4. I/O 가상화\n\n가상 이더넷: 가상 머신 간의 네트워크 통신을 위한 가상 네트워크. LAN 세그먼트를 가상화\n공유 이더넷 어댑터: 하나의 물리적 네트워크 어댑터를 여러 VM이 공유. 병목현상 발생 가능\n가상 디스크 어댑터",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "2 - 데이터 처리 기술"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/01.html#빅데이터의-이해",
    "href": "posts/03_archives/completed_project/adp_필기/notes/01.html#빅데이터의-이해",
    "title": "1 - 데이터의 가치와 미래",
    "section": "빅데이터의 이해",
    "text": "빅데이터의 이해\n\n1. 빅데이터의 정의\n\n\n좁은 범위\n\n데이터 자체의 특성에 초점을 맞춘 정의\n3V(다양성, 속도, 규모)를 강조\n\n중간 범위\n\n데이터 자체뿐 아니라 처리, 분석 방법도 포함하는 정의\n\n넓은 관점\n\n인재, 조직 변화까지 포함한 정의\n\n\n\n∴ 기존 방식으로는 얻을 수 없는 통찰 및 가치 창출\n\n\n2. 출현 배경과 변화\n\n산업계: 고객 데이터가 축적되며 새로운 가치 활용\n학계: 거대 데이터 활용 분야가 늘어나며 통계 도구들이 발전\n기술발전: 관련기술의 발전\n\n\n\n3. 빅데이터의 기능\n\n산업혁명의 석탄, 철: 산업 전반에 혁명적 변화를 가져옴\n21세기의 원유: 생산성을 향상시키고, 기존에 없던 새로운 범주의 산업을 만들어낼 것으로 전망\n렌즈: 데이터가 산업에 영향을 미침\n플랫폼\n\n\n\n4. 빅데이터가 만들어 내는 본질적인 변화\n\n사전처리 → 사후처리\n표본조사 → 전수조사\n질 → 양\n인과관계 → 상관관계",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/01.html#빅데이터의-가치와-영향",
    "href": "posts/03_archives/completed_project/adp_필기/notes/01.html#빅데이터의-가치와-영향",
    "title": "1 - 데이터의 가치와 미래",
    "section": "빅데이터의 가치와 영향",
    "text": "빅데이터의 가치와 영향\n\n1. 빅데이터의 가치\n빅데이터는 아래와 같은 이유로 가치 선정이 어렵다.\n\n데이터 활용방식: 데이터를 언제 어디서 누가 사용할지 미리 예측하기 어려움\n새로운 가치 창출: 기존에 없던 가치를 창출하기 때문에 가치를 예측하기 어려움\n분석 기술 발전: 현재 가치가 없더라도, 추후 기술이 발전하면 가치가 생길 수 있음\n\n\n\n2. 빅데이터의 영향\n빅데이터는 다양한 주체(기업, 정부, 개인)에 영향을 미친다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/01.html#비즈니스-모델",
    "href": "posts/03_archives/completed_project/adp_필기/notes/01.html#비즈니스-모델",
    "title": "1 - 데이터의 가치와 미래",
    "section": "비즈니스 모델",
    "text": "비즈니스 모델\n\n1. 빅데이터 활용 사례\n여러가지 활용 사례가 있다.\n\n\n2. 빅데이터 활용 기본 테크닉\n\n연관규칙학습: 범주형 데이터의 변인들간의 규칙을 발견. 비지도 학습 (ex. 장바구니 분석)\n유형(군집)분석: 데이터를 분류하거나 군집화. 비지도 학습 (not 분류분석)\n유전자 알고리즘: 최적해를 찾는 알고리즘\n기계학습: 훈련한 데이터로 예측\n회귀분석: 연속형 데이터의 독립변수와 종속변수의 관계를 수학적으로 모델링해서 예측\n감정분석: 비정형 데이터 분석\n소셜네트워크분석(사회관계망분석): 비정형 데이터 분석",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/01.html#위기-요인과-통제-방안",
    "href": "posts/03_archives/completed_project/adp_필기/notes/01.html#위기-요인과-통제-방안",
    "title": "1 - 데이터의 가치와 미래",
    "section": "위기 요인과 통제 방안",
    "text": "위기 요인과 통제 방안\n\n1. 빅데이터 시대의 위기 요인과 통제 방안\n\n사생활 침해: 동의에서 책임으로\n책임 원칙 훼손: 결과 기반 책임 원칙 고수\n데이터 오용: 알고리즘 접근 허용, 알고리즈미스트",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/01.html#미래의-빅데이터",
    "href": "posts/03_archives/completed_project/adp_필기/notes/01.html#미래의-빅데이터",
    "title": "1 - 데이터의 가치와 미래",
    "section": "미래의 빅데이터",
    "text": "미래의 빅데이터\n\n1. 빅데이터 활용의 3요소\n\n데이터: 모든것의 데이터화\n기술: 인공지능\n인력: 데이터 사이언티스트, 알고리즈미스트",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터의 가치와 미래"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/14.html#시각화의-정의",
    "href": "posts/03_archives/completed_project/adp_필기/notes/14.html#시각화의-정의",
    "title": "5 - 시각화 디자인",
    "section": "시각화의 정의",
    "text": "시각화의 정의\n\n1. 데이터 시각화의 중요성\n데이터 시각화의 목적은 데이터 분석과 의사소통\n\n\n2. 시각 이해와 시각화\n\n\n데이터: 디자인의 대상이 될 수 없음\n정보\n\n데이터가 의미를 전달하기위한 형태를 가짐\n자기조직화 되지 않은 일반적인 의미를 가지고 있고, 생산자와 사용자의 관점에 따라 다르게 전달될 수 있다.\n\n지식: 다른 영역의 정보가 자기조직화된 형태\n지혜: 지식이 내면화되어 개인적 맥락에 포함된 형태. 명시적으로 상대에게 전달하기 어려움\n\n\n\n\n정보 인터랙션 디자인(사진좀 보이게 올려둬라 좀..)\n\n\n\n\n3. 시각화 분류와 구분\n\n\n데이터 시각화\n정보 시각화\n정보 디자인 \n\n데이터 시각화, 정보 시각화, 인포그래픽도 정보 디자인의 범위에 속한다고 볼 수 있다.\n대표적인 예시로 나폴레옹 행군 다이어그램, 나이팅게일 폴라 지역 다이어그램이 있다.\n\n인포그래픽(뉴스 그래픽): 중요한 정보를 한 장의 그래픽으로 표현한 것. 원 데이터는 취급 안함.\n\n정보형 메세지: 객관적인 정보를 전달하는데 목적을 둠. 대표적인 예시: 워싱턴 지하철 지도\n설득형 메세지: 대충 포스터 생각하면 됨",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/14.html#시각화-프로세스",
    "href": "posts/03_archives/completed_project/adp_필기/notes/14.html#시각화-프로세스",
    "title": "5 - 시각화 디자인",
    "section": "시각화 프로세스",
    "text": "시각화 프로세스\n\n1. 정보 디자인 프로세스\n\n데이터 수집\n모든 것을 읽기\n내리티브 찾기\n문제의 정의\n계층 구조 만들기\n와이어프레임 그리기\n포맷 선택하기\n시각 접근 방법 결정하기\n정제와 테스트\n세상에 선보이기\n\n\n\n2. 빅데이터 시각화 프로세스\n\n\n\n시각화 프로세스\n\n\n\n\n\n방법론\n\n\n\n\n\n에드워드 터프티 시각 정보 디자인 7원칙",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/14.html#시각화-방법",
    "href": "posts/03_archives/completed_project/adp_필기/notes/14.html#시각화-방법",
    "title": "5 - 시각화 디자인",
    "section": "시각화 방법",
    "text": "시각화 방법\n\n2. 정보 구조화\n\n데이터 수집 및 탐색\n데이터 분류: 확장자 맞게 분류\n데이터 배열: LATCH\n\nLocation\nAlphabet\nTime\nCategory\nHierarchy: 정보의 변화에 따라 데이터의 값이나 중요도 순서로 정렬\n\n데이터 재배열(관계 맺기):\n\n\n\n3. 정보 시각화\n\n\n\n\n좋은 그래프 디자인\n\n\n\n범례 만들지 말고, 직접 그려 넣은거\n테두리, 보조선 없는거\n굵은 글씨 대신 글자를 흐리게\n색깔은 최대한 적게 사용\n\n\n\n4. 정보 시각 표현\n\n자크 베르탱의 그래픽 7요소\n\n위치: 가장 중요한거는 좌측 상단에 배치\n크기\n모양\n색\n명도\n기울기\n질감\n\n타이포그래피\n\n산세리프: 돌기가 없음. 제목에 적합\n세리프: 돌기가 있음. 본문에 적합\n\n아이소타이프",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 디자인"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/00.html#데이터와-정보",
    "href": "posts/03_archives/completed_project/adp_필기/notes/00.html#데이터와-정보",
    "title": "1 - 데이터 이해",
    "section": "데이터와 정보",
    "text": "데이터와 정보\n\n1. 데이터\n\n객관적 사실을 나타내는 존재적 특성과, 추론 예측 전망 추정을 위한 근거가 되는 당위적 특성을 모두 포함하는 개념\n단위: 바이트(byte), 킬로바이트(KB), 메가바이트(MB), 기가바이트(GB), 테라바이트(TB), 페타바이트(PB), 엑사바이트(EB), 제타바이트(ZB), 요타바이트(YB)\n유형:\n\n정성적 데이터: 비정형 데이터, 주관적 내용, 통계분석이 어려움\n정량적 데이터: 정형 데이터, 객관적 내용, 통계분석이 용이함\n\n지식 경영의 핵심 이슈인 암묵지와 형식지를 연결하는 역할을 함\n\n\n\n\n\n\n\n\n정형 데이터: 표 형태로 정리된 데이터\n반정형 데이터: HTML, XML, JSON 등의 형태(스키마, 메타데이터)가 있고, 연산이 불가능한 데이터\n비정형 데이터: 형태가 없고, 연산이 불가능한 데이터\n\n\n\n\n\n\n\n\n\n\n\n암묵지:\n\n학습과 경험을 통해 개인에게 체화되어 잇지만 겉으로 드러나지 않는 지식\n개인에게 축적된 내면화된 지식 → 조직의 지식으로 공통화\n\n형식지:\n\n문서나 메뉴얼처럼 형상화된 지식\n언어, 기호, 숫자로 표출화된 지식 → 개인의 지식으로 연결화\n\n\n∴ 내면화 → 공통화 → 표출화 → 연결화 → 내면화\n\n\n\n\n\n2. 데이터와 정보의 관계\n\n데이터(data): 그 자체로는 의미가 중요하지 않은 객관적인 사실\nex) A마트는 100원, B마트는 200원에 휴지를 판다.\n정보(information): 데이터를 가공하여 의미를 부여한 결과물\nex) A마트가 100원에 판 휴지는 B마트보다 100원 싸다.\n지식(knowledge): 정보를 구조화하여 유의미한 정보를 분류하고 개인적인 경험을 결합시켜 고유의 지식으로 내재화된 것\nex) 가격이 더 저렴한 A마트에 가서 휴지를 사야겠다.\n지혜(wisdom): 지식의 축적과 아이디어가 결합된 창의적인 결과물\nex) A마트의 다른 물건도 B마트보다 저렴할 것이다.\n\n\n\n\nDIKW 피라미드",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/00.html#데이터베이스-정의와-특징",
    "href": "posts/03_archives/completed_project/adp_필기/notes/00.html#데이터베이스-정의와-특징",
    "title": "1 - 데이터 이해",
    "section": "데이터베이스 정의와 특징",
    "text": "데이터베이스 정의와 특징\n\n1. 데이터베이스의 정의\n기존에는 정형 데이터 관리의 의미로 사용되다가, 빅데이터의 출현으로 비정형 데이터까지 포함하는 개념으로 확장됨\n\n\n2. 데이터베이스의 일반적인 특징\n\n통합된 데이터: 동일한 내용의 데이터가 중복되어 있지 않다.\n저장된 데이터\n공용 데이터\n변화되는 데이터: 데이터베이스에는 항상 현재의 정확한 데이터를 유지한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/00.html#데이터베이스의-활용",
    "href": "posts/03_archives/completed_project/adp_필기/notes/00.html#데이터베이스의-활용",
    "title": "1 - 데이터 이해",
    "section": "데이터베이스의 활용",
    "text": "데이터베이스의 활용\n\n1. 1980년대 기업 내부 데이터베이스\n\nOLTP(On-Line Transaction Processing)\n\n데이터베이스의 데이터를 실시간으로 갱신하는 프로세싱.\n구조가 복잡하고, 현재의 단기간 데이터.\n갱신이 동적이고, 엑세스 빈도가 높다.\n질의가 단순하고, 주기적이다.\n\nOLAP(On-Line Analytical Processing)\n\n데이터 조회, 분석 위주.\n구조가 단순하고, 과거의 장기간 요약 데이터.\n갱신이 정적이고, 엑세스 빈도가 보통이다.\n질의가 복잡하다.\n\n\n\n\n2. 2000년대 기업 내부 데이터베이스\n\nCRM(Customer Relationship Management): 고객 관리 시스템\nSCM(Supply Chain Management): 공급망 관리 시스템\n\n\n\n3. 각 분야별 내부 데이터베이스\n\n제조부문\n\nERP(Enterprise Resource Planning): 기업 내부 자료를 하나의 통합 시스템으로 재구축\nBI(Business Intelligence): 기업의 수많은 데이터를 정리, 분석해 의사결정에 활용하는 프로세스\nCRM\nRTE(Real-Time Enterprise): ERP, SCM, CRM 등의 부문별 전산화 시스템을 하나로 통합\n\n금융부문\n\nEAI(Enterprise Application Integration)\nEDW(Enterprise Data Warehouse): BPR, CRM, BSC 등의다양한 분석 시스템을 위한 원천\n\n유통부문\n\nKMS(Knowledge Management System)\nRFID(Radio Frequency Identification): 주파수를 이용해 ID를 식별\n\n\n\n\n4. 사회기반구조로서의 데이터베이스\n\nEDI(Electronic Data Interchange): 전자상거래를 위한 표준화된 데이터 포맷\nVAN(Value Added Network): EDI를 위한 통신망 (카드 결제 시, 가맹점과 카드사 사이에서 승인 요청 및 결과 전달을 중계함.)\nCALS(Commerce At Light Speed): 제품의 설계, 생산, 유통, 판매 등의 모든 과정을 통합한 경영정보시스템",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 데이터 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/09.html#데이터-변경-및-요약",
    "href": "posts/03_archives/completed_project/adp_필기/notes/09.html#데이터-변경-및-요약",
    "title": "4 - 데이터 마트",
    "section": "데이터 변경 및 요약",
    "text": "데이터 변경 및 요약\n\n요약 변수: 전체적 특성을 대표하여 aggregate한 변수. 제활용성이 높다.\n파생 변수: 기존 데이터를 변환, 조합, 계산하여 새롭게 만든 변수. 주관이 개입될 수 있다.\n\n\n\n\n요약 변수 예시\n\n\n\n1. reshape 패키지 활용",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 데이터 마트"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/11.html#데이터-마이닝-개요",
    "href": "posts/03_archives/completed_project/adp_필기/notes/11.html#데이터-마이닝-개요",
    "title": "4 - 정형 데이터 마이닝",
    "section": "데이터 마이닝 개요",
    "text": "데이터 마이닝 개요\n\n데이터 마이닝 분석 방법\n\n지도 학습\n\n의사결정 나무\n인공신경망\n회귀 분석\n사례기반 추론\nk-최근접 이웃\n\n비지도 학습\n\nOLAP\n연관성 규칙\n군집 분석\nSOM\n\n\n\n\n데이터 마이닝 추진 단계\n\n목표 설정\n데이터 준비\n가공\n기법 적용\n검증\n\n\n\n데이터 분할\n\n구축용(추정용, 훈련용): 50%\n검정용: 30%\n시험용: 20%\nfold-out\nk-fold\nleave-one-out\n\n\n\n성과 분석\n\n정분류율\n오분류율\n민감도(재현율): 실제 True인데 True라고 예측한 비율\n특이도: 실제 False인데 False라고 예측한 비율\n정밀도: True라고 예측했는데 True인 비율\nF1-score: \\(\\frac{정밀도 * 재현율}{정밀도 + 재현율}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/11.html#앙상블-기법",
    "href": "posts/03_archives/completed_project/adp_필기/notes/11.html#앙상블-기법",
    "title": "4 - 정형 데이터 마이닝",
    "section": "앙상블 기법",
    "text": "앙상블 기법\n\n여러 개의 분석 모델을 결합하여 하나의 모델을 구축하는 기법\n\n\n배깅\n\n여러 부트스트랩(복원 추출된 샘플)에 대해 동일한 모델을 독립적으로 학습시키고, 결과를 투표하여 최종 결과를 결정\n\n\n\n부스팅\n\n부트스트랩을 순차적으로 학습시키며, 이전 모델의 오차를 보완하는 방식\nGradient Boosting\nXGBoost\nLightGBM",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/11.html#분류-분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/11.html#분류-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "분류 분석",
    "text": "분류 분석\n\n지도학습. 데이터의 범주형 속성 값을 예측\n\n\n의사결정 나무\n\n이상치에 민감하지 않다.\n대용량 데이터에 대해 적합하다.\n과적합 문제 발생 가능성\n\n\n성장\n\n분리\n\n이산형 변수\n\n카이제곱량\n지니지수: \\(1 - \\sum_{i=1}^{n} p_i^2\\). 낮춰주는 변수 선택\n엔트로피: \\(-\\sum_{i=1}^{n} p_i \\log_2 p_i\\). 낮춰주는 변수 선택\n\n범주형 변수\n\n분산\nF 통계량\n\n\n정지 기준: 의사경정 나무의 높이, 리프 노드의 최소 갯수\n\n가지치기\n타당성 평가\n예측\n\n\n\n인공신경망 분석",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/11.html#som",
    "href": "posts/03_archives/completed_project/adp_필기/notes/11.html#som",
    "title": "4 - 정형 데이터 마이닝",
    "section": "SOM",
    "text": "SOM\n\n고차원의 데이터를 이해하기 쉬운 저차원의 데이터로 변환\n구성\n\n입력층\n경쟁층",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/11.html#군집-분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/11.html#군집-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "군집 분석",
    "text": "군집 분석\n\n사전정보가 없는 상태에서 관측값들의 거리 또는 유사성을 이용하여 군집을 형성하는 분석 방법\n군집분석은 알고리즘에 따라 결과가 매번 다르고, 명확한 정답이나 정답을 찾기 위한 p-value가 없다.\noutlier에 민감하다.\n\n\n계층적 군집 분석\n\n군집의 갯수를 모를 때, 우선적으로 갯수를 정하기 위해 사용\n가까운 개체끼리 차례로 묶거나 멀리 떨어진 개체를 분리해 가는 방식\n한 번 분류된 개체는 재분류되지 않음\n계층적 군집분석 단계\n\nDistance Measure 결정\n\n연속형 변수\n\n유클리디안 거리\n맨하탄 거리\n민코우스키 거리: 유클리디안 거리(L2)와 맨하탄 거리(L1)의 일반화된 공식.\n표준화 거리: 표준편차로 표준화된 길이의 유클리디안 거리\n마할라노비스 거리: 공분산으로 표준화된 길이의 유클리디안 거리\n체비셰프 거리: x 좌표 차이와 y 좌표 차이 중 최댓 값\n캔버라 거리: 두 벡터의 각 차이의 비율\n\n범주형 변수\n\n자카드 거리: \\(1 - \\frac{A \\cap B}{A \\cup B}\\)\n코사인 거리: \\(1 - \\frac{A \\cdot B}{||A|| \\cdot ||B||}\\)\n\n\nClustering Algorithm 결정\n\n합병에 의한 방법: 가장 가까운 거리를 가진 두 군집을 합침\n\n단일 연결법: 군집의 개체들 사이의 모든 거리 조합 중 최솟값 사용\n완전 연결법: 군집의 개체들 사이의 모든 거리 조합 중 최댓값 사용\n평균 연결법: 군집의 개체들 사이의 모든 거리 조합의 평균 사용\n와드 연결법: ESS(군집 내 제곱합)의 증가량이 최소가 되는 두 군집을 합침\n\n분할에 의한 방법\n\n다이아나 연결법\n\n\n군집의 갯수 결정: 1, 2번 단계에서 나온 dendrogram을 보고 알아서 결정\n분석의 타당성 검토\n\n\n\n\n비계층적(분할적) 군집 분석\n\n군집의 갯수를 알고 있을 때 사용\n판정기준을 최적화 시키는 방법으로 군집을 나눔\n한 번 분류된 개체도 재분류될 수 있음\nk-means\n\nk개의 군집을 사전에 설정\n군집의 초기 시작 포인트를 설정\n각 군집의 중심을 계산하여, 개체들을 다시 가장 가까운 군집에 재할당\n3 반복\n\n혼합분포군집\n\nk-means와 비슷하지만, 군집의 형태가 원형이 아닐 때도 사용 가능\n\nPAM\n\nk-means와 비슷하지만, 중심을 평균이 아닌 중앙값으로 설정\n연속형이 아닌 여러 종류의 변수가 혼합된 경우에도 사용할 수 있음\n\n\n\n\n타당성 지표\n\nsilhouette\nDunn index",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/11.html#연관-분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/11.html#연관-분석",
    "title": "4 - 정형 데이터 마이닝",
    "section": "연관 분석",
    "text": "연관 분석\n\n지지도: \\(\\frac{A \\cap B}{전체}\\)\n신뢰도: \\(\\frac{지지도}{A}\\)\n향상도: \\(\\frac{신뢰도}{B}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/notes/01.html#sql-기본",
    "href": "posts/03_archives/completed_project/sqld/notes/01.html#sql-기본",
    "title": "SQL 기본 및 활용",
    "section": "SQL 기본",
    "text": "SQL 기본\n\n관계형 데이터베이스 개요\n\n\nSELECT 문\n\n\n함수\n\n\nWHERE 절\n\n\nGROUP By, HAVING 절\n\n\nORDER BY 절\n\n\nJOIN\n\n\nSTANDARD JOIN",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비",
      "Notes",
      "SQL 기본 및 활용"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/notes/01.html#sql-활용",
    "href": "posts/03_archives/completed_project/sqld/notes/01.html#sql-활용",
    "title": "SQL 기본 및 활용",
    "section": "SQL 활용",
    "text": "SQL 활용\n\n서브쿼리\n\nScalar subquery\ninline view\nnested subquery\n\n\n\n뷰\n\n\n집합 연산자\n\n\n그룹 함수\n\n\n윈도우 함수\n\n\nTOP-N 쿼리\n\n\nSelf join\n\n\n계층 쿼리",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비",
      "Notes",
      "SQL 기본 및 활용"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/notes/01.html#관리-구문",
    "href": "posts/03_archives/completed_project/sqld/notes/01.html#관리-구문",
    "title": "SQL 기본 및 활용",
    "section": "관리 구문",
    "text": "관리 구문\n\nDML\n\n\nTCL\n\n\nDDL\n\n\nDCL",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비",
      "Notes",
      "SQL 기본 및 활용"
    ]
  },
  {
    "objectID": "all.html",
    "href": "all.html",
    "title": "전체 게시글",
    "section": "",
    "text": "정렬\n       디폴트\n         \n          날짜 - 날짜(오름차순)\n        \n         \n          날짜 - 날짜(내림차순)\n        \n         \n          제목\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n제목\n\n\n날짜\n\n\n분류\n\n\n\n\n\n\n3학년 2학기 후기\n\n\n2026-02-14\n\n\n후기\n\n\n\n\nbasic\n\n\n2026-01-31\n\n\nrust\n\n\n\n\n필기 후기\n\n\n2026-01-30\n\n\n정보처리기사, 후기\n\n\n\n\n프로그래밍 언어 활용 - 응용 SW 기초 기술 활용\n\n\n2026-01-28\n\n\n정보처리기사\n\n\n\n\n정보시스템 구축 관리 - 시스템 보안 구축\n\n\n2026-01-28\n\n\n정보처리기사\n\n\n\n\n정보시스템 구축 관리 - 소프트웨어 개발 보안 구축\n\n\n2026-01-28\n\n\n정보처리기사\n\n\n\n\n정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리\n\n\n2026-01-28\n\n\n정보처리기사\n\n\n\n\n소프트웨어 개발 - 어플리케이션 테스트 관리\n\n\n2026-01-28\n\n\n정보처리기사\n\n\n\n\n소프트웨어 개발 - 데이터 입 출력 구현\n\n\n2026-01-28\n\n\n정보처리기사\n\n\n\n\n데이터베이스 구축 - 물리 데이터베이스 설계\n\n\n2026-01-28\n\n\n정보처리기사\n\n\n\n\n데이터베이스 구축 - 논리 데이터베이스 설계\n\n\n2026-01-28\n\n\n정보처리기사\n\n\n\n\n정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용\n\n\n2026-01-25\n\n\n정보처리기사\n\n\n\n\nZKPs\n\n\n2026-01-22\n\n\n블록 체인\n\n\n\n\n소프트웨어 설계 - 인터페이스 설계\n\n\n2026-01-15\n\n\n정보처리기사\n\n\n\n\n소프트웨어 설계 - 화면 설계\n\n\n2026-01-14\n\n\n정보처리기사\n\n\n\n\n소프트웨어 설계 - 어플리케이션 설계\n\n\n2026-01-14\n\n\n정보처리기사\n\n\n\n\n소프트웨어 설계 - 요구사항 확인\n\n\n2026-01-10\n\n\n정보처리기사\n\n\n\n\n자기소개서\n\n\n2025-12-31\n\n\n대학원, 자기소개서\n\n\n\n\nPARA 폐기\n\n\n2025-12-24\n\n\n블로그\n\n\n\n\ndata windowing & baseline modeling\n\n\n2025-12-20\n\n\ndeep learning\n\n\n\n\n결과\n\n\n2025-12-19\n\n\nadp 실기, 데이터 분석, 후기\n\n\n\n\noverview\n\n\n2025-12-18\n\n\ndeep learning\n\n\n\n\npytorch 기초\n\n\n2025-12-17\n\n\ndeep learning\n\n\n\n\nAutomation 시연\n\n\n2025-12-13\n\n\n프로세스 경영, 보고서\n\n\n\n\n반도체 공정 시뮬레이션\n\n\n2025-12-13\n\n\n시뮬레이션, 보고서\n\n\n\n\nCould Quantum Computing be the Next Paradigm Shift Soon?\n\n\n2025-12-13\n\n\nEnglish, 보고서\n\n\n\n\n기말\n\n\n2025-12-11\n\n\n기초공학수학\n\n\n\n\ncheat seet\n\n\n2025-12-10\n\n\n \n\n\n\n\n블록체인기술의 SCM\n\n\n2025-11-30\n\n\n공급사슬관리, 보고서\n\n\n\n\nProcess mining\n\n\n2025-11-30\n\n\n프로세스 경영\n\n\n\n\n운송 및 창고관리\n\n\n2025-11-19\n\n\n공급사슬관리\n\n\n\n\n신뢰도 공학\n\n\n2025-11-19\n\n\n품질경영\n\n\n\n\n공급사슬 네트워크 설계\n\n\n2025-11-11\n\n\n공급사슬관리\n\n\n\n\n5조 기말과제 제안서\n\n\n2025-11-09\n\n\n보고서, 프로세스 경영\n\n\n\n\n샘플링 검사법\n\n\n2025-11-05\n\n\n품질경영\n\n\n\n\n주제 제안\n\n\n2025-10-31\n\n\n프로세스 경영\n\n\n\n\n공급사슬 의사결정의 조정과 계약\n\n\n2025-10-29\n\n\n공급사슬관리\n\n\n\n\nBPM Suite\n\n\n2025-10-27\n\n\n프로세스 경영\n\n\n\n\ncheat seet\n\n\n2025-10-22\n\n\n \n\n\n\n\ntry 1 후기\n\n\n2025-10-18\n\n\nadp 실기, 데이터 분석, 후기\n\n\n\n\n채찍 효과\n\n\n2025-10-15\n\n\n공급사슬관리\n\n\n\n\n관리도\n\n\n2025-10-15\n\n\n품질경영\n\n\n\n\n재고 관리 2\n\n\n2025-10-12\n\n\n공급사슬관리\n\n\n\n\n회귀분석 템플릿\n\n\n2025-10-05\n\n\n데이터 분석\n\n\n\n\n확률과 통계\n\n\n2025-10-05\n\n\n데이터 분석\n\n\n\n\n전처리 템플릿\n\n\n2025-10-05\n\n\n데이터 분석\n\n\n\n\n시계열 분석\n\n\n2025-10-05\n\n\n데이터 분석\n\n\n\n\n비지도 학습 템플릿\n\n\n2025-10-05\n\n\n데이터 분석\n\n\n\n\n분산 분석 템플릿\n\n\n2025-10-05\n\n\n데이터 분석\n\n\n\n\n기타\n\n\n2025-10-05\n\n\n데이터 분석\n\n\n\n\n프로세스 모델링 표준: WS-BPEL\n\n\n2025-10-02\n\n\n프로세스 경영\n\n\n\n\nDMN\n\n\n2025-10-02\n\n\n프로세스 경영\n\n\n\n\nBPMN\n\n\n2025-10-02\n\n\n프로세스 경영\n\n\n\n\n테일러 급수\n\n\n2025-10-01\n\n\n기초공학수학\n\n\n\n\nEDA 템플릿\n\n\n2025-09-29\n\n\n데이터 분석\n\n\n\n\nBPM 표준\n\n\n2025-09-25\n\n\n프로세스 경영\n\n\n\n\n재고 관리\n\n\n2025-09-23\n\n\n공급사슬관리\n\n\n\n\n프로세스 경영 구축 방법론\n\n\n2025-09-22\n\n\n프로세스 경영\n\n\n\n\n무한 급수\n\n\n2025-09-22\n\n\n기초공학수학\n\n\n\n\n모델링, 평가 템플릿\n\n\n2025-09-21\n\n\n데이터 분석\n\n\n\n\nS&OP: Sales and Operations Planning\n\n\n2025-09-16\n\n\n공급사슬관리\n\n\n\n\n수요 관리\n\n\n2025-09-10\n\n\n공급사슬관리\n\n\n\n\n개인 발표 - 공짜 탄소배출권 발급 사건\n\n\n2025-09-09\n\n\n신재생에너지, 보고서\n\n\n\n\n품질변동과 공정능력\n\n\n2025-09-08\n\n\n품질경영\n\n\n\n\nBPM 개요\n\n\n2025-09-08\n\n\n프로세스 경영\n\n\n\n\n프로세스 경영 개요\n\n\n2025-09-04\n\n\n프로세스 경영\n\n\n\n\n품질관리의 기본개념\n\n\n2025-09-03\n\n\n품질경영\n\n\n\n\nSCM 의사결정\n\n\n2025-09-03\n\n\n공급사슬관리\n\n\n\n\nIntro\n\n\n2025-09-02\n\n\n시뮬레이션\n\n\n\n\n관심 분야 JD\n\n\n2025-08-27\n\n\n금융\n\n\n\n\nToeic 문법\n\n\n2025-08-22\n\n\n영어\n\n\n\n\nBasic\n\n\n2025-08-20\n\n\nmachine learning\n\n\n\n\n데이터 모델의 이해\n\n\n2025-08-19\n\n\nsql\n\n\n\n\nSQL 기본 및 활용\n\n\n2025-08-19\n\n\nsql\n\n\n\n\nword2vec\n\n\n2025-08-17\n\n\ndeep learning\n\n\n\n\n전처리\n\n\n2025-08-16\n\n\n확률 통계\n\n\n\n\n연구실\n\n\n2025-08-14\n\n\n대학원\n\n\n\n\n관심 분야 JD\n\n\n2025-08-14\n\n\n대학원\n\n\n\n\n전치행렬\n\n\n2025-08-13\n\n\n선형 대수\n\n\n\n\n역함수와 역변환\n\n\n2025-08-13\n\n\n선형 대수\n\n\n\n\nlinear transformations\n\n\n2025-08-12\n\n\n선형 대수\n\n\n\n\n자연어와 단어의 분산 표현\n\n\n2025-08-10\n\n\ndeep learning\n\n\n\n\n학습 관련 기술들\n\n\n2025-08-06\n\n\ndeep learning\n\n\n\n\nEDA\n\n\n2025-08-05\n\n\n확률 통계\n\n\n\n\n분류 - 신용 카드 사기 검출\n\n\n2025-08-02\n\n\n머신 러닝\n\n\n\n\n합성곱 신경망\n\n\n2025-08-01\n\n\ndeep learning\n\n\n\n\n토익 스피킹 후기\n\n\n2025-08-01\n\n\n후기\n\n\n\n\n오차역전법\n\n\n2025-08-01\n\n\ndeep learning\n\n\n\n\n3학년 1학기 후기\n\n\n2025-08-01\n\n\n후기\n\n\n\n\n텍스트 분석 - 감성 분석\n\n\n2025-07-30\n\n\n머신 러닝\n\n\n\n\n텍스트 분석 - 20 뉴스그룹 분류\n\n\n2025-07-30\n\n\n머신 러닝\n\n\n\n\n텍스트 분석\n\n\n2025-07-30\n\n\n머신 러닝\n\n\n\n\n신경망 학습\n\n\n2025-07-30\n\n\ndeep learning\n\n\n\n\n차원 축소\n\n\n2025-07-29\n\n\n머신 러닝\n\n\n\n\n회귀\n\n\n2025-07-28\n\n\n머신 러닝\n\n\n\n\n분류 - 앙상블\n\n\n2025-07-27\n\n\n머신 러닝\n\n\n\n\n분류 - 산탄데르 고객 만족 예측\n\n\n2025-07-27\n\n\n머신 러닝\n\n\n\n\n분류 - 결정 트리\n\n\n2025-07-27\n\n\n머신 러닝\n\n\n\n\n대학원 준비\n\n\n2025-07-26\n\n\n대학원\n\n\n\n\n일정 정리\n\n\n2025-07-16\n\n\n영어\n\n\n\n\n다차원 척도법 (Multidimensional Scaling)\n\n\n2025-07-13\n\n\n확률 통계\n\n\n\n\nTips\n\n\n2025-07-13\n\n\n영어\n\n\n\n\n외생 변수 추가하기\n\n\n2025-07-12\n\n\n확률 통계, 시계열 분석\n\n\n\n\n계절성 고려\n\n\n2025-07-12\n\n\n확률 통계, 시계열 분석\n\n\n\n\n자기귀모형\n\n\n2025-07-11\n\n\n확률 통계, 시계열 분석\n\n\n\n\n이동평균과정 모델링\n\n\n2025-07-11\n\n\n확률 통계, 시계열 분석\n\n\n\n\n복잡한 시계열 모델\n\n\n2025-07-11\n\n\n확률 통계, 시계열 분석\n\n\n\n\n확률보행\n\n\n2025-07-09\n\n\n확률 통계, 시계열 분석\n\n\n\n\n단순 미래 예측\n\n\n2025-07-09\n\n\n확률 통계, 시계열 분석\n\n\n\n\nOverview\n\n\n2025-07-09\n\n\n확률 통계, 시계열 분석\n\n\n\n\n두 변수의 연관성과 독립성\n\n\n2025-07-08\n\n\n확률 통계\n\n\n\n\nMCMC\n\n\n2025-07-08\n\n\n확률 통계\n\n\n\n\nHadoop Ecosystem\n\n\n2025-07-07\n\n\nHadoop, Big Data\n\n\n\n\n의사결정분석\n\n\n2025-07-06\n\n\n확률 통계\n\n\n\n\n검정\n\n\n2025-07-06\n\n\n확률 통계\n\n\n\n\n포아송 과정\n\n\n2025-07-04\n\n\n확률 통계\n\n\n\n\n순열검정과 전통적인 비모수통계\n\n\n2025-07-02\n\n\n확률 통계\n\n\n\n\nintro\n\n\n2025-07-02\n\n\n확률 통계\n\n\n\n\n회귀\n\n\n2025-07-01\n\n\n확률 통계\n\n\n\n\n켤레사전분포\n\n\n2025-07-01\n\n\n확률 통계\n\n\n\n\n로지스틱 회귀\n\n\n2025-07-01\n\n\n확률 통계\n\n\n\n\n비교\n\n\n2025-06-30\n\n\n확률 통계\n\n\n\n\n최솟값, 최댓값 그리고 혼합 분포\n\n\n2025-06-24\n\n\n확률 통계\n\n\n\n\n분류\n\n\n2025-06-24\n\n\n확률 통계\n\n\n\n\n설문 script 정리\n\n\n2025-06-21\n\n\n영어\n\n\n\n\n돌발 script 정리\n\n\n2025-06-21\n\n\n영어\n\n\n\n\n오픽 구조 파악\n\n\n2025-06-20\n\n\n영어\n\n\n\n\n공산과 가산\n\n\n2025-06-20\n\n\n확률 통계\n\n\n\n\n수량 추정\n\n\n2025-06-19\n\n\n확률 통계\n\n\n\n\n비율 추정\n\n\n2025-06-19\n\n\n확률 통계\n\n\n\n\n분포\n\n\n2025-06-19\n\n\n확률 통계\n\n\n\n\n베이즈 정리\n\n\n2025-06-19\n\n\n확률 통계\n\n\n\n\n개요\n\n\n2025-06-18\n\n\nHelm, infra\n\n\n\n\n확률\n\n\n2025-06-17\n\n\n확률 통계\n\n\n\n\nEDA\n\n\n2025-06-17\n\n\n데이터 분석\n\n\n\n\n청소년기의 심리·정서적 요인을 통한 성인 진입기 진로 안정형·탐색형 성향 분류 예측\n\n\n2025-06-16\n\n\n보고서, data mining\n\n\n\n\nclustering\n\n\n2025-06-14\n\n\ndata mining\n\n\n\n\nXGBoost\n\n\n2025-06-14\n\n\ndata mining, 학부 정리\n\n\n\n\n데이터 전처리\n\n\n2025-06-13\n\n\ndata mining\n\n\n\n\nrandom forest\n\n\n2025-06-13\n\n\ndata mining\n\n\n\n\n확률과 통계 R 실습 과제\n\n\n2025-06-05\n\n\n확률과 통계, 보고서\n\n\n\n\n수송문제와 할당 문제들\n\n\n2025-06-05\n\n\nOR, 학부 정리\n\n\n\n\n네트워크 최적화 모형\n\n\n2025-06-05\n\n\nOR, 학부 정리\n\n\n\n\nAnalysis of categorical data\n\n\n2025-06-05\n\n\n확률과 통계\n\n\n\n\n시험 범위\n\n\n2025-05-31\n\n\nData Structure, 학부 정리\n\n\n\n\n시험 범위\n\n\n2025-05-31\n\n\nOR, 학부 정리\n\n\n\n\nensemble\n\n\n2025-05-31\n\n\ndata mining, 학부 정리\n\n\n\n\n일정 계획\n\n\n2025-05-28\n\n\n생산시스템관리\n\n\n\n\n기준생산계획 및 자재소요계획\n\n\n2025-05-26\n\n\n생산시스템관리\n\n\n\n\npreprocessing\n\n\n2025-05-22\n\n\ndata mining\n\n\n\n\nanalysis\n\n\n2025-05-22\n\n\ndata mining\n\n\n\n\n총괄생산계획\n\n\n2025-05-19\n\n\n생산시스템관리\n\n\n\n\nOR 과제 - 6\n\n\n2025-05-18\n\n\n보고서, OR\n\n\n\n\nRegression Analysis\n\n\n2025-05-13\n\n\n확률과 통계\n\n\n\n\n예측\n\n\n2025-05-12\n\n\n생산시스템관리\n\n\n\n\n데이터마이닝 1차 팀과제 script\n\n\n2025-05-10\n\n\n보고서, data mining\n\n\n\n\n분류\n\n\n2025-05-05\n\n\ndata mining\n\n\n\n\nSupport vector machine\n\n\n2025-05-05\n\n\ndata mining\n\n\n\n\n수송문제와 할당 문제들\n\n\n2025-05-02\n\n\nOR, 학부 정리\n\n\n\n\n선형계획을 위한 다른 알고리즘들\n\n\n2025-05-02\n\n\nOR, 학부 정리\n\n\n\n\n프로젝트 관리\n\n\n2025-04-30\n\n\n생산시스템관리\n\n\n\n\n프로세스 성과에 미치는 변동성의 영향: 산술 손실\n\n\n2025-04-28\n\n\n생산시스템관리\n\n\n\n\nassociation rule mining\n\n\n2025-04-28\n\n\ndata mining\n\n\n\n\n컴퓨팅적 사고 1차 발표 구현 raw script\n\n\n2025-04-26\n\n\n보고서\n\n\n\n\nOR 과제 - 2\n\n\n2025-04-19\n\n\n보고서, OR\n\n\n\n\n쌍대이론과 민감도 분석 (part 6)\n\n\n2025-04-19\n\n\nOR, 학부 정리\n\n\n\n\nSimplex Method (part 5)\n\n\n2025-04-18\n\n\nOR, 학부 정리\n\n\n\n\nANOVA\n\n\n2025-04-15\n\n\n확률과 통계\n\n\n\n\nclassification with trees\n\n\n2025-04-14\n\n\ndata mining\n\n\n\n\n컴퓨팅적사고 발표 ppt\n\n\n2025-04-13\n\n\n보고서\n\n\n\n\nDataminig 1차 발표 ppt\n\n\n2025-04-13\n\n\ndata mining, 보고서\n\n\n\n\nOR 과제 - 4\n\n\n2025-04-10\n\n\n보고서, OR\n\n\n\n\n프로세스 성과에 미치는 변동성의 영향: 대기시간 문제\n\n\n2025-04-09\n\n\n생산시스템관리\n\n\n\n\nOR 과제 - 3\n\n\n2025-04-06\n\n\n보고서, OR\n\n\n\n\nCoding pipeline\n\n\n2025-04-05\n\n\nAir flow\n\n\n\n\n시험 범위\n\n\n2025-04-03\n\n\nData Structure, 학부 정리\n\n\n\n\nSimplex 표 계산\n\n\n2025-04-03\n\n\nOR, 학부 정리\n\n\n\n\n배치 생산 및 경제적 주문량 모형\n\n\n2025-04-02\n\n\n생산시스템관리\n\n\n\n\n인건비 추정과 감축\n\n\n2025-03-31\n\n\n생산시스템관리\n\n\n\n\n통계적 가설검정\n\n\n2025-03-27\n\n\n확률과 통계\n\n\n\n\n제품 설계 기법 및 기업 프로세스 유형\n\n\n2025-03-26\n\n\n생산시스템관리\n\n\n\n\nGetting Started\n\n\n2025-03-26\n\n\nAir flow\n\n\n\n\n공급 프로세스의 이해: 프로세스 처리능력 평가\n\n\n2025-03-19\n\n\n생산시스템관리\n\n\n\n\n신경망\n\n\n2025-03-18\n\n\ndeep learning\n\n\n\n\n퍼셉트론\n\n\n2025-03-17\n\n\ndeep learning\n\n\n\n\ntitanic\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nUpper Confidence Bound\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nNull space and Column space\n\n\n2025-03-16\n\n\n선형 대수\n\n\n\n\nEclat\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\nApriori\n\n\n2025-03-16\n\n\nmachine learning\n\n\n\n\n안녕하세요. 데이터 분석 전문가(진)입니다.\n\n\n2025-03-14\n\n\nadp, 후기\n\n\n\n\nOR 과제 - 1\n\n\n2025-03-13\n\n\n보고서, OR\n\n\n\n\n통계적 추정\n\n\n2025-03-13\n\n\n확률과 통계\n\n\n\n\n봉사\n\n\n2025-03-13\n\n\n봉사\n\n\n\n\n조직을 프로세스 관점에서 바라보기\n\n\n2025-03-12\n\n\n생산시스템관리\n\n\n\n\n가감법으로 연립방정식을 풀기 위한 행렬\n\n\n2025-03-11\n\n\n선형 대수\n\n\n\n\nMatching Supply with Demand\n\n\n2025-03-10\n\n\n생산시스템관리\n\n\n\n\nvector dot product, cross product\n\n\n2025-03-09\n\n\n선형 대수\n\n\n\n\nk-means clustering\n\n\n2025-03-09\n\n\nmachine learning\n\n\n\n\nhierarchical clustering\n\n\n2025-03-09\n\n\nmachine learning\n\n\n\n\nLinear Programming Algorithm\n\n\n2025-03-08\n\n\nOperational Research\n\n\n\n\nIntro\n\n\n2025-03-07\n\n\nOR, 학부 정리\n\n\n\n\n확률과 통계 1 정리\n\n\n2025-03-06\n\n\n확률과 통계\n\n\n\n\nRandom Forest\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nNaive Bayes\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nIntro\n\n\n2025-03-05\n\n\n생산시스템관리\n\n\n\n\nDecision Tree Classification\n\n\n2025-03-05\n\n\nmachine learning\n\n\n\n\nSubspaces and the basis\n\n\n2025-03-03\n\n\n선형 대수\n\n\n\n\nlinear independence\n\n\n2025-03-02\n\n\n선형 대수\n\n\n\n\nSupport Vector Machine\n\n\n2025-03-02\n\n\nmachine learning\n\n\n\n\nK Nearest Neighbors\n\n\n2025-03-02\n\n\nmachine learning\n\n\n\n\nrandom forest\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nSupport Vector Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nLogistic Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nDecision Tree Regression\n\n\n2025-03-01\n\n\nmachine learning\n\n\n\n\nPolynorminal Linear Regression\n\n\n2025-02-27\n\n\nmachine learning\n\n\n\n\noverview\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\ndata preprocessing\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\nSimple Linear Regression\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\nMultiple Linear Regression\n\n\n2025-02-26\n\n\nmachine learning\n\n\n\n\n시험을 보고 왔습니다.\n\n\n2025-02-22\n\n\nadp, 후기\n\n\n\n\nblock chain basic\n\n\n2025-02-22\n\n\n블록 체인\n\n\n\n\n4 - 정형 데이터 마이닝\n\n\n2025-02-20\n\n\nadp\n\n\n\n\n4 - 비정형 데이터 마이닝\n\n\n2025-02-18\n\n\nadp\n\n\n\n\ninception-of-things part 3\n\n\n2025-02-17\n\n\nvagrant, k8s, argoCD, gitlab, 42 seoul\n\n\n\n\ninception-of-things part 2\n\n\n2025-02-17\n\n\nvagrant, k8s, argoCD, gitlab, 42 seoul\n\n\n\n\ninception-of-things part 1\n\n\n2025-02-17\n\n\nvagrant, k8s, argoCD, gitlab, 42 seoul\n\n\n\n\n4 - 통계분석\n\n\n2025-02-16\n\n\nadp\n\n\n\n\n4 - 데이터 마트\n\n\n2025-02-15\n\n\nadp\n\n\n\n\nTerraform Cloud\n\n\n2025-02-11\n\n\nterraform, terraform cloud, devops, IaC\n\n\n\n\n5 - 시각화 인사이트 프로세스\n\n\n2025-02-11\n\n\nadp\n\n\n\n\n5 - 시각화 디자인\n\n\n2025-02-11\n\n\nadp\n\n\n\n\n3 - 분석 마스터 플랜\n\n\n2025-02-10\n\n\nadp\n\n\n\n\n3 - 데이터 분석 기획의 이해\n\n\n2025-02-10\n\n\nadp\n\n\n\n\n2 - 데이터 처리 프로세스\n\n\n2025-02-08\n\n\nadp\n\n\n\n\n2 - 데이터 처리 기술\n\n\n2025-02-08\n\n\nadp\n\n\n\n\n성적 장학금\n\n\n2025-02-05\n\n\n장학금\n\n\n\n\n1 - 데이터의 가치와 미래\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n1 - 데이터 이해\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트\n\n\n2025-02-04\n\n\nadp\n\n\n\n\n인간 관계론 - 데일 카네기\n\n\n2025-02-02\n\n\n독서, 인간 관계\n\n\n\n\n선형결합과 생성\n\n\n2025-01-31\n\n\n선형 대수\n\n\n\n\n벡터와 공간\n\n\n2025-01-30\n\n\n선형 대수\n\n\n\n\ncloud-1 코드 설명\n\n\n2025-01-30\n\n\naws, packer, terraform, ansible, 42 seoul\n\n\n\n\ncloud-1 개념 설명\n\n\n2025-01-28\n\n\naws, packer, terraform, ansible, 42 seoul\n\n\n\n\n3-몰라\n\n\n2025-01-22\n\n\n선형 대수\n\n\n\n\n자기 소개서\n\n\n2025-01-17\n\n\n자기 소개서\n\n\n\n\nft_transcendence - github action\n\n\n2025-01-17\n\n\nagile, github action, 42 seoul\n\n\n\n\n2-기초(2)\n\n\n2025-01-11\n\n\n선형 대수\n\n\n\n\n2-기초(1)\n\n\n2025-01-10\n\n\n선형 대수\n\n\n\n\nSecond Brain - 티아고 포르테\n\n\n2025-01-09\n\n\n학습, 독서\n\n\n\n\nwhat is linear algebra\n\n\n2025-01-07\n\n\n선형 대수\n\n\n\n\n돈의 심리학 - 모건 하우절\n\n\n2025-01-02\n\n\n금융, 독서\n\n\n\n\nPARA Blog 제작\n\n\n2024-12-26\n\n\n블로그\n\n\n\n\n맥도날드 키오스크 UI 개선 보고서\n\n\n2024-11-27\n\n\n보고서, 인간 공학\n\n\n\n\n숭실대학교 학생식당 식자제 SCM 설계\n\n\n2024-11-26\n\n\n보고서, database\n\n\n\n\nControl\n\n\n2024-11-21\n\n\n인간 공학\n\n\n\n\n표본의 분포\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\n중심 극한 정리\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\n정규 분포\n\n\n2024-11-18\n\n\n확률과 통계\n\n\n\n\nDisplay\n\n\n2024-11-14\n\n\n인간 공학\n\n\n\n\n연속형 확률분포\n\n\n2024-11-05\n\n\n확률과 통계\n\n\n\n\nAttention\n\n\n2024-11-05\n\n\n인간 공학\n\n\n\n\nDatabase Design\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\nDatabase Administration\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\nASP.NET\n\n\n2024-10-31\n\n\ndatabase\n\n\n\n\n4조 기말과제 제안서\n\n\n2024-10-30\n\n\n보고서, database\n\n\n\n\n이산형 확률분포\n\n\n2024-10-28\n\n\n확률과 통계\n\n\n\n\n데이터베이스설계및활용 개인과제 #2\n\n\n2024-10-27\n\n\n보고서, database\n\n\n\n\n확률변수의 기댓값\n\n\n2024-10-16\n\n\n확률과 통계\n\n\n\n\nSignal Detection Theory\n\n\n2024-10-15\n\n\n인간 공학\n\n\n\n\nAuditory Haptic\n\n\n2024-10-15\n\n\n인간 공학\n\n\n\n\nData Modeling and the Entity-Relationship Model\n\n\n2024-10-14\n\n\ndatabase\n\n\n\n\nSQL\n\n\n2024-09-27\n\n\ndatabase\n\n\n\n\nSensor System (Visual)\n\n\n2024-09-24\n\n\n인간 공학\n\n\n\n\nDatabase Normalization\n\n\n2024-09-24\n\n\ndatabase\n\n\n\n\nThe Relational Model\n\n\n2024-09-17\n\n\ndatabase\n\n\n\n\nHuman Information Processing Model\n\n\n2024-09-17\n\n\n인간 공학\n\n\n\n\nResearch Method in Human Factors\n\n\n2024-09-10\n\n\n인간 공학\n\n\n\n\n확률변수와 확률분포\n\n\n2024-09-03\n\n\n확률과 통계\n\n\n\n\nIntroduction to Human Factors\n\n\n2024-09-03\n\n\n인간 공학\n\n\n\n\nAn Overview of Database\n\n\n2024-09-03\n\n\ndatabase\n\n\n\n\n확률과 통계의 정의\n\n\n2024-09-02\n\n\n확률과 통계\n\n\n\n\nmetrics server\n\n\n2024-05-15\n\n\n \n\n\n\n\nmanual scheduling\n\n\n2024-05-15\n\n\n \n\n\n\n\nk8s cluster architecture\n\n\n2024-05-15\n\n\n \n\n\n\n\nfail tolerance\n\n\n2024-05-15\n\n\n \n\n\n\n\ncore DNS\n\n\n2024-05-15\n\n\n \n\n\n\n\nPersistant volume\n\n\n2024-05-15\n\n\n \n\n\n\n\nHA in master node\n\n\n2024-05-15\n\n\n \n\n\n\n\nAuthentication\n\n\n2024-05-15\n\n\n \n\n\n\n\nwhat is ebs\n\n\n2024-04-30\n\n\n \n\n\n\n\nwhat is EC2\n\n\n2024-04-30\n\n\n \n\n\n\n\ndatabase choice in aws\n\n\n2024-04-30\n\n\n \n\n\n\n\naws global infrastructure\n\n\n2024-04-30\n\n\n \n\n\n\n\nVPC\n\n\n2024-04-30\n\n\n \n\n\n\n\nRoute53\n\n\n2024-04-30\n\n\n \n\n\n\n\nPerformance Improvement\n\n\n2024-04-30\n\n\n \n\n\n\n\nOverview\n\n\n2024-04-30\n\n\nvault, devops\n\n\n\n\nKMS(Key Management Service)\n\n\n2024-04-30\n\n\n \n\n\n\n\nELB\n\n\n2024-04-30\n\n\n \n\n\n\n\nDisaster Recovery(DR)\n\n\n2024-04-30\n\n\n \n\n\n\n\nDefine IAM\n\n\n2024-04-30\n\n\n \n\n\n\n\nCloudFront\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon Rekognition\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon RDS\n\n\n2024-04-30\n\n\n \n\n\n\n\nAmazon CloudWatch\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Snow Family\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS SQS\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS S3\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Organization\n\n\n2024-04-30\n\n\n \n\n\n\n\nAWS Lambda\n\n\n2024-04-30\n\n\n \n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/notes/00.html#데이터-모델의-이해",
    "href": "posts/03_archives/completed_project/sqld/notes/00.html#데이터-모델의-이해",
    "title": "데이터 모델의 이해",
    "section": "데이터 모델의 이해",
    "text": "데이터 모델의 이해\n\nOverview\n\n모델링: 현실세계를 관리하고자 하는 데이터를 모델로 단순화해 표현한 것\n특징\n\n추상화\n단순화\n명확화\n\n모델링의 세 가지 관점\n\n데이터 관점: 어떤 데이터들이 업무와 얽혀있는지, 그리고 그 데이터 간에는 어떤 관계가 있는지에 대해서 모델링\n프로세스 관점: 이 업무가 실제로 처리하고 있는 일은 무엇인지, 앞으로 처리해야 하는 일은 무엇인지 모델링\n데이터와 프로세스의 상관 관점: 프로세스의 흐름에 따라 데이터가 어떤 영향을 받는지 모델링\n\n모델링의 세 가지 단계\n\n개념적 모델링: 전사적인 모델링\n논리적 모델링: key, 속성, 관계 등을 모두 표현하는 것\n물리적 모델링: 실제 물리적으로 데이터베이스를 설계\n\n데이터의 독립성\n\n외부 스키마: 각 사용자가 보는 데이터베이스의 스키마를 정의\n개념 스키마: 데이터베이스 싀마를 통합하여 전체 데이터베이스를 나타냄\n내부 스키마: 물리적인 저장 구조(칼럼, 인덱스 등)을 나타냄.\n논리적 독립성: 개념 스키마가 변경되어도 외부 스키마는 영향을 받지 않음.\n물리적 독립성: 내부 스키마가 변경되어도 외부 / 개념 스키마는 영향을 받지 않음.\n\nERD\n\nPeter Chen\nIDEF1X\nIE/Crow’s Foot\nMin-Max/ISO\nUML\nCaseMethod / Barker\n\n\n\n\nEntity\n\n특징\n\n업무에서 쓰이는 정보여야 함\n유니크함을 보장할 수 있는 식별자가 있어야 함\n2개 이상의 인스턴스를 가지고 있어야 함\n반드시 속성을 가지고 있어야 함\n다른 엔티티와 1개 이상의 관계를 가지고 있어야 함\n\n분류\n\n유형 vs 무형\n\n유형 엔티티: 물리적으로 존재\n개념 엔티티: 개념적으로 존재\n사건 엔티티: 행위를 함으로써 발생\n\n발생 시점\n\n기본 엔티티: 독립적으로 생성됨\n중심 엔티티: 기본 엔티티로부터 파생\n행위 엔티티: 2개 이상의 엔티티로부터 파생\n\n\n\n\n\nAttribute\n\n특성에 따른 분류\n\n기본속성\n설계속성: 필요하진 않지만 모델링을 위해 생긴 것\n파생속성: 계산하기 편하게 사용하려고 만든 것\n\n구성방식에 따른 분류\n\nPK\nFK\n일반 속성\n\n\n\n\nRelationship\n\n존재관계\n행위관계\n표기법\n\n관계명\n관계차수\n관계선택사양\n\n\n\n\nIdentifiers\n\n유일성\n식별성\n불변성\n존재성: NULL이 될 수 없음\n분류\n\n주식별자\n보조식별자\n내부식별자\n외부식별자\n단일식별자\n복합식별자\n원조식별자\n대리식별자",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비",
      "Notes",
      "데이터 모델의 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/notes/00.html#데이터-모델과-sql",
    "href": "posts/03_archives/completed_project/sqld/notes/00.html#데이터-모델과-sql",
    "title": "데이터 모델의 이해",
    "section": "데이터 모델과 SQL",
    "text": "데이터 모델과 SQL\n\n정규화\n\n제 1정규형: 모든 속성은 반드시 하나의 값만 가져야 한다.\n제 2정규형: 엔티티의 모든 일반 속성은 반드시 모든 주식별자에 종속되어야 한다.\n제 3정규형: 주 식별자가 아닌 모든 속성간에는 중속될 수 없다.\n\n\n\n반정규화\n\n테이블 반정규화\n\n테이블 병합\n테이블 분할\n테이블 추가\n\n칼럼 반정규화\n\n중복 칼럼 추가\n파생 칼럼 추가: 부하가 예상되는 칼럼을 미리 계산\n이력 테이블 칼럼 추가: 최신 데이터 여부 같은 조회 기준이 될 칼럼을 미리 만드는 것\n\n관계 반정규화\n\n\n\n트랜잭션\n\n\nNULL\n\n가로 연산은 NULL, 세로 연산은 NULL 무시",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비",
      "Notes",
      "데이터 모델의 이해"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/index.html",
    "href": "posts/03_archives/completed_project/sqld/index.html",
    "title": "SQLD 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2025-08-19\n        종료일: 2025-08-23\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증sql",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/index.html#details",
    "href": "posts/03_archives/completed_project/sqld/index.html#details",
    "title": "SQLD 준비",
    "section": "Details",
    "text": "Details\n빠르게 끝내 봅시다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/index.html#tasks",
    "href": "posts/03_archives/completed_project/sqld/index.html#tasks",
    "title": "SQLD 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/sqld/index.html#참고-자료",
    "title": "SQLD 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/sqld/index.html#related-posts",
    "href": "posts/03_archives/completed_project/sqld/index.html#related-posts",
    "title": "SQLD 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "SQLD 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/16.html",
    "href": "posts/03_archives/completed_project/adp_필기/notes/16.html",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "",
    "text": "시험 결과",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/16.html#intro",
    "href": "posts/03_archives/completed_project/adp_필기/notes/16.html#intro",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "Intro",
    "text": "Intro\nadp 필기시험 34회차 결과가 나왔습니다. 당연히 합격했고요. 제가 말했죠? 서술형을 몰라도 객관식을 70개 이상 맞추면 합격이라고요. 저는 69개를 맞췄지만 말입니다. 하하.\n..사실 뭐 딱히 자랑스러운 결과는 아니긴 합니다. 서술형 1.5점 받은 주제에 데이터 분석 전문가라고 말 할 수 있을까요? 하지만 제가 합격한건 오직 필기. 전 아직 데이터 분석 전문가가 아닙니다. (제목을 봐주세요. 데이터 분석 전문가(진)입니다.)\n처음 adp 시험을 준비하려고 생각한 것은 데이터 분석 분야에 입문을 하기 위해서였기 때문에, 그런 측면에서 저의 목적은 달성했다고 생각합니다. 실기 시험은 더 구체적인 통계론이나 머신러닝 로직 등을 꽤 오랜시간 자세히 공부해서 치루려고 합니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/16.html#리뷰",
    "href": "posts/03_archives/completed_project/adp_필기/notes/16.html#리뷰",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "리뷰",
    "text": "리뷰\n생각보다 시험에서 다루는 내용의 범위가 넓었습니다. 암기식 공부가 주가되는 자격증 시험의 특성상, 이는 굉장히 힘든 일이 아닐 수 없었습니다. 그래도 데이터 분석 분야의 내용들을 포괄적으로 다루다 보니, 확실히 입문을 하기에는 괜찮은 resource라고 생각합니다.\n그래도 몇 가지 아쉬운 점을 꼽자면, 2 단원 데이터 처리 기술들이 조금 old한 내용들을 다루는게 아닌가 하는 생각이 듭니다. 또, 5 단원 마지막 부분의 데이터 시각화 tool을 구체적으로 다루는 부분은 이걸 꼭 다 외워야하나 싶은 느낌이 들었고요. (근데 이번 시험에서 이 부분은 안 나왔습니다.) 사실 이 정도만 아는 것으로도 앞으로 학습을 이어나갈 수 있는 좋은 발판이 되어준다고 생각합니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/16.html#outro",
    "href": "posts/03_archives/completed_project/adp_필기/notes/16.html#outro",
    "title": "안녕하세요. 데이터 분석 전문가(진)입니다.",
    "section": "Outro",
    "text": "Outro\n이제 실기 시험을 준비해야죠. 올해 볼까.. 내년에 볼까.. 고민입니다. 처음 목표는 졸업 전 까지 adp 자격증을 따는 거였고, 졸업까지 아직 2년이 남았으니까.. 여유롭게 준비해보려 합니다.\n그럼 저는 adp 실기 준비로 다시 돌아오겠습니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "안녕하세요. 데이터 분석 전문가(진)입니다."
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/15.html",
    "href": "posts/03_archives/completed_project/adp_필기/notes/15.html",
    "title": "시험을 보고 왔습니다.",
    "section": "",
    "text": "높게 솟은 수원 공고의 모습",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/15.html#intro",
    "href": "posts/03_archives/completed_project/adp_필기/notes/15.html#intro",
    "title": "시험을 보고 왔습니다.",
    "section": "Intro",
    "text": "Intro\n수원 공고에서 진행된 adp 시험을 보고 왔습니다.\n고등학교 치고는 왠만한 대규모 성당급으로 상당히 넓다는 느낌이 들었습니다. 그냥 제가 나온 인문계 고등학교가 좁아서 그렇게 느껴진 것일 수도 있고요.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/15.html#시험",
    "href": "posts/03_archives/completed_project/adp_필기/notes/15.html#시험",
    "title": "시험을 보고 왔습니다.",
    "section": "시험",
    "text": "시험\n저번 시험에서는 서술형에서 20점 만점에 4점을 받아서 굉장히 아쉬운 성적이 나와버렸죠. 사실 그렇다고 이번 시험에서 서술형을 엄청 잘 준비하거나 하진 않았습니다. 제가 게을러서 그렇다는걸 딱히 부정하는건 아니지만, 가장 큰 이유는 뭐가 나올지 예상을 할 수 없기 때문입니다.\n서술형에 나올 수 있는 주제 자체는 참고서에 나와있는 내용 안에서 등장하지만, 시험에서는 더 구체적인 수식과 구현 과정을 요구하는 경우가 많습니다. 아마도 adp 시험에서 서술형을 더 잘 대비하려면, 더 많은 외부 자료를 참고하거나, 배경지식을 더 쌓은 상태에서 도전해봐야 할 것 같습니다.\n하지만, 만약 서술형이 0점이 나온다고 해도 객관식을 80 문제중 70개 이상만 맞춘다면 통과할 수 있다는 사실. 뭐.. 그렇게까지 불가능한 것도 아니긴 합니다. 객관식은 참고서 내용으로 잘 커버할 수 있으니까요.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/15.html#outro",
    "href": "posts/03_archives/completed_project/adp_필기/notes/15.html#outro",
    "title": "시험을 보고 왔습니다.",
    "section": "Outro",
    "text": "Outro\n실제로 꽤 느낌이 좋긴 합니다. 이번에도 서술형은 모르는 내용이 나오긴 했지만 말입니다.\n결과가 3월 중순에 나올 예정인데, 그 전까지는 smart contract 쪽으로 공부를 해볼 예정입니다.\n더 자세한 감상은 결과가 나온 후에 작성해보겠습니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "시험을 보고 왔습니다."
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/12.html#텍스트-마이닝",
    "href": "posts/03_archives/completed_project/adp_필기/notes/12.html#텍스트-마이닝",
    "title": "4 - 비정형 데이터 마이닝",
    "section": "텍스트 마이닝",
    "text": "텍스트 마이닝\n\n비정형 데이터를 구조화해서 패턴을 도출한 후 결과를 평가 및 해석하는 일련의 과정\n\n\n기능\n\n목표 기능: 문서 분류, 군집, 정보 추출, 문서 요약\n사용 기술: 자연어 처리, 컴퓨터 언어학\n\n\n\n과정\n\n\n텍스트 수집\n텍스트 전처리\n\ntm 패키지: 문서를 Corpus 객체로 변환해서 관리\n\nVCorpus: 문서를 Corpus로 변환해서 메모리에 저장\nPCorpus: 문서를 Corpus로 변환해서 디스크에 저장\nDirSource, DataframeSource, VectorSource: 데이터 소스 지정\ntm_map(x, FUN): x에 FUN을 적용\nDocumentTermMatrix: 문서-단어 빈도표 생성\nTermDocumentMatrix: 단어-문서 빈도표 생성\n\n전처리\n\n정제: 노이즈 제거\n토큰화\n\n단어 토큰화\n어절 토큰화\n형태소 토큰화\n품사 태깅\n\n불용어 처리: 불필요한 토큰 제거\n정제 / 정규화\n\n표기가 다른 같은 단어 통일\n대소문자 통일\n불필요한 단어 제거\n정규표현식으로 특수문자 제거\n\n어간 / 어근 추출\n텍스트 인코딩\n\none hot 인코딩\n말뭉치(BoW): 단어의 빈도수를 벡터로 표현\nTF-IDF: 문서 내 단어의 빈도 수 / 단어가 등장한 문서 수\n워드 임베딩\n\n\n텍스트 분석\n\n토픽 모델링\n감성 분석\n텍스트 분류\n텍스트 군집화\n\n텍스트 시각화\n\n워드 클라우드\n의미 연결망 분석\n\n\n\n\n\n정보 검색의 적절성",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 비정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/12.html#사회연결망-분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/12.html#사회연결망-분석",
    "title": "4 - 비정형 데이터 마이닝",
    "section": "사회연결망 분석",
    "text": "사회연결망 분석\n\n2. 기법\n\n개인을 노드, 관계를 엣지로 해서 그래프 생성\n아래의 기준에 따라 구조 파악",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 비정형 데이터 마이닝"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/13.html#시각화-인사이트-프로세스의-의미",
    "href": "posts/03_archives/completed_project/adp_필기/notes/13.html#시각화-인사이트-프로세스의-의미",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "시각화 인사이트 프로세스의 의미",
    "text": "시각화 인사이트 프로세스의 의미\n\n1. 인사이트란 무엇인가\n상위 개념을 발견하기 위해, 각 단계의 관계를 이해해야 한다.\n이를 위해 시각화 인사이트 방법이 필요하다.\n\n\n\nDIKW 피라미드와 시각화 관계\n\n\n\n\n2. 시각화와 인사이트",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/13.html#탐색",
    "href": "posts/03_archives/completed_project/adp_필기/notes/13.html#탐색",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "탐색",
    "text": "탐색\n\n상위 개념을 발견하기 위해, 각 단계의 관계를 이해하는 과정\n객관적인 패턴을 찾는 용도\n\n\n1. 사용 가능한 데이터 확인\n\n데이터 접근\n\n이벤트 기록으로서 접근: 데이터로부터 통찰을 이끌어 내기 위해서 데이터 생성 원리를 파악해야 한다고 간주\n객체지향 관점에서의 접근: 데이터로부터 통찰을 이끌어 내기 위해서 전체 구조를 파악해야 한다고 간주\n\n데이터 명세화\n\n모든 데이터는 하나 이상의 차원과 측정값을 가지고 있다.\n이는 분석 형태에 따라, 차원이 될 수도 있고, 측정값이 될 수도 있다.\n\n\n\n\n2. 연결 고리의 확인\n데이터 명세서를 이용해 2개 이상의 데이터간 연결 고리를 확인해 봄\n\n공통 요소 찾기\n공통 요소로 변환하기: 데이터 타입이 달라도 공통 요소로 묶을 수 있다 (더 자세한 데이터를 덜 자세한 데이터로 변환. 반대는 불가)\n\n시간 데이터의 변환\n공간 데이터의 변환(지오코딩, 코로플레스 지도, X-Ray Map 사용 가능)\n계층 관계 변환: 상위 수준(덜 자세한)이라는 공통 요소로 변환. replace, lookup, vlookup 함수 사용 가능\n\n탐색 범위 설정: 차원과 측정값의 전체 조합 종류가 탐색 범위가 됨. 데이터를 구성하는 항목이 늘어날 수록 탐색 범위가 늘어남\n\n여러 데이터를 보유한 경우, 개별 데이터 안에서 먼저 탐색\n측정값 하나의 차원만 연결해 탐색\n같은 데이터 안에서 차원과 측정값을 맞바꾸면 다른 통찰을 얻을 수 있음\n어떤 통찰을 얻기 위해 비주얼 인사이트 프로세스를 사용하는 것인지 살펴본 후, 목표와 관련 있을 법한 조합을 만듦\n상식적으로 의미나 연계성이 없는 조합은 배제\n\n\n\n\n3. 관계의 탐색\n상관관계와 인과관계를 탐색\n\n이상값 처리: 시각화 도구를 통해 전체 구조를 파악한 후 처리\n차원과 측정값 유형에 따른 관계 파악 시각화\n\n시각화 도구 선정\n시간 데이터에서의 관계 파악: 구글 모션차트 사용 가능\n공간 데이터에서의 관계 파악: Arc GIS, X-Ray Map, 파워 맵 사용 가능\n비정형 데이터에서의 관계 파악\n\n워들: 주어진 텍스트에서 형태소 단위를 추출(NLP)해 빈도에 따라 시각화\n\n\n잘라보고 달리보기: 둘 이상의 차원과 측정값으로 이루어진 데이터를 여러 관점으로 살펴본다.\n\n잘라보기(slice): ex) 연령별, 성별 평균 체중 데이터 → 20세 이상, 40세 미만 남성들의 체중 패턴\n달리보기(dice): ex) 연령별, 성별 평균 체중 데이터 → 남성의 연령별 체중 패턴, 여성의 연령별 체중 패턴\nMS excel의 pivot, powerview, spreadsheet의 pivot table report 사용 가능\n\n내려다보고 올려보기\n\n내려다보기(Drill Down): 데이터를 하위 계층으로 세분화한다.\n올려보기(Reverse Driil Down): 데이터를 상위 계층으로 통합한다.\nTree map, Hyperbolic Tree\n\n척도의 조정: 스파크라인 차트 사용 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/13.html#분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/13.html#분석",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "분석",
    "text": "분석\n\n탐색을 통해 발견된 패턴을 분석하는 과정\n\n\n1. 분석 대상의 구체화\n\n2차 탐색: 관계들의 분석 우선순위 결정. 궁극적인 목표는 그냥 다시 한 번 더 검토하는 것\n분석 목표에 따른 분석 기법\n\n \n\n\n2. 분석 시각화 도구\n통계적 도구와 시각적 도구는 상호보완 관계\n\n\n3. 지표 설정과 분석\n\n지표: 어떤 현상의 강도를 평가하는 기준이 되는 수치\n\nex) KPI(Key Performance Indicator): 핵심 성과 지표. 목표 달성을 위한 세부적인 활동 결과물의 추진 정도나 수준을 측정하고 평가\n주로 함수식 구조를 가짐 (ex. 매출액 = 판매단가 * 판매량)\n요인 분석(factor analysis)를 통해 지표가 다른 요인과 설명력이 겹치는지 여부 확인할 수 있다.\n어떤 변화요인에 의해 지표의 흐름에 영향을 미쳤는지 파악하기 어렵다는 단점이 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/13.html#활용",
    "href": "posts/03_archives/completed_project/adp_필기/notes/13.html#활용",
    "title": "5 - 시각화 인사이트 프로세스",
    "section": "활용",
    "text": "활용\n\n도출한 인사이트를 활용하는 과정\n\n\n1. 내부에서 적용\n\n기존 문제 해결 방식이나 설명 모델의 수정\n새로운 문제 해결 방식의 도입\n새롭게 발견한 가능성에 대한 구체적인 탐색과 발전\n\n\n\n2. 외부에 대한 설명, 설득과 시각화 도구\n설득이 필요하기 때문에 스토리텔링이 감미된 시각화 자료나, 인터렉티브 인포그래픽 활용\n\n\n3. 인사이트의 발전과 확장\n계속 잘 검토해 나가야함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "5 - 시각화 인사이트 프로세스"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/10.html#통계분석의-이해",
    "href": "posts/03_archives/completed_project/adp_필기/notes/10.html#통계분석의-이해",
    "title": "4 - 통계분석",
    "section": "통계분석의 이해",
    "text": "통계분석의 이해\n\n1. 표본 추출 방법\n\n단순랜덤 추출법\n계통추출법: k개씩 띄어서 랜덤으로 추출\n집락 추출법: 군집을 나눈 후, 군집 안에서 단순랜덤 추출\n층화 추출법: 이질적인 모집단에서, 비슷한 특성을 가진 층을 나눈 후, 각 층에서 단순랜덤 추출\n\n\n\n2. 척도\n\n명목척도\n순서척도\n구간척도: 더하기, 빼기 가능. 곱셈 나눗셈 불가능\n비율척도: 절대적 기준인 0이 존재, 사칙연산 가능\n\n\n\n3. 비모수 검정\n\n모집단에 대한 가정이 없이, 서열관계나 차이를 검정하는 방법\n분포의 형태가 동일하다, 동일하지 않다로 가정\n관측값들의 순위나 차이의 부호에 의존\n\n\n\n\n비모수 검정 예시",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/10.html#기초-통계분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/10.html#기초-통계분석",
    "title": "4 - 통계분석",
    "section": "기초 통계분석",
    "text": "기초 통계분석\n\n\n\n상관 분석 유형",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/10.html#통계분석의-방법론",
    "href": "posts/03_archives/completed_project/adp_필기/notes/10.html#통계분석의-방법론",
    "title": "4 - 통계분석",
    "section": "통계분석의 방법론",
    "text": "통계분석의 방법론\n\nt 검정\n\n일표본\n대응표본\n독립표본\n\nANOVA\n\n일원분산분석\n이원분산분석\n다원분산분석\n\n다변량분석\n실험계획법\n\n요인배치법\n분할법\n교락법\n난괴법\n\n교차분석\n\n적합성 검정: k개의 범주들에 대한 관측값 갯수가 기댓값과 일치하는지 검정\n\n자유도: k-1\n각 집단의 \\(\\frac{(관측도수 - 기대도수)^2}{기대도수}\\)의 합이 카이제곱 분포를 따름\n\n독립성 검정\n\n자유도: (r - 1)(c - 1)\n\n동질성 검정: 독립성 검정이랑 유사",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/10.html#회귀분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/10.html#회귀분석",
    "title": "4 - 통계분석",
    "section": "회귀분석",
    "text": "회귀분석\n\n1. 가정\n\n선형성\n정규성: qq-plot, 대각선에 가까워야함\n등분산성: 수평선에 가까워야함\n독립성: 더빈 왓슨 검정(0~4), 2에 가까울수록 독립성이 있다.\n\n→ 가정을 충족하지 않을 경우, 회귀모델을 수정해야함\n\n이상치 → 관측값 제거\n선형성 → 독립변수 변환\n정규성, 등분산성 미충족 → 종속변수 변환\n\n변환: \\(x\\) → \\(x^λ\\)\n\n\n2. 회귀식\n\n\n\\(R^2 = \\frac{SSR}{SST}\\)\n\\(R^2_{adj} = 1 - \\frac{(1 - R^2)(n - 1)}{n - k - 1}\\)\n\n\n\n3. 다중공선성\n\n독립변수들 간에 강한 상관관계가 존재하는 경우\n\n상관계수: 변수간 상관계수를 직접 계산\n허용오차: 1 - \\(R^2\\). 0.1 이하면 다중공선성이 존재한다고 판단\nVIF: 허용 오차의 역수. 10 이상이면 다중공선성이 존재한다고 판단 → 변수 제거\n\n\n\n\n4. 최적화 회귀방정식\n\nAIC, BIC나 F-value를 크게 만드는 변수 제거\n\n\n전진 선택법: 상수항부터 시작해, 한번에 한개씩 독립변수 추가\n\n전체 변수 사용할 수 있지만 안정성이 낮음\n\n후진 선택법: 모든 독립변수를 포함한 후, 하나씩 제거. AIC가 더 이상 작아지지 않을 때까지\n\n안정성이 높지만 변수가 많을 때 시간이 오래 걸림\n\n단계 선택법: 전진, 후진 선택법을 혼합.\n\n이미 선택된 변수를 제거할 수 있음\n변수가 많으면 시간이 오래 걸림",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/10.html#고급-회귀분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/10.html#고급-회귀분석",
    "title": "4 - 통계분석",
    "section": "고급 회귀분석",
    "text": "고급 회귀분석\n\n1. 패널티 회귀분석\n지나치게 많은 독립변수를 갖는 모델에 페널티를 부과하는 방식\n\n릿지: 모델의 설명력에 기여하지 못하는 독립변수의 계수 크기를 0에 근접하게 축소 (\\(l_2\\) 규제)\n\n회귀 계수가 비슷하고, 독립변수가 많을 때 효과가 좋다.\n\n라쏘: 모델의 설명력에 기여하지 못하는 독립변수의 계수 크기를 0으로 만듦 (\\(l_1\\) 규제)\n\n회귀 계수 차이가 클 때 효과가 좋다.\n\n엘라스틱넷: 릿지와 라쏘를 혼합한 방법 (\\(l_1\\) + \\(l_2\\) 규제)\n\n\n\n2. 일반화 회귀분석\n\n종속변수가 연속형이면서 정규분포를 따르지 않을 때 사용\n\n\nlogistic 회귀모형\npoisson 회귀모형",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/10.html#시계열-분석",
    "href": "posts/03_archives/completed_project/adp_필기/notes/10.html#시계열-분석",
    "title": "4 - 통계분석",
    "section": "시계열 분석",
    "text": "시계열 분석\n\n시계열 데이터 생성\n탐색적 분석을 통해 데이터 이해\n\n시각화 작업으로 변통 패턴 관찰\n성분분해 작업으로 추세, 계절성분, 불규칙성분 분리\n\n추세(장기)\n계절(단기)\n순환(중장기)\n불규칙(설명 불가)\n\n\n미래 관측값에 대한 예측\n\n이동 평균법\n지수 평활법\nARIMA 기법\n\nAR모델: P시점 전의 자료가 현재에 주는 영향을 시계열 모형으로 구축. 과거 관측값을 이용하여 예측모델 생성. 감절\nMA모델: 시간이 지날수록 관측치의 평균값이 지속적으로 증가하거나 감소하는 경향 표현. 과거 오차항을 이용하여 예측모델 생성. 절감\nARIMA모델: 비정상 시계열. 차분이나 변환을 통해 정상시계열로 변환",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "4 - 통계분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/02.html#빅데이터-분석과-전략-인사이트",
    "href": "posts/03_archives/completed_project/adp_필기/notes/02.html#빅데이터-분석과-전략-인사이트",
    "title": "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트",
    "section": "빅데이터 분석과 전략 인사이트",
    "text": "빅데이터 분석과 전략 인사이트\n빅테이터 분석은 분석을 통해 가치를 창출하는 것이 목적이다.\n\n일차원적인 분석: 해당 부서나 업무 영역에만 효과가 있다. 변화하는 환경에서 새로운 기회를 포착하기 어려움.\n전략도출 가치기반 분석: 일차원적인 분석을 통해 얻은 가치를 기반으로 활용 범위를 더 넓고 전략적으로 확장해야한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/02.html#전략-인사이트-도출을-위한-필요-역량",
    "href": "posts/03_archives/completed_project/adp_필기/notes/02.html#전략-인사이트-도출을-위한-필요-역량",
    "title": "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트",
    "section": "전략 인사이트 도출을 위한 필요 역량",
    "text": "전략 인사이트 도출을 위한 필요 역량\n\n\n\n데이터 사이언티스트의 요구 역량\n\n\n외부 환경이 다음과 같이 변화함에 따라 인사이트 도출을 위한 인문학적 역량이 요구됨.\n\n컨버전스 → 디버전스\n생산 → 서비스\n생산 → 시장창조",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "1 - 가치 창조를 위한 데이터 사이언스와 전략 인사이트"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/06.html#마스터-플랜-수립-프레임-워크",
    "href": "posts/03_archives/completed_project/adp_필기/notes/06.html#마스터-플랜-수립-프레임-워크",
    "title": "3 - 분석 마스터 플랜",
    "section": "마스터 플랜 수립 프레임 워크",
    "text": "마스터 플랜 수립 프레임 워크\n\n\n\n마스터 플랜 수립 개요\n\n\n\n2. 수행 과제 도출 및 우선순위 평가\n\n\n\n일반적인 IT 프로젝트 우선순위 평가\n\n\n\n\n\nROI 관점\n\n\n위 기준에 따라 시급성과 난이도를 평가한 후, 아래 그림에 맞게 우선순위를 정한다.\n\n우선순위 기준을 시급성에 둔다면, 3 → 4 → 2 순, 난이도에 둔다면 3 → 1 → 2 순으로 우선순위를 정한다.\n\n\n3. 이행계획 수립\n\n\n\n로드맵 수립\n\n\n\n세부 이행계획 수립",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "3 - 분석 마스터 플랜"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/notes/06.html#분석-거버넌스-체계-수립",
    "href": "posts/03_archives/completed_project/adp_필기/notes/06.html#분석-거버넌스-체계-수립",
    "title": "3 - 분석 마스터 플랜",
    "section": "분석 거버넌스 체계 수립",
    "text": "분석 거버넌스 체계 수립\n\n1. 거버넌스 체계\n\n\n\n2. 데이터 분석 수준진단\n\n\n분석 준비도\n\n\n\n분석 성숙도\n\n\n\n\nCMMI(Capability Maturity Model Integration)\n\n\n분석 준비도와 성숙도를 통해 현재 분석 수준을 파악한다. 이후 아래의 그림에 맞춰 목표 방향을 설정한다.\n\n\n\n4. 데이터 거버넌스 체계 수립\n\n구성 요소:\n\n원칙\n조직\n프로세스\n\n\n\n\n\n체계\n\n\n\n데이터 표준화: 규칙같은거 통일하는거\n데이터 관리 체계: 라이프사이클 같은거 관리하는거\n레포지토리: 너가 아는 그거\n표준화 활동: 잘 지켜지는지 지속적으로 모니터링하는거\n\n\n\n5. 데이터 조직 및 인력방안 수립\n\n\n\n분석 조직 구조\n\n\n\n\n\n분석 조직 인력 구성\n\n\n\n\n6. 분석과제 관리 프로세스 수립\n\n\n\n분석 과제 관리 프로세스",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비",
      "Notes",
      "3 - 분석 마스터 플랜"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/index.html",
    "href": "posts/03_archives/completed_project/adp_필기/index.html",
    "title": "ADP 필기 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2025-02-02\n        종료일: 2025-02-22\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증데이터 분석",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/index.html#details",
    "href": "posts/03_archives/completed_project/adp_필기/index.html#details",
    "title": "ADP 필기 준비",
    "section": "Details",
    "text": "Details\n1회차 시도는 실패했지만, 이번엔 잘 되겠죠",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/adp_필기/index.html#참고-자료",
    "title": "ADP 필기 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/index.html#tasks",
    "href": "posts/03_archives/completed_project/adp_필기/index.html#tasks",
    "title": "ADP 필기 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    원서 접수 (2025.01.20 10 am)\n                \n                2025.02.22 10:00 수원공업고등학교\n            \n\n            \n            \n                \n                    \n                    경기도 자격증 응시료 지원 신청\n                \n                아마도 5월 쯤 뜨지 않을까",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_필기/index.html#related-posts",
    "href": "posts/03_archives/completed_project/adp_필기/index.html#related-posts",
    "title": "ADP 필기 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 필기 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/01.html#자기소개",
    "href": "posts/03_archives/completed_project/opic/notes/01.html#자기소개",
    "title": "설문 script 정리",
    "section": "자기소개",
    "text": "자기소개\nHello Eva. It’s great to see you.\nMy name is hyunghoon, and I will do my best to answer your questions clearly\nSo, let’s have a great conversation. I’m ready!”",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "설문 script 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/01.html#주제",
    "href": "posts/03_archives/completed_project/opic/notes/01.html#주제",
    "title": "설문 script 정리",
    "section": "주제",
    "text": "주제\n일 경험 없음 학생: 아니요 수강 후 5년 이상 지남\n\n개인주택이나 아파트에 홀로 거주\nQ. I would like to know where you live. Describe your house in detail. What does it look like? How many rooms do you have?\n\nMP: 나는 지금 부모님과 함께 아파트에 살고 있다.\n아파트 주변에는 산과 공원이 있어서 산책하기 좋다.\n침실 3개, 화장실 2개가 있다.\n이 중에서 내 방에는 안락한 소파와 조명이 있는데, 혼자 있는걸 좋아해서 내 방에서 시간을 많이 보낸다.\n\n\nI live in a pretty cozy studio apartment, and I love this place because it feels like my own personal space. It’s not that big, but it’s just the right size for me. I have a small kitchen area, a living room that doubles as my bedroom, and a bathroom. living room에는 안락한 소파와 조명이 있는데, 이곳에 앉아 있으면 편안함을 느낍니다. 이것들이 없었다면 제 집은 정말 단조로웠을 거예요. so, even though it’s small, it has everything I need to feel at home.\n\n\nQ. Can you tell me about the household appliances in your house? What is your favorite one among them? Why do you like it most?\n\nMP: 로봇 청소기가 자동으로 청소해줘서 좋아한다.\n\n\nQ. Tell me about the house or apartment you lived in when you were a child. How was it different from the one you live in now? What are the similarities and differences?\n\n옛날: 시골 / 부모님과 살았음 / 이웃들과 가까웠음\n지금: 도시 / 부모님과 살고 있음 / 이웃들과 친하게 안 지냄\n\n\n10. Now, tell me about the problems that happen at your home. What are those problems? Why do they occur? How do people deal with those problems? How do you personally deal with those problems?\n\n쓰레기를 제때 버리지 않아서 벌레가 생겼다.\n벌레들이 짝찟기를 하며 날라다니는 모습이 역겨웠다.\n쓰레기를 버리고 버려도 몇 주 동안 벌레가 계속 나왔다.\n요즘에는 벌레가 생기지 않도록 미리미리 쓰레기를 버리고 있다.\n\n\n\n영화/공연/콘서트/음악\nQ1. What is your favorite genre of movies? Why do you like those types of movies? (당신이 좋아하는 영화 장르는 무엇인가요? 왜 그 장르를 좋아하나요?)\n\nMP: 요즘 가장 좋아하는 장르는 뮤지컬 이다. 뮤지컬 영화는 사람의 감정을 깊게 느낄 수 있어서 좋다.\n틱틱붐, 라라랜드, 위대한 쇼맨 같은 뮤지컬 영화가 좋다.\n\n\nQ2. I’d like you to tell me about one of the most memorable movies you’ve seen. (당신이 본 영화 중 가장 기억에 남는 영화는 무엇인가요?)\n\nMP: 가장 기억에 남는 영화는 ’라라랜드’이다.\n이 영화는 뮤지컬 영화로, 사랑과 꿈을 쫓는 두 주인공의 이야기를 다룬다.\n영화의 음악과 춤, 그리고 감정 표현이 정말 인상적이었다.\n\n\nQ3. Tell me about a time when you went to listen to some live music. (라이브 음악을 들으러 갔을 때에 대해 말해주세요.)\n\nQ4. Could you compare the movies made today to movies you saw while you were growing up? (요즘 영화와 당신이 자라날 때 보았던 영화를 비교해주세요.)\n\nQ5. How have the performances in your country changed or developed over the last several years? (지난 몇 년간 당신의 나라의 공연들은 어떻게 변해왔나요?)\n\n\n공원가기 캠핑하기\nQ1. You indicated that you like to go to parks. Tell me about one of the parks that you often visit. What does it look like? (자주 가는 공원에 대해 말해주세요. 어떻게 생겼나요?)\n\n집앞 공원에 자주 가고, 조명이 잘 되어 있고, 나무가 많다.\n\n\nQ3. Do you have any memorable experience when you went camping? When was it? Who did you go with? What happened? (캠핑을 갔을 때 기억에 남는 경험이 있나요?)\n\n\n\n\nQ4. I would like to know things you usually bring when you go camping. Why do you take those? (캠핑갈 때 주로 가져가는 것들에 대해 말해주세요.)\n\n최소한의 짐만 챙긴다.\n세면도구와 갈아입을 옷 정도만 챙긴다.\n나머지는 빌린다.\n\n\nQ5. Tell me about your most memorable experience that have happened at the park. (공원에서 발생했던 당신의 가장 기억에 남는 경험에 대해 말해주세요.)\n\n공원에서 조깅을 하고 있었는데 갑자기 모르는 외국인이 다가와서 말을 걸었다.\n그 사람은 나에게 길을 물어봤고, 나는 영어를 잘 못해서 대답을 못했다.\n\n\n\n조깅 걷기 운동을 전혀 하지 않음\nQ1. Where do you often go to for jogging? Describe your favorite place for jogging in detail. (주로 어디로 조깅을 하러 가나요? 그곳을 자세히 묘사해주세요.)\n\n집 앞 공원에서 조깅을 한다.\n공원은 나무가 많고, 조깅하기에 좋은 길이 있다.\n밤에는 조명이 켜져서 안전하게 조깅할 수 있다.\n\n\nQ2. How did you get into jogging initially? Tell me about your motivation to try and continue. (처음 조깅을 시작하게 된 계기는 무엇인가요? 동기에 대해 말해주세요.)\n\n군 시절에 체력 단련을 위해 조깅을 시작했다.\n서울 공항에서 복역했고, 그 곳에는 조깅을 하기 좋은 장소가 있었다.\n매일 저녁 자유 체력 단련 시간에 혼자 조깅을 하거나, 가끔 동기나 후임들을 끌고 나가서 조깅을 했다.\n전역을 한 지금도 계속 조깅을 하고 있다.\n\n\nQ3. Please describe a memorable experience you had while walking. Explain what happened and why it was so memorable. (걷는 동안 있었던 기억에 남는 경험을 묘사해주세요.)\n\n군 시절 조깅 / 걷기 중에 미군 부대 근처를 지나고 있었는데, 갑자기 미군들이 내쪽으로 인사를 했다.\n그날은 휴일이였고, 그들은 자식들과 함께 산책을 하는 것처럼 보였다.\n그때 나는 영어를 할줄 몰라서 그냥 경례를 하고 빠르게 지나갔다.\n\n\nQ4. What do you have to consider when you go jogging? What can you do to avoid an injury? (조깅하러 갈 때 어떤 걸 고려하나요? 부상을 피하기 위해 무엇을 할 수 있나요?)\n\n먼저 나는 주로 야간에 조깅을 하기 때문에, 주변이 잘 보이는지 확인한다.\n또한 운동화는 반드시 편한 것을 입고, 몸에 맞는 옷을 입는다.\n그날의 날씨와 미세먼지도 확인한다.\n\n\n\n집에서 보내는 휴가\nQ1. Most people want to travel during vacations. Tell me why you prefer staying at home. What makes your vacation at home enjoyable? (대부분 여행을 원하는데, 당신이 집에 머무는 걸 더 좋아하는 이유는 무엇인가요?)\n\n나는 사람이 많은 곳에 오래 있으면 어지럽다.\n집에서는 할 수 있는게 꽤 많다.\n주로 집에서 영화나 드라마를 본다.\n\n\nQ3. I would like to know about the memorable vacation you have spent at home. What did you do and who were you with? Why was it so memorable? (집에서 보낸 휴가 중 가장 기억에 남는 휴가에 대해 말해주세요.)\n\n지난 크리스마스에 친한 사람들과 집에 모여서 축구 경기를 관람했다. (물론 이건 구라다)\n\n\nQ4. Why do you think people need vacations? What purposes do you think vacations have for different people? (사람들에게 왜 휴가가 필요하다고 생각하나요? 휴가의 목적은 무엇일까요?)\n\n새로운 환경과 경험은 더 많은 집중력과 창의력을 가져다 준다고 생각한다.\n휴가는 일상에서 벗어나서 재충전할 수 있는 기회라고 생각한다.\n사람마다 휴가의 목적은 다르지만, 대부분은 스트레스를 해소하고 새로운 경험을 찾기 위해 휴가를 떠나는 것 같다.\n\n\n\n국내여행 해외여행\nQ1. What are some things that you do to prepare for trips? (여행을 준비할 때 당신이 하는 일들은 무엇인가요?)\n\n짐은 최소한으로 싸는 편이라 세면도구와 갈아입을 정도만 챙긴다.\n여행의 묘미는 예측하지 못한 일 속에서 즐거움을 찾는 것이라고 생각한다.\n위생을 중요하게 생각해서, ~는 꼭 챙긴다.\n\n\nQ2. You indicated that you enjoy domestic travel. Where do you like to visit? Describe the place and why you love to go there. (국내 여행지 중 어디를 좋아하나요? 그곳을 묘사하고 왜 좋아하는지 설명해주세요.)\n\n바다를 좋아하기 때문에 섬으로 여행 가는 것을 좋아한다.\n경치가 좋고, 주변에 좋은 음식점이 많아서 좋다.\n\n\nQ4. Unexpected things can happen while you are traveling. Talk about an unforgettable happening on a trip. (여행 중 겪은 잊지 못할 일에 대해 말해주세요.)\n\n1 시간이나 기다렸는데 버스가 안왔다.\n주변에 아무도 없었고, 핸드폰 배터리도 없었고, 날씨는 추웠다.\n무작정 기다리다가 결국, 사람이 많은 곳을 찾아서 무작정 걸었다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "설문 script 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/02.html#smart-전략",
    "href": "posts/03_archives/completed_project/opic/notes/02.html#smart-전략",
    "title": "Tips",
    "section": "SMART 전략",
    "text": "SMART 전략\n\nS(start by stating your main point/idea): 주제나 요점 먼저 말하기 (main point1)\nM(mention a few example): 연관된 사례나 예시 제시하여 주제 설명 (body)\nA(address why your MP is important by returning to it): MP가 중요한 이유를 설명하며 주제로 돌아와 그 중요성 강조\nR(Reflect on what you’ve learned or realized): 배운 점이나 깨달은 점 반영. conclusion 가기 전에 언급\nT(tie everything back yo your main point): 모든 내용을 MP에 연결시켜 마무리\n\n(억지로 붙여넣네)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/02.html#main-point",
    "href": "posts/03_archives/completed_project/opic/notes/02.html#main-point",
    "title": "Tips",
    "section": "Main point",
    "text": "Main point\n\n설명/묘사\n\n메인포인트는 간결하고 깔끔하게 해야 한다. 말하고 싶은 내용을 메인 포인트로 말한 뒤, 그것에 대해 내가 어떻게 느끼는지, 왜 그렇게 느끼는지 간단한 질문이 많기 때문에 너무 장황하게 설명할 필요 없이 짧고 간결하게 답변하는 것도 좋다.\n\n습관\n\n행동적인 메인 포인트가 있어야 한다.\n질문 : 당신은 은행에 갈 때 보통 무엇을 하나요? 대답 : 전 은행에 갈 때마다 지갑을 꼭 챙겨요.\nHibit: whenever _\n\n과거 경험\n\n시간의 순서로 줄줄이 말하는 것 보다 경험의 클라이맥스 부분을 메인 포인트로 시작하는 게 좋다. 예를 들어 며칠전 카페에 가서 커피를 마시다가 화장실을 가려고 하는데 어떤 여자와 부딪쳐서 음료를 쏟았고 매우 당황했다는 이야기는 할 때, 사실과 느낀 점을 몇 마디로 정리해서 먼저 말하고 그다음 주절주절 자세하게 말하는 게 좋다.\n\n비교- 시간 비교\n\n현재에만 집중해서 메인 포인트를 말해야 한다. 질문 : 당신의 음악 선호도가 과거부터 지금까지 어떻게 변했나요? 답변 : 전 요즘 KPOP을 듣는게 좋아요. 왜냐면 엄청 신나거든요. 이후 예전에는 어떤 걸 좋아했고 왜 좋아했는데 어떤 계기로 요즘에는 바뀌었는지 등등을 설명하면 된다.\n\n비교- 일반 비교\n\n특별한 포인트가 있다기 보다는 비교하는 대상에 대해서 일반적인 메인 포인트를 딱 이야기하면 된다. 질문 : 당신이 좋아하는 가수 2명을 비교해보세요 답변 : 저는 A와 B를 좋아해요. 둘을 각각 다른 이유로 좋아하는데 왜 그런지 말해 줄게요.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/02.html#role-play",
    "href": "posts/03_archives/completed_project/opic/notes/02.html#role-play",
    "title": "Tips",
    "section": "Role play",
    "text": "Role play\n\n11\n\n대충 내용 설명\n첫 번째 질문\n상대방 말 보여주기\nreally?\n나를 포함시키기\n두번째 질문 하기\n상대방 말 보여주기\nreally?\n나를 포함시키기\n세번째 질문 하기\n상대방 말 보여주기\n나를 포함시키기",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/02.html#그-외",
    "href": "posts/03_archives/completed_project/opic/notes/02.html#그-외",
    "title": "Tips",
    "section": "그 외",
    "text": "그 외\n\n한국 고유말 한국말로 쓰면 감점이 있다고 함. 걍 한국말 쓰지 말자.\nthere’s this []: 어떤 []가 있다.\n어려운 표현 쓸거면 비슷한 쉬운 표현도 뒤에 같이 써서 의미를 명확히 전달할 것\nand 조심해서 써라\n15번은 그냥 스킵해라\nyou expression 많이 써라\n과거 이야기 할 때 main point 먼저 말하고 develope 해라\n생각하고 대답하는 척 하기\n보통 다른 사람 and i 순. i and 다른 사람은 어색\nmoreover, furthermore, in addition은 대화에는 어색함\nwhich because really since 로 길게 말할 수 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/notes/02.html#footnotes",
    "href": "posts/03_archives/completed_project/opic/notes/02.html#footnotes",
    "title": "Tips",
    "section": "각주",
    "text": "각주\n\n\nWhat, Feeling, Why↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비",
      "Notes",
      "Tips"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/index.html",
    "href": "posts/03_archives/completed_project/opic/index.html",
    "title": "OPIc 준비",
    "section": "",
    "text": "FAILED\n    \n    \n        시작일: 2025-06-16\n        종료일: 2025-07-07\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/index.html#details",
    "href": "posts/03_archives/completed_project/opic/index.html#details",
    "title": "OPIc 준비",
    "section": "Details",
    "text": "Details",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/index.html#tasks",
    "href": "posts/03_archives/completed_project/opic/index.html#tasks",
    "title": "OPIc 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/opic/index.html#참고-자료",
    "title": "OPIc 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/index.html#why-failed",
    "href": "posts/03_archives/completed_project/opic/index.html#why-failed",
    "title": "OPIc 준비",
    "section": "Why failed?",
    "text": "Why failed?\n나는 암기식 공부가 더 편하다.\n토익 스피킹을 준비하자.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/opic/index.html#related-posts",
    "href": "posts/03_archives/completed_project/opic/index.html#related-posts",
    "title": "OPIc 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "OPIc 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/dsa/01.html#트리",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/dsa/01.html#트리",
    "title": "시험 범위",
    "section": "트리",
    "text": "트리\n\n차수, 노드의 갯수, 높이 물어봄\n특정 노드의 부모 / 조상 노드, 단말 노드 수\n\n\n\n트리의 차수: 트리의 최대 차수\n\n\n전위, 중위, 후위 순회 무조건 냄\n\n\n전위 / 후위 중 하나의 결과와 중위순위를 통해 원래 트리 추측하는 문제\n\n전위의 맨 앞이랑 후위의 맨 뒤가 루트 노드\n루트 노드를 알면 중위 순회에서 루트 노드 기준으로 왼쪽 서브트리와 오른쪽 서브트리로 나눌 수 있음\n\n트리를 주고 나서 전위 / 중위 / 후위 결과를 기술하라\n\n\n레벨 순위 안냄",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/dsa/01.html#힙트리-1520",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/dsa/01.html#힙트리-1520",
    "title": "시험 범위",
    "section": "힙트리 (15~20)",
    "text": "힙트리 (15~20)\n\n삽입\n\ndef insert(self, n):\n    self.heap.append(n)\n    i = self.size()\n    while (i != 1 and n &gt; self.Parent(i)):\n        self.heap[i] = self.Parent(i)\n        i = i // 2\n    self.heap[i] = n\n\n삭제\n\ndef delete(self):\n    parent = 1\n    child = 2\n    if not self.is_empty():\n        hroot = self.heap[1]\n        last = self.heap[self.size()]\n        while child &lt;= self.size():\n            if (child &lt; self.size() and self.left(parent) &lt; self.right(parent)):\n                child += 1\n            if last &gt;= self.heap[child]:\n                break;\n            self.heap[parent] = self.heap[child]\n            parent = child\n            child = child * 2\n        self.heap[parent] = last\n        self.heap.pop(-1)\n        return hroot\n\n상향식 힙 만들기\nclass Bheap:\n    def __init__(self, a):\n        self.a = a\n        self.N = len(a) - 1\n\n    def create_heap(self, k):\n        for i in range(N // 2, 0, -1):\n            self.downheap(i)\n\n    def insert(self, k):\n        self.a.append(k)\n        self.N += 1\n        self.upheap(self.N)\n\n    def delete_min(self):\n        if self.N == 0:\n            return None\n        minimum = self.a[1]\n        self.a[1], self.a[-1] = self.a[-1], self.a[1]\n        del self.a[-1]\n        self.downheap(1)\n        return minimum\n\n    def upheap(self, i):\n        while i &gt; 1 and self.a[i][0] &lt; self.a[i // 2][0]:\n            self.a[i], self.a[i // 2] = self.a[i // 2], self.a[i]\n            i = i // 2\n\n    def downheap(self, i):\n        while i * 2 &lt;= self.N:\n            k = i * 2\n            if k &lt; self.N and self.a[k][0] &gt; self.a[k + 1][0]:\n                k = k+1\n            if self.a[i][0] &lt; self.a[k][0]:\n                break\n            self.a[i], self.a[k] = self.a[k], self.a[i]\n            i = k\n삽입, 삭제: O(logN)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/dsa/01.html#이진-탐색-트리-1520",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/dsa/01.html#이진-탐색-트리-1520",
    "title": "시험 범위",
    "section": "이진 탐색 트리 (15~20)",
    "text": "이진 탐색 트리 (15~20)\n\n이진 탐색 트리\n\n\nkey 값을 10개정도 쭉 주고 나서 이진 탐색트리로 구현하라\n이진 탐색 트리를 주고 나서, 노드 삭제 후 최종 이진 탐색 트리 기술하라\n\n\n연습 문제: BST 빈칸 채우기 (고급)\n아래 BST 클래스의 메소드들을 완성해보세요:\nclass Node:\n    def __init__(self, key, value, left=None, right=None):\n        self.key = key\n        self.value = value\n        self.left = left\n        self.right = right\n\nclass BST:\n    def __init__(self):\n        self.root = None\n\n    def get(self, k):\n        return self.get_item(self.root, k)\n\n    def get_item(self, i, k):\n        if i == None:\n            return None\n        if k &lt; i.key:\n            return self.get_item(i.left, k)\n        if i.key &lt; k:\n            return self.get_item(i.right, k)\n        return i.value\n\n    def put(self, k, val):\n        self.root = self.put_item(self.root, k, val)\n\n    def put_item(self, i, k, val):\n        if self.root == None:\n            return Node(k, val)\n        if k &lt; i.key:\n            self.left = self.put_item(self,left, k, val)\n        elif i.key &lt; k:\n            self.right = self.put_item(self.right, k, val)\n        else:\n            i.val = val\n        return i\n\n    def min(self):\n        if self.root == None:\n            return None\n        return self.min_item(self.root)\n\n    def min_item(self, i):\n        if i.left == None:\n            return i\n        return self.min_item(i.left)\n\n    def del_minimum(self):\n        if self.root == None:\n            return None\n        return self.del_min(self.root)\n\n    def del_min(self, i):\n        if i.left == None:\n            return i.right\n        i.left = del_min(self.left)\n        return i\n\n    def del(self, k):\n        if self.root == None:\n            return None\n        return self.del_item(self.root, k)\n\n    def del_item(i, k):\n        if k &lt; i.key:\n            i.left = self.del_item(i.left, k)\n        elif i.key &lt; k:\n            i.right = self.del_item(i.right, k)\n        else:\n            if i.right == None:\n                return i.left\n            if i.left == None:\n                return i.right\n            target = i\n            i = self.min_item(target.right)\n            i.left = target.left\n            i.right = self.del_min(target.right)\n        return i\n\nAVL 안냄",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/dsa/01.html#그래프",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/dsa/01.html#그래프",
    "title": "시험 범위",
    "section": "그래프",
    "text": "그래프\n\n그래프 차수: 정점에 인접한 정점의 수\nkruscal, prim(둘 중 하나) 30~40\n\n\nkruscal\nweights = [(0, 1, 9), (0, 2, 10)]\nweights.sort(key = lambda t: t[2])\nmst = []\nN = 7\np = [] * N\ndef find(a):\n    if a != p[a]:\n        p[a] = find(p[a])\n    return p[a]\n\ndef union(u, v):\n    root1 = find(u)\n    root2 = find(v)\n    p[root1] = root2\n\ne = 0\ncost = 0\nwhile True:\n    if e == N - 1:\n        break\n    u, v, wt = weights.pop(0)\n    if find(u) != find(v):\n        union(u, v)\n        mst.append((u, v))\n        cost += wt\n        e += 1\nprint(cost)\nprint(mst)\nO(MlogN)\n\n\nprim\nimport sys\nN=8\ns=0\ng = [None] * N\ng[0] = [(1, 1), (3, 2)]\ng[1] = []\n\nvisited = [False] * N\nD = [sys.maxsize] * N\nD[s] = 0\nprevious = [None] * N\nprevious[s] = s\n\nfor k in range(N):\n    m = -1\n    min_val = sys.maxsize\n    for j in range(N):\n        if not visited[j] and D[j] &lt; min_val:\n            m = j\n            min_val = D[j]\n    visited[m] = True\n    for v, wt in list(g[m]):\n        if not visited[v]:\n            if wt &lt; D[v]:\n                D[v] = wt\n                previous[v] = m\n그래프가 희소그래프이고, 이진힙 쓰면 O(NlogN) 아니면 O(N^2)\n\n다익스트라(필수)\n\nimport sys\nN=8\ns=0\ng = [None] * N\ng[0] = [(1, 1), (3, 2)]\ng[1] = []\n\nvisited = [False] * N\nD = [sys.maxsize] * N\nD[s] = 0\nprevious = [None] * N\nprevious[s] = s\nfor k in range(N):\n    m = -1\n    min_val = sys.maxsize\n    for j in range(N):\n        if not visited[j] and D[j] &lt; min_val:\n            min_val = D[j]\n            m = j\n    visted[m] = True\n    for v, wt in list(g[m]):\n        if not visted[v]:\n            if D[m] + wt &lt; D[v]:\n                D[v] = D[m] + wt\n                previous[v] = m\nO(N^2)\n\nBFS, DFS (5~10)\n\n\nBFS\n\nadj_list = [[2, 1], [3, 0]]\nN = len(adj_list)\nvisited = [None] * N\n\ndef bfs(i):\n    queue = []\n    visited[i] = True\n    queue.append(i)\n    while len(queue) != 0:\n        v = queue.pop(0)\n        print(v, ' ', end='')\n        for i in adj[v]:\n            if not visited[i]:\n                queue.append(i)\n                visited[i] = True\n\nfor i in range(N):\n    if not visited[i]:\n        bfs(i)\nO(N + M)\n\nDFS\n\nadj_list = [[2, 1], [3, 0]]\nN = len(adj_list)\nvisited = [None] * N\n\ndef dfs(v):\n    visited[v] = True\n    print(v, ' ', end='')\n    for i in adj_list[v]:\n        if not visited[i]:\n            dfs(i)\n\nfor i in range(N):\n    if not visited[i]:\n        dfs(i)\nO(N + M)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Dsa",
      "시험 범위"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#통계학",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#통계학",
    "title": "확률과 통계 1 정리",
    "section": "통계학",
    "text": "통계학\n\n불확실한 상황 하에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법의 체계\n모집단으로 부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 한다.\n\n\n\n\n통계적 의사결정 과정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#확률",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#확률",
    "title": "확률과 통계 1 정리",
    "section": "확률",
    "text": "확률\n\n고전적 의미: 표본공간에서 특정 사건이 차지하는 비율\n통계적 의미: 특정 사건이 발생하는 상대도수의 극한\n\n각 원소의 발생 가능성이 동일하지 않아도 무한한 반복을 통해 수렴하는 값을 구할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#확률-분포-정의-단계",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#확률-분포-정의-단계",
    "title": "확률과 통계 1 정리",
    "section": "확률 분포 정의 단계",
    "text": "확률 분포 정의 단계\n\n\nExperiment(확률실험): 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\nSample space(표본공간): 모든 simple event의 집합\nEvent(사건): 실험에서 발생하는 결과 (부분 집합)\nSimple event(단순사건): 원소가 하나인 사건\n확률 변수: 확률실험의 결과를 수치로 나타낸 변수",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#확률-분포",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#확률-분포",
    "title": "확률과 통계 1 정리",
    "section": "확률 분포",
    "text": "확률 분포\n\n이산 확률 분포: 이산 표본 공간, 연속 표본공간에서 정의 가능포\n\n베르누이 분포: 각 시행은 서로 독립적이고, 실패와 성공 두 가지 결과만 존재.\n\n단 모집단의 크기가 충분히 크고, 표본의 크기가 충분히 작다면 비복원 추출에서도 유효\n\n이항 분포: n번의 독립적인 베르누이 시행을 수행하여 성공 횟수를 측정\n기하 분포: 성공 확률이 p인 베르누이 시행에서 첫 성공까지의 시행 횟수\n초기하 분포: 베르누이 시행이 아닌 시행에서 성공하는 횟수\n포아송 분포: 임의의 기간동안 어떤 사건이 간헐적으로 발생할 때, 사건이 발생하는 횟수\n\nn이 매우 크고, p가 매우 작을 때, 이항 분포를 포아송 분포로 근사할 수 있다.\n\n\n연속 확률 분포: 연속 표본 공간에서 정의 가능\n\n균일 분포\n정규 분포\n\n\\(X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)\\)\n\nt 분포\n\n자유도가 커질수록 표준 정규분포에 근사함.\n t(n)\n\nf 분포\n\n\\(F = \\frac{X_1/ν_1}{X_2/ν_2}\\), \\(X_1 \\sim χ^2(ν_1)\\), \\(X_2 \\sim χ^2(ν_2)\\)\n\n감마 분포\n\n카이제곱 분포: α = v/2, θ = 2 인 감마분포\n\n\\(Z_i \\sim N(0,1)\\)일 때, \\(Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)\\)\n\\(X_i\\)가 서로 독립이고, 자유도가 \\(ν_i\\)인 카이제곱분포를 따른다면, \\(X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)\\)\n\n지수 분포: 포아송 분포에서 사건 발생 간격의 분포\n\n\\(\\sum_{i=1}^{n} X_i \\sim Γ(n, θ)\\), \\(θ = 1/λ\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#표본의-분포",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/00.html#표본의-분포",
    "title": "확률과 통계 1 정리",
    "section": "표본의 분포",
    "text": "표본의 분포\n\n샘플링에 따라 통계량이 다른 값을 가질 수 있다. 따라서 통계량의 분포를 이용한 통계적 추론이 가능하다.\n통계량: 표본의 특성을 나타내는 값\n추정량: 아래의 조건을 만족하는 통계량\n\n불편성: 추정량의 기대값이 추정하려는 모수와 같아야 한다.\n효율성: 분산이 작아야 한다. 표본의 갯수가 많아질수록 분산이 작아져야 한다.\n\n\n\n표본 평균의 분포\n\n모집단의 분포와 관계없이, 모집단의 평균이 μ이고, 분산이 \\(σ^2\\)이면, \\(\\bar{X}\\)의 평균은 μ이고, 분산은 \\(σ^2/n\\)인 정규분포를 따른다.\n\n단 모집단의 분포에 따라 표본의 크기가 충분히 커야함. (중심극한정리)\n\n만약 모집단의 분산을 모를 경우, σ를 s로 대체하여, t분포를 따르는 표본 평균의 분포를 구할 수 있다.\n\n\n\n표본 분산의 분포\n\n정규 모집단으로 부터 나온 표본의 분산 S에 대하여, \\(\\frac{(n-1)S^2}{σ^2}\\)은 자유도가 n-1인 카이제곱 분포를 따른다.\n\n모집단이 정규분포를 따르지 않을 경우, 비모수적인 방법을 사용해야 한다.\n\n두 정규 모집단으로부터 계산되는 표본분산의 비율은 f-분포를 따른다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 1 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#통계학습-및-회귀분석",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#통계학습-및-회귀분석",
    "title": "Regression Analysis",
    "section": "통계학습 및 회귀분석",
    "text": "통계학습 및 회귀분석\n\n통계 학습: 관측된 X, Y 데이터로부터 X와 Y의 관계를 추정하는 것\n\\(Y = f(X) + ϵ\\)\n선형 회귀 모형: 독립변수와 종속변수 사이의 관계를 선형으로 가정하는 모형",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#단순-선형회귀모형",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#단순-선형회귀모형",
    "title": "Regression Analysis",
    "section": "단순 선형회귀모형",
    "text": "단순 선형회귀모형\n\n\\(Y_i = β_0 + β_1x_i + ϵ_i\\)\n\\(β_0 + β_1x_i\\): 회귀 계수, 선형 모형으로 설명 가능한 부분\n\\(ϵ_i\\): 오차항, 선형 모형으로 설명할 수 없는 부분\n\n\\(ϵ_i ~^{iid} N(0, σ^2) → Y_i ~ N(β_0 + β_1x_i, σ^2)\\)\n\n\n\n\n\n\\(Y_i\\)는 iid는 아니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#회귀계수의-추정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#회귀계수의-추정",
    "title": "Regression Analysis",
    "section": "회귀계수의 추정",
    "text": "회귀계수의 추정\n\n최소 제곱법: 잔차 제곱합을 최소화하는 방법\n\n→ \\(SS_E\\)를 \\(\\hat{β}_0, \\hat{β}_1\\)에 대해 미분하여 0으로 만드는 \\(\\hat{β}_0, \\hat{β}_1\\).\n\\(β_0 = \\bar{Y} - β_1\\bar{X}\\), \\(β_1 = \\frac{S_{XY}}{S_{XX}}\\)\n\\(Σ_{i=1}^n e_i = 0\\), \\(Σ_{i=1}^n x_ie_i = 0\\)\n\n회귀계수 추정 시에는 오차항의 분포에 대한 가정이 사용되지 않는다.\n독립변수의 단위가 달라지면, 회귀계수의 단위도 달라진다. (설명력은 동일하다)\n표본의 수가 늘어나면, \\(β_1\\)은 수렴한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#회귀모형의-적합성",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#회귀모형의-적합성",
    "title": "Regression Analysis",
    "section": "회귀모형의 적합성",
    "text": "회귀모형의 적합성\n\n\\(SS_T = S_{YY}\\)\n\\(SS_R = \\frac{S_{XY}^2}{S_{XX}}\\)\n\\(SS_E = SS_T - SS_R\\)\n\n - 결정계수: \\(R^2 = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_E}{SS_T}\\) - \\(R^2\\)는 모형이 종속변수의 변동을 얼마나 설명하는지 나타내는 지표 - 오차를 최소화하면 \\(R^2\\) 값 증가 - \\(\\frac{S_{XY}^2}{S_{XX}S_{YY}} = r(X, Y)^2\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#회귀계수에-대한-추론",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#회귀계수에-대한-추론",
    "title": "Regression Analysis",
    "section": "회귀계수에 대한 추론",
    "text": "회귀계수에 대한 추론\n\n\\(H_0: β_1 = 0\\) (독립변수와 종속변수 사이에 관계가 없다)\n\\(β_1\\)은 불편추정량, 정규분포를 따른다.\n\\(Var(β_1) = \\frac{MS_E}{S_{XX}}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#종속변수의-신뢰구간과-예측구간",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#종속변수의-신뢰구간과-예측구간",
    "title": "Regression Analysis",
    "section": "종속변수의 신뢰구간과 예측구간",
    "text": "종속변수의 신뢰구간과 예측구간\n\n\\(\\hat{Y_0} = \\hat{β}_0 + \\hat{β}_1x_0\\)\n\\(Var(\\hat{Y_0}) = MS_E(\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{XX}})\\)\n예측 구간에서의 오차의 분산은 \\(MS_E(1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{XX}})\\)\n\n\n\n\n종속변수의 예측 / 신뢰구간은 평균일 때 가장 짧음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#잔차-분석",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/04.html#잔차-분석",
    "title": "Regression Analysis",
    "section": "잔차 분석",
    "text": "잔차 분석\n\n불편성\n정규성\n독립성\n등분산성\n\n오차의 추정치인 잔차를 통해 위의 가정을 검증할 수 있다.\n\n선형성: 잔차그림이 특정 패턴을 보이지 않는다.\n\n만족하지 않을 경우: 독립변수 변환\n\n등분산성: 잔차그림의 분산이 일정하다.\n\n만족하지 않는 경우: 반응치를 log 변환할 수 있다.\n\n독립성: 시계열 데이터의 경우, 자기상관을 고려해야 한다.\n정규성: normal Q-Q plot / shapiro-wilk test\n\n만족하지 않는 경우: 반응치를 log 변환할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Regression Analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/05.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/05.html",
    "title": "Analysis of categorical data",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "Analysis of categorical data"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/06.html#one-way-anova",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/06.html#one-way-anova",
    "title": "확률과 통계 R 실습 과제",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\n\na\n\nval1 &lt;- c(1.84, 2.67, 2.61, 2.95, 2.25, 2.35, 2.85, 2.58, 3.08, 2.25)\nval2 &lt;- c(1.79, 1.98, 2.01, 1.93, 2.24, 2.03, 2.22, 1.44, 2.24, 2.05)\nval3 &lt;- c(3.31, 2.24, 2.74, 2.36, 2.79, 2.20, 2.32, 2.82, 2.73, 2.78)\ndf &lt;- data.frame(\n  group = c(rep(\"일반 마우스\", 10), rep(\"트랙 패트\", 10), rep(\"버티컬 마우스\", 10)),\n  finger = c('왼중지', '왼검지', '오른약지', '왼약지', '왼엄지', '오른약지', '오른새끼', '오른엄지', '왼중지', '왼새끼', '오른중지', '오른엄지', '오른중지', '오른검지', '왼엄지', '왼검지', '오른새끼', '오른검지', '왼엄지', '오른중지', '왼검지', '왼새끼', '오른검지', '왼약지', '오른새끼', '오른엄지', '왼약지', '왼중지', '왼새끼', '오른약지'),\n  value = c(val1, val2, val3)\n)\ndf\n\n           group   finger value\n1    일반 마우스   왼중지  1.84\n2    일반 마우스   왼검지  2.67\n3    일반 마우스 오른약지  2.61\n4    일반 마우스   왼약지  2.95\n5    일반 마우스   왼엄지  2.25\n6    일반 마우스 오른약지  2.35\n7    일반 마우스 오른새끼  2.85\n8    일반 마우스 오른엄지  2.58\n9    일반 마우스   왼중지  3.08\n10   일반 마우스   왼새끼  2.25\n11     트랙 패트 오른중지  1.79\n12     트랙 패트 오른엄지  1.98\n13     트랙 패트 오른중지  2.01\n14     트랙 패트 오른검지  1.93\n15     트랙 패트   왼엄지  2.24\n16     트랙 패트   왼검지  2.03\n17     트랙 패트 오른새끼  2.22\n18     트랙 패트 오른검지  1.44\n19     트랙 패트   왼엄지  2.24\n20     트랙 패트 오른중지  2.05\n21 버티컬 마우스   왼검지  3.31\n22 버티컬 마우스   왼새끼  2.24\n23 버티컬 마우스 오른검지  2.74\n24 버티컬 마우스   왼약지  2.36\n25 버티컬 마우스 오른새끼  2.79\n26 버티컬 마우스 오른엄지  2.20\n27 버티컬 마우스   왼약지  2.32\n28 버티컬 마우스   왼중지  2.82\n29 버티컬 마우스   왼새끼  2.73\n30 버티컬 마우스 오른약지  2.78\n\n\n\n요인: 클릭하는 장치 종류\n수준: 일반 마우스, 트랙 패드, 버티컬 마우스\n반응치: 클릭 반응속도\n\n10개의 손가락에 대해 3개의 장치를 랜덤으로 사용하여 클릭 반응속도를 측정한다.\n각 손가락의 실험에 대해 랜덤으로 그룹을 나눈 후, 그룹별로 클릭하는 장치 종류를 배정하였다.\n\n\nb\n\ndf.means &lt;- tapply(df$value, INDEX=df$group, FUN=mean)\nboxplot(df$value ~ df$group, col=c(\"lightblue\", \"mistyrose\", \"lightcyan\"))\npoints(1:3, df.means, col=\"red\", pch=4, cex=1.5)\n\n\n\n\n\n\n\n\n결과는 다음과 같이 나온다.\n\n\nc\n\ndf.sd &lt;- tapply(df$value, INDEX=df$group, FUN=sd)\ndf.sd.diff &lt;- max(df.sd) / min(df.sd)\nif (df.sd.diff &gt; 2) \n{\n  print(\"모집단의 분산이 동일하지 않음\")\n} else \n{\n  print(\"모집단의 분산이 동일하다고 가정\")\n}\n\n[1] \"모집단의 분산이 동일하다고 가정\"\n\n\n결과는 다음과 같이 나온다.\n\n\nd\n\nlibrary(ggpubr)\n\nLoading required package: ggplot2\n\nggqqplot(val1)\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\nℹ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\n\n\n\n\n\n\nggqqplot(val2)\n\n\n\n\n\n\n\nggqqplot(val3)\n\n\n\n\n\n\n\n\n그림을 보니 반응치 값들이 정규분포를 따르는 것 같다!\n\n\ne\n\ndf.anova &lt;- aov(value ~ group, data=df)\nsummary(df.anova)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ngroup        2  2.381  1.1907   11.17 0.000292 ***\nResiduals   27  2.878  0.1066                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nis_significant &lt;- summary(df.anova)[[1]][[\"Pr(&gt;F)\"]][1] &lt; 0.05\nif (is_significant)\n{\n  print(\"95%에서 처리별 모평균의 차이가 있음\")\n} else\n{\n  print(\"95%에서 처리별 모평균의 차이가 없음\")\n}\n\n[1] \"95%에서 처리별 모평균의 차이가 있음\"\n\n\n결과는 다음과 같이 나온다.\n\n\nf\n\nif (is_significant)\n{\n  df.bon &lt;- pairwise.t.test(df$value, df$group, p.adjust.method = \"bonferroni\")\n  diff_pair &lt;- which(df.bon$p.value &lt; 0.05, arr.ind = TRUE)\n  if (length(diff_pair) &gt; 0)\n  {\n    cat(\"모평균의 차이가 있는 쌍\", \"\\n\")\n    for (i in 1:nrow(diff_pair)) \n    {\n      cat(colnames(df.bon$p.value)[diff_pair[i, \"col\"]], \"-\", rownames(df.bon$p.value)[diff_pair[i, \"row\"]], \"\\n\")\n    }\n  } else \n  {\n    print(\"모평균의 차이가 있는 처리 쌍 없음\")\n  }\n}\n\n모평균의 차이가 있는 쌍 \n버티컬 마우스 - 트랙 패트 \n일반 마우스 - 트랙 패트 \n\n\n결과는 다음과 같이 나온다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/06.html#anova-for-randomized-block-design",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/06.html#anova-for-randomized-block-design",
    "title": "확률과 통계 R 실습 과제",
    "section": "ANOVA for Randomized Block Design",
    "text": "ANOVA for Randomized Block Design\n\na\n\ndf &lt;- data.frame(\n  block = rep(c(\"아스팔트\", \"트랙\", \"모래밭\", \"잔디밭\"), each=3),\n  treatment = rep(c(\"런닝화\", \"축구화\", \"크록스\"), times=4),\n  value = c(6.1, 6.4, 6.5, 5.5, 5.9, 6.0, 6.9, 7.1, 7.5, 6.3, 6.9, 7.0)\n)\ndf\n\n      block treatment value\n1  아스팔트    런닝화   6.1\n2  아스팔트    축구화   6.4\n3  아스팔트    크록스   6.5\n4      트랙    런닝화   5.5\n5      트랙    축구화   5.9\n6      트랙    크록스   6.0\n7    모래밭    런닝화   6.9\n8    모래밭    축구화   7.1\n9    모래밭    크록스   7.5\n10   잔디밭    런닝화   6.3\n11   잔디밭    축구화   6.9\n12   잔디밭    크록스   7.0\n\n\n\n요인: 운동화 종류\n수준: 런닝화, 축구화, 크록스\nblock: 달리는 땅\n반응치: 50m 달리기 시간(초)\n\n각 block에 대해 3개의 운동화 종류를 사용하는 순서를 랜덤으로 결정해서 달리기 시간을 측정한다.\n\n\nb\n\ndf.anova.rbd &lt;- aov(value ~ treatment + block, data=df)\nsummary(df.anova.rbd)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreatment    2 0.6317  0.3158   27.73  0.00093 ***\nblock        3 3.0492  1.0164   89.24 2.28e-05 ***\nResiduals    6 0.0683  0.0114                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n처리에 의한 변동이 통계적으로 유의미하다.\nblock에 의한 변동 역시 통계적으로 유의미하다.\n\n\nc\n\ndf.anova &lt;- aov(value~treatment, data=df)\nsummary(df.anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ntreatment    2 0.6317  0.3158   0.912  0.436\nResiduals    9 3.1175  0.3464               \n\n\n처리에 의한 변동이 통계적으로 유의미하지 않다고 나온다.\nblock에 의한 변동이 통계적으로 유의미하기 때문에, block에 의한 변동을 제거하고 처리에 의한 변동을 분석해야 하기 때문이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/06.html#simple-linear-regression",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/06.html#simple-linear-regression",
    "title": "확률과 통계 R 실습 과제",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\na\n\nx &lt;- runif(100, 0, 10)\nerror &lt;- rnorm(100, 0, 4)\ny &lt;- 3 * x - 2 + error\nmodel &lt;- lm(y ~ x)\nmodel\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     -2.080        2.957  \n\n\n결과는 다음과 같이 나온다.\n\n\nb\n\ncat(\"B1의 추정 기울기: \", coef(model)[2])\n\nB1의 추정 기울기:  2.957164\n\ntrust &lt;- confint(model, level=0.95)[2, ]\ncat(\"신뢰구간: \", trust, \"\\n\")\n\n신뢰구간:  2.699609 3.21472 \n\nif (3 &gt; trust[1] && 3 &lt; trust[2]) \n{\n  print(\"신뢰구간에 3 포함\")\n} else \n{\n  print(\"신뢰구간에 3 안 포함\")\n}\n\n[1] \"신뢰구간에 3 포함\"\n\n\n결과는 다음과 같이 나온다.\n\n\nc\n\nplot(x, y, cex=0.7)\nabline(model, col = \"red\", lwd = 2)\nabline(-2, 3, col = \"blue\", lwd = 2)\nlegend(\"topleft\", legend = c(\"실제 회귀선\", \"추정 회귀선\"), col = c(\"blue\", \"red\"), lwd = 2)\n\n\n\n\n\n\n\n\n결과는 다음과 같이 나온다.\n\n\nd\n\ncnt &lt;- 0\nfor (i in 1:100) \n{\n  x &lt;- runif(100, 0, 10)\n  error &lt;- rnorm(100, 0, 4)\n  y &lt;- 3 * x - 2 + error\n  model &lt;- lm(y ~ x)\n  trust &lt;- confint(model, level=0.95)[2, ]\n  cnt &lt;- cnt + (3 &gt; trust[1] && 3 &lt; trust[2])\n}\ncnt\n\n[1] 97\n\n\n95에 가깝게 나온다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/statistics/06.html#multiple-linear-regression",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/statistics/06.html#multiple-linear-regression",
    "title": "확률과 통계 R 실습 과제",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\na\n\ndata(iris)\nmodel &lt;- lm(Sepal.Width ~ Sepal.Length + Petal.Length + Petal.Width, data = iris)\nsummary(model)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + Petal.Length + Petal.Width, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88045 -0.20945  0.01426  0.17942  0.78125 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.04309    0.27058   3.855 0.000173 ***\nSepal.Length  0.60707    0.06217   9.765  &lt; 2e-16 ***\nPetal.Length -0.58603    0.06214  -9.431  &lt; 2e-16 ***\nPetal.Width   0.55803    0.12256   4.553  1.1e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3038 on 146 degrees of freedom\nMultiple R-squared:  0.524, Adjusted R-squared:  0.5142 \nF-statistic: 53.58 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nF 값의 p-value가 매우 작으므로, 모델이 통계적으로 유의미하다고 할 수 있다.\n각각의 독립변수에 대한 p-value 역시 모두 매우 작으므로, 종속변수에 통계적으로 유의미한 영향을 미친다고 할 수 있다.\n하지만 r-squared 값이 크지 않아서 모델이 종속변수의 변동을 잘 설명하지 못한다고 할 수 있다.\n\n\nb\n\nplot(model, 1)\n\n\n\n\n\n\n\n\n잔차가 U자 패턴을 보이므로, 선형성이 만족되지 않는 것 같다.\n\n\nc\n\nmodel2 &lt;- lm(Sepal.Width ~ Sepal.Length + I(Sepal.Length^2) + Petal.Length + I(Petal.Length^2) + Petal.Width + I(Petal.Width^2), data = iris)\nsummary(model2)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + I(Sepal.Length^2) + \n    Petal.Length + I(Petal.Length^2) + Petal.Width + I(Petal.Width^2), \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82277 -0.16843 -0.00315  0.15300  0.77761 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -3.10783    1.37460  -2.261 0.025275 *  \nSepal.Length       2.32243    0.50141   4.632 8.08e-06 ***\nI(Sepal.Length^2) -0.15699    0.04264  -3.682 0.000327 ***\nPetal.Length      -0.89155    0.20300  -4.392 2.17e-05 ***\nI(Petal.Length^2)  0.06925    0.02336   2.965 0.003549 ** \nPetal.Width       -0.03666    0.38214  -0.096 0.923708    \nI(Petal.Width^2)   0.13095    0.10529   1.244 0.215648    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2755 on 143 degrees of freedom\nMultiple R-squared:  0.6167,    Adjusted R-squared:  0.6006 \nF-statistic: 38.35 on 6 and 143 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nd\nSepal.Length, \\(\\text{Sepal.Length}^2\\), Petal.Length의 p-value가 모두 0.05보다 작으므로, 이 변수들이 종속변수에 통계적으로 유의미한 영향을 미친다고 할 수 있다.\nr-squared 값도 더 작아져서, c 모델이 b 모델보다 종속변수의 변동을 잘 설명한다고 할 수 있다.\n\n\ne\n\nplot(model2, 1)\n\n\n\n\n\n\n\n\nU자 패턴이 완만해진걸로 보인다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Statistics",
      "확률과 통계 R 실습 과제"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#data-load",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#data-load",
    "title": "preprocessing",
    "section": "Data Load",
    "text": "Data Load\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom matplotlib import font_manager, rc\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nID_VAR = \"id\"\nWEIGHT_VAR = \"wt2\"\nOUTCOME_VAR = \"status_category\"\nRANDOM_STATE = 54321\ntarget_pred_var = [ID_VAR, \"wt1\", \"q33a01\", \"q33a02\", \"q33a03\", \"q33a04\", \"q33a05\", \"q33a06\",\n                    \"q48a07\", \"q48a08\", \"q48a09\", \"q48a10\",\n                    \"q49a01\", \"q49a02\", \"q49a03\", \"q49a04\",\n                    \"q33a07\", \"q33a08\", \"q33a09\",\n                    \"q49a15\", \"q49a16\", \"q49a17\",\n                    \"q49a09\", \"q49a10\", \"q49a11\",\n                    \"q48b1\", \"q48b2\", \"q48b3\",\n                    \"q12a01\", \"q12a02\", \"q12a03\",\n                    \"q48a04\", \"q48a05\", \"q48a06\",\n                    \"q49a05\", \"q49a06\", \"q49a08\"]\ncfa_model = \"\"\"\n  # 1. 부모애착\n  parent_attachment =~ q33a01 + q33a02 + q33a03 + q33a04 + q33a05 + q33a06\n  # 2. 일탈적 자아 낙인\n  deviant_esteem =~ q48a07 + q48a08 + q48a09 + q48a10\n  # 3. 부모에 의한 스트레스\n  parent_stress =~ q49a01 + q49a02 + q49a03 + q49a04\n  # 4. 부모감독\n  parent_monitoring =~ q33a07 + q33a08 + q33a09\n  # 5. 물질적 요인으로 인한 스트레스\n  desire_stress =~ q49a15 + q49a16 + q49a17\n  # 6. 친구로 인한 스트레스\n  friend_stress =~ q49a09 + q49a10 + q49a11\n  # 7. 자기신뢰감\n  self_confidence =~ q48b1 + q48b2 + q48b3\n  # 8. 상급학교 의존도\n  higher_school_dependence =~ q12a01 + q12a02 + q12a03\n  # 9. 부정적 자아존중감\n  neg_esteem =~ q48a04 + q48a05 + q48a06\n  # 10. 학업으로 인한 스트레스\n  academic_stress =~ q49a05 + q49a06 + q49a08\n\"\"\"\ndf1_origin = pd.read_csv('_data/student_1.csv')\ndf2_origin = pd.read_csv('_data/student_2.csv')\ndf3_origin = pd.read_csv('_data/student_3.csv')\ndf4_origin = pd.read_csv('_data/student_4.csv')\ndf5_origin = pd.read_csv('_data/student_5.csv')\ndf6_origin = pd.read_csv('_data/student_6.csv')\ndf_origin = [df1_origin, df2_origin, df3_origin, df4_origin, df5_origin]",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#cfa-confirmatory-factor-analysis",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#cfa-confirmatory-factor-analysis",
    "title": "preprocessing",
    "section": "CFA (Confirmatory Factor Analysis)",
    "text": "CFA (Confirmatory Factor Analysis)\n\nimport rpy2.robjects as ro\nfrom rpy2.robjects.packages import importr\nfrom rpy2.robjects import pandas2ri\nfrom rpy2.robjects.conversion import localconverter\nfrom rpy2.robjects.packages import importr\n\npandas2ri.activate()\nlavaan = importr('lavaan')\nbase_r = importr('base')\n\nmerged_df_pd = pd.DataFrame()\nfor i, df in enumerate(df_origin):\n    wave = i + 1\n    df_analysis = df[target_pred_var].copy()\n    df_clean = df_analysis.dropna()\n    with localconverter(ro.default_converter + pandas2ri.converter):\n        df_clean_r = ro.conversion.py2rpy(df_clean)\n    cfa_fit_r = lavaan.cfa(\n        model=cfa_model,\n        data=df_clean_r,\n        sampling_weights=\"wt1\",\n        estimator=\"MLR\",\n        warn=False,\n        verbose=False\n    )\n    print(f\"\\n--- Wave {wave} CFA 적합도 지수 ---\")\n    desired_fit_measures = ro.StrVector([\n        'cfi.scaled', 'tli.scaled',\n        'rmsea.scaled', 'rmsea.ci.lower.scaled', 'rmsea.ci.upper.scaled',\n        'srmr_bentler'\n    ])\n    fit_measures_values_r = lavaan.fitMeasures(cfa_fit_r, fit_measures=desired_fit_measures)\n    fit_values_py = [val if val is not ro.NA_Real else float('nan') for val in list(fit_measures_values_r)]\n    fit_measures_s = pd.Series(fit_values_py, index=list(desired_fit_measures))\n    print(fit_measures_s.to_string())\n    print(\"------------------------------------\")\n    factor_scores_r = lavaan.lavPredict(cfa_fit_r, type=\"lv\")\n    factor_scores_pd = pd.DataFrame(factor_scores_r, columns=[\"parent_attachment\", \"deviant_esteem\", \"parent_stress\", \"parent_monitoring\",\n                                                             \"desire_stress\", \"friend_stress\", \"self_confidence\",\n                                                             \"higher_school_dependence\", \"neg_esteem\", \"academic_stress\"])\n    ids_for_scores = df_clean[ID_VAR].reset_index(drop=True)\n    factor_scores_pd[ID_VAR] = ids_for_scores\n    new_colnames = {col: f\"{col}_w{wave}\" for col in factor_scores_pd.columns if col != ID_VAR}\n    factor_scores_pd = factor_scores_pd.rename(columns=new_colnames)\n    if wave == 1:\n        merged_df_pd = factor_scores_pd\n    else:\n        merged_df_pd = pd.merge(merged_df_pd, factor_scores_pd, on=ID_VAR, how='outer')\n\ndef classify_status(q11_value):\n    if q11_value in [1, 7, 8, 9, 71, 81, 91, 10, 101, 11, 111]:\n        return \"stable\"\n    elif q11_value in [2, 3, 4, 5, 6, 12, 13, 14]:\n        return \"explorative\"\n    else:\n        return None\n\npandas2ri.deactivate()\n\n\n--- Wave 1 CFA 적합도 지수 ---\ncfi.scaled               0.927641\ntli.scaled               0.916400\nrmsea.scaled             0.041374\nrmsea.ci.lower.scaled    0.040245\nrmsea.ci.upper.scaled    0.042511\nsrmr_bentler             0.042360\n------------------------------------\n\n--- Wave 2 CFA 적합도 지수 ---\ncfi.scaled               0.937524\ntli.scaled               0.927819\nrmsea.scaled             0.039718\nrmsea.ci.lower.scaled    0.038552\nrmsea.ci.upper.scaled    0.040894\nsrmr_bentler             0.037124\n------------------------------------\n\n--- Wave 3 CFA 적합도 지수 ---\ncfi.scaled               0.930504\ntli.scaled               0.919708\nrmsea.scaled             0.039424\nrmsea.ci.lower.scaled    0.038270\nrmsea.ci.upper.scaled    0.040586\nsrmr_bentler             0.043117\n------------------------------------\n\n--- Wave 4 CFA 적합도 지수 ---\ncfi.scaled               0.925239\ntli.scaled               0.913625\nrmsea.scaled             0.045518\nrmsea.ci.lower.scaled    0.044364\nrmsea.ci.upper.scaled    0.046681\nsrmr_bentler             0.046338\n------------------------------------\n\n--- Wave 5 CFA 적합도 지수 ---\ncfi.scaled               0.934303\ntli.scaled               0.924098\nrmsea.scaled             0.042005\nrmsea.ci.lower.scaled    0.040850\nrmsea.ci.upper.scaled    0.043169\nsrmr_bentler             0.045346\n------------------------------------\n\n\n\n상관행렬\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nplt.figure(figsize=(25, 22))\ncorr_matrix = merged_df_pd.drop(columns=[ID_VAR]).corr()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(\n    corr_matrix,\n    annot=False,\n    vmax=1.0, \n    vmin=-1.0,\n    center=0,\n    linewidths=.5,\n    cmap=cmap,\n    cbar_kws={\"shrink\": .5, \"label\": \"Correlation Coefficient\"}\n)\n\nplt.title('Correlation Matrix', fontsize=16, pad=20)\nplt.tight_layout()\nplt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ncorr_pairs = corr_matrix.unstack().reset_index()\ncorr_pairs.columns = ['Variable 1', 'Variable 2', 'Correlation']\n\ncorr_pairs = corr_pairs[corr_pairs['Variable 1'] != corr_pairs['Variable 2']]\ncorr_pairs['Pair'] = corr_pairs.apply(lambda x: tuple(sorted([x['Variable 1'], x['Variable 2']])), axis=1)\ncorr_pairs = corr_pairs.drop_duplicates('Pair')\n\ncorr_pairs = corr_pairs.sort_values(by='Correlation', key=abs, ascending=False)\ntop_corr = corr_pairs[['Variable 1', 'Variable 2', 'Correlation']]\n\nprint(\"Highest Absolute Correlations:\")\nprint(top_corr.head(40))\n\n\n\n\n\n\n\n\nHighest Absolute Correlations:\n                Variable 1            Variable 2  Correlation\n619       parent_stress_w2    academic_stress_w2     0.757157\n109       parent_stress_w1    academic_stress_w1     0.737240\n2043  parent_attachment_w5  parent_monitoring_w5     0.713755\n1533  parent_attachment_w4  parent_monitoring_w4     0.692874\n513   parent_attachment_w2  parent_monitoring_w2     0.689347\n2149      parent_stress_w5    academic_stress_w5     0.663941\n3     parent_attachment_w1  parent_monitoring_w1     0.658326\n1023  parent_attachment_w3  parent_monitoring_w3     0.655997\n1129      parent_stress_w3    academic_stress_w3     0.608676\n1540  parent_attachment_w4  parent_attachment_w5     0.604272\n1030  parent_attachment_w3  parent_attachment_w4     0.601522\n10    parent_attachment_w1  parent_attachment_w2     0.582573\n520   parent_attachment_w2  parent_attachment_w3     0.579379\n719       desire_stress_w2    academic_stress_w2     0.574989\n1639      parent_stress_w4    academic_stress_w4     0.566123\n614       parent_stress_w2      desire_stress_w2     0.559798\n409          neg_esteem_w1    academic_stress_w1     0.559204\n209       desire_stress_w1    academic_stress_w1     0.556277\n1040  parent_attachment_w3  parent_attachment_w5     0.539659\n58       deviant_esteem_w1         neg_esteem_w1     0.534314\n104       parent_stress_w1      desire_stress_w1     0.530970\n715       desire_stress_w2      friend_stress_w2     0.530456\n2144      parent_stress_w5      desire_stress_w5     0.519393\n1693  parent_monitoring_w4  parent_monitoring_w5     0.518281\n1132      parent_stress_w3      parent_stress_w4     0.513876\n20    parent_attachment_w1  parent_attachment_w3     0.512048\n1124      parent_stress_w3      desire_stress_w3     0.507338\n1489    academic_stress_w3    academic_stress_w4     0.507213\n163   parent_monitoring_w1  parent_monitoring_w2     0.507147\n1234      desire_stress_w3      desire_stress_w4     0.505383\n530   parent_attachment_w2  parent_attachment_w4     0.504675\n2     parent_attachment_w1      parent_stress_w1    -0.501466\n673   parent_monitoring_w2  parent_monitoring_w3     0.497952\n540   parent_attachment_w2  parent_attachment_w5     0.492200\n1183  parent_monitoring_w3  parent_monitoring_w4     0.489749\n1642      parent_stress_w4      parent_stress_w5     0.489084\n1744      desire_stress_w4      desire_stress_w5     0.486436\n112       parent_stress_w1      parent_stress_w2     0.481296\n1588     deviant_esteem_w4         neg_esteem_w4     0.480133\n1846    self_confidence_w4    self_confidence_w5     0.478809\n\n\n\nextra_df_pd = df6_origin[[ID_VAR, \"q11\", WEIGHT_VAR, \"sex\", \"yy\", \"area\"]].copy()\nextra_df_pd['status_category'] = extra_df_pd['q11'].apply(classify_status)\nmerged_df = pd.merge(merged_df_pd, extra_df_pd, on=ID_VAR, how='left').dropna()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#인구통계학적-분포-분석",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#인구통계학적-분포-분석",
    "title": "preprocessing",
    "section": "인구통계학적 분포 분석",
    "text": "인구통계학적 분포 분석\n\n분포 테이블\n\nsex_dist_py = merged_df['sex'].value_counts().rename_axis('sex').reset_index(name='n')\nsex_dist_py['percentage'] = (sex_dist_py['n'] / sex_dist_py['n'].sum()) * 100\n\nbirth_year_dist_py = merged_df['yy'].value_counts().rename_axis('yy').reset_index(name='n')\nbirth_year_dist_py['percentage'] = (birth_year_dist_py['n'] / birth_year_dist_py['n'].sum()) * 100\nbirth_year_dist_py = birth_year_dist_py.sort_values(by='yy').reset_index(drop=True)\n\narea_dist_py = merged_df['area'].value_counts().rename_axis('area').reset_index(name='n')\narea_dist_py['percentage'] = (area_dist_py['n'] / area_dist_py['n'].sum()) * 100\narea_dist_py = area_dist_py.sort_values(by='area').reset_index(drop=True) # 지역 순으로 정렬\n\nstatus_dist_py = merged_df['status_category'].value_counts(dropna=False).rename_axis('status_category').reset_index(name='n')\nstatus_dist_py['percentage'] = (status_dist_py['n'] / status_dist_py['n'].sum()) * 100\n\nprint(sex_dist_py)\nprint(birth_year_dist_py)\nprint(status_dist_py)\n\n   sex     n  percentage\n0  2.0  1280   52.523595\n1  1.0  1157   47.476405\n     yy     n  percentage\n0  88.0     2    0.082068\n1  89.0  1884   77.308166\n2  90.0   551   22.609766\n  status_category     n  percentage\n0          stable  1369   56.175626\n1     explorative  1068   43.824374\n\n\n\n\n분포 시각화\n\narea_mapping_py = {\n    100: \"서울\", 110: \"서울\", 120: \"서울\", 121: \"서울\", 122: \"서울\",\n    130: \"서울\", 131: \"서울\", 132: \"서울\", 133: \"서울\", 134: \"서울\",\n    135: \"서울\", 136: \"서울\", 137: \"서울\", 138: \"서울\", 139: \"서울\",\n    140: \"서울\", 142: \"서울\", 143: \"서울\", 150: \"서울\", 151: \"서울\",\n    152: \"서울\", 153: \"서울\", 156: \"서울\", 157: \"서울\", 158: \"서울\",\n    200: \"강원\", 209: \"강원\", 210: \"강원\", 215: \"강원\", 217: \"강원\",\n    219: \"강원\", 220: \"강원\", 225: \"강원\", 230: \"강원\", 232: \"강원\",\n    233: \"강원\", 235: \"강원\", 240: \"강원\", 245: \"강원\", 250: \"강원\",\n    252: \"강원\", 255: \"강원\", 269: \"강원\",\n    300: \"대전\", 301: \"대전\", 302: \"대전\", 305: \"대전\", 306: \"대전\",\n    312: \"충남\", 314: \"충남\", 320: \"충남\", 321: \"충남\", 323: \"충남\",\n    325: \"충남\", 330: \"충남\", 336: \"충남\", 339: \"충남\", 340: \"충남\",\n    343: \"충남\", 345: \"충남\", 350: \"충남\", 355: \"충남\", 356: \"충남\",\n    357: \"충남\",\n    360: \"충북\", 361: \"충북\", 363: \"충북\", 365: \"충북\", 367: \"충북\",\n    368: \"충북\", 369: \"충북\", 370: \"충북\", 373: \"충북\", 376: \"충북\",\n    380: \"충북\", 390: \"충북\", 395: \"충북\",\n    400: \"인천\", 401: \"인천\", 402: \"인천\", 403: \"인천\", 404: \"인천\",\n    405: \"인천\", 406: \"인천\", 407: \"인천\", 409: \"인천\", 417: \"인천\",\n    411: \"경기\", 412: \"경기\", 413: \"경기\", 415: \"경기\",\n    420: \"경기\", 421: \"경기\", 422: \"경기\", 423: \"경기\", 425: \"경기\",\n    426: \"경기\", 427: \"경기\", 429: \"경기\", 430: \"경기\", 431: \"경기\",\n    435: \"경기\", 437: \"경기\", 440: \"경기\", 441: \"경기\", 442: \"경기\",\n    443: \"경기\", 445: \"경기\", 447: \"경기\", 449: \"경기\", 456: \"경기\",\n    459: \"경기\", 461: \"경기\", 462: \"경기\", 463: \"경기\", 464: \"경기\",\n    465: \"경기\", 467: \"경기\", 469: \"경기\", 471: \"경기\", 472: \"경기\",\n    476: \"경기\", 477: \"경기\", 480: \"경기\", 481: \"경기\", 482: \"경기\",\n    483: \"경기\", 487: \"경기\",\n    500: \"광주\", 501: \"광주\", 502: \"광주\", 503: \"광주\", 506: \"광주\",\n    513: \"전남\", 515: \"전남\", 516: \"전남\", 517: \"전남\", 519: \"전남\",\n    520: \"전남\", 525: \"전남\", 526: \"전남\", 527: \"전남\", 529: \"전남\",\n    530: \"전남\", 534: \"전남\", 535: \"전남\", 536: \"전남\", 537: \"전남\",\n    539: \"전남\", 540: \"전남\", 542: \"전남\", 545: \"전남\", 546: \"전남\",\n    548: \"전남\", 550: \"전남\",\n    560: \"전북\", 561: \"전북\", 565: \"전북\", 566: \"전북\", 567: \"전북\",\n    568: \"전북\", 570: \"전북\", 573: \"전북\", 576: \"전북\", 579: \"전북\",\n    580: \"전북\", 585: \"전북\", 590: \"전북\", 595: \"전북\", 597: \"전북\",\n    600: \"부산\", 601: \"부산\", 602: \"부산\", 604: \"부산\", 606: \"부산\",\n    607: \"부산\", 608: \"부산\", 609: \"부산\", 611: \"부산\", 612: \"부산\",\n    613: \"부산\", 614: \"부산\", 616: \"부산\", 617: \"부산\", 618: \"부산\",\n    619: \"부산\",\n    621: \"경남\", 626: \"경남\", 627: \"경남\", 631: \"경남\", 635: \"경남\",\n    636: \"경남\", 637: \"경남\", 638: \"경남\", 641: \"경남\", 645: \"경남\",\n    650: \"경남\", 656: \"경남\", 660: \"경남\", 664: \"경남\", 666: \"경남\",\n    667: \"경남\", 668: \"경남\", 670: \"경남\", 676: \"경남\", 678: \"경남\",\n    680: \"울산\", 681: \"울산\", 682: \"울산\", 683: \"울산\", 689: \"울산\",\n    690: \"제주\", 695: \"제주\", 697: \"제주\", 699: \"제주\",\n    700: \"대구\", 701: \"대구\", 702: \"대구\", 703: \"대구\", 704: \"대구\",\n    705: \"대구\", 706: \"대구\", 711: \"대구\",\n    712: \"경북\", 714: \"경북\", 716: \"경북\", 717: \"경북\", 718: \"경북\",\n    719: \"경북\", 730: \"경북\", 740: \"경북\", 742: \"경북\", 745: \"경북\",\n    750: \"경북\", 755: \"경북\", 757: \"경북\", 760: \"경북\", 763: \"경북\",\n    764: \"경북\", 766: \"경북\", 767: \"경북\", 769: \"경북\", 770: \"경북\",\n    780: \"경북\", 790: \"경북\", 791: \"경북\", 799: \"경북\",\n    999: \"국외\"\n}\n\nsns.barplot(x='sex', y='percentage', data=sex_dist_py, palette='pastel')\nplt.title('Gender Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(ticks=[0, 1], labels=['Male', 'Female'])\nplt.tight_layout()\nplt.savefig('sex_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nsns.barplot(x='yy', y='percentage', data=birth_year_dist_py, palette='viridis')\nplt.title('Birth Year Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.savefig('birth_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nmerged_df['area_name'] = merged_df['area'].map(area_mapping_py)\narea_name_dist_py = merged_df['area_name'].value_counts(dropna=False).rename_axis('area_name').reset_index(name='n')\narea_name_dist_py['percentage'] = (area_name_dist_py['n'] / area_name_dist_py['n'].sum()) * 100\narea_name_dist_py = area_name_dist_py.sort_values(by='percentage', ascending=False).reset_index(drop=True)\nsns.barplot(x='area_name', y='percentage', data=area_name_dist_py, palette='colorblind')\nplt.title('Regional Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.savefig('area_dis.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nsns.barplot(x='status_category', y='percentage', data=status_dist_py, palette='Set2')\nplt.title('Dependent Variable Distribution (%)')\nplt.ylabel('Percentage (%)')\nplt.tight_layout()\nplt.savefig('status_dis.png', dpi=300, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#데이터-전처리-및-train-test-split",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#데이터-전처리-및-train-test-split",
    "title": "preprocessing",
    "section": "데이터 전처리 및 train test split",
    "text": "데이터 전처리 및 train test split\n\nimport re\nfrom sklearn.model_selection import train_test_split\n\npred_vars = [col for col in merged_df.columns if re.search(r\"_w[1-5]$\", col)]\nmerged_df[OUTCOME_VAR] = merged_df[OUTCOME_VAR].astype('category')\n\nX = merged_df[pred_vars]\ny = merged_df[OUTCOME_VAR]\ncomposite_stratify_key = y.astype(str) + '_' + \\\n                         merged_df['area_name'].astype(str) + '_' + \\\n                         merged_df['sex'].astype(str)\nweights = merged_df[WEIGHT_VAR]\nX_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(X, y, weights, test_size=0.3, random_state=RANDOM_STATE, stratify=composite_stratify_key)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#데이터-저장",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/14.html#데이터-저장",
    "title": "preprocessing",
    "section": "데이터 저장",
    "text": "데이터 저장\n\ntrain_data_to_save = X_train.copy()\ntrain_data_to_save.insert(0, 'y', y_train)\ntrain_data_to_save.insert(1, 'weights', weights_train)\ntrain_data_to_save.to_csv('_data/train_data.csv', index=False)\ntest_data_to_save = X_test.copy()\ntest_data_to_save.insert(0, 'y', y_test)\ntest_data_to_save.insert(1, 'weights', weights_test)\ntest_data_to_save.to_csv('_data/test_data.csv', index=False)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#데이터-load",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#데이터-load",
    "title": "analysis",
    "section": "데이터 load",
    "text": "데이터 load\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nRANDOM_STATE = 54321\nnp.random.seed(RANDOM_STATE)\n\nX_train_df = pd.read_csv(\"_data/train_data.csv\")\nX_test_df = pd.read_csv(\"_data/test_data.csv\")\ny_train = X_train_df['y'].astype('category')\ny_test = X_test_df['y'].astype('category')\nweights_train = X_train_df['weights']\nweights_test = X_test_df['weights']\nX_train = X_train_df.drop(columns=['y', 'weights'])\nX_test = X_test_df.drop(columns=['y', 'weights'])",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#무작위-분류",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#무작위-분류",
    "title": "analysis",
    "section": "무작위 분류",
    "text": "무작위 분류\n\nclass_proportions_py = y_train.value_counts(normalize=True)\ntest_categories = list(y_test.cat.categories)\nprop_values_ordered = class_proportions_py.reindex(test_categories, fill_value=0).values\nrandom_predictions = np.random.choice(\n    a=test_categories,\n    size=len(y_test),\n    replace=True,\n    p=prop_values_ordered\n)\nrandom_predictions_cat = pd.Categorical(random_predictions, categories=test_categories, ordered=False)\ncm_random = confusion_matrix(y_test, random_predictions_cat, sample_weight=weights_test, labels=test_categories)\n\nprint(\"\\n=== 무작위 분류기 혼동 행렬 ===\\n\")\ncm_df_random = pd.DataFrame(cm_random, index=[f\"Actual: {cat}\" for cat in test_categories], columns=[f\"Predicted: {cat}\" for cat in test_categories])\nprint(cm_df_random.round(2))\n\naccuracy_random = accuracy_score(y_test, random_predictions_cat, sample_weight=weights_test)\nprecision_random_sk = precision_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nrecall_random_sk = recall_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nf1_score_random_sk = f1_score(y_test, random_predictions_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\n\nprint(\"\\n=== 무작위 분류기 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_random:.4f}\")\n\nprint(\"\\n무작위 분류기 범주별 정밀도\")\nprint(pd.Series(precision_random_sk, index=test_categories).round(4))\n\nprint(\"\\n무작위 분류기 범주별 재현율\")\nprint(pd.Series(recall_random_sk, index=test_categories).round(4))\n\nprint(\"\\n무작위 분류기 범주별 F1-score\")\nprint(pd.Series(f1_score_random_sk, index=test_categories).round(4))\n\n\n=== 무작위 분류기 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                37372.17           44867.14\nActual: stable                     41616.52           49155.97\n\n=== 무작위 분류기 성능 지표 ===\n\n정확도: 0.5001\n\n무작위 분류기 범주별 정밀도\nexplorative    0.4731\nstable         0.5228\ndtype: float64\n\n무작위 분류기 범주별 재현율\nexplorative    0.4544\nstable         0.5415\ndtype: float64\n\n무작위 분류기 범주별 F1-score\nexplorative    0.4636\nstable         0.5320\ndtype: float64",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#random-forest",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#random-forest",
    "title": "analysis",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nbase_rf = RandomForestClassifier(\n    random_state=RANDOM_STATE,\n    oob_score=True,\n    class_weight='balanced_subsample',\n    n_jobs=-1\n)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 10, None],\n    'max_features': ['sqrt', 'log2', None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(\n    estimator=base_rf,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=False\n)\n\ngrid_search.fit(X_train, y_train, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nrf_model_py = grid_search.best_estimator_\nprint(f\"OOB Score: {rf_model_py.oob_score_:.4f}\")\n\nimportances = rf_model_py.feature_importances_\nfeature_names = X_train.columns\n\nmax_importance = importances.max()\nscaled_importances = (importances / max_importance) * 100\nforest_importances = pd.Series(scaled_importances, index=feature_names).sort_values(ascending=False)\n\nprint(\"\\n=== Random Forest 중요도 ===\")\ntop_10_importances = forest_importances\ntop_10_df = pd.DataFrame({'Feature': top_10_importances.index, 'Importance': top_10_importances.values})\nprint(top_10_df.round(2))\n\nsns.barplot(x=top_10_importances.values, y=top_10_importances.index)\nplt.title('Feature Importances (Random Forest)', fontsize=10)\nplt.xlabel('Importance Score', fontsize=8)\nplt.ylabel('Feature', fontsize=8)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('ran_imp.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ny_pred_rf = rf_model_py.predict(X_test)\ny_pred_proba_rf = rf_model_py.predict_proba(X_test)\n\ny_pred_rf_cat = pd.Categorical(y_pred_rf, categories=test_categories, ordered=False)\n\ncm_rf = confusion_matrix(y_test, y_pred_rf_cat, sample_weight=weights_test, labels=test_categories)\nprint(\"\\n=== Random Forest 혼동 행렬 ===\\n\")\ncm_df_rf = pd.DataFrame(cm_rf, index=[f\"Actual: {cat}\" for cat in test_categories], columns=[f\"Predicted: {cat}\" for cat in test_categories])\nprint(cm_df_rf.round(2))\n\naccuracy_rf = accuracy_score(y_test, y_pred_rf_cat, sample_weight=weights_test)\nprecision_rf_sk = precision_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nrecall_rf_sk = recall_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\nf1_score_rf_sk = f1_score(y_test, y_pred_rf_cat, sample_weight=weights_test, average=None, labels=test_categories, zero_division=0)\n\nprint(\"\\n=== Random Forest 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_rf:.4f}\")\nprint(\"\\nRandom Forest 범주별 정밀도:\")\nprint(pd.Series(precision_rf_sk, index=test_categories).round(4))\nprint(\"\\nRandom Forest 범주별 재현율:\")\nprint(pd.Series(recall_rf_sk, index=test_categories).round(4))\nprint(\"\\nRandom Forest 범주별 F1-score:\")\nprint(pd.Series(f1_score_rf_sk, index=test_categories).round(4))\n\n\n최적의 하이퍼파라미터: {'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n최적 교차 검증 점수: 0.6129\nOOB Score: 0.5865\n\n=== Random Forest 중요도 ===\n                        Feature  Importance\n0            self_confidence_w4      100.00\n1              desire_stress_w1       89.19\n2          parent_attachment_w5       83.50\n3          parent_monitoring_w5       83.40\n4              friend_stress_w4       82.51\n5          parent_monitoring_w4       81.66\n6             deviant_esteem_w3       80.23\n7                 neg_esteem_w4       79.71\n8          parent_monitoring_w3       79.27\n9          parent_attachment_w4       78.51\n10  higher_school_dependence_w1       76.60\n11             desire_stress_w4       75.87\n12         parent_attachment_w3       74.15\n13  higher_school_dependence_w2       73.13\n14         parent_monitoring_w1       72.00\n15             friend_stress_w3       71.55\n16           self_confidence_w1       69.76\n17  higher_school_dependence_w4       67.74\n18             desire_stress_w2       67.66\n19            deviant_esteem_w4       66.96\n20           self_confidence_w3       66.83\n21  higher_school_dependence_w5       66.33\n22         parent_monitoring_w2       64.70\n23           self_confidence_w2       64.55\n24             parent_stress_w2       64.32\n25             friend_stress_w2       63.39\n26             parent_stress_w3       63.12\n27             parent_stress_w4       62.80\n28            deviant_esteem_w5       62.53\n29         parent_attachment_w1       62.00\n30             friend_stress_w1       61.78\n31                neg_esteem_w5       61.55\n32           academic_stress_w3       60.54\n33            deviant_esteem_w2       60.54\n34         parent_attachment_w2       60.46\n35  higher_school_dependence_w3       60.34\n36           academic_stress_w2       59.86\n37             parent_stress_w5       59.49\n38            deviant_esteem_w1       59.29\n39                neg_esteem_w3       58.54\n40           academic_stress_w4       58.47\n41           self_confidence_w5       57.98\n42                neg_esteem_w2       57.81\n43           academic_stress_w5       55.73\n44             desire_stress_w5       55.68\n45           academic_stress_w1       55.06\n46             friend_stress_w5       54.55\n47                neg_esteem_w1       52.13\n48             parent_stress_w1       52.07\n49             desire_stress_w3       51.97\n\n\n\n\n\n\n\n\n\n\n=== Random Forest 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                37334.03           44905.28\nActual: stable                     27479.79           63292.70\n\n=== Random Forest 성능 지표 ===\n\n정확도: 0.5816\n\nRandom Forest 범주별 정밀도:\nexplorative    0.576\nstable         0.585\ndtype: float64\n\nRandom Forest 범주별 재현율:\nexplorative    0.4540\nstable         0.6973\ndtype: float64\n\nRandom Forest 범주별 F1-score:\nexplorative    0.5078\nstable         0.6362\ndtype: float64",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#xgboost",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#xgboost",
    "title": "analysis",
    "section": "XGBoost",
    "text": "XGBoost\n\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_numeric = le.fit_transform(y_train)\ny_test_numeric = le.transform(y_test)\n\nclass_labels_ordered = list(le.classes_) \nnum_classes = len(class_labels_ordered)\n\nprint(f\"Label Encoder 클래스: {class_labels_ordered} -&gt; {list(range(num_classes))}\")\n\nxgb_model = xgb.XGBClassifier(\n    objective='multi:softprob',\n    eval_metric='mlogloss',\n    num_class=num_classes,\n    seed=RANDOM_STATE,\n    use_label_encoder=False,\n    verbosity=0\n)\n\nparam_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [3, 5, 10, None],\n    'learning_rate': [0.05, 0.1],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0]\n}\n\ngrid_search = GridSearchCV(\n    estimator=xgb_model,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train_numeric, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nxgb_model_py = grid_search.best_estimator_\n\nimportance_scores = xgb_model_py.feature_importances_\nxgb_importance_df = pd.DataFrame({\n    'Feature': X_train.columns,\n    'Importance': importance_scores\n})\n\nmax_importance = xgb_importance_df['Importance'].max()\nxgb_importance_df['Importance'] = (xgb_importance_df['Importance'] / max_importance) * 100\nxgb_importance_df = xgb_importance_df.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nXGBoost 변수 중요도\")\nprint(xgb_importance_df)\n\nsns.barplot(x='Importance', y='Feature', data=xgb_importance_df)\nplt.title('Feature Importances (XGBoost)', fontsize=10)\nplt.xlabel('Importance', fontsize=8)\nplt.ylabel('Feature', fontsize=8)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\nplt.tight_layout()\nplt.savefig('xg_imp.png', dpi=300, bbox_inches='tight')\nplt.show()\n\ny_pred_proba_xgb = xgb_model_py.predict_proba(X_test)\ny_pred_xgb_numeric = np.argmax(y_pred_proba_xgb, axis=1)\ny_pred_xgb = le.inverse_transform(y_pred_xgb_numeric)\ny_pred_xgb_cat = pd.Categorical(y_pred_xgb, categories=class_labels_ordered, ordered=False)\n\ncm_xgb = confusion_matrix(y_test, y_pred_xgb_cat, sample_weight=weights_test, labels=class_labels_ordered)\nprint(\"\\n=== XGBoost 모델 혼동 행렬 ===\\n\")\ncm_df_xgb = pd.DataFrame(cm_xgb, index=[f\"Actual: {cat}\" for cat in class_labels_ordered], columns=[f\"Predicted: {cat}\" for cat in class_labels_ordered])\nprint(cm_df_xgb.round(2))\n\naccuracy_xgb = accuracy_score(y_test, y_pred_xgb_cat, sample_weight=weights_test)\nprecision_xgb_sk = precision_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\nrecall_xgb_sk = recall_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\nf1_score_xgb_sk = f1_score(y_test, y_pred_xgb_cat, sample_weight=weights_test, average=None, labels=class_labels_ordered, zero_division=0)\n\nprint(\"\\n=== XGBoost 모델 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_xgb:.4f}\")\nprint(\"\\nXGBoost 모델 범주별 정밀도:\")\nprint(pd.Series(precision_xgb_sk, index=class_labels_ordered).round(4))\nprint(\"\\nXGBoost 모델 범주별 재현율:\")\nprint(pd.Series(recall_xgb_sk, index=class_labels_ordered).round(4))\nprint(\"\\nXGBoost 모델 범주별 F1-score:\")\nprint(pd.Series(f1_score_xgb_sk, index=class_labels_ordered).round(4))\n\nLabel Encoder 클래스: ['explorative', 'stable'] -&gt; [0, 1]\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n최적의 하이퍼파라미터: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n최적 교차 검증 점수: nan\n\nXGBoost 변수 중요도\n                        Feature  Importance\n40         parent_attachment_w5  100.000000\n38                neg_esteem_w4   89.684937\n36           self_confidence_w4   87.710533\n33         parent_monitoring_w4   84.155891\n30         parent_attachment_w4   83.655487\n35             friend_stress_w4   83.593765\n23         parent_monitoring_w3   80.530685\n15             friend_stress_w2   80.180794\n12             parent_stress_w2   80.104950\n6            self_confidence_w1   78.258446\n17  higher_school_dependence_w2   77.525246\n21            deviant_esteem_w3   75.736404\n22             parent_stress_w3   74.421280\n4              desire_stress_w1   74.244553\n43         parent_monitoring_w5   74.036301\n14             desire_stress_w2   73.580856\n7   higher_school_dependence_w1   73.197128\n25             friend_stress_w3   72.865379\n47  higher_school_dependence_w5   72.309631\n31            deviant_esteem_w4   71.178528\n39           academic_stress_w4   69.950974\n48                neg_esteem_w5   69.804626\n32             parent_stress_w4   69.397591\n3          parent_monitoring_w1   68.932785\n46           self_confidence_w5   68.488029\n45             friend_stress_w5   68.380692\n26           self_confidence_w3   67.708191\n27  higher_school_dependence_w3   66.273537\n19           academic_stress_w2   65.726303\n41            deviant_esteem_w5   65.552612\n20         parent_attachment_w3   65.515610\n11            deviant_esteem_w2   65.489426\n9            academic_stress_w1   64.981789\n34             desire_stress_w4   63.373531\n49           academic_stress_w5   63.285591\n42             parent_stress_w5   62.527847\n13         parent_monitoring_w2   62.383938\n24             desire_stress_w3   62.092834\n18                neg_esteem_w2   61.555267\n29           academic_stress_w3   60.970982\n37  higher_school_dependence_w4   60.602821\n44             desire_stress_w5   59.860622\n5              friend_stress_w1   59.424625\n0          parent_attachment_w1   55.924370\n10         parent_attachment_w2   55.653919\n8                 neg_esteem_w1   52.370869\n28                neg_esteem_w3   50.224579\n2              parent_stress_w1   50.090385\n1             deviant_esteem_w1   49.659233\n16           self_confidence_w2   49.203815\n\n\n\n\n\n\n\n\n\n\n=== XGBoost 모델 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                40510.53           41728.78\nActual: stable                     25198.53           65573.96\n\n=== XGBoost 모델 성능 지표 ===\n\n정확도: 0.6132\n\nXGBoost 모델 범주별 정밀도:\nexplorative    0.6165\nstable         0.6111\ndtype: float64\n\nXGBoost 모델 범주별 재현율:\nexplorative    0.4926\nstable         0.7224\ndtype: float64\n\nXGBoost 모델 범주별 F1-score:\nexplorative    0.5476\nstable         0.6621\ndtype: float64",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#logistic-regression",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#logistic-regression",
    "title": "analysis",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm\n\nbase_log_reg = LogisticRegression(\n    solver='lbfgs',\n    max_iter=5000,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\nparam_grid = [\n    {\n        'penalty': ['l1'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['liblinear', 'saga'],\n        'class_weight': ['balanced', None]\n    },\n    {\n        'penalty': ['l2'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['lbfgs', 'liblinear', 'saga'],\n        'class_weight': ['balanced', None]\n    },\n    {\n        'penalty': ['elasticnet'],\n        'C': [0.001, 0.01, 0.1, 1, 10],\n        'solver': ['saga'],\n        'l1_ratio': [0.2, 0.5, 0.8],\n        'class_weight': ['balanced', None]\n    }\n]\n\ngrid_search = GridSearchCV(\n    estimator=base_log_reg,\n    param_grid=param_grid,\n    cv=3,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=False\n)\n\ngrid_search.fit(X_train, y_train, sample_weight=weights_train)\n\nprint(f\"\\n최적의 하이퍼파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\n\nlog_reg_model_py = grid_search.best_estimator_\nclass_labels_logreg = log_reg_model_py.classes_\ncoef_series = pd.Series(log_reg_model_py.coef_[0], index=X_train.columns)\nsorted_coefs = coef_series.reindex(coef_series.abs().sort_values(ascending=False).index)\n\nX_train_sm = sm.add_constant(X_train)\nX_test_sm = sm.add_constant(X_test)\n\ny_train_binary = (y_train == class_labels_logreg[1]).astype(int)\n\nlogit_model = sm.Logit(y_train_binary, X_train_sm)\nlogit_result = logit_model.fit(disp=0)\n\ncoef_summary = logit_result.summary2().tables[1]\ncoef_summary_df = pd.DataFrame(coef_summary)\ncoef_summary_sorted = coef_summary_df.sort_values('Coef.', key=abs, ascending=False)\nprint(\"\\n계수 및 p-value (절댓값이 큰 순서):\")\nprint(coef_summary_sorted[['Coef.', 'P&gt;|z|']])\n\ny_pred_logistic_py = log_reg_model_py.predict(X_test)\n\ntest_categories_logreg = list(y_test.cat.categories) if hasattr(y_test, 'cat') else sorted(list(y_test.unique()))\ny_pred_logistic_cat = pd.Categorical(y_pred_logistic_py, categories=test_categories_logreg, ordered=False)\n\ncm_logistic = confusion_matrix(y_test, y_pred_logistic_cat, sample_weight=weights_test, labels=test_categories_logreg)\nprint(\"\\n=== 로지스틱 회귀 모델 혼동 행렬 ===\\n\")\ncm_df_logistic = pd.DataFrame(cm_logistic, index=[f\"Actual: {cat}\" for cat in test_categories_logreg], columns=[f\"Predicted: {cat}\" for cat in test_categories_logreg])\nprint(cm_df_logistic.round(2))\n\naccuracy_logistic = accuracy_score(y_test, y_pred_logistic_cat, sample_weight=weights_test)\nprecision_logistic_sk = precision_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\nrecall_logistic_sk = recall_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\nf1_score_logistic_sk = f1_score(y_test, y_pred_logistic_cat, sample_weight=weights_test, average=None, labels=test_categories_logreg, zero_division=0)\n\nprint(\"\\n=== 로지스틱 회귀 모델 성능 지표 ===\\n\")\nprint(f\"정확도: {accuracy_logistic:.4f}\")\nprint(\"\\n로지스틱 회귀 모델 범주별 정밀도:\")\nprint(pd.Series(precision_logistic_sk, index=test_categories_logreg).round(4))\nprint(\"\\n로지스틱 회귀 모델 범주별 재현율:\")\nprint(pd.Series(recall_logistic_sk, index=test_categories_logreg).round(4))\nprint(\"\\n로지스틱 회귀 모델 범주별 F1-score:\")\nprint(pd.Series(f1_score_logistic_sk, index=test_categories_logreg).round(4))\n\n\n최적의 하이퍼파라미터: {'C': 0.1, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}\n최적 교차 검증 점수: 0.6006\n\n계수 및 p-value (절댓값이 큰 순서):\n                                Coef.     P&gt;|z|\ndesire_stress_w1             0.499608  0.000015\nself_confidence_w4          -0.332916  0.010294\nparent_stress_w3             0.271418  0.057369\nneg_esteem_w4                0.263964  0.031803\ndesire_stress_w2             0.255400  0.018759\nself_confidence_w3          -0.254064  0.030035\nfriend_stress_w4            -0.247249  0.024386\nconst                        0.245302  0.000002\ndeviant_esteem_w3           -0.237610  0.068700\nparent_monitoring_w3         0.211999  0.050412\nparent_attachment_w3         0.184476  0.274346\nparent_monitoring_w4         0.180603  0.106346\nhigher_school_dependence_w5 -0.164889  0.024659\nacademic_stress_w2          -0.156225  0.223868\nparent_attachment_w5         0.151586  0.345721\ndesire_stress_w3            -0.149660  0.181854\nself_confidence_w2          -0.140414  0.185298\ndeviant_esteem_w1           -0.138070  0.263940\nparent_stress_w4            -0.137391  0.383677\nfriend_stress_w1            -0.136979  0.134193\nacademic_stress_w5           0.118084  0.270881\nparent_monitoring_w1         0.113677  0.264741\nacademic_stress_w1          -0.112715  0.410824\ndesire_stress_w4             0.110917  0.295629\nself_confidence_w1           0.109299  0.356000\nparent_stress_w1            -0.104439  0.436255\nparent_stress_w2            -0.101390  0.460526\nparent_monitoring_w2        -0.098915  0.363357\ndesire_stress_w5             0.096226  0.359037\nneg_esteem_w1                0.096029  0.462255\nacademic_stress_w4           0.091028  0.451824\nhigher_school_dependence_w2 -0.086012  0.175999\nhigher_school_dependence_w1 -0.082377  0.182057\nparent_attachment_w4        -0.078615  0.648722\ndeviant_esteem_w4           -0.077995  0.525561\nneg_esteem_w5               -0.077245  0.532354\nparent_monitoring_w5         0.075278  0.495239\nfriend_stress_w5            -0.073577  0.475849\nfriend_stress_w2            -0.069986  0.466378\nhigher_school_dependence_w4 -0.059155  0.445485\nparent_stress_w5            -0.053443  0.710032\nneg_esteem_w3               -0.045219  0.687428\nparent_attachment_w1        -0.035101  0.822711\nhigher_school_dependence_w3  0.033887  0.623319\ndeviant_esteem_w5           -0.024602  0.843547\nneg_esteem_w2                0.021549  0.824973\nacademic_stress_w3          -0.017071  0.879205\nself_confidence_w5           0.015841  0.893445\nfriend_stress_w3            -0.010240  0.924892\nparent_attachment_w2         0.006187  0.967933\ndeviant_esteem_w2            0.002349  0.982333\n\n=== 로지스틱 회귀 모델 혼동 행렬 ===\n\n                     Predicted: explorative  Predicted: stable\nActual: explorative                42190.16           40049.15\nActual: stable                     26537.42           64235.07\n\n=== 로지스틱 회귀 모델 성능 지표 ===\n\n정확도: 0.6151\n\n로지스틱 회귀 모델 범주별 정밀도:\nexplorative    0.6139\nstable         0.6160\ndtype: float64\n\n로지스틱 회귀 모델 범주별 재현율:\nexplorative    0.5130\nstable         0.7076\ndtype: float64\n\n로지스틱 회귀 모델 범주별 F1-score:\nexplorative    0.5589\nstable         0.6586\ndtype: float64",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#모델-성능-비교",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#모델-성능-비교",
    "title": "analysis",
    "section": "모델 성능 비교",
    "text": "모델 성능 비교\n\nimport matplotlib.cm as cm\n\nmodel_metrics_list = []\nclass_labels_ordered = list(y_test.cat.categories)\n\ndef compile_metrics(model_name, accuracy, precision_arr, recall_arr, f1_arr):\n    metrics = {'Model': model_name, 'Accuracy': accuracy}\n    for i, label in enumerate(class_labels_ordered):\n        metrics[f'Precision ({label})'] = precision_arr[i]\n        metrics[f'Recall ({label})'] = recall_arr[i]\n        metrics[f'F1-score ({label})'] = f1_arr[i]\n    return metrics\n\nmodel_metrics_list.append(compile_metrics(\n    \"Random Classifier\",\n    accuracy_random,\n    precision_random_sk,\n    recall_random_sk,\n    f1_score_random_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"Random Forest\",\n    accuracy_rf,\n    precision_rf_sk,\n    recall_rf_sk,\n    f1_score_rf_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"XGBoost\",\n    accuracy_xgb,\n    precision_xgb_sk,\n    recall_xgb_sk,\n    f1_score_xgb_sk\n))\nmodel_metrics_list.append(compile_metrics(\n    \"Logistic Regression\",\n    accuracy_logistic,\n    precision_logistic_sk,\n    recall_logistic_sk,\n    f1_score_logistic_sk\n))\n\ncomparison_df = pd.DataFrame(model_metrics_list).set_index('Model')\nmodels = comparison_df.index.tolist()\nall_categories = ['Accuracy']\nfor label in class_labels_ordered:\n    all_categories.extend([\n        f'Precision ({label})',\n        f'Recall ({label})',\n        f'F1-score ({label})'\n    ])\nangles = np.linspace(0, 2*np.pi, len(all_categories), endpoint=False).tolist()\nangles += angles[:1]\nax = plt.subplot(111, polar=True)\ncolors = cm.tab10(np.linspace(0, 1, len(models)))\nfor i, model in enumerate(models):\n    values = comparison_df.loc[model, all_categories].values.flatten().tolist()\n    values += values[:1]\n    ax.plot(angles, values, 'o-', linewidth=2, color=colors[i], label=model, alpha=0.8)\n    ax.fill(angles, values, color=colors[i], alpha=0.1)\nax.set_xticks(angles[:-1])\nax.set_xticklabels(all_categories, fontsize=10)\n\nax.set_ylim(0, 1)\nax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\nax.grid(True, linestyle='-', alpha=0.3)\nplt.title('모델 성능 비교', size=15, y=1.1)\nplt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\nplt.savefig('model_met.png', dpi=300, bbox_inches='tight')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#roc-커브",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/15.html#roc-커브",
    "title": "analysis",
    "section": "ROC 커브",
    "text": "ROC 커브\n\nrandom_pred_proba = np.zeros((len(y_test), len(class_labels_ordered)))\nfor i, cls in enumerate(class_labels_ordered):\n    random_pred_proba[:, i] = prop_values_ordered[i]\n\npred_probas = {\n    \"Random Classifier\": random_pred_proba,\n    \"Random Forest\": y_pred_proba_rf,\n    \"XGBoost\": y_pred_proba_xgb,\n    \"Logistic Regression\": log_reg_model_py.predict_proba(X_test)\n}\n\ncolors = {\n    \"Random Classifier\": \"grey\",\n    \"Random Forest\": \"forestgreen\",\n    \"XGBoost\": \"darkorange\",\n    \"Logistic Regression\": \"navy\"\n}\n\ny_test_numeric = y_test.cat.codes if hasattr(y_test, 'cat') else y_test\n\nfor model_name, proba in pred_probas.items():\n    if proba.shape[1] &gt; 1:\n        y_score = proba[:, 1]\n    else:\n        y_score = proba.ravel()\n    fpr, tpr, _ = roc_curve(y_test_numeric, y_score, sample_weight=weights_test)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})', color=colors[model_name])\n\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC 커브', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.gcf().set_size_inches(7, 7)\nplt.tight_layout()\nplt.savefig('roc_curve_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "analysis"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/07.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/07.html",
    "title": "ensemble",
    "section": "",
    "text": "다수의 모델을 학습시켜 결과를 종합하여 예측 성능을 높이는 방법",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "ensemble"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/07.html#techniques",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/07.html#techniques",
    "title": "ensemble",
    "section": "Techniques",
    "text": "Techniques\n\nstacking:\n\n여러 모델을 학습시킨 후, 각 모델의 예측 결과를 입력으로 하는 메타 모델을 학습시킨다.\n메타 모델은 다른 모델들의 예측 결과를 종합하여 최종 예측을 수행한다.\n\nbagging\n\nvs cross validation:\n\ncross validation은 이미 생성된 모델을 검증하기 위한 방법. 모델 구축 방법은 아님\nbagging은 분산을 줄이기 위해 사용함\n\nbagging의 voting, averaging은 unsupervised learning\n\nboosting: sequentially 학습\n\n이전 모델의 오차를 보완하는 방식으로 학습한다.\nsupervised learning",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "ensemble"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/12.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/12.html",
    "title": "Dataminig 1차 발표 ppt",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "Dataminig 1차 발표 ppt"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/05.html#pattern-minig",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/05.html#pattern-minig",
    "title": "association rule mining",
    "section": "Pattern minig",
    "text": "Pattern minig\n\nBasic Concepts\n\npattern: dataset 안에서 함께 자주 발생하는 subsequences, substructures, set of items\n\n이 pattern은 인과관계를 의미하진 않는다.\n\nAssociation rule minig: 최소 지지도나 신뢰도를 넘는 모든 항목에 대해 pattern을 찾는다.\n\n\n\nApplications\n\nassociation rule, correlation, classification, clustering data mining의 기반이 될 수 있다.\n장바구니 분석\n연속 구매 분석\n\n\n\nTerminologies\n\n지지도(Support): 전체 거래 중 특정 항목 집합이 포함된 거래의 비율.\n신뢰도(Confidence): 항목 X를 포함하는 거래 중에서 항목 Y도 함께 포함하는 거래의 비율.\n빈발 패턴(frequent): 최소 지지도를 넘는 pattern\n\n빈발 항목 집합(frequent itemset): 단순한 묶음\n빈발 시퀀스\n\n\n\n\nclosed pattern\n\nx가 빈발이고, 지지도가 상위 집합들과 다른 집합 (지지도는 상위로 갈 수록 떨어짐)\n지지도 정보를 유지할 수 있다.\n\n신뢰도 계산할 때 사용할 수 있음\n\n\n\n\nmax patterns\n\nx가 빈발이고, 상위 집합들 모두가 빈발 집합이 아닌 집합\n지지도 정보는 유지되지 않음.\n\n신뢰도 계산할 때 사용할 수 없어서 사실 상 결과 요약 외의 용도는 없음\n\nDownward closure property: 어떤 itemset이 빈발하지 않으면, 그 모든 superset은 무조건 빈발하지 않는다. 대우도 성립\n\n교수님은 anti-monotone property로 설명하셨지만 이게 더 자주 사용되는 용어",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/05.html#association-rule",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/05.html#association-rule",
    "title": "association rule mining",
    "section": "Association Rule",
    "text": "Association Rule\n\nfind frequent itemsets\n\nApriori (breadth-first search)\nFP-Growth\nEclat (depth-first search)\n\ngenerate association rules\n\n모든 빈발 itemset I에 대해 모든 I의 subset s로 ‘s -&gt; (I - s)’ 규칙을 생성\n최소 신뢰도 조건을 만족하는 규칙만 남김",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/05.html#algorithm",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/05.html#algorithm",
    "title": "association rule mining",
    "section": "Algorithm",
    "text": "Algorithm\n\nApirori\n\nMonotone 성질을 이용하여 빈발하지 않는 집합은 후보에서 제거\n\n\nscan DB once to get 1-itemsets\n반복\n\nk개의 itemset에 대해 k+1-itemset의 후보를 생성\n\nself-join: k-itemset을 두 개 합쳐서 k+1-itemset을 생성\nprune: k+1-itemset을 생성할 때, k-itemset의 subset이 모두 빈발해야 k+1-itemset이 빈발할 수 있다.\n\nk+1-itemset 후보에 대해 DB를 scan하여 빈발한 itemset을 찾는다.\nk += 1\n빈발 itemset이 없으면 종료\n\n\n\nApriori의 단점: DB scan을 여러 번 해야함, 후보가 많아질 수 있음\n\n후보 수를 줄이는 방법: Hashing\n\n\n\n\nDHP (Direct Hashing and Pruning)\n\nHash값이 같은 itemset의 count를 합하고, minimum support를 넘는 itemset만 남김\nHash table을 매번 만드는 번거로움이 있지만, apriori보다 빠름\n반복\n\n빈발 항목 찾기, 후보 해시 테이블 생성\n\n데이터베이스를 scan하여 최소 지지도를 넘는 1-itemset 후보를 찾음\n동시에 조합 가능한 2-itemset을 만들어 mapping된 해시 테이블 bucket에 count += 1\n\n가지치기\n\n1-itemset 후보를 이용해 self-join, prune으로 2-itemset 후보 생성\n완성된 후보를 1단계에서 만든 hash table에 매핑해서 최소 지지도를 넘는 2-itemset bucket이 아닐 경우 배제",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "association rule mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/03.html#장점",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/03.html#장점",
    "title": "random forest",
    "section": "장점",
    "text": "장점\n\nClassification, Regression문제 모두 해결 가능\nAccuracy, out-of-bag error에 우수한 결과\nValidation을 위한 별도의 data set이 필요하지 않음\nBuilt-in validation set\nOverfitting이 없다\nOutlier에 강함\nMissing data를 잘 처리\n선처리 작업을 최소화\nFeature의 선택을 자동처리\n변수 삭제 없이 수천 개의 입력 변수를 처리",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "random forest"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/03.html#단점",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/data_mining/03.html#단점",
    "title": "random forest",
    "section": "단점",
    "text": "단점\n\n속도가 느림\n해석이 어렵다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Data Mining",
      "random forest"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/02.html#business-analytics",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/02.html#business-analytics",
    "title": "Intro",
    "section": "Business Analytics",
    "text": "Business Analytics\n\nData Analysis\n\nDescriptive Analytics: What happened?\nPredictive Analytics: What will happen?\n\nOperations Research\n\nPrescriptive Analytics: What should we do? (Optimization)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/02.html#process-of-or-study",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/02.html#process-of-or-study",
    "title": "Intro",
    "section": "Process of OR Study",
    "text": "Process of OR Study\n\n\n\n\n\nflowchart LR\n  A(Collect data) --&gt; B(Define the problem)\n  B --&gt; C{Data are sufficient?}\n  C --&gt;|No| A\n  C --&gt;|Yes| D(Formulate a model)\n  D --&gt; E(Solve the model)\n  E --&gt; F{Model is good?}\n  F --&gt;|Yes| G(Interpret results make suggestions)\n  F --&gt;|No| D",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/02.html#lp-model-표준형",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/02.html#lp-model-표준형",
    "title": "Intro",
    "section": "LP Model (표준형)",
    "text": "LP Model (표준형)\n\n제한된 자원을 경쟁하는 활동들에게 가능한 최적으로 분배하거나 이와 비슷한 수학적 구조를 가진 문제를 다루는 방법\n\n\\[\\begin{aligned}\nmax & \\sum_{i=1}^{n} c_i x_i \\\\\ns.t. & \\sum_{i=1}^{n} a_{ij} x_i \\leq b_j, j ≤ m \\\\\n& x_1, x_2, ..., x_n ≥ 0\n\\end{aligned}\\]",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/02.html#lp의-가정",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/02.html#lp의-가정",
    "title": "Intro",
    "section": "LP의 가정",
    "text": "LP의 가정\n\n선형계획은 현실을 단순화한 모델로, 아래의 네 가지 가정이 완벽히 맞지 않을 수 있음.\n작은 불일치는 허용 가능하며, 민감도 분석으로 보완.\n심각한 위반 시 대안 모델(비선형계획, 정수계획 등)을 사용하나, 선형계획의 강력한 알고리즘이 유리하므로 초기 분석에 활용 후 필요 시 복잡한 모델로 전환.\n\n\n비례성(Proportionality)\n\n정의: 목적함수와 제약식에서 활동 수준(예: xx)에 대한 기여도가 선형(비례적)으로 표현됨.\n위반 사례:\n\n초기 투자비용(고정비용)이 있어 \\(Z=3x_1−1\\)이 되는 경우, 비례성이 깨짐.\n규모의 경제로 한계 이익이 증가하면 비례성이 위반됨.\n한계 이익이 감소(예: 마케팅 비용 증가)하면 역시 비례성이 깨짐.\n\n대안: 비례성이 깨지면 비선형계획(12장)이나 혼합정수계획(11장)을 고려.\n\n가합성(Additivity)\n\n정의: 목적함수와 제약식의 값이 각 활동의 개별 기여도의 합으로 표현됨. 즉, 변수 간 교차곱이 없음.\n위반 사례:\n\n제품 간 보완적 상호작용(예: 공동 광고 효과)으로 \\(Z=3x_1+5x_2+x_1x_2\\)가 됨.\n경쟁적 상호작용(예: 설비 공유로 비효율 발생)으로 \\(Z=3x_1+5x_2−x_1x_2\\)가 됨.\n\n대안: 가합성이 위반되면 비선형계획(12장)으로 전환.\n\n가분성(Divisibility)\n\n정의: 의사결정 변수가 실수 값을 가질 수 있음. 즉, 활동 수준이 정수로 제한되지 않음.\n위반 사례: 변수가 정수로 제한되면(예: 배치 단위가 1, 2, 3만 가능) 가분성이 깨짐.\n대안: 정수계획(11장) 사용.\n\n확실성(Certainty)\n\n정의: 모델의 매개변수(예: \\(c_j, a_{ij}, b_i\\))가 알려진 상수로 고정. 해당 상수는 미래 예측에 기반하므로 불확실성이 존재.\n대응: 불확실성이 크면 민감도 분석(6.7절)으로 최적해의 변화를 확인하거나, 확률변수를 도입한 모델(23장) 사용.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/16.html#최단-경로-문제",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/16.html#최단-경로-문제",
    "title": "네트워크 최적화 모형",
    "section": "3. 최단 경로 문제",
    "text": "3. 최단 경로 문제\n\n그냥 다익스트라",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/16.html#최대-흐름-문제-augmenting-path-method",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/16.html#최대-흐름-문제-augmenting-path-method",
    "title": "네트워크 최적화 모형",
    "section": "5. 최대 흐름 문제 (augmenting path method)",
    "text": "5. 최대 흐름 문제 (augmenting path method)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/16.html#네트워크-심플렉스-해법",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/16.html#네트워크-심플렉스-해법",
    "title": "네트워크 최적화 모형",
    "section": "7. 네트워크 심플렉스 해법",
    "text": "7. 네트워크 심플렉스 해법\n모든 실행가능 해는 n-1개의 기저변수를 가지고, spanning tree를 형성한다.\n\n최대흐름문제 심플렉스 (9.7.2) 이거 어케 품\nmingamdo variable add?",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "네트워크 최적화 모형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/15.html#overview",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/15.html#overview",
    "title": "수송문제와 할당 문제들",
    "section": "Overview",
    "text": "Overview\n\n수송문제: 여러 공급지로부터 여러 수요지까지 상품을 운송하는 최적의 방법을 결정하는 문제를 다루지만, 그 적용 범위는 생산 일정 계획과 같이 실제 수송과 직접적인 관련이 없는 경우까지 확장된다.\n할당문제: 주로 인력이나 자원을 특정 과업(tasks)에 배정하는 문제를 다룬다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/15.html#수송문제를-위한-능률적인-심플렉스-방법",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/15.html#수송문제를-위한-능률적인-심플렉스-방법",
    "title": "수송문제와 할당 문제들",
    "section": "2. 수송문제를 위한 능률적인 심플렉스 방법",
    "text": "2. 수송문제를 위한 능률적인 심플렉스 방법\n\n가정\n\n공급량과 수요량은 일치\n\n일치하지 않으면 dummy 공급지를 추가, 비용은 0으로 설정\n불가능한 연결은 무한대 비용으로 설정\n수요가 정수가 아닌 범위일 경우\n\n비용은 분배되는 상품 양(\\(x_{ij}\\))에 비례\n\n\n\n수식\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n창고 (트럭당 운송비용, $)\n\n생산량 (트럭분)\n\n\n\n\n\n\n새크라멘토\n솔트레이크시티\n래피드시티\n앨버커키\n\n\n\n공장 1 (벨링햄)\n\n464\n513\n654\n867\n75\n\n\n공장 2 (유진)\n\n352\n416\n690\n791\n125\n\n\n공장 3 (앨버트 리)\n\n995\n682\n388\n685\n100\n\n\n창고 배정량 (트럭분)\n\n80\n65\n70\n85\n(총 300)\n\n\n\n\n\n\n수송문제 example\n\n\n\n\\(Z= \\sum_{i=1}^{m} \\sum_{j=1}^{n} c_{ij} x_{ij}\\)\n공급 제약식: \\(\\sum_{j=1}^{n} x_{ij} = s_i \\quad \\text{for } i=1,2,\\ldots,m\\)\n수요 제약식: \\(\\sum_{i=1}^{m} x_{ij} = d_j \\quad \\text{for } j=1,2,\\ldots,n\\)\n비음 제약식: \\(x_{ij} \\geq 0 \\quad \\text{for } i=1,2,\\ldots,m, j=1,2,\\ldots,n\\)\n\n\n\n수송문제를 위한 매개변수 표\n\n\n일반적인 심플렉스 방법도 수송문제를 푸는 데 적용할 수 있지만, 수송문제의 규모가 크고 제약식 행렬이 대부분 0과 일부 1로 구성된 희소 행렬(sparse matrix)이라는 특성 때문에 비효율적이다. 일반 심플렉스 방법은 초기화 단계에서 많은 인공 변수를 필요로 할 수 있다.\n수송문제에서 기저가능해는 항상 m+n−1개의 기저 변수를 가진다.\n\n\n\n\n초기 bfs를 만들기 위한 절차\n\n\n\n문제 예시\n\n\n\n여전히 고려중인 행들과 열들로부터, 어떤 기준에 따라 다음 기저변수(할당)를 선택한다.\n\n북서모서리법\n\n할당을 충분히 크게 하여 그 행에 있는 남은 공급이나 그 열에 있는 남은 수요를 다 써버리게 한다(둘 중 더 작은 것).\n더 이상의 고려에서 그 행이나 열(둘 중에서 더 작게 남은 공급 혹은 수요를 가진 것)을 제거한다(만약 행과 열이 같은 남은 공급과 수요를 가지면, 임의로 행을 선택하여 제거한다. 열은 후에 퇴화기저변수, 즉 할당 0으로 표시됨을 제공하는 것으로 사용될 것이다).\n만약 단지 하나의 행 혹은 하나의 열이 고려 대상으로 남아 있으면, 가능한 할당과 함께 기저가 되는 그 행이나 열과 연관된 모든 남아 있는 변수(즉 전에 기저로 선택되지 않았고 행 과 열을 제거함으로써 고려 대상에서 제외되지 않은 변수들)를 선택함으로써 절차는 종결된다.\n\n\n\n\n최적화 검사 절차\n\n가장 많은 할당이 일어난 행의 변수 하나를 0으로 설정\n기저인 \\(x_{ij}\\)의 \\({i, j}\\)에 대해 \\(c_{ij} = u_i + v_j\\)를 만족한다는 성질로 \\(u_i\\)와 \\(v_j\\)를 계산한다.\n비기저 변수들의 \\(c_{ij} - u_i - v_j\\)를 계산한다.\n모두 양수이면 최적.\n\n\n\n반복\n\n진입기저변수를 결정하라: 가장 큰(절댓값으로) 음의 값 \\(C_{jj} - u_i - v_j\\)를 가지는 비기저변수 \\(x_{ij}\\)를 선택하라.\n탈락기저변수를 결정하라: 진입기저변수가 증가할 때 가능을 유지하기 위해 요구되는 연쇄반응을 식별하라. 기증셀들 중에서, 가장 작은 값을 가지는 기저변수를 선택하라.\n새 기저가능해를 결정하라: 탈락변수의 값을 각 수신셀의 할당에 더하라. 그 값을 각 기증 셀의 할당에서 빼어라.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "수송문제와 할당 문제들"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/10.html#쌍대-심플렉스-방법",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/10.html#쌍대-심플렉스-방법",
    "title": "선형계획을 위한 다른 알고리즘들",
    "section": "쌍대 심플렉스 방법",
    "text": "쌍대 심플렉스 방법\n\n쌍대 문제 제약식에 -1 하고 slack 변수 추가해서 반대로 품.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "선형계획을 위한 다른 알고리즘들"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/10.html#상한-기법",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/10.html#상한-기법",
    "title": "선형계획을 위한 다른 알고리즘들",
    "section": "상한 기법",
    "text": "상한 기법\n\n일단은 대수적으로 푸는 방법만 배움.\n변수가 0일 때 뿐만 아니라 upper bound일 때도 non-basic variable로 취급.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "선형계획을 위한 다른 알고리즘들"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/04.html#overview",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/04.html#overview",
    "title": "Simplex Method (part 5)",
    "section": "Overview",
    "text": "Overview\n\n심플렉스 방법의 기하학적·대수적 원리, 행렬형 알고리즘, 그리고 그 실용적 응용(민감도 분석 등)을 체계적으로 설명\n심플렉스 방법은 선형계획 문제에서 최적해를 꼭짓점 가능해(CPF)에서 찾으며, 행렬 연산을 통해 컴퓨터로 효율적으로 구현할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/04.html#simplex-방법의-기초",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/04.html#simplex-방법의-기초",
    "title": "Simplex Method (part 5)",
    "section": "Simplex 방법의 기초",
    "text": "Simplex 방법의 기초\n\n꼭짓점 가능해와 제약식 경계\n\n선형계획 문제의 해는 가능해 영역(feasible region)의 경계에 존재한다.\n이 제약식들을 등호(=)로 바꾼 제약식 경계식을 만들 수 있다.\n\n함수 제약식\n\n≤: slack variable\n=: artificial variable\n≥: surplus variable, artificial variable\n\n비음 제약식\n\nnon-restricted: \\(x_i = x_i^{+} - x_i^{-}\\)\n≤ 0: \\(x_i = -x_i^{+}\\)\n\n\n이 경계식들은 2차원에서는 선, 3차원에서는 평면, n차원에서는 초평면(hyperplane)을 형성한다.\n\n\n\n꼭짓점 가능해의 세 가지 주요 속성\n\n최적해의 위치\n\n최적해가 유일하면, 그것은 꼭짓점 가능해이다.\n최적해가 여러 개라면, 그 중 적어도 두 개는 인접 꼭짓점 가능해이다.\n즉, 최적해는 항상 꼭짓점 가능해(혹은 그 선분)에 존재한다.\n\n유한성\n\n꼭짓점 가능해의 개수는 유한합니다.\nm+n개의 제약식 중 n개를 선택하는 조합의 수는 유한하므로, 이론적으로 모든 꼭짓점 가능해를 열거해 비교할 수도 있습니다. 하지만 실제로는 심플렉스 방법이 훨씬 적은 수만 탐색한다.\n\n최적성의 충분조건\n\n인접 꼭짓점 중 더 좋은 해가 없으면, 현재 해가 최적해임이 보장된다.\n\n\n\n\n심플렉스 방법의 핵심 알고리즘 구조\n심플렉스 방법은 다음과 같은 반복 구조를 가진다\n\n초기 꼭짓점 가능해(기저해) 선택\n인접 꼭짓점으로 이동(목적함수 값이 개선되는 방향)\n더 이상 개선이 불가능하면 종료, 그 해가 최적해임을 보장",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/04.html#행렬형의-simplex",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/04.html#행렬형의-simplex",
    "title": "Simplex Method (part 5)",
    "section": "행렬형의 Simplex",
    "text": "행렬형의 Simplex\n\n행렬형 심플렉스 방법의 기본 구조\n표준형 선형계획 문제를 다음과 같이 쓸 수 있다.\n\\[\n\\begin{aligned}\n\\text{Maximize} \\quad &Z=c^Tx \\\\\n\\text{Subject to} \\quad &Ax=b, x≥0 \\\\\n\\end{aligned}\n\\]\n\n여기서 A는 m×n 행렬, x는 n차원 변수 벡터, b는 m차원 상수 벡터, c는 n차원 계수 벡터이다.\n여유변수(slack variable)등을 도입해 모든 제약식을 등식으로 바꾼다.\n\n\n\n행렬 연산을 활용한 반복 과정\n\n(제일 처음 단계의 경우 3, 4단계 먼저 진행)\n\n\n진입기저변수(Entering Variable) 선택\n탈락기저변수(Leaving Variable) 선택\n\n최소비율법(minimum ratio test) 사용\n\n새로운 기저 가능해 결정\n\n기저변수 식별\n기저행렬(Basic Matrix, B): m개의 기저변수에 대해 m×m 행렬 \\(B\\)와 \\(B^{-1}\\)를 만든다. 1\n기저해(Basic Solution) 계산: \\(x_B=B^{-1}b\\)\n목적함수 값 계산: \\(Z=c_B^TB^{−1}b\\)\n\n최적화 검사\n\n비기저변수의 계수(감소계수, reduced cost)를 계산\n\n계산식: \\(c_B^TB^{−1}a_n - c_n\\)\nslack 변수: \\(c_B^TB^{-1}\\)\n\n최적일 경우 종료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/04.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/04.html#footnotes",
    "title": "Simplex Method (part 5)",
    "section": "각주",
    "text": "각주\n\n\n역행렬 구하는 법. 2차원 말고는 그냥 그 방식으로 풀자.↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex Method (part 5)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/01.html#general",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/01.html#general",
    "title": "Simplex 표 계산",
    "section": "General",
    "text": "General\n\nimport numpy as np\nfrom fractions import Fraction\nfrom tabulate import tabulate\n\n# Convert all elements to Fraction\ndef to_fraction(array):\n    return [Fraction(x).limit_denominator() if isinstance(x, (int, float)) else x for x in array]\n\n# 초기 설정\nobj = [5, -5, -13, 0, 0, -10, 0]\nA = [\n    [-1, 1, 3, 1, 0, 3, 20],\n    [12, 4, 10, 0, 1, 5, 90],\n]\nbasic = np.array([4, 5]) - 1  # x5, x6\nnon_basic = np.array([1, 2, 3]) - 1  # x1, x2, x3, x4\n\n\n# 초기 배열을 분수로 변환\nobj = to_fraction(obj)\nA = [to_fraction(row) for row in A]\n\ndef print_table():\n    headers = [\"\", \"Z\"] + [f\"x{i+1}\" for i in range(len(obj)-1)] + [\"RHS\"]\n    table = [[\"Z\", 1] + [str(x) for x in obj]]\n    for i in range(len(A)):\n        row = [f\"x{basic[i]+1}\", 0] + [str(x) for x in A[i]]\n        table.append(row)\n    print(\"\\nCurrent Simplex Tableau:\")\n    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n\ndef simplex():\n    global obj, A, basic, non_basic\n    \n    iteration = 1\n    while True:\n        print(f\"\\nIteration {iteration}\")\n        print(\"=\" * 60)\n        print_table()\n\n        # 음의 계수 찾기 (entering variable)\n        min_rc_idx = None\n        min_rc = Fraction(0)\n        for j in range(len(obj) - 1):  # RHS 제외\n            if obj[j] &lt; min_rc:\n                min_rc = obj[j]\n                min_rc_idx = j\n        \n        # 종료 조건: 음수 계수가 없으면 최적\n        if min_rc_idx is None or min_rc &gt;= 0:\n            print(\"Optimal solution reached.\")\n            solution = {f\"x{i+1}\": Fraction(0) for i in range(len(obj)-1)}\n            for i, var_idx in enumerate(basic):\n                solution[f\"x{var_idx+1}\"] = A[i][-1]\n            print(\"Optimal Solution:\")\n            for var, val in solution.items():\n                print(f\"{var} = {val}\")\n            print(f\"Objective Value = {obj[-1]}\")\n            break\n\n        # Pivot 열 선택 및 ratio 계산\n        ratios = []\n        for i in range(len(A)):\n            if A[i][min_rc_idx] &gt; 0:\n                ratios.append((A[i][-1] / A[i][min_rc_idx], i))\n            else:\n                ratios.append((float('inf'), i))\n        \n        min_ratio, pivot_row = min(ratios)\n        if min_ratio == float('inf'):\n            print(\"Unbounded solution detected.\")\n            break\n\n        print(f\"Entering variable: x{min_rc_idx + 1}\")\n        print(f\"Leaving variable: x{basic[pivot_row] + 1}\")\n\n        # Pivot 연산\n        pivot = A[pivot_row][min_rc_idx]\n        A[pivot_row] = [x / pivot for x in A[pivot_row]]\n        \n        # 다른 행 업데이트\n        for i in range(len(A)):\n            if i != pivot_row:\n                factor = A[i][min_rc_idx]\n                A[i] = [A[i][j] - factor * A[pivot_row][j] for j in range(len(obj))]\n        \n        # 목적함수 업데이트\n        factor = obj[min_rc_idx]\n        obj = [obj[j] - factor * A[pivot_row][j] for j in range(len(obj))]\n        \n        # 기본 변수 업데이트\n        leaving_var = basic[pivot_row]\n        basic[pivot_row] = min_rc_idx\n        non_basic[non_basic == min_rc_idx] = leaving_var\n        \n        iteration += 1\n\nsimplex()\n\n\nIteration 1\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+------+------+------+-------+\n|    |   Z |   x1 |   x2 |   x3 |   x4 |   x5 |   x6 |   RHS |\n+====+=====+======+======+======+======+======+======+=======+\n| Z  |   1 |    5 |   -5 |  -13 |    0 |    0 |  -10 |     0 |\n+----+-----+------+------+------+------+------+------+-------+\n| x4 |   0 |   -1 |    1 |    3 |    1 |    0 |    3 |    20 |\n+----+-----+------+------+------+------+------+------+-------+\n| x5 |   0 |   12 |    4 |   10 |    0 |    1 |    5 |    90 |\n+----+-----+------+------+------+------+------+------+-------+\nEntering variable: x3\nLeaving variable: x4\n\nIteration 2\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+-------+------+------+-------+\n|    |   Z | x1   | x2   |   x3 | x4    |   x5 |   x6 | RHS   |\n+====+=====+======+======+======+=======+======+======+=======+\n| Z  |   1 | 2/3  | -2/3 |    0 | 13/3  |    0 |    3 | 260/3 |\n+----+-----+------+------+------+-------+------+------+-------+\n| x3 |   0 | -1/3 | 1/3  |    1 | 1/3   |    0 |    1 | 20/3  |\n+----+-----+------+------+------+-------+------+------+-------+\n| x5 |   0 | 46/3 | 2/3  |    0 | -10/3 |    1 |   -5 | 70/3  |\n+----+-----+------+------+------+-------+------+------+-------+\nEntering variable: x2\nLeaving variable: x3\n\nIteration 3\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+------+------+------+-------+\n|    |   Z |   x1 |   x2 |   x3 |   x4 |   x5 |   x6 |   RHS |\n+====+=====+======+======+======+======+======+======+=======+\n| Z  |   1 |    0 |    0 |    2 |    5 |    0 |    5 |   100 |\n+----+-----+------+------+------+------+------+------+-------+\n| x2 |   0 |   -1 |    1 |    3 |    1 |    0 |    3 |    20 |\n+----+-----+------+------+------+------+------+------+-------+\n| x5 |   0 |   16 |    0 |   -2 |   -4 |    1 |   -7 |    10 |\n+----+-----+------+------+------+------+------+------+-------+\nOptimal solution reached.\nOptimal Solution:\nx1 = 0\nx2 = 20\nx3 = 0\nx4 = 0\nx5 = 10\nx6 = 0\nObjective Value = 100",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/01.html#big-m-method",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/01.html#big-m-method",
    "title": "Simplex 표 계산",
    "section": "Big M Method",
    "text": "Big M Method\n\nimport numpy as np\nfrom fractions import Fraction\nfrom tabulate import tabulate\nfrom sympy import symbols, simplify, oo\n\nM = symbols('M')\n\n# 초기 설정\nobj = [20, 10, 0, M, 0, M, 0]\nA = [\n    [5, 1, -1, 1, 0, 0, 6],\n    [2, 2, 0, 0, -1, 1, 8],\n]\nbasic = np.array([4, 6]) - 1  # x5, x6\nnon_basic = np.array([1, 2, 3, 5]) - 1  # x1, x2, x3, x4\n\ndef to_fraction(array):\n    return [Fraction(x).limit_denominator() if isinstance(x, (int, float)) else x for x in array]\n\n# 초기 배열을 분수로 변환\nobj = to_fraction(obj)\nA = [to_fraction(row) for row in A]\n\ndef print_table():\n    headers = [\"\", \"Z\"] + [f\"x{i+1}\" for i in range(len(obj)-1)] + [\"RHS\"]\n    table = [[\"Z\", 1] + [str(x) for x in obj]]\n    for i in range(len(A)):\n        row = [f\"x{basic[i]+1}\", 0] + [str(x) for x in A[i]]\n        table.append(row)\n    print(\"\\nCurrent Simplex Tableau:\")\n    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n\ndef adjust_obj_for_big_m():\n    global obj\n    print(\"\\nAdjusting Objective Function for Big M Method\")\n    for i in range(len(basic)):\n        basic_var_idx = basic[i]\n        if obj[basic_var_idx] != 0:  # 인공변수일 경우(M이 포함된 경우)\n            factor = obj[basic_var_idx]\n            obj = [simplify(obj[j] - factor * A[i][j]) for j in range(len(obj))]\n\ndef simplex():\n    global obj, A, basic, non_basic\n    \n    adjust_obj_for_big_m()\n    print_table()\n    \n    iteration = 1\n    while True:\n        print(f\"\\nIteration {iteration}\")\n        print(\"=\" * 60)\n        print_table()\n\n        # 음의 계수 찾기 (entering variable)\n        eval_obj = [x.evalf(subs={M: 1e6}) if x.has(M) else float(x) for x in obj[:-1]]\n        min_rc_idx = min(range(len(obj)-1), key=lambda j: eval_obj[j])\n        \n        if eval_obj[min_rc_idx] &gt;= 0:\n            print(\"Optimal solution reached.\")\n            # 최적 해 출력\n            solution = {f\"x{i+1}\": 0 for i in range(len(obj)-1)}\n            for i, var_idx in enumerate(basic):\n                solution[f\"x{var_idx+1}\"] = A[i][-1]\n            print(\"Optimal Solution:\")\n            for var, val in solution.items():\n                print(f\"{var} = {val}\")\n            print(f\"Objective Value = {obj[-1]}\")\n            break\n\n        # Pivot 열 선택 및 ratio 계산\n        ratios = []\n        for i in range(len(A)):\n            if A[i][min_rc_idx] &gt; 0:\n                ratios.append((A[i][-1] / A[i][min_rc_idx], i))\n            else:\n                ratios.append((oo, i))\n        \n        min_ratio, pivot_row = min(ratios)\n        if min_ratio == oo:\n            print(\"Unbounded solution detected.\")\n            break\n\n        print(f\"Entering variable: x{min_rc_idx + 1}\")\n        print(f\"Leaving variable: x{basic[pivot_row] + 1}\")\n\n        # Pivot 연산\n        pivot = A[pivot_row][min_rc_idx]\n        A[pivot_row] = [simplify(x / pivot) for x in A[pivot_row]]\n        \n        # 다른 행 업데이트\n        for i in range(len(A)):\n            if i != pivot_row:\n                factor = A[i][min_rc_idx]\n                A[i] = [simplify(A[i][j] - factor * A[pivot_row][j]) for j in range(len(obj))]\n        \n        # 목적함수 업데이트\n        factor = obj[min_rc_idx]\n        obj = [simplify(obj[j] - factor * A[pivot_row][j]) for j in range(len(obj))]\n        \n        # 기본 변수 업데이트\n        leaving_var = basic[pivot_row]\n        basic[pivot_row] = min_rc_idx\n        non_basic[non_basic == min_rc_idx] = leaving_var\n        \n        iteration += 1\n\nsimplex()\n\n\nAdjusting Objective Function for Big M Method\n\nCurrent Simplex Tableau:\n+----+-----+----------+----------+------+------+------+------+-------+\n|    |   Z | x1       | x2       | x3   |   x4 | x5   |   x6 | RHS   |\n+====+=====+==========+==========+======+======+======+======+=======+\n| Z  |   1 | 20 - 7*M | 10 - 3*M | M    |    0 | M    |    0 | -14*M |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x4 |   0 | 5        | 1        | -1   |    1 | 0    |    0 | 6     |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x6 |   0 | 2        | 2        | 0    |    0 | -1   |    1 | 8     |\n+----+-----+----------+----------+------+------+------+------+-------+\n\nIteration 1\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+----------+----------+------+------+------+------+-------+\n|    |   Z | x1       | x2       | x3   |   x4 | x5   |   x6 | RHS   |\n+====+=====+==========+==========+======+======+======+======+=======+\n| Z  |   1 | 20 - 7*M | 10 - 3*M | M    |    0 | M    |    0 | -14*M |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x4 |   0 | 5        | 1        | -1   |    1 | 0    |    0 | 6     |\n+----+-----+----------+----------+------+------+------+------+-------+\n| x6 |   0 | 2        | 2        | 0    |    0 | -1   |    1 | 8     |\n+----+-----+----------+----------+------+------+------+------+-------+\nEntering variable: x1\nLeaving variable: x4\n\nIteration 2\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n|    |   Z |   x1 | x2        | x3        | x4        | x5   |   x6 | RHS          |\n+====+=====+======+===========+===========+===========+======+======+==============+\n| Z  |   1 |    0 | 6 - 8*M/5 | 4 - 2*M/5 | 7*M/5 - 4 | M    |    0 | -28*M/5 - 24 |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n| x1 |   0 |    1 | 1/5       | -1/5      | 1/5       | 0    |    0 | 6/5          |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\n| x6 |   0 |    0 | 8/5       | 2/5       | -2/5      | -1   |    1 | 28/5         |\n+----+-----+------+-----------+-----------+-----------+------+------+--------------+\nEntering variable: x2\nLeaving variable: x6\n\nIteration 3\n============================================================\n\nCurrent Simplex Tableau:\n+----+-----+------+------+------+---------+------+----------+-------+\n|    |   Z |   x1 |   x2 | x3   | x4      | x5   | x6       | RHS   |\n+====+=====+======+======+======+=========+======+==========+=======+\n| Z  |   1 |    0 |    0 | 5/2  | M - 5/2 | 15/4 | M - 15/4 | -45   |\n+----+-----+------+------+------+---------+------+----------+-------+\n| x1 |   0 |    1 |    0 | -1/4 | 1/4     | 1/8  | -1/8     | 1/2   |\n+----+-----+------+------+------+---------+------+----------+-------+\n| x2 |   0 |    0 |    1 | 1/4  | -1/4    | -5/8 | 5/8      | 7/2   |\n+----+-----+------+------+------+---------+------+----------+-------+\nOptimal solution reached.\nOptimal Solution:\nx1 = 1/2\nx2 = 7/2\nx3 = 0\nx4 = 0\nx5 = 0\nx6 = 0\nObjective Value = -45",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/01.html#grubi",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/01.html#grubi",
    "title": "Simplex 표 계산",
    "section": "Grubi",
    "text": "Grubi\n\nfrom gurobipy import *\n\nmodel = Model(\"ex4.4-6\")\nmodel.setParam(GRB.Param.OutputFlag, 0)\n\nxab = model.addVar(vtype=GRB.CONTINUOUS, name=\"xab\")\nxac = model.addVar(vtype=GRB.CONTINUOUS, name=\"xac\")\nxbd = model.addVar(vtype=GRB.CONTINUOUS, name=\"xbd\")\nxbe = model.addVar(vtype=GRB.CONTINUOUS, name=\"xbe\")\nxcd = model.addVar(vtype=GRB.CONTINUOUS, name=\"xcd\")\nxce = model.addVar(vtype=GRB.CONTINUOUS, name=\"xce\")\nxde = model.addVar(vtype=GRB.CONTINUOUS, name=\"xde\")\nxdf = model.addVar(vtype=GRB.CONTINUOUS, name=\"xdf\")\nxef = model.addVar(vtype=GRB.CONTINUOUS, name=\"xef\")\nxfa = model.addVar(vtype=GRB.CONTINUOUS, name=\"xfa\")\n\nmodel.setObjective(xfa, GRB.MAXIMIZE)\n\nmodel.addConstr(xab + xac - xfa == 0)\nmodel.addConstr(xbd + xbe - xab == 0)\nmodel.addConstr(xcd + xce - xac == 0)\nmodel.addConstr(xde + xdf - xbd - xcd == 0)\nmodel.addConstr(xef - xbe - xce - xde == 0)\nmodel.addConstr(xfa - xdf - xef == 0)\n\nmodel.addConstr(xab &lt;= 9)\nmodel.addConstr(xac &lt;= 7)\nmodel.addConstr(xbd &lt;= 7)\nmodel.addConstr(xbe &lt;= 2)\nmodel.addConstr(xcd &lt;= 4)\nmodel.addConstr(xce &lt;= 6)\nmodel.addConstr(xde &lt;= 3)\nmodel.addConstr(xdf &lt;= 6)\nmodel.addConstr(xef &lt;= 9)\n\nmodel.optimize()\n\nfor var in model.getVars():\n    print(f\"{var.varName}: {var.x}\")\nprint(\"Obj: \", model.objVal)\n\nRestricted license - for non-production use only - expires 2026-11-23\nxab: 8.0\nxac: 7.0\nxbd: 6.0\nxbe: 2.0\nxcd: 1.0\nxce: 6.0\nxde: 1.0\nxdf: 6.0\nxef: 9.0\nxfa: 15.0\nObj:  15.0",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Simplex 표 계산"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/03.html#선형-계획을-푸는-알고리즘",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/03.html#선형-계획을-푸는-알고리즘",
    "title": "Linear Programming Algorithm",
    "section": "선형 계획을 푸는 알고리즘",
    "text": "선형 계획을 푸는 알고리즘\n\n그래프를 사용하는 방법\n변수가 3개 이하인 경우, 그래프를 그려서 해를 찾을 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/OR/03.html#simplex-method",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/OR/03.html#simplex-method",
    "title": "Linear Programming Algorithm",
    "section": "Simplex Method",
    "text": "Simplex Method\n\n기하학적 이해\n\n\nInitialization: Collect 1 CFP. 일반적으로 원점을 선택\nOptimality test: find better adj\n\nObj ⋅ (adj - cur) &gt; 0: better\nObj ⋅ (adj - cur) = 0: not changed\nObj ⋅ (adj - cur) &lt; 0: worse\n\n\n\n\n대수적 풀이\n\nbasic solution: 제약식의 변수 중 일부를 기저 변수로 선택하고, 나머지를 0으로 설정하여 얻는 해.\n\n만약 기저변수가 0인 경우, 이를 퇴화라고 부른다.\n\nbasic feasible solution: 모든 변수가 0 이상인 basic solution. 즉, 제약식을 모두 만족하는 해.\n비 기저변수(non basic variable): free variable. 변수의 수 - 방정식의 수 만큼 존재.\n기저 변수(bais variable): pivot variable.\n풀이는 생략\n\n\n\nSimplex Tableau\n\n풀이는 생략\n\n\n\n비 표준형 모델에서의 적용\n\n제약식이 = 인 경우: 인공 변수 추가. 목적 함수에 Big-M 방법을 사용하여 표현.\n\nTableau에서 인공변수의 계수를 0으로 만들어서 진행.\n\n제약식이 ≥ 인 경우: slack 변수랑 surplus variable 추가. surplus variable에 대하여 목적 함수에 Big-M 방법을 사용하여 표현.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "OR",
      "Linear Programming Algorithm"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/computer/02.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/computer/02.html",
    "title": "컴퓨팅적사고 발표 ppt",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Computer",
      "컴퓨팅적사고 발표 ppt"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/00.html#서론",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/00.html#서론",
    "title": "3학년 1학기 후기",
    "section": "서론",
    "text": "서론\n\n\n\n학기 점수\n\n\n\n\n\n과목 점수\n\n\n수강 학점이 24.5 학점이긴 한데, 사이버 강의로 가득 채워서 높게 나왔다.\n실제로는 전공 과목 5개, 교양 과목 2개를 들었다.\n\n\n\n시간표\n\n\n이 중에서도 배경지식이 전혀 없는 과목은 생산 시스템 관리 1과목 뿐이라 은근히 수월하게 이번학기를 마무리 한 것 같다.\n나의 상대적으로 높은 배경지식은, 그나마 뽑을 수 있는 휴학을 오래한 경험의 장점 중 하나가 아닌가 생각한다.\n물론 다시 과거로 돌아가서 어린 김형훈에게 ‘이 장점 하나만 보고 휴학을 4년동안 하겠습니까?’ 라고 한다면 내 대답은 no.다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "3학년 1학기 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/00.html#점수",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/00.html#점수",
    "title": "3학년 1학기 후기",
    "section": "점수",
    "text": "점수\n컴퓨팅적사고 과목은 코딩 기초 과목인데, 재수강 과목이라 A- 점수가 최대 점수였다.\n그래서 데이터 마이닝 과목만 A+이 나왔다면 이번 학기 받을 수 있는 최대 점수를 얻을 수 있었다.\n점수가 잘 안나온 이유를 몇 가지 생각해 보자면,\n\n이 과목이 발표 중심이라, 상대적으로 발표 능력이 약했던 점에서 마이너스가 됐다.\n기말 분석 과제의 성과가 좋지 않았다.\n마지막 필기 시험에서 검토도 한 번 하지 않고 1등으로 시험지를 제출하고 나왔다.\n\n..정도가 있지 않을까.\n특히 3번은, 나는 시험을 볼 때, 검토를 하면 보통 실수한 문제 1, 2개가 반드시 나오는데, 감히 검토를 하지 않은것은 상당히 큰 문제였다고 할 수 있다.\n사실 이건 이 과목 시험이 마지막 시험이라 빨리 모든걸 끝내고 싶다는 나의 안일하고 연약한 마음가짐이 가장 큰 결점사항이였다고 생각한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "3학년 1학기 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/00.html#다른-부족했던-점",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/00.html#다른-부족했던-점",
    "title": "3학년 1학기 후기",
    "section": "다른 부족했던 점",
    "text": "다른 부족했던 점\n다른 과목은 A+이 나오긴 했지만, 그렇다고 완벽하게 모든걸 잘 처리하지는 않았다.\nOR 과목은 과제를 2번 정도 제출을 못했고, 생산시스템관리는 중간 퀴즈에서 평균보다 못한 점수를 받았다.\n시간이 없어서.. 보다는 시간을 효율적으로 사용하지 못한 원인이 컸다.\n복습도 그날그날 해야 하는데, 이 부분이 잘 이뤄지지 않았다.\n복습을 집에서 하려고 하면 시간이 너무 늦어져서 잘 안하게 되는거 같고, 다음 학기는 학교 안에서 복습까지 하고 집에 가는 방법을 생각해 봐야겠다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "3학년 1학기 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/00.html#결론",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/00.html#결론",
    "title": "3학년 1학기 후기",
    "section": "결론",
    "text": "결론\n그래도 저번 학기에 비하면 시간적으로도 여유가 있었고, 요령도 어느정도 터특을 한것 같다.\n요령은 1학년때 터득을 했어야 하는데, 뭐.. 지난일에 미련을 갖지 말자.\n이정도 추세라면 내가 목표하는 학점 4점은 넘기고 졸업 과업은 무난하게 수행해낼 수 있어 보인다.\n다음학기에는 재수강 과목이 전혀 없기 때문에 드디어 4.5점을 받고, 학기 1등을 해볼 수 있는 기회가 찾아왔다.\n열심히 한 번 해 보자.\n\n\n\n이번에도 어김없이 장학금은 25%",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "3학년 1학기 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/11.html#총괄생산계획",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/11.html#총괄생산계획",
    "title": "총괄생산계획",
    "section": "총괄생산계획",
    "text": "총괄생산계획\n\n\n\n중기 범위(6-18개월) 기간에 대한 유사한 제품 묶음 수준에서 수요-공급 균형을 맞추기 위한 러프한 생산 계획\n계획 수립 후 주기적으로 업데이트\n\n\n의사결정\n\n목적: 비용, 인력변동의 최소화, 이윤의 최대화, 바람직한 고객 서비스 수준 유지\n\n보통 trade-off 관계\n\n결정 사항: 고용 수준, 시간당 생산량, 재고수준, 외주 생산량 등\n\n\n\n\n전략\n\n수요대안\n\n가격책정\n판촉\n백오더(납기지연)\n신규수요 창출\n\n공급대안\n\n수요추종전략\n\n생산량을 수요에 일치되도록 조정하는 전략. 재고가 안쌓임\n재고 유지비용이 높고, 생산용량 변경에 따른 비용이 적을 때 효과적\n\n생산평준화전략\n\n생산량을 평균으로 일정하게 유지하는 전략\n안정적 산출량\n재고비용 증가, 납기 지연이나 품절에 따른 고객 서비스 저하\n\n혼합전략",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "총괄생산계획"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/11.html#최적화-기법을-통한-총괄생산계획-수립",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/11.html#최적화-기법을-통한-총괄생산계획-수립",
    "title": "총괄생산계획",
    "section": "최적화 기법을 통한 총괄생산계획 수립",
    "text": "최적화 기법을 통한 총괄생산계획 수립",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "총괄생산계획"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/00.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/00.html",
    "title": "Intro",
    "section": "",
    "text": "과목 목표: 전통적인 생산 시스템 관리 방법론을 학습\n\n생산: 유, 무형의 제품을 만드는 것\n\n제조업, 서비스업 등\n\n시스템: 투입물을 산출물로 만드는 구성 요소와 프로세스의 총체적 집합\n관리: 목표를 달성하기 위한 체계적인 의사결정\n\n목표: 비용, 품질, 납품, 유연성\n\n네 가지 모두 고려해야하고, 이 사이에는 trade-off가 존재.\n\n\n\n중간 시험범위: lec2 ~ lec9\n\n질문:\n\n\n재고 유지의 다섯 가지 이유 - 수송중 vs 안전\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/12.html#기준생산계획master-production-schedule-mps",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/12.html#기준생산계획master-production-schedule-mps",
    "title": "기준생산계획 및 자재소요계획",
    "section": "기준생산계획(Master Production Schedule, MPS)",
    "text": "기준생산계획(Master Production Schedule, MPS)\n\n총괄생산계획을 분해한 것\n보통 개별 제품에 대한 계획\n기간은 총괄생산계획에 상대적으로 정함.(분기 - 월, 월 - 주, 주 - 일 등)\n\n부품 주문부터 제품의 최종 조립이 완료될 떼까지의 총 기간은 포함해야함\n\n이어지는 부품의 제조활동이나 자재 조달 활동의 기준이 되는 계획\nRCCP를 통해 초안 생산계획을 수정\n가까운 시기의 계획은 동결하는게 바람직하다. (동결 시작 시기는 agility에 따라 다름)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "기준생산계획 및 자재소요계획"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/12.html#자재소요계획material-requirements-planning-mrp",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/12.html#자재소요계획material-requirements-planning-mrp",
    "title": "기준생산계획 및 자재소요계획",
    "section": "자재소요계획(Material Requirements Planning, MRP)",
    "text": "자재소요계획(Material Requirements Planning, MRP)\n\n부품에 대한 수요가 발생하는 양상이 완제품인 MPS랑 다름. 종속수요(다른 품목의 수요에 따라 수요가 발생)\n기준생산계획을 지키기 위한 자재로서의 품목 종류, 수량, 시점을 결정\n부품의 공급이 100% 확실하지 않음 (lead time 지연, 품질 등)\n투입물\n\n자재명세서(BOM): 한 단위의 품목 생산에 필요한 자재종류, 품목과 자재의 상하관계, 자재 사용량을 기록한 명세서\n기준생산계획\n재고 기록: 보유량, 주문량, 공급자, 리드타임, 로트사이즈 결정 방침 등 포함\n\n\n\n고려사항\n\n공통부품: LLC(Low Level Coding) 기준으로 MRP를 작성\n로트크기 규칙결정\n\nFixed Order Quantity\nPeriodic Order Quantity: 주문시 향후 T period만큼 필요한 양을 주문\nLot-for-Lot: 주문시 수요량만큼 주문\n\n안전재고: 순소요량에 안전재고 만큼 더함.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "기준생산계획 및 자재소요계획"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#생산시스템관리를-어떤-관점에서-바라보며-학습하는지",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#생산시스템관리를-어떤-관점에서-바라보며-학습하는지",
    "title": "Matching Supply with Demand",
    "section": "생산시스템관리를 어떤 관점에서 바라보며 학습하는지",
    "text": "생산시스템관리를 어떤 관점에서 바라보며 학습하는지\n\n운영하는 관점에서 수요와 공급을 바라볼 예정\n기업을 바라보는 관점: 유/무형의 제품을 생산해서 수요(양, timing, 품질, …)에 맞게 공급하기 위해 노력하는 집단",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#수요와-공급-법칙",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#수요와-공급-법칙",
    "title": "Matching Supply with Demand",
    "section": "수요와 공급 법칙",
    "text": "수요와 공급 법칙\n\n\n\n파란색 - 수요, 빨간색 - 공급\n\n\n\n경제학 기본적인 법칙. 가격 조정은 건강한 시스템의 증거라고 봄\n운영 관리자(OM)인 우리는 이거랑 다르게 바라봄.\n\nExcess demand = lost revenue\nExcess supply = wasted resources\n가격 조정만으로 수요와 공급 맞추기 어려움\n\n끊임없이 변하는 수요와 비 탄력적인 공급으로 인해 수요와 공급을 맞추기 어렵다.\n과학적 도구로 최대한 수요를 예측하고, 탄력적인 공급을 하는 방법을 찾아야 한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#수요와-공급이-안-맞는-사례",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#수요와-공급이-안-맞는-사례",
    "title": "Matching Supply with Demand",
    "section": "수요와 공급이 안 맞는 사례",
    "text": "수요와 공급이 안 맞는 사례\n\n푸바오를 보기 위해 사람들이 몰림\n\n수요 공급의 불균형은 안 좋은 효과를 가져옴\n\n마스크, 먹태깡: 수요는 빠르게 변하는데, 공급은 느리게 변함\n\n수요를 예측하고, 설비를 미리 준비하는 과학적 도구가 필요\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n소매업\n철광석 공장\n응급실\n심박조율기\n항공 여행\n\n\n\n\n공급\n소비자 전자제품\n철광석\n의료 서비스\n의료 장비\n특정 항공편 좌석\n\n\n수요\n새로운 비디오 시스템을 구매하는 소비자\n제철소\n긴급한 의료 서비스 수요\n심박조율기가 특정 시간과 장소에서 필요한 심장외과 의사\n특정 시간과 목적지로의 여행\n\n\n공급이 수요를 초과\n재고 비용이 높고, 재고 회전율이 낮음\n가격 하락\n의사, 간호사 및 인프라가 충분히 활용되지 않음\n심박조율기가 재고로 남아 있음\n빈 좌석 발생\n\n\n수요가 공급을 초과\n포기한 이익 기회; 소비자 불만족\n가격 상승\n응급실 혼잡 및 지연; 구급차 우회 가능성\n포기된 이익 (일반적으로 의료적 위험과는 관련 없음)\n초과 예약으로 인해 고객이 다른 항공편을 이용해야 함 (이익 손실)\n\n\n공급과 수요를 맞추기 위한 조치\n수요 예측; 신속한 대응\n가격이 지나치게 하락하면 생산 시설이 폐쇄됨\n예측된 수요에 맞춘 인력 배치; 우선순위 설정\n여러 장소에서 심박조율기를 보관하는 유통 시스템\n동적 가격 책정; 예약 정책\n\n\n관리적 중요성\n소비자 전자제품 소매업의 단위당 재고 비용이 종종 순이익을 초과함\n가격 경쟁이 치열하여 주요 초점은 공급 비용 절감에 맞춰짐\n치료 또는 이송 지연이 사망과 연관된 사례 있음\n대부분의 제품(가치 2만 달러)이 사용되기 전에 영업 사원의 차량 트렁크에서 4~5개월 동안 대기함1\n전체 좌석의 약 30%가 빈 채로 운항되며, 좌석 이용률이 1~2%만 증가해도 이익과 손실이 갈림2",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#생산-시스템의-performance",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#생산-시스템의-performance",
    "title": "Matching Supply with Demand",
    "section": "생산 시스템의 performance",
    "text": "생산 시스템의 performance\n\n서로 상충됨. business 목표에 맞게 balance를 잘 맞춰야함\n\ncost\nquality: 품질이 얼마나 좋고 일관되냐\nvariety: 다양한 사용자의 니즈를 얼마나 잘 맞추냐\ntime",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#생산-시스템-관리를-배우면-할-수-있는-것",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#생산-시스템-관리를-배우면-할-수-있는-것",
    "title": "Matching Supply with Demand",
    "section": "생산 시스템 관리를 배우면 할 수 있는 것",
    "text": "생산 시스템 관리를 배우면 할 수 있는 것\n\n비효율성 분석\n상충관계에 대한 의사결정\n신기술 등에 대한 평가",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/01.html#footnotes",
    "title": "Matching Supply with Demand",
    "section": "각주",
    "text": "각주\n\n\n뭔소리지↩︎\n뭔소리지↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "Matching Supply with Demand"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/04.html#제품-설계-및-개발",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/04.html#제품-설계-및-개발",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "제품 설계 및 개발",
    "text": "제품 설계 및 개발\n\n설계의 중요성\n\n총 제품 비용 중 설계가 치지하는 비중은 적지만, 설계의 영향을 받는 비중이 높다.\n\n\n\n제품 설계 프로세스\n\n\n\n제품 설계 프로세스\n\n\n\n아이디어 선정: 소비자의 니즈, 경쟁사 제품 등 벤치마킹(reverse engineering)\n제품 선정: 시장 분석, 경제성 분석, 기술 분석\n\n\n\n제조 고려 설계\n\n제조 과정 단순화 및 비용 절감을 고려해서 설계해야 한다.\n\n부품 개수 최소화\n모듈화 표준화\n조립, 재활용, 분해 고려\n\nsubtract manufacturing보단 additive manufacturing(적층제조, DFAM)을 고려\n\n\n\n표준화 모듈화\n\n표준화: 부품 호환성 및 운영 효율성\n모듈화: 표준화된 부품의 집합 &lt;-&gt; integral\n\n\n\n지연전략\n\n차별화 지연 전략: 수요를 알기 전까지 많은 종류 생산은 지연",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/04.html#생산-프로세스의-유형",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/04.html#생산-프로세스의-유형",
    "title": "제품 설계 기법 및 기업 프로세스 유형",
    "section": "생산 프로세스의 유형",
    "text": "생산 프로세스의 유형\n\n주문충족 방식에 따른 분류\n\nmake to stock: 수요가 발생하기 전에 생산. 수요가 예측이 쉬울 경우 적합 (push)\nassemble / configure / build to order: 제품 구성요소를 재고로 보유. 고객의 요구에 따라 조립하여 생산. 지연 전략에 맞닿아 있다 (pull)\nmake to order: 주문에 따른 생산. 설계가 완료된 걸 다른 옵션으로 제공. 옵션이 많거나 고가 제품에 적합 (push-pull)\nengineer to order: 고객의 요구에 따라 설계 및 생산. 일회성 프로젝트에 적합 (pull)\n\n위로 갈 수록 제공 시간은 짧아지고, 아래로 갈 수록 유연성이 높아진다.\n\n\n생산 흐름에 따른 분류\n\n프로젝트 프로세스: 일회성 생산. 흐름이라고 할 수는 없다.\n개별작업 프로세스(job shop): 공정별 배치. 높은 유연성, 낮은 규모\n배치 프로세스: job shop과 라인 프로세스의 중간 형태. batch 수가 맞춰지기 전까지 대기 / 유휴시간 존재.\n라인 프로세스: 제품별 배치. 낮은 유연성, 대규모\n연속 흐름 프로세스: 멈춤, 수정, 변경 최소화\n\n\n\n다양한 유형의 프로세스와 설비배치를 혼합 적용하는 것이 일반적\n\n\n\n다품종소량생산, 개인맞춤생산 시대 도래\n\n\nreconfigurable manufacturing system: 다양한 제품을 생산할 수 있는 유연한 생산 시스템\n\n\n\ncell manufacturing\n\nline process랑 job shop이 혼합된거\n비슷한 작업이 필요한 부품들을 하나의 그룹으로 묶어서 전용 셀에서 생산",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "제품 설계 기법 및 기업 프로세스 유형"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/05.html#조립공정의-분석",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/05.html#조립공정의-분석",
    "title": "인건비 추정과 감축",
    "section": "조립공정의 분석",
    "text": "조립공정의 분석\n\n처리 능력: \\(\\frac{자원의 수}{처리시간}\\)\nbottleneck은 처리능력이 제일 낮은 자원\nX개를 생산하는데 걸리는 시간\n\n가동중인 생산 시스템: \\(\\frac{X}{R}\\)\n비어있는 생산 시스템: 비어있는 시스템을 흘러 가는데 걸리는 시간 + \\(\\frac{X - 1}{R}\\)\n\n\n\n비어있는 시스템을 흘러가는데 걸리는 시간\n\nWorker-paced process: 모든 작업의 처리시간의 합\nMachine-paced process(컨베이어 벨트): 프로세스상의 단계 수 * 병목공정의 처리시간",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/05.html#노동량과-유휴시간",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/05.html#노동량과-유휴시간",
    "title": "인건비 추정과 감축",
    "section": "노동량과 유휴시간",
    "text": "노동량과 유휴시간\n\n이상적인 노동비: 작업자의 처리시간의 합 * 시간당 평균 임금\n\n유휴시간을 고려하지 않았을 때\n\n직접 노동 인건비= \\(\\frac{단위시간당 총 임금}{단위시간당 흐름률}\\)\n\n실제 노동시간 + 유휴시간(idle time)\n흐름률이 높아지면 유휴시간이 줄어들면서 직접 노동 인건비가 줄어든다.\n\n\n\n유휴시간 종류\n\nbottleneck에 맞추기 위한 유휴시간\n수요에 맞추기 위해 발생하는 유휴시간\n\n이런 경우 작업시간을 줄이는 방법을 생각할 수 있다.\n하지만 flexible하게 맞추기는 어려울 것이다.\n\n\n\n\nCycle time(주기 시간)\n\n프로세스에서 산출되는 연속된 두 제품 간의 시간간격\n프로세스가 얼마나 빨리 생산하는지를 나타내는 지표\nflow rate의 역수\n1인 작업자의 유휴시간 = cycle time - 1인 작업자의 작업시간\n\n\n\n평균 노동 활용률\n\n제품 생산에만 들어가는 노동의 양과 실제 인건비 지불의 기준이 되는 노동의 양(노동량 + 유휴시간)을 비교\n\\(\\frac{노동량}{노동량 + 모든 작업자의 유휴시간 총합}\\)\n\\(\\frac{1}{노동자 수}(활용률의 합)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/05.html#line-balancing",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/05.html#line-balancing",
    "title": "인건비 추정과 감축",
    "section": "Line Balancing",
    "text": "Line Balancing\n\nbottleneck에 맞추기 위한 유휴시간으로 이상적인 노동비를 산출할 수 없음. → line balancing을 맞춰줌\n프로세스 내부적인 수요(요구되는 노동량)와 공급(작업자의 처리 능력)을 맞추는 것\n\n\n대량생산으로의 확장\n\n라인의 병렬적 배치\n프로세스 단계별 작업자 추가\n과업의 분화 및 전문화\n\n전문화될수록 라인 밸런싱이 어려워지고 평균 노동 활용률이 낮아짐(작업이 평탄하지 않음)\n→ 전문화 정도를 감소시켜 라인 밸런싱을 쉬워지게 한다.\n\n작업 셀: 한 명이 모든 과업을 수행함. 한 명의 작업시간이 노동량에 해당하고, 노동 활용률은 100%",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "인건비 추정과 감축"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/03.html#프로세스-처리-능력-및-활용률",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/03.html#프로세스-처리-능력-및-활용률",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "프로세스 처리 능력 및 활용률",
    "text": "프로세스 처리 능력 및 활용률\n\nprocess capacity: 흐름률의 upper bound(유량).\n병목(bottleneck): 제일 낮은 처리능력의 자원\n전체 프로세스의 처리 능력 = 병목의 처리능력 (단 작업이 일렬로 수행될 때)\nproduct mix:\n\n다양한 제품이 input으로 들어와 처리능력이 달라짐\nbottleneck을 계산하기는 어려움\n\n비율이 매번 달라질 수도 있어서\n작업이 일렬로만 수행되지 않아서\n\n\n실제 생산한 양(흐름률)은 capacity에서만 결정되지 않는다.\n\n수요(market + 계절 / 안전 재고 같은 내부적 수요)\n원자재 투입량\n\n흐름률 = min(시간 당 수요, 프로세스 처리 능력)\n공급능력: 투입량, 처리 능력\n\n\n수요 / 공급 제약적 상황\n\n\n수요(가) 제약적: 수요 &lt; 공급\n\nbottleneck 활용률 &lt; 100%\nflow rate == Demand rate\n\n공급(이) 제약적: 수요 &gt; 공급\n\n투입 제약적\n처리능력 제약적\n\nbottleneck 활용률 == 100%\nflow rate = capacity\n\n\n\n\n\n활용률\n\n\n실제 생산하는 양을 capacity로 나눈 것\n\n\\(\\frac{흐름률}{처리능력}\\)\n활용률을 100% 달성하려면 쉬지 않고 프로세스가 돌아가야하지만 현실적으로 쉽지 않다.\n\n수요가 공급보다 적을 수 있다.\n투입물이 충분하지 않다.\n몇몇 공정의 사용이 공장이나 수리로 제한될 수 있다.\n불확실성\n\n\n병목을 제외한 다른 하위 작업은 활용률이 떨어질 수 있다.\n\n모든 프로세스가 병목인 것이 가장 이상적인 상황 (과도하게 높은 공급능력)\n\n공급 제약적 상황에서 수요가 얼마나 많은지 알 수 없음.\n→ implied utilization\n\n\n\nimplied utilization\n\n\\(U = \\frac{R}{Capacity}\\)\n\\(IU = \\frac{Demand or workload}{Capacity} (≤100% or &gt; 100%)\\)\nif min(demmand, capacity, input) = demand then U = IU\n이 외에도 잠재적 수요 못따라가는 작업도 알 수 있음\n\nIU 100 넘는거 개선 필요\n\n또, 작업이 sequential하게 진행되지 않을 때 병목현상을 확인할 수 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/product/03.html#여러-종류의-흐름-단위",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/product/03.html#여러-종류의-흐름-단위",
    "title": "공급 프로세스의 이해: 프로세스 처리능력 평가",
    "section": "여러 종류의 흐름 단위",
    "text": "여러 종류의 흐름 단위\ninput 당 뭐가 다르면 다른 단위로 치환",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Product",
      "공급 프로세스의 이해: 프로세스 처리능력 평가"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/notes/others/2.html",
    "href": "posts/03_archives/completed_project/bs_3_1/notes/others/2.html",
    "title": "성적 장학금",
    "section": "",
    "text": "오~예~ (남은 등록금 300만원을 대출 받으며)\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기",
      "Notes",
      "Others",
      "성적 장학금"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/index.html",
    "href": "posts/03_archives/completed_project/bs_3_1/index.html",
    "title": "학부 3학년 1학기",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2024-12-21\n        종료일: 2025-06-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/index.html#details",
    "href": "posts/03_archives/completed_project/bs_3_1/index.html#details",
    "title": "학부 3학년 1학기",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 3학년 1학기 개념 정리, 과제, 할 일 등을 총 정리한 노트 모음입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/index.html#tasks",
    "href": "posts/03_archives/completed_project/bs_3_1/index.html#tasks",
    "title": "학부 3학년 1학기",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                \n                    푸른등대 기부장학금 - 두나무UDC 신청 (~2025-01-20 18:00)\n                \n                불합격\n            \n            \n            \n                \n                \n                    2025 DB 드림리더 장학생 신청 (~2025-01-10)\n                \n                잘할 자신이 없다\n            \n            \n            \n                \n                \n                    경기도 학자금대출 이자 지원 신청 (~2025.02.14 18:00)\n                \n                신청 완료\n            \n            \n            \n                \n                    \n                    학과 근로 신청\n                \n                신청 완료\n            \n\n            \n            \n                \n                    \n                    KMOOC 학점 인정 신청 (2025-03-10~)\n                \n                done\n            \n\n            \n            \n                \n                \n                    봉사활동 계획서 작성 (2025-03-11~)\n                \n                작성 완료\n            \n            \n            \n                \n                \n                    OR 과제 1 (~2025-03-16 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                \n                    OR 과제 2 (~2025-03-23 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                    \n                    교수님과 커피\n                \n                나 커피 못 마시는데\n            \n\n            \n            \n                \n                \n                    데이터마이닝 팀과제 script\n                \n                일단 완성\n            \n            \n            \n                \n                \n                    OR 과제 3 (~2025-04-06 23:59)\n                \n                제출 완료\n            \n            \n            \n                \n                \n                    OR 과제 4 (~2025-04-13 23:59)\n                \n                그만...\n            \n            \n            \n                \n                \n                    data mining 1차 과제 ppt 완성\n                \n                done\n            \n            \n            \n                \n                    \n                    진로 지도 상담 받기\n                \n                \n            \n\n            \n            \n                \n                \n                    컴퓨팅적 사고 발표 ppt 만들기\n                \n                \n            \n            \n            \n                \n                    \n                    데이터마이닝 2차 과제 준비\n                \n                노션에 정리 중\n            \n\n            \n            \n                \n                    \n                    교통비 지원금 신청\n                \n                \n            \n\n            \n            \n                \n                \n                    OR 과제 6 (~2025-05-18 23:59)\n                \n                힘들다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/index.html#필요한-자료",
    "href": "posts/03_archives/completed_project/bs_3_1/index.html#필요한-자료",
    "title": "학부 3학년 1학기",
    "section": "필요한 자료",
    "text": "필요한 자료\n\n자기소개서 작성 1\n교육 이수 증빙자료\nPortfolio: 큰일 났다. 진짜 못만들었다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/bs_3_1/index.html#참고-자료",
    "title": "학부 3학년 1학기",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_1/index.html#related-posts",
    "href": "posts/03_archives/completed_project/bs_3_1/index.html#related-posts",
    "title": "학부 3학년 1학기",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 1학기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/core/01.html#결측치-처리",
    "href": "posts/03_archives/completed_project/adp_실기/notes/core/01.html#결측치-처리",
    "title": "전처리",
    "section": "결측치 처리",
    "text": "결측치 처리\n\n대푯값으로 대체\n단순확률대치법\n다른 모델로 예측\n보간법: 시계열에서 주로 사용.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "전처리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/core/01.html#이상치-처리",
    "href": "posts/03_archives/completed_project/adp_실기/notes/core/01.html#이상치-처리",
    "title": "전처리",
    "section": "이상치 처리",
    "text": "이상치 처리\n\nESD\nIQR\nDBSCAN",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "전처리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/core/01.html#클래스-불균형",
    "href": "posts/03_archives/completed_project/adp_실기/notes/core/01.html#클래스-불균형",
    "title": "전처리",
    "section": "클래스 불균형",
    "text": "클래스 불균형",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Core",
      "전처리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/statistics/00.html#plot을-통한-자료-요약",
    "href": "posts/03_archives/completed_project/adp_실기/notes/statistics/00.html#plot을-통한-자료-요약",
    "title": "EDA",
    "section": "plot을 통한 자료 요약",
    "text": "plot을 통한 자료 요약\n\n범주형 자료 요약\n\n도수분포표\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom seaborn import color_palette\n\ndf = pd.DataFrame(range(1, 11)).sample(100, replace=True)\ntable = pd.crosstab(index=df.values.flatten(), colnames=['질병'], columns='도수')\ntable.index = [\"감염\", \"심장\", \"호흡기\", \"소화기\", \"신경\", \"근골격\", \"내분비\", \"정신\", \"피부\", \"기타\"]\nprint(table)\n\n질병   도수\n감염   12\n심장    7\n호흡기   4\n소화기  10\n신경    9\n근골격  13\n내분비  10\n정신    7\n피부   14\n기타   14\n\n\n\n원형 그래프\n\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ntable.plot(kind='bar', color='skyblue', legend=False)\nplt.xlabel('사망 원인')\nplt.ylabel('빈도수')\nplt.title('사망 원인별 빈도수')\nplt.xticks(rotation=0)\nplt.show()\n\n\n\n\n\n\n\n\n\n막대 그래프\n\n\nplt.pie(table.iloc[:, 0], labels=list(table.index), colors=color_palette(\"pastel\"), autopct='%1.1f%%')\nplt.title('사망 원인별 빈도수')\nplt.show()\n\n\n\n\n\n\n\n\n\n파레토그림 (명목형)\n\n\n가장 큰 영향을 미치는 범주를 파악할 수 있는 그래프\n라이브러리는 딱히 없는거 같고, 뭐 많이 쓰지도 않는거 같아서 구현은 생략\n\n\n\n이산형 자료 요약\n\n관측값의 종료가 적은 경우 그냥 범주형으로 처리할 수 있다. (단 파레토그림같은 순서가 바뀌는 기법은 사용하지 않는다.)\n관측값의 종류가 많을 경우, 연속형 자료로 처리할 수 있다.\n\n\n\n연속형 자료 요약\n\n점도표\n도수분포표\n\n\n구간을 정하고 각 구간에 속하는 관측값의 개수를 세어 도수분포표를 만든다.\n\n\n히스토그램\n\n\n상대도수를 계급구간의 폭으로 나눈 값을 막대의 높이로 사용하는 그래프\n\n\n도수다각형\n\n\n중심의 위치, 퍼진 정도 등을 파악하는데 유용하다.\n또한 여러 자료를 비교하는 경우 히스토그램보다 유용하다.\n\n\n줄기-잎 그림\n\n\n개개의 관측값에 대한 정보를 유지하면서 분포를 파악할 수 있는 그래프\n하지만 관측값의 갯수가 많거나 지나치게 흩어져 있는 경우 제한된 공간에 그리는 것이 불가능하다.\n\n\n\n분포의 모양\n\n종모양\n이봉형: 두 개의 다른 집단이 섞여 있을 때 종종 나타난다.\n균일형\n오른쪽 편중\n왼쪽 편중",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Statistics",
      "EDA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/statistics/00.html#수치를-통한-연속형-자료-요약",
    "href": "posts/03_archives/completed_project/adp_실기/notes/statistics/00.html#수치를-통한-연속형-자료-요약",
    "title": "EDA",
    "section": "수치를 통한 연속형 자료 요약",
    "text": "수치를 통한 연속형 자료 요약\n\ndf.describe(include='all')\n\n\n\n\n\n\n\n\n0\n\n\n\n\ncount\n100.000000\n\n\nmean\n5.930000\n\n\nstd\n2.989054\n\n\nmin\n1.000000\n\n\n25%\n4.000000\n\n\n50%\n6.000000\n\n\n75%\n9.000000\n\n\nmax\n10.000000\n\n\n\n\n\n\n\n\n중심위치의 측도\n\n평균, 중앙값, 최빈값\n\n\nfrom scipy import stats\n\nprint(f'평균: {np.mean(df)}, 중앙값: {np.median(df)}, 최빈값: {stats.mode(df)}')\n\n평균: 5.93, 중앙값: 6.0, 최빈값: ModeResult(mode=array([9]), count=array([14]))\n\n\n\n\n퍼진 정도의 측도\n\n분산, 표준편차, 범위, 사분위수, 변동계수\n\n\n# 자유도 -1\nprint(f'분산: {np.var(df, ddof=1)}, 표준편차: {np.std(df, ddof=1)}, 범위: {np.ptp(df)}, 1,3분위수: {np.quantile(df, [0.25, 0.75])}, 변동계수: {np.std(df, ddof=1) / np.mean(df)}')\n\n분산: 0    8.934444\ndtype: float64, 표준편차: 0    2.989054\ndtype: float64, 범위: 9, 1,3분위수: [4. 9.], 변동계수: 0    0.504056\ndtype: float64\n\n\n\n\nbox plot\n\n종모양의 데이터의 분포를 나타내는 데 적절하다.\n사전에 도수분포표, 히스토그램, 줄기-잎 그림으로 봉우리를 파악해야 한다.\n\n\nplt.boxplot(df)\n\n{'whiskers': [&lt;matplotlib.lines.Line2D at 0x7dceb70234a0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x7dceb70237a0&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D at 0x7dceb7023a70&gt;,\n  &lt;matplotlib.lines.Line2D at 0x7dceb7023d70&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D at 0x7dceb7023230&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D at 0x7dceb7054080&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D at 0x7dceb70542f0&gt;],\n 'means': []}",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Statistics",
      "EDA"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/03.html#overview",
    "href": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/03.html#overview",
    "title": "두 변수의 연관성과 독립성",
    "section": "overview",
    "text": "overview\n표본 상관계수를 대체할 수 있는 비모수적 상관계수와 이를 이용한 두 변수에 대한 독립성 검정법을 알아본다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "두 변수의 연관성과 독립성"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/03.html#상관계수와-단순선형회귀",
    "href": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/03.html#상관계수와-단순선형회귀",
    "title": "두 변수의 연관성과 독립성",
    "section": "상관계수와 단순선형회귀",
    "text": "상관계수와 단순선형회귀\n\n스피어만 순위상관\n\n두 변수를 순위변환 한 뒤 표본상관계수를 계산\n비선형 추세일지라도 순증가 또는 순감소하는 추세를 측정할 수 있다.\n순서가 있는 범주형일 때도 적용할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "두 변수의 연관성과 독립성"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/00.html#비모수-통계란",
    "href": "posts/03_archives/completed_project/adp_실기/notes/Nonparametric/00.html#비모수-통계란",
    "title": "intro",
    "section": "비모수 통계란",
    "text": "비모수 통계란\n\n모집단에 대한 가정을 적게 한 상태에서 통계적 추론을 하는 방법론\n\n모집단이 정규분포를 따른다는 모수적 모형 가정 없이 정확한 추론을 할 수 있다.\n\n\n\n두 표본이 정규분포를 따르지 않을 때 검정\n\n\n순위와 부호를 이용한 검정 방법\n순열 검정\n\n\n추정량의 불확실성을 계량화\n비모수적 함수 추정\n\n\n모수의 종류에 따른 비모수통계학의 소주제\n\n위치와 산포 모수\n\n\n순열검정\n\n\n모수모형 하에서 적용 가능\n\n\n붓스트랩\n\n\n비모수모형 공간이 매우 크기 때문에, 복잡한 모형에서의 모수 추정 가능\n\n\n평활법",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Nonparametric",
      "intro"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/03.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/03.html",
    "title": "이동평균과정 모델링",
    "section": "",
    "text": "이동편균 모델: 현잿 값이 현재와 과거 오차에 선형적으로 비례한다.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ndf = pd.read_csv('_data/widget.csv')\nsns.lineplot(data=df, x=df.index, y='widget_sales')\nplt.xticks(\n    [0, 30, 57, 87, 116, 145, 175, 204, 234, 264, 293, 323, 352, 382, 409, 439, 468, 498], \n    ['Jan 2019', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan 2020', 'Feb', 'Mar', 'Apr', 'May', 'Jun'])\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\nADF_result = adfuller(df['widget_sales'])\n\nADF_result[0], ADF_result[1]\n\n(np.float64(-1.5121662069359048), np.float64(0.5274845352272605))\n\n\n\n정상 시계열이 아님. 차분 진행\n\n\ndiff_df = np.diff(df['widget_sales'], n=1)\n\nADF_result = adfuller(diff_df)\nADF_result[0], ADF_result[1]\n\n(np.float64(-10.576657780341959), np.float64(7.076922818587193e-19))\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(diff_df, lags=30)\nplt.show()\n\n\n\n\n\n\n\n\n\n지연 2 이후 유의하지 않음.\nMA(2) 진행\n\n\nfrom sklearn.model_selection import train_test_split\n\ndiff_df = pd.DataFrame({'widget_sales_diff': diff_df})\ntrain, test = train_test_split(diff_df, test_size=0.1)\n\n\nMA(q)는 q 크기까지만 예측 가능.\n회귀적으로 예측을 진행해야함\n\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    if method == 'mean':\n        pred_mean = []\n        for i in range(train_len, total_len, window):\n            mean = np.mean(df[:i].values)\n            pred_mean.extend(mean for _ in range(window))\n        return pred_mean\n    if method == 'last':\n        pred_last_value = []\n        for i in range(train_len, total_len, window):\n            last_value = df.iloc[i-1].values[0]\n            pred_last_value.extend(last_value for _ in range(window))\n        return pred_last_value\n    if method == 'MA':\n        pred_MA = []\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(df[:i], order=(0,0,2))\n            res = model.fit(disp=False)\n            predictions = res.get_prediction(0, i + window - 1)\n            oos_pred = predictions.predicted_mean.iloc[-window:]\n            pred_MA.extend(oos_pred)\n        return pred_MA\n\n\npred_df = test.copy()\nTRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 2\n\npred_mean = rolling_forecast(diff_df, TRAIN_LEN, HORIZON, WINDOW, 'mean')\npred_last = rolling_forecast(diff_df, TRAIN_LEN, HORIZON, WINDOW, 'last')\npred_MA = rolling_forecast(diff_df, TRAIN_LEN, HORIZON, WINDOW, 'MA')\n\npred_df['pred_mean'] = pred_mean\npred_df['pred_last'] = pred_last\npred_df['pred_MA'] = pred_MA\n\n\ndf['pred_widget_sales'] = pd.Series()\ndf['pred_widget_sales'].iloc[450:] = df['widget_sales'].iloc[450] + pred_df['pred_MA'].cumsum()\n\n\nsns.lineplot(data=df, x=df.index, y='widget_sales', label='실제 값')\nsns.lineplot(data=df, x=df.index, y='pred_widget_sales', label='MA(2)')\nplt.xticks(\n    [409, 439, 468, 498], \n    ['Mar', 'Apr', 'May', 'Jun'])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "이동평균과정 모델링"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/05.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/05.html",
    "title": "복잡한 시계열 모델",
    "section": "",
    "text": "from statsmodels.tsa.arima_process import ArmaProcess\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nar1 = np.array([1, -0.33])\nma1 = np.array([1, 0.9])\n\nARMA_1_1 = ArmaProcess(ar1, ma1).generate_sample(nsample=1000)\nfrom statsmodels.tsa.stattools import adfuller\n\nADF_result = adfuller(ARMA_1_1)\n\nADF_result[0], ADF_result[1]\n\n(np.float64(-7.360446175397818), np.float64(9.531402483929456e-11))\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(ARMA_1_1, lags=20)\nplt.show()\nplot_pacf(ARMA_1_1, lags=20)\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "복잡한 시계열 모델"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/05.html#일반적-모델링-절차",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/05.html#일반적-모델링-절차",
    "title": "복잡한 시계열 모델",
    "section": "일반적 모델링 절차",
    "text": "일반적 모델링 절차\n\n\\(AIC = 2k - 2ln(\\hat{L})\\)\nk = p + q\nL = max(likelihood)\n\n\nfrom itertools import product\n\nps = range(0, 4, 1)\nqs = range(0, 4, 1)\n\norder_list = list(product(ps, qs))\n\n\nfrom typing import Union\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef optimize_ARMA(endog: Union[pd.Series, list], order_list: list) -&gt; pd.DataFrame:\n    results = []\n    for order in order_list:\n        try:\n            model = SARIMAX(endog, order=(order[0], 0, order[1]), simple_differencing=False).fit(disp=False)\n        except:\n            continue\n        aic = model.aic\n        results.append([order, aic])\n    result_df = pd.DataFrame(results)\n    result_df.columns = ['(p, q)', 'AIC']\n    result_df = result_df.sort_values(by=\"AIC\").reset_index(drop=True)\n\n    return result_df\n\nresult_df = optimize_ARMA(ARMA_1_1, order_list)\nresult_df\n\n/home/cryscham123/.local/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n\n\n\n\n\n\n\n\n\n(p, q)\nAIC\n\n\n\n\n0\n(1, 1)\n2857.099212\n\n\n1\n(1, 2)\n2859.093087\n\n\n2\n(2, 1)\n2859.093715\n\n\n3\n(3, 3)\n2859.877153\n\n\n4\n(3, 1)\n2860.879065\n\n\n5\n(1, 3)\n2860.952599\n\n\n6\n(2, 2)\n2861.093731\n\n\n7\n(0, 3)\n2861.418546\n\n\n8\n(3, 2)\n2863.052817\n\n\n9\n(2, 3)\n2863.093082\n\n\n10\n(0, 2)\n2865.171193\n\n\n11\n(0, 1)\n2956.271652\n\n\n12\n(3, 0)\n3008.178796\n\n\n13\n(2, 0)\n3074.047023\n\n\n14\n(1, 0)\n3234.247867\n\n\n15\n(0, 0)\n3812.185210\n\n\n\n\n\n\n\n\n잔차 분석\n\nmodel = SARIMAX(ARMA_1_1, order=(1, 0, 1), simple_differencing=False)\nmodel_fit = model.fit(disp=False)\n\nmodel_fit.plot_diagnostics(figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\ntr = acorr_ljungbox(model_fit.resid, np.arange(1, 11))\nprint(tr)\n\n     lb_stat  lb_pvalue\n1   0.001234   0.971975\n2   0.057762   0.971532\n3   0.529364   0.912395\n4   1.095239   0.895027\n5   2.128972   0.831027\n6   2.561838   0.861484\n7   3.201337   0.865773\n8   3.227127   0.919306\n9   3.346862   0.948943\n10  3.676792   0.960752",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "복잡한 시계열 모델"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/05.html#예시---대역폭-사용량-예측",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/05.html#예시---대역폭-사용량-예측",
    "title": "복잡한 시계열 모델",
    "section": "예시 - 대역폭 사용량 예측",
    "text": "예시 - 대역폭 사용량 예측\n\ndf = pd.read_csv('_data/bandwidth.csv')\nsns.lineplot(data=df, x=df.index, y='hourly_bandwidth')\n\nplt.xlabel('시간')\nplt.ylabel('시간당 대역폭 사용량(MBps)')\nplt.xticks(\n    np.arange(0, 10000, 730), \n    ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', '2020', 'Feb'])\nplt.show()\n\n\n\n\n\n\n\n\n\nADF_result = adfuller(df['hourly_bandwidth'])\n\nADF_result[0], ADF_result[1]\n\n(np.float64(-0.8714653199452735), np.float64(0.7972240255014551))\n\n\n\nbandwidth_diff = np.diff(df['hourly_bandwidth'], n=1)\n\nADF_result = adfuller(bandwidth_diff)\n\nADF_result[0], ADF_result[1]\n\n(np.float64(-20.694853863789017), 0.0)\n\n\n\ndf_diff = pd.DataFrame({'bandwidth_diff': bandwidth_diff})\ntrain = df_diff.iloc[:-168]\ntest = df_diff.iloc[-168:]\n\n\nps = range(0, 4, 1)\nqs = range(0, 4, 1)\norder_list = list(product(ps, qs))\nresult_df = optimize_ARMA(train['bandwidth_diff'], order_list)\nresult_df\n\n\n\n\n\n\n\n\n(p, q)\nAIC\n\n\n\n\n0\n(3, 2)\n27991.063879\n\n\n1\n(2, 3)\n27991.287509\n\n\n2\n(2, 2)\n27991.603598\n\n\n3\n(3, 3)\n27993.416924\n\n\n4\n(1, 3)\n28003.349550\n\n\n5\n(1, 2)\n28051.351401\n\n\n6\n(3, 1)\n28071.155496\n\n\n7\n(3, 0)\n28095.618186\n\n\n8\n(2, 1)\n28097.250766\n\n\n9\n(2, 0)\n28098.407664\n\n\n10\n(1, 1)\n28172.510044\n\n\n11\n(1, 0)\n28941.056983\n\n\n12\n(0, 3)\n31355.802141\n\n\n13\n(0, 2)\n33531.179284\n\n\n14\n(0, 1)\n39402.269523\n\n\n15\n(0, 0)\n49035.184224\n\n\n\n\n\n\n\n\nmodel = SARIMAX(train['bandwidth_diff'], order=(2, 0, 2), simple_differencing=False)\nmodel_fit = model.fit(disp=False)\nmodel_fit.plot_diagnostics(figsize=(12, 8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nacorr_ljungbox(model_fit.resid, np.arange(1, 11))\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\n\n\n\n\n1\n0.042190\n0.837257\n\n\n2\n0.418364\n0.811247\n\n\n3\n0.520271\n0.914416\n\n\n4\n0.850554\n0.931545\n\n\n5\n0.850841\n0.973678\n\n\n6\n1.111754\n0.981019\n\n\n7\n2.124864\n0.952607\n\n\n8\n3.230558\n0.919067\n\n\n9\n3.248662\n0.953615\n\n\n10\n3.588289\n0.964015\n\n\n\n\n\n\n\n\ndef rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, window: int, method: str) -&gt; list:\n    total_len = train_len + horizon\n    if method == 'mean':\n        pred_mean = []\n        for i in range(train_len, total_len, window):\n            mean = np.mean(df[:i].values)\n            pred_mean.extend(mean for _ in range(window))\n        return pred_mean\n    if method == 'last':\n        pred_last_value = []\n        for i in range(train_len, total_len, window):\n            last_value = df.iloc[i-1].values[0]\n            pred_last_value.extend(last_value for _ in range(window))\n        return pred_last_value\n    if method == 'ARMA':\n        pred_MA = []\n        for i in range(train_len, total_len, window):\n            model = SARIMAX(df[:i], order=(2,0,2))\n            res = model.fit(disp=False)\n            predictions = res.get_prediction(0, i + window - 1)\n            oos_pred = predictions.predicted_mean.iloc[-window:]\n            pred_MA.extend(oos_pred)\n        return pred_MA\n\npred_df = test.copy()\nTRAIN_LEN = len(train)\nHORIZON = len(test)\nWINDOW = 2\n\npred_mean = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'mean')\npred_last = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'last')\npred_ARMA = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'ARMA')\n\npred_df['pred_mean'] = pred_mean\npred_df['pred_last'] = pred_last\npred_df['pred_ARMA'] = pred_ARMA\n\nsns.lineplot(data=pred_df, x=pred_df.index, y='bandwidth_diff', label='실제값')\nsns.lineplot(data=pred_df, x=pred_df.index, y='pred_mean', label='평균 예측')\nsns.lineplot(data=pred_df, x=pred_df.index, y='pred_last', label='마지막 값 예측')\nsns.lineplot(data=pred_df, x=pred_df.index, y='pred_ARMA', label='ARMA(2, 2) 예측')\nplt.xlabel('시간')\nplt.ylabel('시간당 대역폭 사용량(MBps)')\nplt.xticks(\n    [9802, 9850, 9898, 9946, 9994],\n    ['2020-02-13', '2020-02-15', '2020-02-17', '2020-02-19', '2020-02-21'])\nplt.xlim(9800, 9999)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmse_mean = mean_squared_error(pred_df['bandwidth_diff'], pred_df['pred_mean'])\nmse_last = mean_squared_error(pred_df['bandwidth_diff'], pred_df['pred_last'])\nmse_ARMA = mean_squared_error(pred_df['bandwidth_diff'], pred_df['pred_ARMA'])\nmse_mean, mse_last, mse_ARMA\n\n(np.float64(6.306526957989325),\n np.float64(2.2297582947733656),\n np.float64(1.7690462114420604))\n\n\n\n역변환\n\ndf['pred_bandwidth'] = pd.Series()\ndf['pred_bandwidth'].iloc[9832:] = df['hourly_bandwidth'].iloc[9832] + pred_df['pred_ARMA'].cumsum()\n\nsns.lineplot(data=df, x=df.index, y='hourly_bandwidth', label='실제 값')\nsns.lineplot(data=df, x=df.index, y='pred_bandwidth', label='ARMA(2, 2) 예측')\nplt.xticks(\n    [9802, 9850, 9898, 9946, 9994],\n    ['2020-02-13', '2020-02-15', '2020-02-17', '2020-02-19', '2020-02-21'])\nplt.xlim(9800, 9999)\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "복잡한 시계열 모델"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/07.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/07.html",
    "title": "외생 변수 추가하기",
    "section": "",
    "text": "import statsmodels.api as sm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nmacro_econ_data = sm.datasets.macrodata.load_pandas().data\nmacro_econ_data\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.980\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.150\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.350\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.370\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.540\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n198\n2008.0\n3.0\n13324.600\n9267.7\n1990.693\n991.551\n9838.3\n216.889\n1474.7\n1.17\n6.0\n305.270\n-3.16\n4.33\n\n\n199\n2008.0\n4.0\n13141.920\n9195.3\n1857.661\n1007.273\n9920.4\n212.174\n1576.5\n0.12\n6.9\n305.952\n-8.79\n8.91\n\n\n200\n2009.0\n1.0\n12925.410\n9209.2\n1558.494\n996.287\n9926.4\n212.671\n1592.8\n0.22\n8.1\n306.547\n0.94\n-0.71\n\n\n201\n2009.0\n2.0\n12901.504\n9189.0\n1456.678\n1023.528\n10077.5\n214.469\n1653.6\n0.18\n9.2\n307.226\n3.37\n-3.19\n\n\n202\n2009.0\n3.0\n12990.341\n9256.0\n1486.398\n1044.088\n10040.6\n216.385\n1673.9\n0.12\n9.6\n308.013\n3.56\n-3.44\n\n\n\n\n203 rows × 14 columns\n\n\n\n\ntarget = macro_econ_data[['realgdp', 'realcons', 'realinv', 'realgovt', 'realdpi', 'cpi']]\ntarget.plot(subplots=True, figsize=(10, 8), layout=(3, 2))\nplt.xticks(np.arange(0, 208, 16), np.arange(1959, 2010, 4))\nplt.show()\n\n\n\n\n\n\n\n\n\n이 다음 부분은 오류인거 같음. 다른 책 보자.\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "외생 변수 추가하기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/time_series/06.html#sarima-모델",
    "href": "posts/03_archives/completed_project/adp_실기/notes/time_series/06.html#sarima-모델",
    "title": "계절성 고려",
    "section": "SARIMA 모델",
    "text": "SARIMA 모델\n\n\\(SARIMA(p, d, q)(P, D, Q)_m\\) 형태로 표현\n\\(m\\): 계절성 주기\n\\(P, D, Q\\): 계절성 AR, 차분, MA 차수\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ndf = pd.read_csv('_data/air.csv')\n\n\nfrom statsmodels.tsa.seasonal import STL\n\ndecomposition = STL(df['Passengers'], period=12).fit()\ndecomposition.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\ndf_diff = np.diff(df['Passengers'], n=1)\n\nADF_result = adfuller(df_diff)\nADF_result[0], ADF_result[1]\n\n(np.float64(-2.8292668241700047), np.float64(0.05421329028382478))\n\n\n\ndf_diff = np.diff(df_diff, n=12)\n\nADF_result = adfuller(df_diff)\nADF_result[0], ADF_result[1]\n\n(np.float64(-17.62486236026156), np.float64(3.823046855601547e-30))\n\n\n\nfrom typing import Union\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndef optimize_SARIMA(endog: Union[pd.Series, list], order_list: list, d: int, D: int, s: int) -&gt; pd.DataFrame:\n    results = []\n    for order in order_list:\n        try:\n            model = SARIMAX(endog, \n                            order=(order[0], d, order[1]), \n                            seasonal_order=(order[2], D, order[3], s),\n                            simple_differencing=False).fit(disp=False)\n        except:\n            continue\n        aic = model.aic\n        results.append([order, aic])\n    result_df = pd.DataFrame(results)\n    result_df.columns = ['(p, q, P, Q)', 'AIC']\n    result_df = result_df.sort_values(by=\"AIC\").reset_index(drop=True)\n\n    return result_df\n\n\nfrom itertools import product\n\ntrain = df.iloc[:-12]['Passengers']\ntest = df.iloc[-12:]\n#  ps = range(0, 4, 1)\n#  qs = range(0, 4, 1)\n#  Ps = range(0, 4, 1)\n#  Qs = range(0, 4, 1)\n# \n#  SARIMA_order_list = list(product(ps, qs, Ps, Qs))\n# \n# \n#  d = 1\n#  D = 1\n#  s = 12\n#  SARIMA_result_df = optimize_SARIMA(train, SARIMA_order_list, d, D, s)\n#  SARIMA_result_df\n\n\nSARIMA_model = SARIMAX(train, order=(2, 1, 1), seasonal_order=(1, 1, 2, 12), simple_differencing=False)\nSARIMA_model_fit = SARIMA_model.fit(disp=False)\n\nSARIMA_model_fit.plot_diagnostics(figsize=(10, 8))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\ntr = acorr_ljungbox(SARIMA_model_fit.resid, np.arange(1, 11))\ntr\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\n\n\n\n\n1\n0.004688\n0.945412\n\n\n2\n0.743919\n0.689382\n\n\n3\n1.018359\n0.796810\n\n\n4\n1.223784\n0.874167\n\n\n5\n1.435211\n0.920423\n\n\n6\n1.710187\n0.944332\n\n\n7\n2.304834\n0.941063\n\n\n8\n2.715298\n0.950935\n\n\n9\n2.731198\n0.974006\n\n\n10\n4.968094\n0.893299\n\n\n\n\n\n\n\n\nSARIMA_pred = SARIMA_model_fit.get_prediction(132, 143).predicted_mean\n\ntest['SARIMA_pred'] = SARIMA_pred\n\nsns.lineplot(data=df, x=df.index, y='Passengers', label='실제 값')\nsns.lineplot(data=test, x=test.index, y='SARIMA_pred', label='SARIMA 예측값')\nplt.xticks(np.arange(0, 145, 12), np.arange(1949, 1962, 1))\nplt.xlim(120, 143)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Time Series",
      "계절성 고려"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/09.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/09.html",
    "title": "텍스트 분석 - 감성 분석",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석 - 감성 분석"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/08.html#전처리",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/08.html#전처리",
    "title": "텍스트 분석 - 20 뉴스그룹 분류",
    "section": "전처리",
    "text": "전처리\n\nfrom sklearn.datasets import fetch_20newsgroups\n\ntrain_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quates'))\nX_train = train_news.data\ny_train = train_news.target\n\ntest_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quates'))\nX_test = test_news.data\ny_test = test_news.target",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석 - 20 뉴스그룹 분류"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/08.html#학습",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/08.html#학습",
    "title": "텍스트 분석 - 20 뉴스그룹 분류",
    "section": "학습",
    "text": "학습\n\nCount Vector\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncnt_vect = CountVectorizer()\nX_train_cnt_vect = cnt_vect.fit_transform(X_train)\nX_test_cnt_vect = cnt_vect.transform(X_test)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train_cnt_vect, y_train)\npred = lr_clf.predict(X_test_cnt_vect)\nprint(f'{accuracy_score(y_test, pred):.3f} ')\n\n0.731 \n\n\n\n\nTF-IDF\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer()\nX_train_tfidf_vect = tfidf_vect.fit_transform(X_train)\nX_test_tfidf_vect = tfidf_vect.transform(X_test)\n\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(X_train_tfidf_vect, y_train)\npred = lr_clf.predict(X_test_tfidf_vect)\nprint(f'{accuracy_score(y_test, pred):.3f} ')\n\n0.778",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "텍스트 분석 - 20 뉴스그룹 분류"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/04.html#under-over-sampling",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/04.html#under-over-sampling",
    "title": "분류 - 신용 카드 사기 검출",
    "section": "under, over sampling",
    "text": "under, over sampling\n\nunder sampling: 많은 비중을 차지하는 레이블을 작은 비중의 레이블에 맞추는것\nover sampling: 반대\n\nsmote: k 최근접 이웃 진행 후, 이웃 간 간격을 맞추는 record를 새로 생성하는 방식\n\n\n\nimport pandas as pd",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 신용 카드 사기 검출"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/01.html#개요",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/01.html#개요",
    "title": "분류 - 결정 트리",
    "section": "개요",
    "text": "개요\n\n결정 노드가 많아지면 과적합이 발생할 수 있음\n가능한 한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 규칙을 정해야 함\n균일하게 데이터 세트를 구성할 수 있도록 분할하는 것이 필요\n균일도만 신경쓰면 되기 때문에 전처리 작업이 필요 없음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 결정 트리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/01.html#파라미터",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/01.html#파라미터",
    "title": "분류 - 결정 트리",
    "section": "파라미터",
    "text": "파라미터\n\nmin_samples_split: 노드를 분할하기 위한 최소한의 샘플 수.\nmin_samples_leaf: 말단 노드가 되기 위한 최소한의 샘플 수. 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요\nmax_features: 분할을 고려할 feature의 수. default는 None으로 모든 feature를 고려함\nmax_depth\nmax_leaf_nodes",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 결정 트리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/01.html#시각화",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/01.html#시각화",
    "title": "분류 - 결정 트리",
    "section": "시각화",
    "text": "시각화\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport graphviz\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\ndt_clf = DecisionTreeClassifier()\n\niris_data = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2)\n\ndt_clf.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\n\nexport_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport numpy as np\n\nfor name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):\n    print(f'{name}: {value:.3f}')\nsns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)\n\nsepal length (cm): 0.000\nsepal width (cm): 0.000\npetal length (cm): 0.525\npetal width (cm): 0.475",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 결정 트리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/01.html#examples",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/01.html#examples",
    "title": "분류 - 결정 트리",
    "section": "examples",
    "text": "examples\n\nimport pandas as pd\n\n\ndef get_new_feature_name_df(old):\n    df = pd.DataFrame(data=old.groupby('column_name').cumcount(), columns=['dup_cnt'])\n    df = df.reset_index()\n    new_df = pd.merge(old.reset_index(), df, how='outer')\n    new_df['column_name'] = new_df[['column_name', 'dup_cnt']].apply(lambda x: x[0] + '_' + str(x[1]) if x[1] &gt; 0 else x[0], axis=1)\n    new_df = new_df.drop(['index'], axis=1)\n    return new_df\n\ndef get_human_dataset():\n    feature_name_df = pd.read_csv('_data/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n\n    X_train = pd.read_csv('_data/human_activity/train/X_train.txt', sep='\\s+', names=feature_name)\n    X_test = pd.read_csv('_data/human_activity/test/X_test.txt', sep='\\s+', names=feature_name)\n\n    y_train = pd.read_csv('_data/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n    y_test = pd.read_csv('_data/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = get_human_dataset()\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7352 entries, 0 to 7351\nColumns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean)\ndtypes: float64(561)\nmemory usage: 31.5 MB\n\n\n\ny_train['action'].value_counts()\n\naction\n6    1407\n5    1374\n4    1286\n1    1226\n2    1073\n3     986\nName: count, dtype: int64\n\n\n\ndefault 파라미터 예측\n\nfrom sklearn.metrics import accuracy_score\n\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\npred = dt_clf.predict(X_test)\naccuracy = accuracy_score(y_test, pred)\naccuracy\n\n0.8547675602307431\n\n\n\n\n하이퍼파라미터 최적화\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'max_depth': [6, 8, 10, 12, 16, 20, 24],\n    'min_samples_split': [16]\n}\n\ngrid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy',cv=5, verbose=1)\ngrid_cv.fit(X_train, y_train)\n\ncv_results_df = pd.DataFrame(grid_cv.cv_results_)\ncv_results_df[['param_max_depth', 'mean_test_score']]\ncv_results_df\n\nFitting 5 folds for each of 7 candidates, totalling 35 fits\n\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_max_depth\nparam_min_samples_split\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n1.467849\n0.357629\n0.004141\n0.000225\n6\n16\n{'max_depth': 6, 'min_samples_split': 16}\n0.813732\n0.873555\n0.797279\n0.867347\n0.870068\n0.844396\n0.032238\n5\n\n\n1\n2.062000\n0.023422\n0.004424\n0.000407\n8\n16\n{'max_depth': 8, 'min_samples_split': 16}\n0.812373\n0.825969\n0.866667\n0.875510\n0.893878\n0.854879\n0.030751\n1\n\n\n2\n2.503345\n0.075515\n0.004747\n0.000570\n10\n16\n{'max_depth': 10, 'min_samples_split': 16}\n0.808973\n0.802855\n0.855782\n0.886395\n0.889796\n0.848760\n0.036986\n3\n\n\n3\n2.586922\n0.467468\n0.004648\n0.000843\n12\n16\n{'max_depth': 12, 'min_samples_split': 16}\n0.794018\n0.808294\n0.863265\n0.874150\n0.897279\n0.847401\n0.039583\n4\n\n\n4\n3.141098\n0.264828\n0.004753\n0.000360\n16\n16\n{'max_depth': 16, 'min_samples_split': 16}\n0.806934\n0.816451\n0.834694\n0.877551\n0.885034\n0.844133\n0.031714\n6\n\n\n5\n2.976725\n0.501986\n0.004743\n0.000759\n20\n16\n{'max_depth': 20, 'min_samples_split': 16}\n0.802855\n0.819850\n0.834694\n0.874150\n0.880272\n0.842364\n0.030246\n7\n\n\n6\n3.119848\n0.335677\n0.004394\n0.000332\n24\n16\n{'max_depth': 24, 'min_samples_split': 16}\n0.804895\n0.813052\n0.856463\n0.889796\n0.882313\n0.849304\n0.034833\n2\n\n\n\n\n\n\n\n\nbest_df_clf = grid_cv.best_estimator_\n\nftr_importances_values = best_df_clf.feature_importances_\nftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)\nftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\nsns.barplot(x=ftr_top20, y=ftr_top20.index)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "분류 - 결정 트리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/06.html#pca",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/06.html#pca",
    "title": "차원 축소",
    "section": "PCA",
    "text": "PCA\n\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\niris = load_iris()\ncolumns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\ndf = pd.DataFrame(iris.data, columns=columns)\ndf['target'] = iris.target\nmarkers = ['^', 's', 'o']\n\nfor i, marker in enumerate(markers):\n    x_axis_data = df[df['target'] == i]['sepal_length']\n    y_axis_data = df[df['target'] == i]['sepal_width']\n    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])\nplt.legend()\nplt.xlabel('sepal length')\nplt.ylabel('sepal width')\nplt.show()\n\n\n\n\n\n\n\n\n\nPCA는 scaling의 영향을 받음.\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nscaled_df = StandardScaler().fit_transform(df.iloc[:, :-1])\n\npca = PCA(n_components=2)\ndf = pca.fit_transform(scaled_df)\n\n\npca_columns = ['pca_component_1', 'pca_component_2']\ndf = pd.DataFrame(df, columns=pca_columns)\ndf['target'] = iris.target\n\n\nmarkers = ['^', 's', 'o']\n\nfor i, marker in enumerate(markers):\n    x_axis_data = df[df['target'] == i]['pca_component_1']\n    y_axis_data = df[df['target'] == i]['pca_component_2']\n    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])\nplt.legend()\nplt.xlabel('pca_component_1')\nplt.ylabel('pca_component_2')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "차원 축소"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/06.html#신용카드-고객-데이터",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/06.html#신용카드-고객-데이터",
    "title": "차원 축소",
    "section": "신용카드 고객 데이터",
    "text": "신용카드 고객 데이터\n\ndf = pd.read_excel('_data/creadit_card.xls', header=1, sheet_name='Data').iloc[:, 1:]\ndf.rename(columns={'PAY_0': 'PAY_1', 'default payment next month': 'default'}, inplace=True)\ntarget = df['default']\nfeatures = df.drop('default', axis=1)\n\n\nimport seaborn as sns\n\ncorr = features.corr()\nsns.heatmap(corr, annot=True, fmt='.1g')\n\n\n\n\n\n\n\n\n\nBILL_AMT1~6, PAY_1~6의 상관도가 높다.\n\n\ncols_bill = ['BILL_AMT' + str(i) for i in range(1, 7)]\nscaler = StandardScaler()\ndf_cols_scaled = scaler.fit_transform(features[cols_bill])\npca = PCA(n_components=2)\npca.fit(df_cols_scaled)\npca.explained_variance_ratio_\n\narray([0.90555253, 0.0509867 ])\n\n\n\nPCA 할 때 column 전부 다 안 넣어도 되나?\n다 넣어야 하는 듯",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "차원 축소"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/06.html#lda",
    "href": "posts/03_archives/completed_project/adp_실기/notes/machine_learning/06.html#lda",
    "title": "차원 축소",
    "section": "LDA",
    "text": "LDA\n\n클래스 분리를 최대화하는 축을 찾음\nPCA와 다르게 지도 학습임.\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import StandardScaler\n\niris = load_iris()\niris_scaled = StandardScaler().fit_transform(iris.data)\n\n\nlda = LinearDiscriminantAnalysis(n_components=2)\niris_lda = lda.fit_transform(iris_scaled, iris.target)\n\n\nlda_columns = ['lda_components_1', 'lda_components_2']\ndf = pd.DataFrame(iris_lda, columns=lda_columns)\ndf['target'] = iris.target\n\nmarkers = ['^', 's', 'o']\n\nfor i, marker in enumerate(markers):\n    x_axis_data = df[df['target'] == i]['lda_components_1']\n    y_axis_data = df[df['target'] == i]['lda_components_2']\n    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])\nplt.legend()\nplt.xlabel('lda_components_1')\nplt.ylabel('lda_components_2')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Machine Learning",
      "차원 축소"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/09.html#동전",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/09.html#동전",
    "title": "검정",
    "section": "동전",
    "text": "동전\n\nimport numpy as np\nfrom empiricaldist import Pmf\nfrom scipy.stats import binom\n\nk, n = 140, 250\nlike_fair = binom.pmf(k, n, 0.5)\n\n\nramp_up = np.arange(50)\nramp_down = np.arange(50, -1, -1)\na = np.append(ramp_up, ramp_down)\n\nxs = np.linspace(0, 1, 101)\ntriangle = Pmf(a, xs)\ntriangle.normalize()\nbiased_triangle = triangle.copy()\nbiased_triangle[0.5] = 0\nbiased_triangle.normalize()\n\nlikelihood = binom.pmf(k, n, xs)\nlike_triangle = np.sum(biased_triangle * likelihood)\n\n\nk = like_fair / like_triangle\nk, k / (k + 1)\n\n(np.float64(1.1970766535647133), np.float64(0.5448497446015277))\n\n\n\n250번 중 140번이 앞면이 나왔는데도 여전히 공정할 확률이 높다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "검정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/09.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/09.html#연습문제",
    "title": "검정",
    "section": "연습문제",
    "text": "연습문제\n\n10-2",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "검정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/16.html#더-많은-눈이-내렸을까",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/16.html#더-많은-눈이-내렸을까",
    "title": "회귀",
    "section": "더 많은 눈이 내렸을까?",
    "text": "더 많은 눈이 내렸을까?\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\ndf = pd.read_csv('https://github.com/AllenDowney/ThinkBayes2/raw/master/data/2239075.csv', parse_dates=[2])\ndf['YEAR'] = df['DATE'].dt.year\nsnow = df.groupby('YEAR')['SNOW'].sum()\nsnow = snow.iloc[1:-1]\nsnow.plot(ls='', marker='o', label='강설량')\nplt.legend()\n\n\n\n\n\n\n\n\n\nfrom empiricaldist import Pmf\nimport statsmodels.formula.api as smf\ndata = snow.reset_index()\noffset = data['YEAR'].mean().round()\ndata['x'] = data['YEAR'] - offset\ndata['y'] = data['SNOW']\n\nformula = 'y ~ x'\nresults = smf.ols(formula, data=data).fit()\nresults.params\n\nIntercept    64.446325\nx             0.511880\ndtype: float64\n\n\n\n사전분포\n\nqs = np.linspace(-0.5, 1.5, 51)\nprior_slope = Pmf.from_seq(qs)\nqs = np.linspace(54, 75, 41)\nprior_inter = Pmf.from_seq(qs)\nqs = np.linspace(20, 35, 31)\nprior_sigma = Pmf.from_seq(qs)\n\n\ndef make_joint(pmf1, pmf2):\n    X, Y = np.meshgrid(pmf1, pmf2)\n    return pd.DataFrame(X * Y, columns=pmf1.index, index=pmf2.index)\n\ndef make_joint3(pmf1, pmf2, pmf3):\n    joint2 = make_joint(pmf2, pmf1).stack()\n    joint3 = make_joint(pmf3, joint2).stack()\n    return Pmf(joint3)\n\nprior = make_joint3(prior_slope, prior_inter, prior_sigma)\nprior\n\n\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n-0.5\n54.0\n20.0\n0.000015\n\n\n20.5\n0.000015\n\n\n21.0\n0.000015\n\n\n21.5\n0.000015\n\n\n22.0\n0.000015\n\n\n...\n...\n...\n...\n\n\n1.5\n75.0\n33.0\n0.000015\n\n\n33.5\n0.000015\n\n\n34.0\n0.000015\n\n\n34.5\n0.000015\n\n\n35.0\n0.000015\n\n\n\n\n64821 rows × 1 columns",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/15.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/15.html",
    "title": "로지스틱 회귀",
    "section": "",
    "text": "\\(logO(H|x) = β_0 + β_1x\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "로지스틱 회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/15.html#우주-왕복선-문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/15.html#우주-왕복선-문제",
    "title": "로지스틱 회귀",
    "section": "우주 왕복선 문제",
    "text": "우주 왕복선 문제\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter2_MorePyMC/data/challenger_data.csv')\npred = df.iloc[-1]\ndf = df[:-1].dropna()\noffset = df['Temperature'].mean().round()\ndf['x'] = df['Temperature'] - offset\ndf['y'] = df['Damage Incident'].astype(int)\n\ndf\n\n\n\n\n\n\n\n\nDate\nTemperature\nDamage Incident\nx\ny\n\n\n\n\n0\n04/12/1981\n66\n0\n-4.0\n0\n\n\n1\n11/12/1981\n70\n1\n0.0\n1\n\n\n2\n3/22/82\n69\n0\n-1.0\n0\n\n\n4\n01/11/1982\n68\n0\n-2.0\n0\n\n\n5\n04/04/1983\n67\n0\n-3.0\n0\n\n\n6\n6/18/83\n72\n0\n2.0\n0\n\n\n7\n8/30/83\n73\n0\n3.0\n0\n\n\n8\n11/28/83\n70\n0\n0.0\n0\n\n\n9\n02/03/1984\n57\n1\n-13.0\n1\n\n\n10\n04/06/1984\n63\n1\n-7.0\n1\n\n\n11\n8/30/84\n70\n1\n0.0\n1\n\n\n12\n10/05/1984\n78\n0\n8.0\n0\n\n\n13\n11/08/1984\n67\n0\n-3.0\n0\n\n\n14\n1/24/85\n53\n1\n-17.0\n1\n\n\n15\n04/12/1985\n67\n0\n-3.0\n0\n\n\n16\n4/29/85\n75\n0\n5.0\n0\n\n\n17\n6/17/85\n70\n0\n0.0\n0\n\n\n18\n7/29/85\n81\n0\n11.0\n0\n\n\n19\n8/27/85\n76\n0\n6.0\n0\n\n\n20\n10/03/1985\n79\n0\n9.0\n0\n\n\n21\n10/30/85\n75\n1\n5.0\n1\n\n\n22\n11/26/85\n76\n0\n6.0\n0\n\n\n23\n01/12/1986\n58\n1\n-12.0\n1\n\n\n\n\n\n\n\n\n전통 로지스틱\n\nimport statsmodels.formula.api as smf\n\nformula = 'y ~ x'\nresults = smf.logit(formula, data=df).fit(disp=False)\nresults.params\n\nIntercept   -1.208490\nx           -0.232163\ndtype: float64\n\n\n\nfrom scipy.special import expit\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\ninter = results.params['Intercept']\nslope = results.params['x']\nxs = np.arange(53, 83) - offset\nps = expit(inter + slope * xs)\nplt.plot(xs + offset, ps)\nplt.scatter(df['x'] + offset, df['y'])\n\n\n\n\n\n\n\n\n\n\n사전 분포\n\nfrom empiricaldist import Pmf\n\ndef make_joint(pmf1, pmf2):\n    X, Y = np.meshgrid(pmf1, pmf2)\n    return pd.DataFrame(X * Y, columns=pmf1.qs, index=pmf2.qs)\n\nqs = np.linspace(-5, 1, 101)\nprior_inter = Pmf.from_seq(qs)\nqs = np.linspace(-0.8, 0.1, 101)\nprior_slope = Pmf.from_seq(qs)\njoint = make_joint(prior_inter, prior_slope)\njoint_pmf = Pmf(joint.stack())\njoint_pmf\n\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n-0.8\n-5.00\n0.000098\n\n\n-4.94\n0.000098\n\n\n-4.88\n0.000098\n\n\n-4.82\n0.000098\n\n\n-4.76\n0.000098\n\n\n...\n...\n...\n\n\n0.1\n0.76\n0.000098\n\n\n0.82\n0.000098\n\n\n0.88\n0.000098\n\n\n0.94\n0.000098\n\n\n1.00\n0.000098\n\n\n\n\n10201 rows × 1 columns\n\n\n\n\n\n가능도\n\nfrom scipy.stats import binom\n\ngrouped = df.groupby('x')['y'].agg(['count', 'sum'])\nns = grouped['count']\nks = grouped['sum']\nxs = grouped.index\nps = expit(inter + slope * xs)\nlikes = binom.pmf(ks, ns, ps)\nlikes\n\narray([0.93924781, 0.85931657, 0.82884484, 0.60268105, 0.56950687,\n       0.24446388, 0.67790595, 0.72637895, 0.18815003, 0.8419509 ,\n       0.87045398, 0.15645171, 0.86667894, 0.95545945, 0.96435859,\n       0.97729671])\n\n\n\nlikelihood = joint_pmf.copy()\nfor slope, inter in joint_pmf.index:\n    ps = expit(inter + slope * xs)\n    likes = binom.pmf(ks, ns, ps)\n    likelihood[slope, inter] = likes.prod()\n\n\n\n갱신\n\nposterior_pmf = joint_pmf * likelihood\nposterior_pmf.normalize()\njoint_posterior = posterior_pmf.unstack()\n\nmarginal_inter = Pmf(joint_posterior.sum(axis=0))\nmarginal_inter.normalize()\nmarginal_slope = Pmf(joint_posterior.sum(axis=1))\nmarginal_slope.normalize()\n\nmarginal_inter.plot()\n\n\n\n\n\n\n\n\n\nmarginal_slope.plot()\n\n\n\n\n\n\n\n\n\n\n분포 변환\n\nmarginal_probs = marginal_inter.transform(expit)\nmarginal_lr = marginal_slope.transform(np.exp)\n\n\n\n예측 분포\n\nsample = posterior_pmf.choice(101)\ntemps = np.arange(31, 83)\nxs = temps - offset\npred = np.empty((len(sample), len(xs)))\nfor i, (slope, inter) in enumerate(sample):\n    pred[i] = expit(inter + slope * xs)\nlow, median, high = np.percentile(pred, [5, 50, 95], axis=0)\nplt.fill_between(temps, low, high, color='C1', alpha=0.2, label='95% 신뢰구간')\nplt.plot(temps, median, color='C1', label='logistic model')\nplt.legend()\nplt.scatter(df['x'] + offset, df['y'], label='data')",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "로지스틱 회귀"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html",
    "title": "비율 추정",
    "section": "",
    "text": "from scipy.stats import binom\nfrom empiricaldist import Pmf\nimport numpy as np\n\ndef make_binominal(n, p):\n  ks = np.arange(n + 1)\n  ps = binom.pmf(ks, n, p)\n  return Pmf(ps, ks)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#유로-동전-문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#유로-동전-문제",
    "title": "비율 추정",
    "section": "유로 동전 문제",
    "text": "유로 동전 문제\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\npmf_k = make_binominal(n=250, p=0.5)\npmf_k.plot(label='coin', color='C5')\nplt.title('coin toss 이항분포')\nplt.xlabel('앞면이 나온 횟수(k)')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n동전을 250번 던져서 앞면의 횟수가 140과 같은 극단적인 값이 나올 확률\n\n\npmf_k.prob_ge(140) + pmf_k.prob_le(110)\n\nnp.float64(0.06642115124004333)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#베이지안-추정",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#베이지안-추정",
    "title": "비율 추정",
    "section": "베이지안 추정",
    "text": "베이지안 추정\n\n동전 앞면의 비율을 균등분포로 가정하고 시작\n\n\nhypos = np.linspace(0, 1, 101)\nprior = Pmf(1, hypos)\nlikelihood_heads = hypos\nlikelihood_tails = 1 - hypos\nlikelihood = {\n  'H': likelihood_heads,\n  'T': likelihood_tails\n}\nprior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\n0.00\n1\n\n\n0.01\n1\n\n\n0.02\n1\n\n\n0.03\n1\n\n\n0.04\n1\n\n\n...\n...\n\n\n0.96\n1\n\n\n0.97\n1\n\n\n0.98\n1\n\n\n0.99\n1\n\n\n1.00\n1\n\n\n\n\n101 rows × 1 columns\n\n\n\n\ndef update_euro(pmf, dataset):\n  for data in dataset:\n    pmf *= likelihood[data]\n  pmf.normalize()\n\n\nposterior = prior.copy()\ndataset = 'H' * 140 + 'T' * 110\n\nupdate_euro(posterior, dataset)\n\nposterior.plot(label='coin', color='C5')\nplt.title('동전 앞면 비율의 사후확률 분포')\nplt.xlabel('동전 앞면의 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\nprint(f'동전의 앞면이 250번 중 140번 등장했다면 앞면의 비율은 {posterior.max_prob()}일 확률이 가장 높다.')\n\n동전의 앞면이 250번 중 140번 등장했다면 앞면의 비율은 0.56일 확률이 가장 높다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#삼각사전분포",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#삼각사전분포",
    "title": "비율 추정",
    "section": "삼각사전분포",
    "text": "삼각사전분포\n\n사전확률을 균등분포로 설정했지만, 실제로는 정규분포에 가까울 것이다.\n근데 책에서는 일단 삼각분포를 사용한다.\n\n\nramp_up = np.arange(50)\nramp_down = np.arange(50, -1, -1)\na = np.append(ramp_up, ramp_down)\ntriangle = Pmf(a, hypos, name='triangle')\ntriangle.normalize()\n\nuniform = Pmf(1, hypos, name='uniform')\nuniform.normalize()\n\nuniform.plot(label='균등사전', color='C4')\ntriangle.plot(label='삼각사전', color='C5')\nplt.legend()\nplt.title('삼각사전분포 및 균등사전분포')\nplt.xlabel('동전 앞면의 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\nupdate_euro(uniform, dataset)\nupdate_euro(triangle, dataset)\n\nuniform.plot(label='균등사전', color='C4')\ntriangle.plot(label='삼각사전', color='C5')\nplt.legend()\nplt.title('삼각 및 균등 사후분포')\nplt.xlabel('동전 앞면의 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n두 확률분포의 사후 분포간 차이는 미미함.\n데이터가 충분하다면 서로 다른 사전확률로 시작한다고 해도 동일한 사후확률로 수렴하는 경향이 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#이항가능도함수",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#이항가능도함수",
    "title": "비율 추정",
    "section": "이항가능도함수",
    "text": "이항가능도함수\n\n갱신을 굳이 250번 하지 않아도, 이항분포를 통해 가능도를 한번에 계산할 수 있다.\n\n\ndef update_binomial(pmf, data):\n  k, n = data\n  xs = pmf.qs\n  likelihood = binom.pmf(k, n, xs)\n  pmf *= likelihood\n  pmf.normalize()\n\n\nuniform2 = Pmf(1, hypos, name='uniform2')\ndata = 140, 250\nupdate_binomial(uniform2, data)\nnp.allclose(uniform, uniform2)\n\nTrue",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/03.html#연습문제",
    "title": "비율 추정",
    "section": "연습문제",
    "text": "연습문제\n\n4-1\n\nhypos = np.linspace(0.2, 0.33, 101)\nuniform = Pmf(1, hypos)\nprior = uniform.copy()\nprior.normalize()\n\ndata = 3, 3\nupdate_binomial(uniform, data)\nprior.plot(label='사전', color='C4')\nuniform.plot(label='사후', color='C5')\nplt.legend()\nplt.title('사전 vs 사후 분포')\nplt.xlabel('안타 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n\n4-2\n\ndef update_randomized_response(pmf, data):\n    yes_count, total = data\n    ps = pmf.qs\n    prob_yes = 0.5 + 0.5 * ps\n    likelihood = binom.pmf(yes_count, total, prob_yes)\n    pmf *= likelihood\n    pmf.normalize()\n\nhypos = np.linspace(0, 1, 101)\nuniform = Pmf(1, hypos)\nprior = uniform.copy()\nprior.normalize()\n\ndata = 80, 100\nupdate_randomized_response(uniform, data)\n\nprior.plot(label='사전', color='C4')\nuniform.plot(label='사후', color='C5')\nplt.legend()\nplt.title('Randomized Response - 사전 vs 사후 분포')\nplt.xlabel('탈세자 비율 (p)')\nplt.ylabel('PMF')\nplt.grid(True, alpha=0.3)\n\n\n\n\n\n\n\n\n\nmost_likely_rate = uniform.max_prob()\nprint(f'가장 가능성이 높은 탈세자 비율: {most_likely_rate:.3f}')\n\ncredible_interval = uniform.credible_interval(0.95)\nprint(f'95% 신뢰구간: [{credible_interval[0]:.3f}, {credible_interval[1]:.3f}]')\n\n가장 가능성이 높은 탈세자 비율: 0.600\n95% 신뢰구간: [0.420, 0.730]\n\n\n\n\n4-3\n\ndef update_machine_response(pmf, data, y):\n    k, n = data\n    xs = pmf.qs * (1 - y) + (1 - pmf.qs) * y\n    likelihood = binom.pmf(k, n, xs)\n    pmf *= likelihood\n    pmf.normalize()\n\nhypos = np.linspace(0, 1, 101)\nuniform = Pmf(1, hypos)\n\ndata = 140, 250\nfor y in np.linspace(0, 0.5, 6):\n    dist = uniform.copy()\n    update_machine_response(dist, data, y)\n    dist.plot(label=f'y={y:.1f}')\nplt.legend()\nplt.title('y에 따른 앞면의 비율')\nplt.xlabel('동전 앞면의 비율')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\n\n4-4\n\nhypos = np.linspace(0.1, 0.4, 101)\nuniform = Pmf(1, hypos)\nprior = Pmf(1, hypos)\nprior.normalize()\n\nprob_both_0 = (1 - hypos) ** 4\nprob_both_1 = (2 * hypos * (1 - hypos)) ** 2\nprob_both_2 = hypos ** 4\n\nlikelihood = prob_both_0 + prob_both_1 + prob_both_2\nposterior = uniform * likelihood\nposterior.normalize()\n\nprior.plot(label='사전분포', color='C1')\nposterior.plot(label='사후분포', color='C2')\nplt.legend()\nplt.title('사전 분포와 사후분포')\nplt.xlabel('우주선을 맞출 확률 (x)')\nplt.ylabel('PMF')\n\nprior_mean = prior.mean()\nposterior_mean = posterior.mean()\n\nprint(\"\\n=== 연습문제 4-4 답변 ===\")\nif posterior_mean &gt; prior_mean:\n    print(\"✅ 데이터는 좋은 소식입니다 (GOOD)\")\n    print(f\"✅ x의 추정값이 {prior_mean:.3f}에서 {posterior_mean:.3f}로 증가했습니다 (INCREASE)\")\nelse:\n    print(\"❌ 데이터는 나쁜 소식입니다 (BAD)\")\n    print(f\"❌ x의 추정값이 {prior_mean:.3f}에서 {posterior_mean:.3f}로 감소했습니다 (DECREASE)\")\n\n\n=== 연습문제 4-4 답변 ===\n❌ 데이터는 나쁜 소식입니다 (BAD)\n❌ x의 추정값이 0.250에서 0.235로 감소했습니다 (DECREASE)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "비율 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/04.html#기관차-문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/04.html#기관차-문제",
    "title": "수량 추정",
    "section": "기관차 문제",
    "text": "기관차 문제\n\n각 철도를 지나가는 기관차에 1부터 N까지의 순서로 번호를 붙인다.\n60번 번호가 붙은 기관차를 보았다.\n이 철도에 몇 개의 기관차가 지나가는지 추정해보자\n\n\nimport numpy as np\nfrom empiricaldist import Pmf\n\nhypos = np.arange(1, 1001)\nprior = Pmf(1, hypos)\n\n\n가정: N은 1부터 1000까지의 값 중 한 값이 동일한 확률로 선택될 수 있다.\n\n\ndef update_train(pmf, data):\n  hypos = pmf.qs\n  likelihood = 1 / hypos\n  likelihood[(data &gt; hypos)] = 0\n  pmf *= likelihood\n  pmf.normalize()\n\n\ndata = 60\nposterior = prior.copy()\nupdate_train(posterior, data)\n\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nposterior.plot(label='60번 기관차 발견 시 전체 기관차 수의 사후확률', color='C5')\nplt.legend()\nplt.title('사후 확률')\nplt.xlabel('기관차 수')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\nposterior.max_prob()\n\nnp.int64(60)\n\n\n\n당연하다는 듯이 60이 최선의 선택. 하지만 이는 별로 도움이 안됨.\n대안으로 사후확률의 평균을 구해본다.\n\n\nposterior.mean()\n\nnp.float64(333.41989326370776)\n\n\n\n해당 값을 선택하는 것이 장기적으로 좋은 선택.\n\n\nimport pandas as pd\n\ndf = pd.DataFrame(columns=['사후확률 분포 평균'])\ndf.index.name = '상한값'\n\ndataset = [30, 60, 90]\n\nfor high in [500, 1000, 2000]:\n    hypos = np.arange(1, high+1)\n    pmf = Pmf(1, hypos)\n    for data in dataset:\n        update_train(pmf, data)\n    df.loc[high] = pmf.mean()\ndf\n\n\n\n\n\n\n\n\n사후확률 분포 평균\n\n\n상한값\n\n\n\n\n\n500\n151.849588\n\n\n1000\n164.305586\n\n\n2000\n171.338181\n\n\n\n\n\n\n\n\n하지만 상한값의 범위의 변화에 따른 사후확률 분포의 평균값이 크게 달라진다.\n이럴때는 2가지 방법이 있다.\n\n데이터를 더 확보\n배경지식을 더 확보해서 더 나은 사전확률을 선택",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "수량 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/04.html#멱법칙-사전확률",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/04.html#멱법칙-사전확률",
    "title": "수량 추정",
    "section": "멱법칙 사전확률",
    "text": "멱법칙 사전확률\n\n기관차 수는 멱법칙을 주로 따르는 것으로 알려져 있음\n더 적합한 사전확률은 안정적인 사전확률을 제공할 수 있다.\n\n\nalpha = 1.0\nps = hypos ** (-alpha)\npower = Pmf(ps, hypos, name='power law')\npower.normalize()\n\nnp.float64(8.178368103610282)\n\n\n\nuniform = Pmf(1, hypos, name='uniform')\nuniform.normalize()\n\npower.plot(label='power', color='skyblue')\nuniform.plot(label='uniform', color='pink')\nplt.legend()\nplt.title('사전확률 분포')\nplt.xlabel('기관차 수')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\nupdate_train(uniform, 60)\nupdate_train(power, 60)\n\npower.plot(label='power', color='skyblue')\nuniform.plot(label='uniform', color='pink')\nplt.legend()\nplt.title('사후확률 분포')\nplt.xlabel('기관차 수')\nplt.ylabel('PMF')\n\nText(0, 0.5, 'PMF')\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(columns=['사후확률 분포 평균'])\ndf.index.name = '상한값'\n\nalpha = 1.0\ndataset = [30, 60, 90]\n\nfor high in [500, 1000, 2000]:\n    hypos = np.arange(1, high+1)\n    ps = hypos**(-alpha)\n    power = Pmf(ps, hypos)\n    for data in dataset:\n        update_train(power, data)\n    df.loc[high] = power.mean()\ndf\n\n\n\n\n\n\n\n\n사후확률 분포 평균\n\n\n상한값\n\n\n\n\n\n500\n130.708470\n\n\n1000\n133.275231\n\n\n2000\n133.997463",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "수량 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/04.html#신뢰구간",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/04.html#신뢰구간",
    "title": "수량 추정",
    "section": "신뢰구간",
    "text": "신뢰구간\n\npower.credible_interval(0.9)\n\narray([ 91., 243.])",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "수량 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/04.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/04.html#연습문제",
    "title": "수량 추정",
    "section": "연습문제",
    "text": "연습문제\n\n5-1\n\nfrom scipy.stats import binom\n\ndef update(pmf, k, p):\n    likelihood = binom.pmf(k, pmf.qs, p)\n    pmf *= likelihood\n    pmf.normalize()\n\nhypos = np.arange(1, 2001)\nprior = Pmf(1, hypos)\n\nposterior = prior.copy()\n\nupdate(posterior, 2, 1/365)\nprint(f\"5월 11일 데이터 적용 후 평균 인원수: {posterior.mean():.1f}\")\n\nupdate(posterior, 1, 1/365)\nprint(f\"5월 23일 데이터 적용 후 평균 인원수: {posterior.mean():.1f}\")\n\nupdate(posterior, 0, 1/365)\nprint(f\"8월 1일 데이터 적용 후 평균 인원수: {posterior.mean():.1f}\")\n\n5월 11일 데이터 적용 후 평균 인원수: 957.1\n5월 23일 데이터 적용 후 평균 인원수: 721.7\n8월 1일 데이터 적용 후 평균 인원수: 486.2\n\n\n\nestimated_people = posterior.mean()\nprint(f\"추정된 강당 인원수: {estimated_people:.1f}명\")\n\nprob_over_1200 = posterior[posterior.qs &gt; 1200].sum()\nprint(f\"1200명을 초과할 확률: {prob_over_1200:.4f} ({prob_over_1200*100:.2f}%)\")\n\nci_90 = posterior.credible_interval(0.9)\nprint(f\"90% 신뢰구간: [{ci_90[0]:.0f}, {ci_90[1]:.0f}]\")\n\n추정된 강당 인원수: 486.2명\n1200명을 초과할 확률: 0.0112 (1.12%)\n90% 신뢰구간: [166, 942]\n\n\n\n\n5-2\n\ndef rabbit_likelihood(n):\n    return ((n - 1) / n) * (1/n) * (1/n) * 3\n\nhypos = np.arange(4, 11)\nprior = Pmf(1, hypos)\n\nlikelihood = [rabbit_likelihood(n) for n in hypos]\n\nposterior = prior.copy()\nposterior *= likelihood\nposterior.normalize()\n\nprint(f\"\\n추정 토끼 수 (평균): {posterior.mean():.2f}마리\")\n\n\n추정 토끼 수 (평균): 5.92마리\n\n\n\n\n5-3\n\ndef update_remain(pmf, data):\n    hypos = pmf.qs\n    likelihood = 1 / hypos\n    likelihood[(hypos &lt; data)] = 0\n    pmf *= likelihood\n    pmf.normalize()\n\nhypos = np.arange(0, 1096)\nprior = Pmf(1, hypos)\nprior.normalize()\n\nupdate_remain(prior, 1095)\nprior.plot(label='사후확률', color='pink')\nplt.legend()\nplt.show()\n\n\n\n5-5\n\nimport numpy as np\nfrom empiricaldist import Pmf\n\nhypos_short = np.arange(0, 201) # 10억 단위\nprior_short = Pmf(1, hypos_short, name=\"short\")\nprior_short.normalize()\n\nhypos_long = np.arange(0, 2001)\nprior_long = Pmf(1, hypos_long, name=\"long\")\nprior_long.normalize()\n\nlikelihood_ps = {}\nfor pmf in [prior_short, prior_long]:\n    likelihood = 1 / pmf.qs\n    likelihood[(pmf.qs &lt; 108)] = 0\n    pmf *= likelihood\n    pmf.normalize()\n    likelihood_ps[pmf.name] = pmf(108)\n    pmf.plot(label=f\"{pmf.name}: {pmf(108)}\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nprior = Pmf.from_seq(['short', 'long'])\nfor hypos in prior.index:\n    prior.loc[hypos] *= likelihood_ps[hypos]\n\nprior.normalize()\nprior\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nlong\n0.175733\n\n\nshort\n0.824267",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "수량 추정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#공산",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#공산",
    "title": "공산과 가산",
    "section": "공산",
    "text": "공산\n\ndef odds(p):\n  return p / (1 - p)\n\ndef prob(o):\n  return o / (1 + o)\n\ndef prob2(a, b):\n  return a / (a + b)\n\nodds(0.6)\n\n1.4999999999999998",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#베이즈-규칙",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#베이즈-규칙",
    "title": "공산과 가산",
    "section": "베이즈 규칙",
    "text": "베이즈 규칙\n\\(odds(A|D) = odds(A) \\cdot \\frac{P(D|A)}{P(D|B)}\\)\n\nprior_odds = 1\nlikelihood_ratio = (3/4) / (1/2)\npost_odds = prior_odds * likelihood_ratio\npost_odds\n\n1.5\n\n\n\nprob(post_odds)\n\n0.6\n\n\n\nlikelihood_ratio = (1/4) / (1/2)\npost_odds *= likelihood_ratio\npost_odds\n\n0.75\n\n\n\nprob(post_odds)\n\n0.42857142857142855",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#올리버의-혈액형",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#올리버의-혈액형",
    "title": "공산과 가산",
    "section": "올리버의 혈액형",
    "text": "올리버의 혈액형\n\n범죄 현장에서 두 사람의 혈흔을 발견했다.\n발견된 혈흔은 O형과 AB형이다.\n해당 지역에서 O형의 비율은 0.6, AB형의 비율은 0.01이다.\n올리버는 O형이다.\n범죄 현장의 혈흔(데이터)이 올리버가 범인 중 한 명이라는 질문(가정)에 대한 증거가 될 수 있는가\n\n\\(odds(A|D) = odds(A) \\cdot \\frac{P(D|A)}{P(D|B)}\\) \\(→ \\frac{odds(A|D)}{odds(A)} = \\frac{P(D|A)}{P(D|B)}\\)\n\n베이즈 요인 &gt; 1: A의 가정 하에 존재하는게 더 가깝다.\n베이즈 요인 &lt; 1: B의 가정 하에 존재하는게 더 가깝다.\n베이즈 요인 == 1: 양쪽 가설 하에서 동일한 가능성을 가진다.\n\n\nlike1 = 0.01 # oliver가 혈흔을 남긴 경우\nlike2 = 2 * 0.6 * 0.01 # oliver가 혈흔을 남기지 않은 경우\n\nlikelihood_ratio = like1 / like2\nlikelihood_ratio\n\n0.8333333333333334\n\n\n\n가능도비가 1보다 낮기 때문에, oliver가 혈흔을 남기지 않은 쪽에 더 가깝다.\n\n\npost_odds = 1 * like1 / like2\nprob(post_odds)\n\n0.45454545454545453",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#글루텐-민감도",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#글루텐-민감도",
    "title": "공산과 가산",
    "section": "글루텐 민감도",
    "text": "글루텐 민감도\n\n글루텐에 민감한 사람은 블라이든 검사에서 글루텐 밀가루를 정확히 식별할 확률이 95%다\n글루텐에 민감하지 않은 사람이 우연히 글루텐 밀가루를 식별할 확률은 40%다.\n\n\nn = 35\nnum_sensitive = 10\nnum_insensitive = n - num_sensitive\n\n\nfrom scipy.stats import binom\nfrom empiricaldist import Pmf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef make_binomial(n, p):\n  ks = np.arange(n+1)\n  ps = binom.pmf(ks, n, p)\n  return Pmf(ps, ks)\n\ndist_sensitive = make_binomial(num_sensitive, 0.95)\ndist_insensitive = make_binomial(num_insensitive, 0.40)\n\ndist_total = Pmf.add_dist(dist_sensitive, dist_insensitive)\n\ndist_sensitive.plot(label='sensitive', ls=':')\ndist_insensitive.plot(label='insensitive', ls='--')\ndist_total.plot(label='total')\nplt.legend()\n\n\n\n\n\n\n\n\n\n역산\n\n35명의 피험자중 12명이 글루텐이 있다고 했을 때, 글루텐에 민감한 사람의 비율은?\n\n\nimport pandas as pd\n\ntable = pd.DataFrame()\nfor num_sensitive in range(0, n+1):\n  num_insensitive = n - num_sensitive\n  dist_sensitive = make_binomial(num_sensitive, 0.95)\n  dist_insensitive = make_binomial(num_insensitive, 0.40)\n  dist_total = Pmf.add_dist(dist_sensitive, dist_insensitive)\n  table[num_sensitive] = dist_total\n\nfor dist in table:\n  table[dist].plot(label=f'num_sensitive = {dist}')\n\n\n\n\n\n\n\n\n\nlikelihood1 = table.loc[12]\nhypos = np.arange(n+1)\nprior = Pmf(1, hypos)\n\nposterior1 = prior * likelihood1\nposterior1.normalize()\n\nlikelihood2 = table.loc[20]\nposterior2 = prior * likelihood2\nposterior2.normalize()\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nposterior1.plot(label='12개가 정확히 분류한 경우의 사후분포', color='C4')\nposterior2.plot(label='20개가 정확히 분류한 경우의 사후분포', color='C5')\nplt.legend()\n\n\n\n\n\n\n\n\n\nprint(f'12명이 정확히 분류했을 때 글루텐 민감한 사람은 {posterior1.max_prob()}명, 20명이 정확히 분류했을 때 {posterior2.max_prob()}명일 확률이 높음')\n\n12명이 정확히 분류했을 때 글루텐 민감한 사람은 0명, 20명이 정확히 분류했을 때 11명일 확률이 높음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/05.html#연습문제",
    "title": "공산과 가산",
    "section": "연습문제",
    "text": "연습문제\n\n6-1\n\nfor trust in [0.9, 0.5, 0.1]:\n  prior_odds = odds(trust)\n  post_odds =  prior_odds * likelihood_ratio\n  print(f'{trust}, {prior_odds}: {prob(post_odds)}')\n\n0.9, 9.000000000000002: 0.8823529411764706\n0.5, 1.0: 0.45454545454545453\n0.1, 0.11111111111111112: 0.08474576271186442\n\n\n\n\n6-2\n\nprior_odds = odds(1/3)\npost_odds = prior_odds * 2\npost_odds *= 1.25\n\nprob(post_odds)\n\n0.5555555555555555\n\n\n\n\n6-3\n\nprior_odds = odds(1/10)\npost_odds = prior_odds * (2 ** 3)\n\nprob(post_odds)\n\n0.4705882352941177\n\n\n\n베이즈 방법을 이용하나 빈도주의적 방법을 이용하나, 확률을 구하는 문제에 있어서는 차이가 없다.\n하지만 통계적 추론 방법을 선택하는데 있어서는 차이가 있다.\n\n\n\n6-4\n\nprior_odds = odds(14/100)\npost_odds = prior_odds * 25\n\nprob(post_odds)\n\n0.8027522935779816\n\n\n\n\n6-5\n\ndice6 = Pmf.from_seq(np.arange(1, 7))\ngoblin_health = dice6.add_dist(dice6)\nremaining_health = Pmf.sub_dist(goblin_health, 3)\nattack_damage = dice6\n\ndefeat_probability = 0\nfor damage, damage_prob in attack_damage.items():\n    for remaining, remaining_prob in remaining_health.items():\n        if remaining &gt; 0 and damage &gt; remaining:\n            defeat_probability += damage_prob * remaining_prob\n\nprint(f\"\\n고블린을 물리칠 확률: {defeat_probability:.3f}\")\n\n\n고블린을 물리칠 확률: 0.292\n\n\n\n\n6-6\n\nimport numpy as np\n\nhypos = [6, 8, 12]\npmf = Pmf(1/3, hypos)\ndice = [Pmf([1/6] * 6, np.arange(1, 7)),\n        Pmf([1/8] * 8, np.arange(1, 9)),\n        Pmf([1/12] * 12, np.arange(1, 13))]\nlike = [d.mul_dist(d) for d in dice]\ndf = pd.DataFrame(like).fillna(0).transpose()\ndf *= pmf.ps\ndf[1][12]\n\nnp.float64(0.020833333333333332)\n\n\n\n\n6-7\n\npmf = Pmf(1, ['long', 'zost', 'bell'])\npmf.normalize()\ndice = Pmf([1/3] * 3, np.arange(0, 3))\nlike = [dice.add_dist(dice).add_dist(dice).add_dist(dice).add_dist(dice),\n        dice.add_dist(dice).add_dist(dice).add_dist(dice),\n        dice.add_dist(dice).add_dist(dice)]\ndf = pd.DataFrame(like).fillna(0).transpose()\nfor i in [3, 4, 5]:\n  pmf *= np.array(df.loc[i])\n  pmf.normalize()\npmf\n\n\n\n\n\n\n\n\nprobs\n\n\n\n\nlong\n0.235762\n\n\nzost\n0.449704\n\n\nbell\n0.314534\n\n\n\n\n\n\n\n\n\n6-8\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import binom\nfrom empiricaldist import Pmf\n\nn_total = 538\nn_outperform = 312\n\nhypos = np.arange(n_total + 1)\nprior = Pmf(1, hypos)\nprior.normalize()\n\nlikelihoods = np.zeros(prior.shape[0])\nfor n_honest in hypos:\n    n_dishonest = n_total - n_honest\n    honest_dist = make_binomial(n_honest, 0.5)\n    dishonest_dist = make_binomial(n_dishonest, 0.9)\n    total_dist = Pmf.add_dist(honest_dist, dishonest_dist)\n    likelihood = total_dist[n_outperform]\n    likelihoods[n_honest] = likelihood\n\nposterior = prior * likelihoods\nposterior.normalize()\n\nmost_likely_honest = posterior.max_prob()\nposterior_prob = posterior[most_likely_honest]\n\nprint(f\"가장 가능성이 높은 정직한 의원 수: {most_likely_honest}명\")\nprint(f\"해당 확률: {posterior_prob:.4f}\")\n\n# 사후분포 시각화\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12, 6))\nposterior.plot()\nplt.xlabel('정직한 의원 수')\nplt.ylabel('확률')\nplt.title('정직한 의원 수에 대한 사후분포')\nplt.axvline(most_likely_honest, color='red', linestyle='--', \n           label=f'최빈값: {most_likely_honest}명')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 95% 신용구간\ncredible_interval = posterior.credible_interval(0.95)\nprint(f\"95% 신용구간: {credible_interval[0]}명 - {credible_interval[1]}명\")\n\n가장 가능성이 높은 정직한 의원 수: 430명\n해당 확률: 0.0147\n\n\n\n\n\n\n\n\n\n95% 신용구간: 380.0명 - 486.0명",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "공산과 가산"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/07.html#연습문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/07.html#연습문제",
    "title": "포아송 과정",
    "section": "연습문제",
    "text": "연습문제\n\n8-1\n\nfrom scipy.stats import gamma\nimport numpy as np\nfrom empiricaldist import Pmf\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.family'] = 'Noto Sans KR'\n\nalpha = 1.4\nqs = np.linspace(0, 10, 101)\nps = gamma.pdf(qs, alpha)\nprior = Pmf(ps, qs)\nprior.normalize()\n\nnp.float64(9.889360237140306)\n\n\n\ndef expo_pdf(t, lam):\n    return lam * np.exp(-lam * t)\n\nt1 = 11/90\nlikelihood1 = expo_pdf(t1, prior.qs)\npost1 = prior * likelihood1\npost1.normalize()\n\n# 지수분포에서 모든 시행은 독립적\nt2 = 12/90\nlikelihood2 = expo_pdf(t2, post1.qs)\npost2 = post1 * likelihood2\npost2.normalize()\n\nprior.plot(label=\"사전분포\", alpha=0.7)\npost1.plot(label=\"1골 득점 후 사후분포\", alpha=0.7)\npost2.plot(label=\"2골 득점 후 사후분포\", alpha=0.7)\nplt.xlabel(\"λ (게임당 골 수)\")\nplt.ylabel(\"확률밀도\")\nplt.title(\"독일의 골 득점률 분포\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.stats import poisson\nimport pandas as pd\nimport numpy as np\n\ndef make_poisson_pmf(lam, qs):\n    ps = poisson.pmf(qs, lam)\n    pmf = Pmf(ps, qs)\n    pmf.normalize()\n    return pmf\n\ndef make_mixture(pmf, pmf_seq):\n    df = pd.DataFrame(pmf_seq).fillna(0).transpose()\n    df *= np.array(pmf)\n    total = df.sum(axis=1)\n    return Pmf(total)\n\nremaining_time = (90 - 23) / 90\ngoals = np.arange(15)\npmf_seq = [make_poisson_pmf(lam * remaining_time, goals) for lam in post2.qs]\npred = make_mixture(post2, pmf_seq)\n\nplt.plot(pred)\nplt.title('독일의 남은 시간 동안의 골 득점 예측')\nplt.xlabel('골 수')\nplt.ylabel('확률')\nplt.show()\n\n\n\n\n\n\n\n\n\npred[5:].sum()\n\nnp.float64(0.09386958056810232)\n\n\n\n\n8-2\n\ndef update_goal(prior, data):\n    posterior = prior.copy()\n    for goals in data:\n        likelihood = [poisson.pmf(goals, lam) for lam in posterior.qs]\n        posterior = posterior * likelihood\n        posterior.normalize()\n    return posterior\n\n\nfrance = update_goal(prior, [4])\ncroatia = update_goal(prior, [2])\n\nqs = np.linspace(0, 1, 101)\nlikelihood = expo_pdf(qs, prior.qs)\n\nfrance_post = france * likelihood\nfrance_post.normalize()\ncroatia_post = croatia * likelihood\ncroatia_post.normalize()\n\nPmf.prob_lt(france_post, croatia_post)\n\nnp.float64(0.2633977680689847)\n\n\n\n\n8-3\n\nalpha = 2.8\nqs = np.linspace(0, 10, 101)\n\nprior = Pmf(gamma.pdf(qs, alpha), qs)\nprior.normalize()\n\n\nboston_posterior = update_goal(prior, [0, 2, 8, 4])\nvancouver_posterior = update_goal(prior, [1, 3, 1, 0])\n\nprior.plot(label='prior')\nboston_posterior.plot(label='boston')\nvancouver_posterior.plot(label='vancouver')\nplt.legend()\n\n\n\n\n\n\n\n\n\npmf_seq = [make_poisson_pmf(lam, goals) for lam in prior.qs]\npred_boston = make_mixture(boston_posterior, pmf_seq)\npred_vancouver = make_mixture(vancouver_posterior, pmf_seq)\nwin = Pmf.prob_gt(pred_boston, pred_vancouver)\nwin\n\nnp.float64(0.703863141313654)\n\n\n챔피언십 우승 확률은 아직은 못 구하겠다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "포아송 과정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/17.html#월드컵-문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/17.html#월드컵-문제",
    "title": "켤레사전분포",
    "section": "월드컵 문제",
    "text": "월드컵 문제\n\nfrom scipy.stats import gamma\nimport numpy as np\nfrom empiricaldist import Pmf\n\nalpha = 1.4\ndist = gamma(alpha)\n\nlams = np.linspace(0, 10, 101)\nprior = Pmf(dist.pdf(lams), lams)\nprior.normalize()\n\nnp.float64(9.889360237140306)\n\n\n\nfrom scipy.stats import poisson\n\nk = 4\nlikelihood = poisson(lams).pmf(k)\nposterior = prior * likelihood\nposterior.normalize()\n\nnp.float64(0.05015532557804499)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "켤레사전분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/bayse/17.html#켤레사전분포",
    "href": "posts/03_archives/completed_project/adp_실기/notes/bayse/17.html#켤레사전분포",
    "title": "켤레사전분포",
    "section": "켤레사전분포",
    "text": "켤레사전분포\n\ndef make_gamma_dist(alpha, beta):\n    dist = gamma(alpha, scale=1/beta)\n    dist.alpha = alpha\n    dist.beta = beta\n    return dist\n\ndef update_gamma(prior, daata):\n    k, t = data\n    alpha = prior.alpha + k\n    beta = prior.beta + t\n    return make_gamma_dist(alpha, beta)\n\nprior_gamma = make_gamma_dist(1.4, 1)\ndata = 4, 1\nposterior_gamma = update_gamma(prior_gamma, data)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Bayse",
      "켤레사전분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/12.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/12.html",
    "title": "결과",
    "section": "",
    "text": "시험 결과(75점 이상 합격)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "결과"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/12.html#intro",
    "href": "posts/03_archives/completed_project/adp_실기/notes/12.html#intro",
    "title": "결과",
    "section": "Intro",
    "text": "Intro\n결과가 나오고 1달 정도 지나서 후기를 작성한다. 이번 학기가 너무 바빴다.\n뭐 어쨌든 결과는...불합격! 예상은 하고 있었으니 뭐 크게 실망스럽진 않지만 한 문제 차이로 떨어질거라곤 생각도 못했다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "결과"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/12.html#틀린-문제",
    "href": "posts/03_archives/completed_project/adp_실기/notes/12.html#틀린-문제",
    "title": "결과",
    "section": "틀린 문제",
    "text": "틀린 문제\n기계 학습 분야에서 21점이 감점되었는데 이건 오히려 잘한 것이다. 왜냐하면 20점 짜리 파트 하나를 통으로 날렸기 때문에, 내가 푼 문제에서는 1점만 까였다는 뜻이니까.\n통계 분석 파트에서는 아마도 로지스틱 회귀분석 문제에서 점수가 까인것 같은데...사실 이 문제는 틀릴만한 문제는 아니였다. 심지어...시험 전날 adp 커뮤니티에서 누군가 이 문제가 나온다고 예상 글을 올렸을 때도 ‘이런걸 누가 틀려’ 하고 대수롭지 않게 여겼는데 막상 시험에 나와서 풀어보니 내가 예상한대로 동작하지 않았다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "결과"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/12.html#outro",
    "href": "posts/03_archives/completed_project/adp_실기/notes/12.html#outro",
    "title": "결과",
    "section": "Outro",
    "text": "Outro\n저 문제를 맞았다면 아마 합격했을테지만 뭐 이제와서 그게 중요한가? 어쨌든 이번 시험 준비 과정에서 많이 성장했다는 것을 체감하고 있고, 떨어진거야 뭐 아쉬운거지.\n다음 시험을 준비할지는 아직 잘 모르겠다. 시험을 보긴 하겠지만 이번에 준비한만큼 하진 않을 것 같다.\n들리는 소문에 의하면 이번 회차 합격자는 10명이라고 하던데 정말 양심이 없는 시험이 아닐 수 없다. 하지만 이런 시험에서 합격한다면 실력 증명 하나는 확실히 하는게 되는 것 아닐까? 하는 욕심은 남는다. 그래도 멈춰야할 때를 확실히 아는 것도 필요하다고 본다. 여기에만 온전히 집중할 수는 없으니 말이다.\n앞으로는 인공지능 쪽 공부에 더 집중해보려 한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "결과"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/11.html#intro",
    "href": "posts/03_archives/completed_project/adp_실기/notes/11.html#intro",
    "title": "try 1 후기",
    "section": "Intro",
    "text": "Intro\n개같이 망했다.\n사실 조금 방심했다.\n통계파트가 다 아는 문제가 나와서 음..이거 잘 하면 합격하겠는데? 라는 생각이 들었다.\n그래서 여유롭게 1시간동안 풀고 화장실도 다녀왔다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "try 1 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/11.html#머신러닝",
    "href": "posts/03_archives/completed_project/adp_실기/notes/11.html#머신러닝",
    "title": "try 1 후기",
    "section": "머신러닝",
    "text": "머신러닝\n이런 젠장. 머신러닝 파트 왜 이렇게 오래 걸리는거야?\n분명 풀 수 있는 문제지만, 시간을 보니 10분밖에 안 남아 있었다.\n결국 2번 문제는 전처리조차 하지 못하고 통으로 버릴 수 밖에 없었다.\n물론 통으로 버려도 합격은 할 수 있다. 그런 사람이 실제로 있는지는 잘 모르겠지만.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "try 1 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/11.html#outro",
    "href": "posts/03_archives/completed_project/adp_실기/notes/11.html#outro",
    "title": "try 1 후기",
    "section": "Outro",
    "text": "Outro\nADP 실기 시험까지 오는 여정은 의미가 깊었다. 진심으로 그렇게 생각한다.\n그래서 결과가 좋지 않더라도 실망은 10% 정도만 할 것 같다.\n하지만 계속 시험을 준비하는 것은 다른 이야기다. 내가 6개월을 더 준비할 가치가 아직 남아있을까?\n솔직히 잘 모르겠다. 일단은 다른 것들을 병행해보면서 생각을 정리해보려 한다.\n당장 눈 앞에 닥친 중간고사나 제대로 준비해보자.\n아아! 한 번에 붙을 수 있었는데! (결과는 아직 안나오긴 했다.)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "try 1 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/01.html",
    "href": "posts/03_archives/completed_project/adp_실기/notes/01.html",
    "title": "모델링, 평가 템플릿",
    "section": "",
    "text": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', StandardScaler(), num_features), # outlier가 많다면 RobustScaler를 고려하자\n    ('cat', OneHotEncoder(drop='first'), cat_features),\n    ('ord', OrdinalEncoder(), ord_features) # 순서형 변수. 하지만 이렇게 처리하는 것보다 그냥 DataFrame.map()으로 직접 인코딩해주는게 더 나을듯\n])\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model_name', YourModel(random_state=42))\n])\n\n\ncategory 형을 있는 그대로 처리하고 싶다면, pipeline보다는 LGBM, CatBoost, XGBoost 등의 모델을 바로 사용하는게 더 나음.\n\nsklearn은 데이터들을 numpy array로 변환하기 때문에 category 형을 유지하지 못함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "모델링, 평가 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/01.html#모델-정의",
    "href": "posts/03_archives/completed_project/adp_실기/notes/01.html#모델-정의",
    "title": "모델링, 평가 템플릿",
    "section": "",
    "text": "from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', StandardScaler(), num_features), # outlier가 많다면 RobustScaler를 고려하자\n    ('cat', OneHotEncoder(drop='first'), cat_features),\n    ('ord', OrdinalEncoder(), ord_features) # 순서형 변수. 하지만 이렇게 처리하는 것보다 그냥 DataFrame.map()으로 직접 인코딩해주는게 더 나을듯\n])\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model_name', YourModel(random_state=42))\n])\n\n\ncategory 형을 있는 그대로 처리하고 싶다면, pipeline보다는 LGBM, CatBoost, XGBoost 등의 모델을 바로 사용하는게 더 나음.\n\nsklearn은 데이터들을 numpy array로 변환하기 때문에 category 형을 유지하지 못함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "모델링, 평가 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/01.html#grid-search-bayesian-optimization",
    "href": "posts/03_archives/completed_project/adp_실기/notes/01.html#grid-search-bayesian-optimization",
    "title": "모델링, 평가 템플릿",
    "section": "Grid Search & Bayesian Optimization",
    "text": "Grid Search & Bayesian Optimization\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\n# 파라미터 이름 앞에 pipeline에서 사용한 '모델이름__'을 붙여야 함\nparams = {\n    'classifier__n_estimators': [100, 200, 300],\n    'classifier__max_depth': [10, 50, 100],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(estimator=pipeline, \n                           param_grid=params, \n                           cv=5, \n                           scoring='accuracy') # 분류: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, 회귀: neg_root_mean_squared_error, r2, neg_mean_absolute_error\ngrid_search.fit(X_train, y_train)\nbest_model = grid_search.best_estimator_\n\ny_pred = best_model.predict(X_test)\n\n# ============ 분류 scoring ============\n\ny_pred_proba = best_model.predict_proba(X_test)[:, 1] # 다중클래스인 경우 [:, 1] 제거\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n    y_test, \n    y_pred, \n    average='binary' # 다중 클래스인 경우 'macro', 'micro', 'weighted' 중 선택\n)\ntest_auc = roc_auc_score(y_test, y_pred_proba) # 다중 클래스인 경우 multi_class='ovr', 'ovo' 중 선택\n\nprint(f\"test_accuracy: {test_acc:.4f}\")\nprint(f\"test_precision: {test_precision:.4f}\")\nprint(f\"test_recall: {test_recall:.4f}\")\nprint(f\"test_f1_score: {test_f1:.4f}\")\nprint(f\"test_auc: {test_auc:.4f}\")\n\n# ============ 회귀 scoring ============\n\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"test_rmse: {rmse:.4f}\")\nprint(f\"test_mae: {mae:.4f}\")\nprint(f\"test_r2: {r2:.4f}\")\n\n\n👇 시험에서 사용 불가. 하지만 kaggle이나 dacon에서 주로 사용됨\n\n\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\ndef objective(trial):\n    # 모델에 적합한 파라미터에 맞게 수정\n    params = {\n        'classifier__n_estimators': trial.suggest_int(\"n_estimators\", 100, 500, 100),\n        'classifier__max_features': trial.suggest_categorical(\"max_features\", ['sqrt', 'log2']),\n        'classifier__max_depth': trial.suggest_int(\"max_depth\", 10, 110, 20),\n        'classifier__min_samples_split': trial.suggest_int(\"min_samples_split\", 2, 10, 2),\n        'classifier__min_samples_leaf': trial.suggest_int(\"min_samples_leaf\", 1, 4, 1)\n    }\n    pipeline.set_params(**params)\n\n    cv_score = cross_val_score(pipeline,\n                               X_train,\n                               y_train,\n                               cv=5,\n                               scoring='accuracy') # 분류: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, 회귀: neg_root_mean_squared_error, r2, neg_mean_absolute_error\n    mean_cv_accuracy = cv_score.mean()\n    return mean_cv_accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\npipeline.set_params(**study.best_params)\npipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)\n\n# ============ 분류 scoring ============\n\ny_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n    y_test, \n    y_pred, \n    average='binary' # 다중 클래스인 경우 'macro', 'micro', 'weighted' 중 선택해서 사용\n)\ntest_auc = roc_auc_score(y_test, y_pred_proba) # 다중 클래스인 경우 multiclass='ovr', 'ovo' 중 선택해서 사용\n\nprint(\"test_accuracy:\", test_acc)\nprint(\"test_precision:\", test_precision)\nprint(\"test_recall:\", test_recall)\nprint(\"test_f1_score:\", test_f1)\nprint(\"test_auc:\", test_auc)\n\n# ============ 회귀 scoring ============\n\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"test_rmse: {rmse:.4f}\")\nprint(f\"test_mae: {mae:.4f}\")\nprint(f\"test_r2: {r2:.4f}\")\n\n\n👇 시험에서 사용 가능\n\n\nfrom hyperopt import hp, STATUS_OK, fmin, tpe, Trials\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\n# 파라미터 공간 정의\nsearch_space = {\n    'classifier__n_estimators': hp.quniform('n_estimators', 100, 500, 100),\n    'classifier__max_features': hp.choice('max_features', ['sqrt', 'log2']),\n    'classifier__max_depth': hp.quniform('max_depth', 10, 110, 20),\n    'classifier__min_samples_split': hp.quniform('min_samples_split', 2, 10, 2),\n    'classifier__min_samples_leaf': hp.quniform('min_samples_leaf', 1, 4, 1)\n}\n\ndef objective_func(params):\n    # 파라미터 타입 변환 (hyperopt는 float로 반환하므로 int로 변환 필요)\n    params_int = {\n        'classifier__n_estimators': int(params['classifier__n_estimators']),\n        'classifier__max_features': params['classifier__max_features'],\n        'classifier__max_depth': int(params['classifier__max_depth']),\n        'classifier__min_samples_split': int(params['classifier__min_samples_split']),\n        'classifier__min_samples_leaf': int(params['classifier__min_samples_leaf'])\n    }\n    pipeline.set_params(**params_int)\n    \n    cv_score = cross_val_score(pipeline,\n                               X_train,\n                               y_train,\n                               cv=5,\n                               scoring='accuracy') # 분류: accuracy, f1, roc_auc, f1_macro, roc_auc_ovr, roc_auc_ovo, 회귀: neg_root_mean_squared_error, r2, neg_mean_absolute_error\n    mean_cv_score = cv_score.mean()\n    \n    # hyperopt는 최소화하므로 음수 반환 (최대화하려는 경우)\n    return {'loss': -mean_cv_score, 'status': STATUS_OK}\n\n# 최적화 실행\ntrials = Trials()\nbest = fmin(fn=objective_func,\n            space=search_space,\n            algo=tpe.suggest,\n            max_evals=100,\n            trials=trials)\n\n# 최적 파라미터 적용 (타입 변환 포함)\nbest_params = {\n    'classifier__n_estimators': int(best['n_estimators']),\n    'classifier__max_features': ['sqrt', 'log2'][int(best['max_features'])],\n    'classifier__max_depth': int(best['max_depth']),\n    'classifier__min_samples_split': int(best['min_samples_split']),\n    'classifier__min_samples_leaf': int(best['min_samples_leaf'])\n}\n\npipeline.set_params(**best_params)\npipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)\n\n# ============ 분류 scoring ============\n\ny_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n\ntest_acc = accuracy_score(y_test, y_pred)\ntest_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n    y_test, \n    y_pred, \n    average='binary' # 다중 클래스인 경우 'macro', 'micro', 'weighted' 중 선택해서 사용\n)\ntest_auc = roc_auc_score(y_test, y_pred_proba) # 다중 클래스인 경우 multiclass='ovr', 'ovo' 중 선택해서 사용\n\nprint(\"test_accuracy:\", test_acc)\nprint(\"test_precision:\", test_precision)\nprint(\"test_recall:\", test_recall)\nprint(\"test_f1_score:\", test_f1)\nprint(\"test_auc:\", test_auc)\n\n# ============ 회귀 scoring ============\n\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"test_rmse: {rmse:.4f}\")\nprint(f\"test_mae: {mae:.4f}\")\nprint(f\"test_r2: {r2:.4f}\")",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "모델링, 평가 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/01.html#모델-평가-visualization",
    "href": "posts/03_archives/completed_project/adp_실기/notes/01.html#모델-평가-visualization",
    "title": "모델링, 평가 템플릿",
    "section": "모델 평가 visualization",
    "text": "모델 평가 visualization\n\nROC AUC\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# ROC 곡선 계산\ny_pred_proba = best_model.predict_proba(X_test)[:, 1]  # 양성 클래스 확률\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n\n# ROC 곡선 시각화\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()\n\n\n\nFeature Importance\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimportances = pipeline.named_steps['classifier'].feature_importances_\nftr_importance = pd.Series(importances, index=X.columns)\nftr_top = ftr_importance.sort_values(ascending=False)[:20] # 원하는 수 만큼 설정\nsns.barplot(ftr_top20, y=ftr_top.index)\nplt.show()\n\n\n\nDrop Column importance\n\nfrom sklearn.base import clone\n\nfor col in X_train.columns:\n    X_train_dropped = X_train.drop(columns=[col])\n    X_test_dropped = X_test.drop(columns=[col])\n\n    # 시간의 단축을 위해 하이퍼파라미터는 그냥 그대로 사용\n    # 엄밀하게 하고 싶다면 각 iteration마다 다시 튜닝\n    model_dropped = clone(best_model)\n    model_dropped.fit(X_train_dropped, y_train)\n\n    score = model_dropped.score(X_test_dropped, y_test)\n    print(f\"{col}'s difference:\", best_model.score(X_test, y_test) - score) # 혹은 다른 평가 지표 사용\n\n\n\nPermutation importance\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(best_model).fit(X_test, y_test)\neli5.show_weights(perm, feature_names=X_test.columns.tolist())\n\n\n음수 값은 제거해도 된다는 뜻\n단점: 상관관계가 높은 feature에 대해서 비현실적인 데이터 조합이 생성될 가능성이 높다.\n\n(ex. shuffle을 통해 키 180cm, 몸무게 30kg의 데이터가 조합되는 경우)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "모델링, 평가 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/01.html#ensemble",
    "href": "posts/03_archives/completed_project/adp_실기/notes/01.html#ensemble",
    "title": "모델링, 평가 템플릿",
    "section": "ensemble",
    "text": "ensemble\n\nstacking:\n\n여러 모델을 학습시킨 후, 각 모델의 예측 결과를 입력으로 하는 메타 모델을 학습시킨다.\n메타 모델은 다른 모델들의 예측 결과를 종합하여 최종 예측을 수행한다.\n\nvoting, averaging:\n\n여러 모델을 학습시킨 후, 각 모델의 예측 결과를 투표(voting)하거나 평균(averaging)하여 최종 예측을 수행한다.\n분류 문제에서는 다수결 투표(hard voting) 또는 확률 평균(soft voting)을 사용하고, 회귀 문제에서는 단순 평균 또는 가중 평균을 사용한다.\n\nbagging\n\nvs cross validation:\n\ncross validation은 이미 생성된 모델을 검증하기 위한 방법. 모델 구축 방법은 아님\nbagging은 분산을 줄이기 위해 사용함\n\n\nboosting: sequentially 학습\n\n이전 모델의 오차를 보완하는 방식으로 학습한다.\n\n\n\nfrom sklearn.ensemble import StackingClassifier, StackingRegressor\n\nstacking_lf = StackingClassifier(estimators=[\n    ('lr', LogisticRegression()),\n    ('dt', DecisionTreeClassifier()),\n    ('rf', RandomForestClassifier())\n], final_estimator=LogisticRegression())\n\nstacking_rf = StackingRegressor(estimators=[\n    ('lr', LinearRegression()),\n    ('dt', DecisionTreeRegressor()),\n    ('rf', RandomForestRegressor())\n], final_estimator=LinearRegression())\n\nstacking_lf.fit(X_train, y_train)\nstacking_rf.fit(X_train, y_train)\nstacking_lf.predict(X_test)\nstacking_rf.predict(X_test)\n\n\nfrom sklearn.ensemble import VotingClassifier, VotingRegressor\n\nvoting_lf = VotingClassifier(estimators=[\n    ('lr', LogisticRegression()),\n    ('dt', DecisionTreeClassifier()),\n    ('rf', RandomForestClassifier())\n], voting='soft') # or hard\n\n\nvoting_rf = VotingRegressor(estimators=[\n    ('lr', LinearRegression()),\n    ('dt', DecisionTreeRegressor()),\n    ('rf', RandomForestRegressor())\n], weights=[1, 1, 2])\n\n\nvoting_lf.fit(X_train, y_train)\nvoting_rf.fit(X_train, y_train)\nvoting_lf.predict(X_test)\nvoting_rf.predict(X_test)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "모델링, 평가 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/01.html#모델-파라미터",
    "href": "posts/03_archives/completed_project/adp_실기/notes/01.html#모델-파라미터",
    "title": "모델링, 평가 템플릿",
    "section": "모델 파라미터",
    "text": "모델 파라미터\n\n아래부터는 ai의 도움을 받았습니다.\n적당히 몇 개만 골라서 사용하면 됩니다.\n\n\nLogistic 회귀분석\n\n# Grid Search용\nparams = {\n    \"classifier__penalty\": ['l1', 'l2', 'elasticnet', 'none'],  # 정규화 방법\n    \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100],  # 정규화 강도 (작을수록 강한 정규화)\n    \"classifier__l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],  # elasticnet일 때만 사용\n    \"classifier__solver\": ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga'],  # 최적화 알고리즘\n    \"classifier__max_iter\": [100, 500, 1000, 2000]  # 최대 반복 횟수\n}\n\n# Bayesian Optimization용 (hyperopt)\nparams_bayes = {\n    \"classifier__penalty\": hp.choice(\"penalty\", ['l1', 'l2', 'elasticnet']),\n    \"classifier__C\": hp.loguniform(\"C\", np.log(0.001), np.log(100)),\n    \"classifier__l1_ratio\": hp.uniform(\"l1_ratio\", 0.1, 0.9),\n    \"classifier__solver\": hp.choice(\"solver\", ['liblinear', 'saga']),\n    \"classifier__max_iter\": hp.quniform(\"max_iter\", 100, 2000, 100)\n}\n\n\n\nKNN\n\n# Grid Search용\nparams = {\n    \"classifier__n_neighbors\": [3, 5, 7, 9, 11, 15, 21],  # k값 (이웃의 수)\n    \"classifier__weights\": ['uniform', 'distance'],  # 가중치 방법\n    \"classifier__algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],  # 알고리즘\n    \"classifier__leaf_size\": [20, 30, 40, 50],  # 리프 크기 (ball_tree, kd_tree용)\n    \"classifier__p\": [1, 2],  # 거리 측도 (1: 맨하탄, 2: 유클리드)\n    \"classifier__metric\": ['minkowski', 'euclidean', 'manhattan']  # 거리 함수\n}\n\n# Bayesian Optimization용 (hyperopt)\nparams_bayes = {\n    \"classifier__n_neighbors\": hp.quniform(\"n_neighbors\", 3, 21, 2),\n    \"classifier__weights\": hp.choice(\"weights\", ['uniform', 'distance']),\n    \"classifier__algorithm\": hp.choice(\"algorithm\", ['auto', 'ball_tree', 'kd_tree']),\n    \"classifier__leaf_size\": hp.quniform(\"leaf_size\", 20, 50, 10),\n    \"classifier__p\": hp.choice(\"p\", [1, 2]),\n    \"classifier__metric\": hp.choice(\"metric\", ['minkowski', 'euclidean', 'manhattan'])\n}\n\n\n\nSupport Vector Machine\n\n# Grid Search용\nparams = {\n    \"classifier__C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],  # 정규화 파라미터 (작을수록 강한 정규화)\n    \"classifier__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],  # 커널 함수\n    \"classifier__degree\": [2, 3, 4, 5],  # poly 커널 차수 (poly일 때만 사용)\n    \"classifier__gamma\": ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10],  # 커널 계수 (rbf, poly, sigmoid용)\n    \"classifier__coef0\": [0.0, 0.1, 0.5, 1.0],  # 커널의 독립항 (poly, sigmoid용)\n    \"classifier__shrinking\": [True, False],  # 수축 휴리스틱 사용 여부\n    \"classifier__tol\": [1e-5, 1e-4, 1e-3],  # 정지 기준 허용 오차\n    \"classifier__max_iter\": [100, 500, 1000, 2000, -1]  # 최대 반복 횟수 (-1: 제한 없음)\n}\n\n# Bayesian Optimization용 (hyperopt)\nparams_bayes = {\n    \"classifier__C\": hp.loguniform(\"C\", np.log(0.001), np.log(1000)),\n    \"classifier__kernel\": hp.choice(\"kernel\", ['linear', 'poly', 'rbf', 'sigmoid']),\n    \"classifier__degree\": hp.quniform(\"degree\", 2, 5, 1),  # poly일 때만\n    \"classifier__gamma\": hp.choice(\"gamma\", ['scale', 'auto'] + [hp.loguniform(\"gamma_float\", np.log(0.001), np.log(10))]),\n    \"classifier__coef0\": hp.uniform(\"coef0\", 0.0, 1.0),\n    \"classifier__shrinking\": hp.choice(\"shrinking\", [True, False]),\n    \"classifier__tol\": hp.loguniform(\"tol\", np.log(1e-5), np.log(1e-3)),\n    \"classifier__max_iter\": hp.quniform(\"max_iter\", 100, 2000, 100)\n}\n\n\n\nRandom Forest\n\n# Grid Search용\nparams = {\n    \"classifier__n_estimators\": [50, 100, 200, 300, 500],  # 트리 개수\n    \"classifier__criterion\": ['gini', 'entropy', 'log_loss'],  # 불순도 측정 기준\n    \"classifier__max_depth\": [None, 5, 10, 20, 30, 50],  # 트리의 최대 깊이 (None: 제한 없음)\n    \"classifier__min_samples_split\": [2, 5, 10, 20],  # 내부 노드 분할에 필요한 최소 샘플 수\n    \"classifier__min_samples_leaf\": [1, 2, 4, 10],  # 리프 노드에 필요한 최소 샘플 수\n    \"classifier__min_weight_fraction_leaf\": [0.0, 0.1, 0.2],  # 리프 노드에 필요한 최소 가중치 비율\n    \"classifier__max_features\": ['sqrt', 'log2', None, 0.3, 0.5, 0.7],  # 각 분할에서 고려할 피처 수\n    \"classifier__max_leaf_nodes\": [None, 50, 100, 200],  # 최대 리프 노드 수\n    \"classifier__min_impurity_decrease\": [0.0, 0.01, 0.05, 0.1],  # 분할에 필요한 최소 불순도 감소량\n    \"classifier__bootstrap\": [True, False],  # 부트스트랩 사용 여부\n    \"classifier__oob_score\": [True, False],  # OOB 스코어 계산 여부\n    \"classifier__max_samples\": [None, 0.5, 0.7, 0.9],  # 각 트리에 사용할 샘플 비율\n    \"classifier__ccp_alpha\": [0.0, 0.01, 0.05, 0.1]  # 복잡도 가지치기 파라미터\n}\n\n# Bayesian Optimization용 (hyperopt)\nparams_bayes = {\n    \"classifier__n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 50),\n    \"classifier__criterion\": hp.choice(\"criterion\", ['gini', 'entropy']),\n    \"classifier__max_depth\": hp.choice(\"max_depth\", [None, hp.quniform(\"max_depth_val\", 5, 50, 5)]),\n    \"classifier__min_samples_split\": hp.quniform(\"min_samples_split\", 2, 20, 2),\n    \"classifier__min_samples_leaf\": hp.quniform(\"min_samples_leaf\", 1, 10, 1),\n    \"classifier__max_features\": hp.choice(\"max_features\", ['sqrt', 'log2', None]),\n    \"classifier__min_impurity_decrease\": hp.uniform(\"min_impurity_decrease\", 0.0, 0.1),\n    \"classifier__bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n    \"classifier__max_samples\": hp.choice(\"max_samples\", [None, hp.uniform(\"max_samples_val\", 0.5, 1.0)]),\n    \"classifier__ccp_alpha\": hp.uniform(\"ccp_alpha\", 0.0, 0.1)\n}\n\n\n\nXGBOOST\n\n# Grid Search용\nparams = {\n    \"classifier__n_estimators\": [50, 100, 200, 300, 500],  # 부스팅 라운드 수\n    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],  # 학습률 (eta)\n    \"classifier__max_depth\": [3, 4, 5, 6, 7, 8],  # 트리 최대 깊이\n    \"classifier__min_child_weight\": [1, 3, 5, 7],  # 리프 노드의 최소 가중치 합\n    \"classifier__gamma\": [0, 0.1, 0.2, 0.3, 0.4],  # 분할에 필요한 최소 손실 감소\n    \"classifier__subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 행 샘플링 비율\n    \"classifier__colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 열 샘플링 비율 (트리별)\n    \"classifier__colsample_bylevel\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 열 샘플링 비율 (레벨별)\n    \"classifier__colsample_bynode\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 열 샘플링 비율 (노드별)\n    \"classifier__reg_alpha\": [0, 0.01, 0.1, 1, 10],  # L1 정규화 파라미터\n    \"classifier__reg_lambda\": [0, 0.01, 0.1, 1, 10],  # L2 정규화 파라미터\n    \"classifier__max_delta_step\": [0, 1, 2, 5, 10],  # 각 트리 가중치 변화의 최대값\n    \"classifier__scale_pos_weight\": [1, 2, 3, 5]  # 양성 클래스 가중치 (불균형 데이터용)\n}\n\n# Bayesian Optimization용 (hyperopt)\nparams_bayes = {\n    \"classifier__n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 50),\n    \"classifier__learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.3)),\n    \"classifier__max_depth\": hp.quniform(\"max_depth\", 3, 8, 1),\n    \"classifier__min_child_weight\": hp.quniform(\"min_child_weight\", 1, 7, 1),\n    \"classifier__gamma\": hp.uniform(\"gamma\", 0, 0.4),\n    \"classifier__subsample\": hp.uniform(\"subsample\", 0.6, 1.0),\n    \"classifier__colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.6, 1.0),\n    \"classifier__colsample_bylevel\": hp.uniform(\"colsample_bylevel\", 0.6, 1.0),\n    \"classifier__colsample_bynode\": hp.uniform(\"colsample_bynode\", 0.6, 1.0),\n    \"classifier__reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.01), np.log(10)),\n    \"classifier__reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10)),\n    \"classifier__max_delta_step\": hp.quniform(\"max_delta_step\", 0, 10, 1),\n    \"classifier__scale_pos_weight\": hp.quniform(\"scale_pos_weight\", 1, 5, 1)\n}\n\n\n\nLightGBM\n\n# Grid Search용\nparams = {\n    \"classifier__n_estimators\": [50, 100, 200, 300, 500],  # 부스팅 라운드 수\n    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],  # 학습률\n    \"classifier__max_depth\": [3, 5, 7, 10, -1],  # 트리 최대 깊이 (-1: 제한 없음)\n    \"classifier__num_leaves\": [15, 31, 63, 127, 255],  # 리프 노드 수 (2^max_depth - 1보다 작게)\n    \"classifier__min_child_samples\": [10, 20, 30, 50],  # 리프 노드의 최소 샘플 수\n    \"classifier__min_child_weight\": [1e-3, 1e-2, 1e-1, 1],  # 리프 노드의 최소 가중치 합\n    \"classifier__subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 행 샘플링 비율\n    \"classifier__colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 열 샘플링 비율\n    \"classifier__reg_alpha\": [0, 0.01, 0.1, 1, 10],  # L1 정규화\n    \"classifier__reg_lambda\": [0, 0.01, 0.1, 1, 10],  # L2 정규화\n    \"classifier__min_split_gain\": [0, 0.01, 0.1, 0.5],  # 분할에 필요한 최소 gain\n    \"classifier__min_data_in_leaf\": [10, 20, 50, 100],  # 리프의 최소 데이터 수\n    \"classifier__boosting_type\": ['gbdt', 'dart', 'goss'],  # 부스팅 타입\n    \"classifier__feature_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 피처 샘플링 비율\n    \"classifier__bagging_fraction\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 데이터 샘플링 비율\n    \"classifier__bagging_freq\": [0, 1, 3, 5]  # 배깅 빈도\n}\n\n# Bayesian Optimization용 (hyperopt)\nparams_bayes = {\n    \"classifier__n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 50),\n    \"classifier__learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.3)),\n    \"classifier__max_depth\": hp.choice(\"max_depth\", [-1, hp.quniform(\"max_depth_val\", 3, 10, 1)]),\n    \"classifier__num_leaves\": hp.quniform(\"num_leaves\", 15, 255, 16),\n    \"classifier__min_child_samples\": hp.quniform(\"min_child_samples\", 10, 50, 10),\n    \"classifier__min_child_weight\": hp.loguniform(\"min_child_weight\", np.log(1e-3), np.log(1)),\n    \"classifier__subsample\": hp.uniform(\"subsample\", 0.6, 1.0),\n    \"classifier__colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.6, 1.0),\n    \"classifier__reg_alpha\": hp.loguniform(\"reg_alpha\", np.log(0.01), np.log(10)),\n    \"classifier__reg_lambda\": hp.loguniform(\"reg_lambda\", np.log(0.01), np.log(10)),\n    \"classifier__min_split_gain\": hp.uniform(\"min_split_gain\", 0, 0.5),\n    \"classifier__min_data_in_leaf\": hp.quniform(\"min_data_in_leaf\", 10, 100, 10),\n    \"classifier__boosting_type\": hp.choice(\"boosting_type\", ['gbdt', 'dart', 'goss']),\n    \"classifier__feature_fraction\": hp.uniform(\"feature_fraction\", 0.6, 1.0),\n    \"classifier__bagging_fraction\": hp.uniform(\"bagging_fraction\", 0.6, 1.0),\n    \"classifier__bagging_freq\": hp.quniform(\"bagging_freq\", 0, 5, 1)\n}\n\n\n\nCatboost\n\n# Grid Search용\nparams = {\n    \"classifier__n_estimators\": [50, 100, 200, 300, 500],  # 부스팅 라운드 수\n    \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],  # 학습률\n    \"classifier__depth\": [3, 4, 5, 6, 7, 8],  # 트리 깊이 (max_depth 대신 depth 사용)\n    \"classifier__l2_leaf_reg\": [1, 3, 5, 10, 20],  # L2 정규화 파라미터\n    \"classifier__border_count\": [32, 64, 128, 255],  # 수치형 피처의 분할점 개수\n    \"classifier__bagging_temperature\": [0, 0.5, 1, 2, 5],  # 배깅 온도 (0: 비활성화)\n    \"classifier__random_strength\": [1, 2, 5, 10],  # 트리 구조의 무작위성\n    \"classifier__od_type\": ['IncToDec', 'Iter'],  # 조기 종료 타입\n    \"classifier__od_wait\": [10, 20, 50],  # 조기 종료 대기 라운드\n    \"classifier__bootstrap_type\": ['Bayesian', 'Bernoulli', 'MVS', 'Poisson'],  # 부트스트랩 타입\n    \"classifier__subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],  # 샘플링 비율 (Bernoulli일 때만)\n    \"classifier__rsm\": [0.5, 0.7, 0.9, 1.0],  # 무작위 서브스페이스 방법 (피처 샘플링)\n    \"classifier__leaf_estimation_iterations\": [1, 3, 5, 10],  # 리프 값 추정 반복 횟수\n    \"classifier__grow_policy\": ['SymmetricTree', 'Depthwise', 'Lossguide'],  # 트리 성장 정책\n    \"classifier__min_data_in_leaf\": [1, 5, 10, 20],  # 리프 노드의 최소 샘플 수\n    \"classifier__max_leaves\": [16, 31, 64, 127]  # 최대 리프 수 (Lossguide일 때만)\n}\n\n# Bayesian Optimization용 (hyperopt)\nparams_bayes = {\n    \"classifier__n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 50),\n    \"classifier__learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.3)),\n    \"classifier__depth\": hp.quniform(\"depth\", 3, 8, 1),\n    \"classifier__l2_leaf_reg\": hp.quniform(\"l2_leaf_reg\", 1, 20, 1),\n    \"classifier__border_count\": hp.choice(\"border_count\", [32, 64, 128, 255]),\n    \"classifier__bagging_temperature\": hp.uniform(\"bagging_temperature\", 0, 5),\n    \"classifier__random_strength\": hp.quniform(\"random_strength\", 1, 10, 1),\n    \"classifier__od_type\": hp.choice(\"od_type\", ['IncToDec', 'Iter']),\n    \"classifier__od_wait\": hp.quniform(\"od_wait\", 10, 50, 10),\n    \"classifier__bootstrap_type\": hp.choice(\"bootstrap_type\", ['Bayesian', 'Bernoulli', 'MVS']),\n    \"classifier__subsample\": hp.uniform(\"subsample\", 0.6, 1.0),\n    \"classifier__rsm\": hp.uniform(\"rsm\", 0.5, 1.0),\n    \"classifier__leaf_estimation_iterations\": hp.quniform(\"leaf_estimation_iterations\", 1, 10, 1),\n    \"classifier__grow_policy\": hp.choice(\"grow_policy\", ['SymmetricTree', 'Depthwise', 'Lossguide']),\n    \"classifier__min_data_in_leaf\": hp.quniform(\"min_data_in_leaf\", 1, 20, 1),\n    \"classifier__max_leaves\": hp.quniform(\"max_leaves\", 16, 127, 1)\n}\n\n# 주요 특징:\n# - GPU 지원 (gpu_device_id=-1로 설정하면 GPU 사용)\n# - 범주형 변수 자동 처리 (cat_features 파라미터로 지정)\n# - 내장된 교차검증 및 조기 종료\n# - 텍스트 및 임베딩 피처 지원",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "모델링, 평가 템플릿"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#다차원-척도법mds이란",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#다차원-척도법mds이란",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "다차원 척도법(MDS)이란?",
    "text": "다차원 척도법(MDS)이란?\n다차원 척도법(Multidimensional Scaling, MDS)은 고차원 데이터를 저차원 공간에 시각화하는 차원 축소 기법입니다. 객체들 간의 거리나 유사도를 보존하면서 2차원 또는 3차원 공간에 데이터를 투영합니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#필요한-라이브러리-설치-및-임포트",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#필요한-라이브러리-설치-및-임포트",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "필요한 라이브러리 설치 및 임포트",
    "text": "필요한 라이브러리 설치 및 임포트\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import MDS\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.datasets import make_classification\nfrom scipy.spatial.distance import pdist, squareform\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['font.family'] = 'Noto Sans KR'",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#샘플-데이터-생성",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#샘플-데이터-생성",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "샘플 데이터 생성",
    "text": "샘플 데이터 생성\n다양한 케이스를 위한 샘플 데이터를 생성합니다.\n\n# 1. 연속변수만 포함하는 데이터\nnp.random.seed(42)\ncontinuous_data = make_classification(\n    n_samples=100, \n    n_features=5, \n    n_classes=3, \n    n_redundant=0, \n    n_informative=5,\n    random_state=42\n)[0]\n\ncontinuous_df = pd.DataFrame(\n    continuous_data, \n    columns=[f'feature_{i+1}' for i in range(5)]\n)\n\nprint(\"연속변수 데이터셋 shape:\", continuous_df.shape)\nprint(continuous_df.head())\n\n연속변수 데이터셋 shape: (100, 5)\n   feature_1  feature_2  feature_3  feature_4  feature_5\n0   0.051936  -1.797511  -1.855638  -1.396449  -1.196204\n1   0.403789   0.921306   3.200886   1.984403   0.106783\n2   0.300321  -0.930015   0.162936  -0.576956   2.232421\n3  -0.199444  -0.496488  -1.928236   0.929103  -1.480070\n4   1.144153  -1.221289  -0.581620  -0.475414   1.675759\n\n\n\n# 2. 명목변수를 포함하는 혼합 데이터\nnp.random.seed(42)\n\n# 연속변수\nage = np.random.normal(35, 10, 100)\nincome = np.random.normal(50000, 15000, 100)\nexperience = np.random.normal(5, 3, 100)\n\n# 명목변수\neducation = np.random.choice(['고등학교', '대학교', '대학원'], 100)\ndepartment = np.random.choice(['영업', '마케팅', '개발', 'HR'], 100)\nlocation = np.random.choice(['서울', '부산', '대구', '광주'], 100)\n\nmixed_df = pd.DataFrame({\n    'age': age,\n    'income': income,\n    'experience': experience,\n    'education': education,\n    'department': department,\n    'location': location\n})\n\nprint(\"\\n혼합 데이터셋 shape:\", mixed_df.shape)\nprint(mixed_df.head())\n\n\n혼합 데이터셋 shape: (100, 6)\n         age        income  experience education department location\n0  39.967142  28769.438869    6.073362      고등학교         개발       서울\n1  33.617357  43690.320159    6.682354       대학교        마케팅       서울\n2  41.476885  44859.282252    8.249154      고등학교        마케팅       광주\n3  50.230299  37965.840962    8.161406      고등학교         개발       서울\n4  32.658466  47580.714325    0.866992       대학원         개발       광주\n\n\n\n# 3. 거리 행렬 데이터 (도시간 거리 예시)\ncities = ['서울', '부산', '대구', '인천', '광주', '대전', '울산']\n# 실제 도시간 거리 (km)\ndistance_matrix = np.array([\n    [0, 325, 237, 28, 267, 140, 340],      # 서울\n    [325, 0, 88, 353, 158, 185, 45],       # 부산\n    [237, 88, 0, 265, 215, 97, 85],        # 대구\n    [28, 353, 265, 0, 295, 168, 368],      # 인천\n    [267, 158, 215, 295, 0, 168, 200],     # 광주\n    [140, 185, 97, 168, 168, 0, 230],      # 대전\n    [340, 45, 85, 368, 200, 230, 0]       # 울산\n])\n\ndistance_df = pd.DataFrame(distance_matrix, \n                          index=cities, \n                          columns=cities)\n\nprint(\"\\n도시간 거리 행렬:\")\nprint(distance_df)\n\n\n도시간 거리 행렬:\n     서울   부산   대구   인천   광주   대전   울산\n서울    0  325  237   28  267  140  340\n부산  325    0   88  353  158  185   45\n대구  237   88    0  265  215   97   85\n인천   28  353  265    0  295  168  368\n광주  267  158  215  295    0  168  200\n대전  140  185   97  168  168    0  230\n울산  340   45   85  368  200  230    0",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#연속변수만-포함하는-기본-mds-분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#연속변수만-포함하는-기본-mds-분석",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "1. 연속변수만 포함하는 기본 MDS 분석",
    "text": "1. 연속변수만 포함하는 기본 MDS 분석\n연속변수로만 구성된 데이터에 MDS를 적용하는 예시입니다.\n\n# 데이터 표준화\nscaler = StandardScaler()\ncontinuous_scaled = scaler.fit_transform(continuous_df)\n\n# 기본 MDS 적용 (2차원)\nmds = MDS(n_components=2, random_state=42)\nmds_result = mds.fit_transform(continuous_scaled)\n\n# 결과를 DataFrame으로 변환\nmds_df = pd.DataFrame(mds_result, columns=['MDS1', 'MDS2'])\n\nprint(\"MDS 결과:\")\nprint(mds_df.head())\nprint(f\"\\nStress 값: {mds.stress_:.4f}\")\n\nMDS 결과:\n       MDS1      MDS2\n0  0.329495 -1.755421\n1  0.049463  3.238964\n2 -0.946161  0.369966\n3  0.357709 -0.007320\n4 -0.769899 -0.435848\n\nStress 값: 3208.7256\n\n\n\n# 시각화\nplt.figure(figsize=(10, 8))\nplt.scatter(mds_df['MDS1'], mds_df['MDS2'], alpha=0.7, s=50)\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('MDS 결과 - 연속변수 데이터')\nplt.grid(True, alpha=0.3)\n\n# 각 점에 인덱스 번호 표시\nfor i, (x, y) in enumerate(zip(mds_df['MDS1'], mds_df['MDS2'])):\n    if i % 10 == 0:  # 10개마다 번호 표시\n        plt.annotate(str(i), (x, y), xytext=(5, 5), \n                    textcoords='offset points', fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 클래스별로 색상을 다르게 하여 시각화 (원본 클래스 정보 사용)\n_, y_true = make_classification(\n    n_samples=100, \n    n_features=5, \n    n_classes=3, \n    n_redundant=0, \n    n_informative=5,\n    random_state=42\n)\n\nplt.figure(figsize=(10, 8))\ncolors = ['red', 'blue', 'green']\nfor i in range(3):\n    mask = y_true == i\n    plt.scatter(mds_df.loc[mask, 'MDS1'], \n                mds_df.loc[mask, 'MDS2'], \n                c=colors[i], label=f'Class {i}', alpha=0.7, s=50)\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('MDS 결과 - 클래스별 색상 구분')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 거리 보존 정도 확인\nfrom sklearn.metrics import pairwise_distances\n\n# 원본 데이터의 거리 행렬\noriginal_distances = pairwise_distances(continuous_scaled)\n# MDS 결과의 거리 행렬\nmds_distances = pairwise_distances(mds_result)\n\n# 거리 상관계수 계산\ndistance_correlation = np.corrcoef(\n    original_distances.flatten(), \n    mds_distances.flatten()\n)[0, 1]\n\nprint(f\"원본 거리와 MDS 거리의 상관계수: {distance_correlation:.4f}\")\n\n# Shepard diagram 그리기\nplt.figure(figsize=(8, 6))\nplt.scatter(original_distances.flatten(), \n            mds_distances.flatten(), \n            alpha=0.3, s=1)\nplt.xlabel('Original Distances')\nplt.ylabel('MDS Distances')\nplt.title('Shepard Diagram - 거리 보존 정도')\nplt.plot([0, original_distances.max()], \n         [0, original_distances.max()], \n         'r--', alpha=0.8)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n원본 거리와 MDS 거리의 상관계수: 0.8435",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#명목변수를-포함하는-혼합-데이터-mds-분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#명목변수를-포함하는-혼합-데이터-mds-분석",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "2. 명목변수를 포함하는 혼합 데이터 MDS 분석",
    "text": "2. 명목변수를 포함하는 혼합 데이터 MDS 분석\n명목변수와 연속변수가 혼합된 데이터에 MDS를 적용하는 예시입니다.\n\n# 혼합 데이터 전처리\ndef preprocess_mixed_data(df):\n    \"\"\"혼합 데이터를 MDS에 적합하도록 전처리\"\"\"\n    processed_df = df.copy()\n    \n    # 연속변수 표준화\n    continuous_cols = ['age', 'income', 'experience']\n    scaler = StandardScaler()\n    processed_df[continuous_cols] = scaler.fit_transform(processed_df[continuous_cols])\n    \n    # 명목변수 원핫 인코딩\n    categorical_cols = ['education', 'department', 'location']\n    for col in categorical_cols:\n        dummies = pd.get_dummies(processed_df[col], prefix=col)\n        processed_df = pd.concat([processed_df, dummies], axis=1)\n        processed_df.drop(col, axis=1, inplace=True)\n    \n    return processed_df\n\n# 데이터 전처리\nmixed_processed = preprocess_mixed_data(mixed_df)\nprint(\"전처리된 혼합 데이터 shape:\", mixed_processed.shape)\nprint(\"\\n컬럼 목록:\")\nprint(mixed_processed.columns.tolist())\n\n전처리된 혼합 데이터 shape: (100, 14)\n\n컬럼 목록:\n['age', 'income', 'experience', 'education_고등학교', 'education_대학교', 'education_대학원', 'department_HR', 'department_개발', 'department_마케팅', 'department_영업', 'location_광주', 'location_대구', 'location_부산', 'location_서울']\n\n\n\n# 혼합 데이터에 MDS 적용\nmds_mixed = MDS(n_components=2, random_state=42)\nmds_mixed_result = mds_mixed.fit_transform(mixed_processed)\n\n# 결과를 DataFrame으로 변환\nmds_mixed_df = pd.DataFrame(mds_mixed_result, columns=['MDS1', 'MDS2'])\n\nprint(\"혼합 데이터 MDS 결과:\")\nprint(mds_mixed_df.head())\nprint(f\"\\nStress 값: {mds_mixed.stress_:.4f}\")\n\n혼합 데이터 MDS 결과:\n       MDS1      MDS2\n0  1.112396 -1.853207\n1  0.199845 -1.172801\n2 -0.073573 -1.903613\n3 -0.293398 -2.665801\n4  1.287388  1.284487\n\nStress 값: 3843.1912\n\n\n\n# 부서별로 색상을 다르게 하여 시각화\nplt.figure(figsize=(12, 8))\ndepartments = mixed_df['department'].unique()\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, dept in enumerate(departments):\n    mask = mixed_df['department'] == dept\n    plt.scatter(mds_mixed_df.loc[mask, 'MDS1'], \n                mds_mixed_df.loc[mask, 'MDS2'], \n                c=colors[i], label=dept, alpha=0.7, s=60)\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('MDS 결과 - 부서별 색상 구분 (혼합 데이터)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 학력별로 마커를 다르게 하여 시각화\nplt.figure(figsize=(12, 8))\neducations = mixed_df['education'].unique()\nmarkers = ['o', 's', '^']\n\nfor i, edu in enumerate(educations):\n    mask = mixed_df['education'] == edu\n    plt.scatter(mds_mixed_df.loc[mask, 'MDS1'], \n                mds_mixed_df.loc[mask, 'MDS2'], \n                marker=markers[i], label=edu, alpha=0.7, s=60)\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('MDS 결과 - 학력별 마커 구분 (혼합 데이터)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 부서와 학력을 함께 시각화\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n\n# 부서별 시각화\ndepartments = mixed_df['department'].unique()\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, dept in enumerate(departments):\n    mask = mixed_df['department'] == dept\n    ax1.scatter(mds_mixed_df.loc[mask, 'MDS1'], \n                mds_mixed_df.loc[mask, 'MDS2'], \n                c=colors[i], label=dept, alpha=0.7, s=60)\n\nax1.set_xlabel('MDS Dimension 1')\nax1.set_ylabel('MDS Dimension 2')\nax1.set_title('부서별 구분')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 학력별 시각화\neducations = mixed_df['education'].unique()\ncolors2 = ['purple', 'brown', 'pink']\n\nfor i, edu in enumerate(educations):\n    mask = mixed_df['education'] == edu\n    ax2.scatter(mds_mixed_df.loc[mask, 'MDS1'], \n                mds_mixed_df.loc[mask, 'MDS2'], \n                c=colors2[i], label=edu, alpha=0.7, s=60)\n\nax2.set_xlabel('MDS Dimension 1')\nax2.set_ylabel('MDS Dimension 2')\nax2.set_title('학력별 구분')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Gower 거리를 사용한 혼합 데이터 MDS\ndef gower_distance(X):\n    \"\"\"Gower 거리 계산 (연속변수와 명목변수 혼합용)\"\"\"\n    n_samples, n_features = X.shape\n    distances = np.zeros((n_samples, n_samples))\n    \n    # 각 피처가 연속변수인지 이진변수인지 판단\n    is_continuous = []\n    for j in range(n_features):\n        unique_vals = np.unique(X[:, j])\n        is_continuous.append(len(unique_vals) &gt; 2)\n    \n    for i in range(n_samples):\n        for j in range(i+1, n_samples):\n            distance = 0\n            for k in range(n_features):\n                if is_continuous[k]:\n                    # 연속변수: 절대차이를 범위로 나눔\n                    range_k = np.max(X[:, k]) - np.min(X[:, k])\n                    if range_k &gt; 0:\n                        distance += abs(X[i, k] - X[j, k]) / range_k\n                else:\n                    # 명목변수: 같으면 0, 다르면 1\n                    distance += 0 if X[i, k] == X[j, k] else 1\n            \n            distances[i, j] = distances[j, i] = distance / n_features\n    \n    return distances\n\n# 원본 혼합 데이터로 Gower 거리 계산\nmixed_array = mixed_processed.values\ngower_dist = gower_distance(mixed_array)\n\n# Gower 거리 기반 MDS\nmds_gower = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\nmds_gower_result = mds_gower.fit_transform(gower_dist)\n\n# 결과 시각화\nplt.figure(figsize=(10, 8))\ndepartments = mixed_df['department'].unique()\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor i, dept in enumerate(departments):\n    mask = mixed_df['department'] == dept\n    plt.scatter(mds_gower_result[mask, 0], \n                mds_gower_result[mask, 1], \n                c=colors[i], label=dept, alpha=0.7, s=60)\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('Gower 거리 기반 MDS 결과')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Gower 거리 기반 MDS Stress 값: {mds_gower.stress_:.4f}\")\n\n\n\n\n\n\n\n\nGower 거리 기반 MDS Stress 값: 70.5540",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#거리-행렬-기반-mds-분석",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#거리-행렬-기반-mds-분석",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "3. 거리 행렬 기반 MDS 분석",
    "text": "3. 거리 행렬 기반 MDS 분석\n미리 계산된 거리 행렬을 사용하여 MDS를 적용하는 예시입니다.\n\n# 도시간 거리 행렬로 MDS 수행\nmds_distance = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\ncity_mds_result = mds_distance.fit_transform(distance_matrix)\n\n# 결과를 DataFrame으로 변환\ncity_mds_df = pd.DataFrame(city_mds_result, \n                          columns=['MDS1', 'MDS2'], \n                          index=cities)\n\nprint(\"도시 MDS 결과:\")\nprint(city_mds_df)\nprint(f\"\\nStress 값: {mds_distance.stress_:.4f}\")\n\n도시 MDS 결과:\n          MDS1        MDS2\n서울  -85.433007 -154.346568\n부산   59.327174  130.031955\n대구  -27.112189   81.647524\n인천  -99.393194 -179.287057\n광주  145.126739  -10.586077\n대전  -24.860674  -32.769460\n울산   32.345151  165.309682\n\nStress 값: 2023.2616\n\n\n\n# 도시 위치 시각화\nplt.figure(figsize=(12, 10))\ncolors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']\n\nfor i, city in enumerate(cities):\n    plt.scatter(city_mds_df.loc[city, 'MDS1'], \n                city_mds_df.loc[city, 'MDS2'], \n                c=colors[i], s=200, alpha=0.7, \n                label=city, edgecolors='black', linewidth=1)\n    \n    # 도시 이름 표시\n    plt.annotate(city, \n                (city_mds_df.loc[city, 'MDS1'], city_mds_df.loc[city, 'MDS2']),\n                xytext=(10, 10), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\nplt.xlabel('MDS Dimension 1')\nplt.ylabel('MDS Dimension 2')\nplt.title('도시간 거리 기반 MDS 결과')\nplt.grid(True, alpha=0.3)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 실제 거리와 MDS 거리 비교\nmds_city_distances = pairwise_distances(city_mds_result)\n\n# 거리 상관계수 계산\ncity_distance_correlation = np.corrcoef(\n    distance_matrix.flatten(), \n    mds_city_distances.flatten()\n)[0, 1]\n\nprint(f\"실제 거리와 MDS 거리의 상관계수: {city_distance_correlation:.4f}\")\n\n# Shepard diagram for city distances\nplt.figure(figsize=(8, 6))\nplt.scatter(distance_matrix.flatten(), \n            mds_city_distances.flatten(), \n            alpha=0.6, s=30)\nplt.xlabel('Original Distances (km)')\nplt.ylabel('MDS Distances')\nplt.title('Shepard Diagram - 도시간 거리 보존 정도')\nplt.plot([0, distance_matrix.max()], \n         [0, distance_matrix.max()], \n         'r--', alpha=0.8)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n실제 거리와 MDS 거리의 상관계수: 0.9970\n\n\n\n\n\n\n\n\n\n\n# 3차원 MDS 수행\nmds_3d = MDS(n_components=3, dissimilarity='precomputed', random_state=42)\ncity_mds_3d_result = mds_3d.fit_transform(distance_matrix)\n\n# 3D 시각화\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111, projection='3d')\n\ncolors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']\n\nfor i, city in enumerate(cities):\n    ax.scatter(city_mds_3d_result[i, 0], \n               city_mds_3d_result[i, 1], \n               city_mds_3d_result[i, 2],\n               c=colors[i], s=200, alpha=0.7, \n               label=city, edgecolors='black', linewidth=1)\n    \n    # 도시 이름 표시\n    ax.text(city_mds_3d_result[i, 0], \n            city_mds_3d_result[i, 1], \n            city_mds_3d_result[i, 2], \n            city, fontsize=10, fontweight='bold')\n\nax.set_xlabel('MDS Dimension 1')\nax.set_ylabel('MDS Dimension 2')\nax.set_zlabel('MDS Dimension 3')\nax.set_title('3차원 MDS 결과 - 도시간 거리')\nax.legend(bbox_to_anchor=(1.1, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\nprint(f\"3차원 MDS Stress 값: {mds_3d.stress_:.4f}\")\n\n\n\n\n\n\n\n\n3차원 MDS Stress 값: 1974.2860\n\n\n\n# 차원별 Stress 값 비교\ndimensions = range(1, 6)\nstress_values = []\n\nfor dim in dimensions:\n    mds_temp = MDS(n_components=dim, dissimilarity='precomputed', random_state=42)\n    mds_temp.fit(distance_matrix)\n    stress_values.append(mds_temp.stress_)\n\n# Stress plot (Scree plot)\nplt.figure(figsize=(10, 6))\nplt.plot(dimensions, stress_values, 'bo-', linewidth=2, markersize=8)\nplt.xlabel('차원 수')\nplt.ylabel('Stress 값')\nplt.title('차원 수에 따른 Stress 값 변화')\nplt.grid(True, alpha=0.3)\nplt.xticks(dimensions)\n\n# 각 점에 stress 값 표시\nfor i, stress in enumerate(stress_values):\n    plt.annotate(f'{stress:.3f}', \n                (dimensions[i], stress), \n                xytext=(0, 10), textcoords='offset points', \n                ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"차원별 Stress 값:\")\nfor dim, stress in zip(dimensions, stress_values):\n    print(f\"{dim}차원: {stress:.4f}\")\n\n\n\n\n\n\n\n\n차원별 Stress 값:\n1차원: 340937.8571\n2차원: 2023.2616\n3차원: 1974.2860\n4차원: 1980.2935\n5차원: 1982.6011",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#다양한-mds-변형-기법",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#다양한-mds-변형-기법",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "4. 다양한 MDS 변형 기법",
    "text": "4. 다양한 MDS 변형 기법\n다양한 MDS 알고리즘을 비교해보는 예시입니다.\n\n# 다양한 MDS 알고리즘 비교\nfrom sklearn.manifold import MDS\n\n# 연속 데이터를 사용하여 다양한 MDS 알고리즘 비교\nalgorithms = {\n    'Classical MDS': MDS(metric=True, random_state=42),\n    'Non-metric MDS': MDS(metric=False, random_state=42),\n    'MDS with different init': MDS(metric=True, n_init=10, random_state=42)\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nfor idx, (name, mds_algo) in enumerate(algorithms.items()):\n    # MDS 수행\n    result = mds_algo.fit_transform(continuous_scaled)\n    \n    # 클래스별 색상으로 시각화\n    colors = ['red', 'blue', 'green']\n    for i in range(3):\n        mask = y_true == i\n        axes[idx].scatter(result[mask, 0], result[mask, 1], \n                         c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n    \n    axes[idx].set_xlabel('MDS Dimension 1')\n    axes[idx].set_ylabel('MDS Dimension 2')\n    axes[idx].set_title(f'{name}\\nStress: {mds_algo.stress_:.4f}')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# 거리 메트릭 비교\nfrom sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances\n\n# 다양한 거리 메트릭으로 MDS 수행\ndistance_metrics = {\n    'Euclidean': euclidean_distances(continuous_scaled),\n    'Manhattan': manhattan_distances(continuous_scaled),\n    'Cosine': cosine_distances(continuous_scaled)\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nfor idx, (metric_name, dist_matrix) in enumerate(distance_metrics.items()):\n    # 거리 행렬을 사용한 MDS\n    mds_metric = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n    result = mds_metric.fit_transform(dist_matrix)\n    \n    # 클래스별 색상으로 시각화\n    colors = ['red', 'blue', 'green']\n    for i in range(3):\n        mask = y_true == i\n        axes[idx].scatter(result[mask, 0], result[mask, 1], \n                         c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n    \n    axes[idx].set_xlabel('MDS Dimension 1')\n    axes[idx].set_ylabel('MDS Dimension 2')\n    axes[idx].set_title(f'{metric_name} Distance MDS\\nStress: {mds_metric.stress_:.4f}')\n    axes[idx].legend()\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#mds-결과-해석-및-평가",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#mds-결과-해석-및-평가",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "5. MDS 결과 해석 및 평가",
    "text": "5. MDS 결과 해석 및 평가\nMDS 결과를 해석하고 평가하는 방법들을 소개합니다.\n\n# Stress 값 해석 기준\ndef interpret_stress(stress_value):\n    \"\"\"Stress 값을 해석하는 함수\"\"\"\n    if stress_value &lt; 0.05:\n        return \"매우 좋음 (Excellent)\"\n    elif stress_value &lt; 0.1:\n        return \"좋음 (Good)\"\n    elif stress_value &lt; 0.2:\n        return \"보통 (Fair)\"\n    else:\n        return \"나쁨 (Poor)\"\n\n# 각 MDS 결과의 Stress 값 해석\nprint(\"=== MDS 결과 평가 ===\")\nprint(f\"연속변수 MDS Stress: {mds.stress_:.4f} - {interpret_stress(mds.stress_)}\")\nprint(f\"혼합데이터 MDS Stress: {mds_mixed.stress_:.4f} - {interpret_stress(mds_mixed.stress_)}\")\nprint(f\"Gower 거리 MDS Stress: {mds_gower.stress_:.4f} - {interpret_stress(mds_gower.stress_)}\")\nprint(f\"도시 거리 MDS Stress: {mds_distance.stress_:.4f} - {interpret_stress(mds_distance.stress_)}\")\n\n=== MDS 결과 평가 ===\n연속변수 MDS Stress: 3208.7256 - 나쁨 (Poor)\n혼합데이터 MDS Stress: 3843.1912 - 나쁨 (Poor)\nGower 거리 MDS Stress: 70.5540 - 나쁨 (Poor)\n도시 거리 MDS Stress: 2023.2616 - 나쁨 (Poor)\n\n\n\n# 차원 축소 효과 비교\ndef calculate_explained_variance_ratio(original_data, mds_result):\n    \"\"\"MDS로 설명되는 분산 비율 계산\"\"\"\n    original_var = np.var(original_data, axis=0).sum()\n    mds_var = np.var(mds_result, axis=0).sum()\n    return mds_var / original_var\n\n# 연속변수 데이터의 분산 설명 비율\nexplained_ratio = calculate_explained_variance_ratio(continuous_scaled, mds_result)\nprint(f\"\\n연속변수 MDS 분산 설명 비율: {explained_ratio:.4f} ({explained_ratio*100:.2f}%)\")\n\n# 원본 데이터 차원과 MDS 결과 비교\nprint(f\"원본 데이터 차원: {continuous_scaled.shape[1]}차원\")\nprint(f\"MDS 결과 차원: {mds_result.shape[1]}차원\")\nprint(f\"차원 축소율: {(1 - mds_result.shape[1]/continuous_scaled.shape[1])*100:.1f}%\")\n\n\n연속변수 MDS 분산 설명 비율: 0.9358 (93.58%)\n원본 데이터 차원: 5차원\nMDS 결과 차원: 2차원\n차원 축소율: 60.0%\n\n\n\n# MDS vs PCA 비교\nfrom sklearn.decomposition import PCA\n\n# PCA 수행\npca = PCA(n_components=2, random_state=42)\npca_result = pca.fit_transform(continuous_scaled)\n\n# 비교 시각화\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# MDS 결과\ncolors = ['red', 'blue', 'green']\nfor i in range(3):\n    mask = y_true == i\n    ax1.scatter(mds_result[mask, 0], mds_result[mask, 1], \n                c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n\nax1.set_xlabel('MDS Dimension 1')\nax1.set_ylabel('MDS Dimension 2')\nax1.set_title(f'MDS 결과\\nStress: {mds.stress_:.4f}')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# PCA 결과\nfor i in range(3):\n    mask = y_true == i\n    ax2.scatter(pca_result[mask, 0], pca_result[mask, 1], \n                c=colors[i], label=f'Class {i}', alpha=0.7, s=30)\n\nax2.set_xlabel('PC1')\nax2.set_ylabel('PC2')\nax2.set_title(f'PCA 결과\\n설명분산비율: {pca.explained_variance_ratio_.sum():.4f}')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"PCA 설명 분산 비율: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.2f}%)\")\nprint(f\"각 주성분별 설명 분산 비율: PC1={pca.explained_variance_ratio_[0]:.4f}, PC2={pca.explained_variance_ratio_[1]:.4f}\")\n\n\n\n\n\n\n\n\nPCA 설명 분산 비율: 0.5028 (50.28%)\n각 주성분별 설명 분산 비율: PC1=0.2727, PC2=0.2301",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#결론",
    "href": "posts/03_archives/completed_project/adp_실기/notes/etc/00.html#결론",
    "title": "다차원 척도법 (Multidimensional Scaling)",
    "section": "결론",
    "text": "결론\n다차원 척도법(MDS)은 다양한 유형의 데이터에 적용할 수 있는 강력한 차원 축소 기법입니다:\n\n주요 특징:\n\n거리 보존: 원본 데이터의 거리 관계를 저차원에서 최대한 보존\n유연성: 연속변수, 명목변수, 거리 행렬 등 다양한 데이터 타입 지원\n직관적 해석: 2D/3D 시각화를 통한 직관적인 데이터 이해\n\n\n\n적용 사례:\n\n연속변수: 표준화 후 직접 적용\n혼합 데이터: 원핫 인코딩 또는 Gower 거리 사용\n거리 행렬: 미리 계산된 거리 정보 활용\n\n\n\n평가 지표:\n\nStress 값: 낮을수록 좋음 (&lt; 0.1이 권장)\n거리 상관계수: 원본과 MDS 거리의 상관관계\nShepard diagram: 거리 보존 정도 시각화",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1",
      "Notes",
      "Etc",
      "다차원 척도법 (Multidimensional Scaling)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/index.html",
    "href": "posts/03_archives/completed_project/adp_실기/index.html",
    "title": "ADP 실기 준비 - try 1",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2025-06-16\n        종료일: 2025-10-18\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증데이터 분석python",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/index.html#details",
    "href": "posts/03_archives/completed_project/adp_실기/index.html#details",
    "title": "ADP 실기 준비 - try 1",
    "section": "Details",
    "text": "Details\n빠르게 끝내 봅시다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/index.html#tasks",
    "href": "posts/03_archives/completed_project/adp_실기/index.html#tasks",
    "title": "ADP 실기 준비 - try 1",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    원서 접수 (2025.09.15 10 am)\n                \n                신청 완료\n            \n\n            \n            \n                \n                    \n                    사전 점수 공개 (2025.11.07)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/adp_실기/index.html#참고-자료",
    "title": "ADP 실기 준비 - try 1",
    "section": "참고 자료",
    "text": "참고 자료\n\n확률 통계\n  \n\n위 책은 코드에 오류가 좀 있는거 같다. 단계적으로 잘 설명은 해주지만 오류가 너무 치명적인거 같다.\n깃허브에 오류 관련 질문들이 올라와 있다.\n\n..오류라고 생각했는데 공부하다보니 내가 잘 못 이해한 거였다.\nrolling prediction에 test data로 예측하는 부분이 잘못되었다고 생각했었다.\n실제로 예측을 train data로 진행한 이후 test data를 계속 추가하는 로직이 맞다.\n\n\n\n\n\n머신 러닝",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/adp_실기/index.html#related-posts",
    "href": "posts/03_archives/completed_project/adp_실기/index.html#related-posts",
    "title": "ADP 실기 준비 - try 1",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "ADP 실기 준비 - try 1"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/신재생에너지/00.html",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/신재생에너지/00.html",
    "title": "개인 발표 - 공짜 탄소배출권 발급 사건",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "신재생에너지",
      "개인 발표 - 공짜 탄소배출권 발급 사건"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#프로세스-마이닝process-mining이란",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#프로세스-마이닝process-mining이란",
    "title": "Process mining",
    "section": "프로세스 마이닝(Process Mining)이란?",
    "text": "프로세스 마이닝(Process Mining)이란?\n\n이벤트 로그로부터 프로세스 관련 정보(meaningful & useful)를 추출하는 것\n\n\n\n\n\n이벤트 로그의 다양한 활용 방법\n\n\n\n프로세스 마이닝을 활용하면 사전 지식 없이 process model이나 organization 정보를 도출할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#bpm-life-cycle-완성을-위한-process-mining-역할",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#bpm-life-cycle-완성을-위한-process-mining-역할",
    "title": "Process mining",
    "section": "BPM Life-cycle 완성을 위한 process mining 역할",
    "text": "BPM Life-cycle 완성을 위한 process mining 역할\n\n\n\nBPM Life-cycle\n\n\n\n프로세스 실행 결과 생성된 데이터에 기초해서 현황 진단 및 추가 요구사항 분석이 수행되어야 함.\n실제로는 심각한 문제점이 발생하거나, 주요 외부 환경 변화가 있을 경우에만 life-cycle 순환이 일어나고, 재설계 의사결정 과정에 프로세스에 대한 실제 정보가 잘 사용 안됨\n프로세스 마이닝이 truly close the BPM life-cycle 실현에 기여",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#프로세스-마이닝-유형",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#프로세스-마이닝-유형",
    "title": "Process mining",
    "section": "프로세스 마이닝 유형",
    "text": "프로세스 마이닝 유형\n\nDiscovery\n\n사전 지식이 없는 상태에서 이벤트 로그로부터 프로세스 모델(control flow)을 도출\nα-algorithm -&gt; petri net 모델 도출 -&gt; BPMN 다이어그램 변환\n\nConformance checking\n\n기존 프로세스 모델과 이벤트 로그에 기록된 현실(reality)이 일치하는지 확인\n규정 준수 여부 확인 가능\nDeviations이 어디서 발생하는지 감지하여 설명할 수 있다.\n\nEnhancement\n\nrepair: 현실을 잘 반영하기 위해 프로세스 모델을 수정\nextension: 프로세스 모델에 새로운 관점이나 성과 지표를 추가함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#event-log",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#event-log",
    "title": "Process mining",
    "section": "Event log",
    "text": "Event log\n\n\n\nEvent log\n\n\n\n반드시 케이스 또는 프로세스 인스턴스를 식별할 수 있어야 함.\n\ncase id, timestamp(하나만 있으면 완료 시간 의미), acitivity 이름이 필수로 있어야 한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#복잡도-제어",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#복잡도-제어",
    "title": "Process mining",
    "section": "복잡도 제어",
    "text": "복잡도 제어\n\nSpagethetti 프로세스에서 Lasagna 프로세스로 개선하는 방법\nFuzzy mining: 저빈도 activity들이 제거되거나 클러스터 노드로 병함됨. 저빈도 아크는 설정된 임계치 값에 따라 제거됨",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#challenges-of-process-discovery-techniques",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#challenges-of-process-discovery-techniques",
    "title": "Process mining",
    "section": "Challenges of Process Discovery Techniques",
    "text": "Challenges of Process Discovery Techniques\n\n\n\n옳은 모델\n\n\n\n\n\n안좋은 모델(overfitting: too specific)\n\n\n\n\n\n안좋은 모델(underfitting: too general)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#적합성적합도-검사",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#적합성적합도-검사",
    "title": "Process mining",
    "section": "적합성(적합도) 검사",
    "text": "적합성(적합도) 검사\n\n불일치 시 두 가지 해석 가능\n\n이벤트 로그가 틀림: 기준이 되는 바람직한 모델을 따르지 않음\n\nmodel is normative (de jure model)\n교정:\n\n문제 인식\n원인 분석\n교정 조치 실행\n재검증 및 모니터링\n\n\n모델이 틀림: 모델이 현실을 잘 설명하지 못함\n\nmodel is descriptive (de facto model)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#enhancement---extension",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#enhancement---extension",
    "title": "Process mining",
    "section": "Enhancement - extension",
    "text": "Enhancement - extension\n\n조직 관점\n\n특정 activity를 담당하며 업무를 수행하는 역할(role)은 무엇이며, 어떤 작업자들이 업무 수행을 담당하는가?\nlog 데이터에서 resource가 기록되어 있어야 함\n\ncase 관점\n\n케이스의 어떤 특징이 의사결정에 영향을 미치는가?\nlog 데이터에 부가적 속성이 기록되어 있어야 함. 데이터 마이닝 분류 기법을 사용하여 분기 규칙 발견(의사결정 마이닝)\n\n시간 관점\n\n프로세스 내 병목 지점이 어디인가?\n애니메이션 등을 추가\n\n\n\n\n\n예시\n\n\n\nactivity 대기시간, 수행시간, 분기 확률 파악 가능\ndiscovery에서 발견한 control-flow 관점을 포함해 세 관점을 통합해서 통찰력 제공\n\nobtain an event log\ncreate or discover a process model\nconnect events in the log to activities in the model\nextend the model\nreturn integrated model\n\n규정 준수 여부와 연계 가능\n사생활 보호 이슈를 고려해야 함\n\n비식별화 또는 익명화 요구됨",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#소셜-네트워크-분석sna",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#소셜-네트워크-분석sna",
    "title": "Process mining",
    "section": "소셜 네트워크 분석(SNA)",
    "text": "소셜 네트워크 분석(SNA)\n\n작업 이양(work handover) 지표를 이용하여 소셜 네트워크 생성\n업무에 참여한 사람/역할/부서/자원 사이의 상관 관계 분석\n\n\n\n\noval: entity. 크기가 중요도, arc: relationship. 두께가 중요도",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#dotted-chart",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#dotted-chart",
    "title": "Process mining",
    "section": "Dotted Chart",
    "text": "Dotted Chart\n\n절대적 혹은 상대적 시간에 따른 특성을 점으로 찍은거\n\n\n\nDesire lines in process model",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#프로세스-마이닝-사례",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#프로세스-마이닝-사례",
    "title": "Process mining",
    "section": "프로세스 마이닝 사례",
    "text": "프로세스 마이닝 사례\n\n외래진료 프로세스 분석(분당 서울대 병원)\n\n적합성 검증\n\n도출된 진료 프로세스 모델과 병원의 진료 지침(CPG: Clinical Practice Guideline)이 얼마나 일치하는지 검증\n약 90% 일치(잘 지켜지고 있다.)\n\n진료 프로세스 패턴 분석\n\n환자 유형별 다양한 진료 패턴이 존재함\n이 분석 결과는 환자 안내 시스템 개선에 활용\n\n시뮬레이션 모델 작성\n\n로그 분석을 바탕으로 시뮬레이션 모델 작성. 환자가 늘어날 경우 진료 시간에 어떤 변화가 있는지 분석\n병원의 키오스크 적정 대수 산정\n\n\n\n\n건강검진센터 프로세스 분석(삼성서울병원)\n\n스마트 건강검진 시스템 도입 효과 검증\n\n프로세스 마이닝을 통해 프로세스 개선 효과 정량적 / 객관적 검증\n수검자 대기시간, 검사시간 및 가동률 등의 성과 비교 분석\n\n적합성 검증\n\n검사간 선후관계를 따르지 않는 Deviation 발견\n고도화 전후의 프로세스 비교 분석(주요 sequence 패턴 대상)\n\n성과 분석\n\n전반적으로 각 검사실의 대기시간이 감소한 것을 정량적으로 확인\n고도화 이후 평균 검사 개수에 따른 검진 시간 및 특정 검사 항목 예측\n\n\n\n\n모바일 게임 이용자 여정 분석\n\n모바일 게임 신규 이용자들의 이용 패턴 및 이탈 패턴 분석\n\n과금 유저 전환율 제고를 위한 시사점 도출\n\n\n\n기대효과: 신규 이용자를 이해하는데 핵심적인 역할. 수익 극대화에 기여\n\n\n\n중소 숙박업소 운영 프로세스 분석\n\n중소 숙박업소 특징\n\n상품 = 객식 사용 시간\n재고 소멸성: 시간이 지나면 활용 불가능\n표준 프로세스 모델 부재\n국내 중소숙박업소 정보 시스템은 매우 빈약\n\n실제 수행 프로세스 분석\n\n객실 이용형태에 따른 절차상 공통점, 차이점 발견\n발견된 모델을 활용하여 마케팅 자료로 이용\n\n이용 패턴 발견에 따른 고객 행동 패턴 예측\n\n\n\n삼성전자 MES 분석\n\nMES(Manufacturing Execution System) 로그 데이터를 활용한 반도체 제조 공정 분석\n\n\n\n멜론 서비스 여정 분석\n\n사용자를 행동 패턴에 따라 5개 그룹으로 세부화 하고, 세부 그룹별 전환율을 높이기 위한 타켓 마케팅 캠페인 계획 수립",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#프로세스-마이닝-도구",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#프로세스-마이닝-도구",
    "title": "Process mining",
    "section": "프로세스 마이닝 도구",
    "text": "프로세스 마이닝 도구\n\nCelonis\nMinit\nmyInvenio\nPerceptive",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#disco-demo",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/09.html#disco-demo",
    "title": "Process mining",
    "section": "Disco Demo",
    "text": "Disco Demo\n\n예제 목적\n\n프로세스에 대한 상세한 이해를 통한 효율화 방안 모색\n가이드라인 준수 여부 확인\n병목 지점 발견 / 개선을 통한 프로세스 성능 목표치 통제\n\nperformance filter로 확인 가능\n\n\n\n\n\n\n프로세스 마이닝 Phases\n\n\n\n\n\ncase 탭에서 가능한 프로세스들의 세부적인 내용을 파악할 수 있다.\n\n\n\nactivity에 activity, role 등을 설정해서 어떤 관점으로 분석할지 선택 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "Process mining"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/07.html#what-is-dmn",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/07.html#what-is-dmn",
    "title": "DMN",
    "section": "what is DMN?",
    "text": "what is DMN?\n\nDecision Model and Notation\nDecision Logic을 시각적으로 모델링 하는 것\n\n\n\n\nDecision Logic\n\n\n\nXML 기반\noperational decisions에 적합함.\nstrategic decisions에는 적합하지 않음.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "DMN"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/07.html#why-dmn",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/07.html#why-dmn",
    "title": "DMN",
    "section": "why DMN?",
    "text": "why DMN?\n\nTransparency 향상\n\nDecision Logic과 code 분리 가능\n\nEfficiency\n\n자동 의사결정 가능\n\nAgility\n\n새로운 규칙이나 변경이 쉽게 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "DMN"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/07.html#dmn-구성-요소",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/07.html#dmn-구성-요소",
    "title": "DMN",
    "section": "DMN 구성 요소",
    "text": "DMN 구성 요소\n\n\n\nDMN Element\n\n\n\nDRD: general structure of decision\nDecision Table (Decision Logic):\n\nrules of a decision 설정\nFEEL(Friendly Enough Expression Language) 사용\n\nDecision Box: 사각형\n\n주어진 입력으로 부터 출력을 결정하는 decision\n\n입력: value or the result of another decision\n\n\nDecision Requirement: 화살표\n\n연결된 input과의 관계 설정. input이 decision에 required 하다는 것을 의미\n\n\n\n\n\nDecision Table\n\n\n\n한 row가 하나의 rule을 나타냄\ninput과 output은 특정 type을 가짐\n\nstring, number, boolean 등\n\nmultiple outputs 가능\nmultiple columns는 and로 연결됨\nHit Policy: 여러 rule이 매칭될 때 어떻게 처리할지 결정\n\n\nFEEL 문법\n\nExpressions\nUnary Tests: 특정 조건을 만족하는지 확인\n\nComparision\nInterval\nDisjunction: , 쓰면 or로 연결\nNegation: not(조건이나 값)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "DMN"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/05.html#soaservice-oriented-architecture",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/05.html#soaservice-oriented-architecture",
    "title": "프로세스 모델링 표준: WS-BPEL",
    "section": "SOA(Service-Oriented Architecture)",
    "text": "SOA(Service-Oriented Architecture)\n\n분산컴퓨팅 환경에서 느슨하게 결합된 서비스들의 집합으로 구성된 아키텍처\nbuilding block으로 loosely coupled 된다.\n\n\n\n서비스: 독립적으로 반복사용 가능하고, 네트워크를 통해 접근 가능한 비즈니스 기능 단위\n\n\n\n\nSOA 구성요소\n\n\n\n개방형 표준을 통해 정의된 인터페이스로 접근\n\n\nSOA 실현 인터페이스\n\n\n\n웹 서비스 개방형 표준 구성\n\n\n\nSOAP: Request / Response 메시지 포맷\nBPEL: 서비스들을 순서를 정해서 연결할 때 사용\nUDDI: 표준 서비스 레지스트리\nWSDL: 서비스의 인터페이스를 정의할 때 사용. 사용자에게 사용법을 XML로 알려줌\n\nprocedure 이름: &lt;operation&gt;\n입력 인자 타입, 반환 인자 타입 : &lt;message&gt;\n전송 프로토콜: &lt;binding&gt;\nEndopoint URL: &lt;port&gt;",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 모델링 표준: WS-BPEL"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/05.html#ws-bpelweb-services-business-process-execution-language",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/05.html#ws-bpelweb-services-business-process-execution-language",
    "title": "프로세스 모델링 표준: WS-BPEL",
    "section": "WS-BPEL(Web Services Business Process Execution Language)",
    "text": "WS-BPEL(Web Services Business Process Execution Language)\n\nWSDL로 표현된 서비스들 사이의 상호작용을 모델링하는 프로세스\nBPEL 프로세스와 외부 파트너 제공 서비스를 결합하여 WSDL을 가지는 서비스로 모델링\n\n\n종류\n\nAbstract Processes View(Public Process): 서비스 간의 상호작용만 정의. 내부 프로세스 로직은 정의하지 않음\nExecutable Processes View(Private Process): 서비스 간의 상호작용과 내부 프로세스 로직 모두 정의\n\n\n\n구성 요소\n\n\n\nBPEL Language 구성 요소\n\n\n\nBasic Activities\n\ninvoke: 외부 서비스 호출만 하는 1-way 또는 request-response 메시지 전송\nreceive: 외부 서비스로부터 메시지 수신\nreply: 외부 서비스에 메시지 응답\n\nStructured Activities(Nesting Structured Activities)\n\nflow: 병렬적 활동\nsequence: 순차적 활동\nif-elseif-else: 조건 분기\nwhile: 선행 조건이 만족되는 한 반복 활동",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 모델링 표준: WS-BPEL"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-경영을-위한-전략visioning",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-경영을-위한-전략visioning",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 경영을 위한 전략(Visioning)",
    "text": "프로세스 경영을 위한 전략(Visioning)\n\n1. 대내외 환경 분석\n\n\n\nSWOT\n\n\n\n\n\n5-Forces 모델\n\n\n\n\n2. 기업 비전 수립\n\n기업의 총체적인 방향과 목표 설정\n보통 2~5년 주기로 변화관리 수행\n다음의 요소를 정의해야 함\n\n비전: 기업이 달성하고자 하는 미래상\n미션 또는 사명: 기업의 존재 이유\n가치 제안: 경쟁사와 차별화되는 요소\n목적(Goal): 실현하고자 하는 일\n목표(Objective): 목적을 이루기 위해 구체적으로 해야하는 일\n\n\n\n\n3. 기업 전략 탐색\n\n가치규율모델(value discipline model): 세 가지 전략 중 하나를 선택하여 집중\n\n운영 우수성: 제조 프로세스 및 업무 절차를 자동화하여 운영 업무를 간소화하고 비용을 절감하는데 집중. cost leadership 달성\n고객 친밀도: 서비스의 개인화(personalization)와 맞춤화(customization)를 통해 각기 다른 고객의 요구를 충족하는데 집중\n제품 리더십: 프리미엄 전략\n\nBSC(Balanced Score Card)\n\n재무적인 지표에만 치중하지 않고, 균형된 성과 지표의 조합을 보는거\n4가지 영역(핵심 성공요소(CSF))\n\n재무\n고객\n프로세스\n학습과 성장\n\n\n\n\n\n4. 전략측정지표 정의(KPI)\n\n상위 수준의 전략 성과 측정지표 정의",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-아키텍처",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-아키텍처",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 아키텍처",
    "text": "프로세스 아키텍처\n\n프로세스 관리의 목표, 원칙, 프로세스 모델 및 계층 구조를 총칭\n\n\n1. 프로세스 관리 목표 수립\n\n비전수립 단계에서 수립된 기업 전략을 프로세스 관점에서 해석하여 프로세스 관리 목표 수립\n\n\n\n2. 프로세스 관리 원칙 수립\n\n\n3. 프로세스 모델 수립\n\n프로세스 분류 체계: 프로세스 간 계층관계\n프로세스 스키마\n프로세스 콘텐츠\n\n\n\n4. 프로세스 통합 및 검증\n\n\n5. 프로세스 아키텍처 운영\n\n프로세스 아키텍처 위원회를 구성해서 지속적으로 관리",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-모델링-분석",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-모델링-분석",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 모델링 / 분석",
    "text": "프로세스 모델링 / 분석\n\n개별 프로세스와 세부 액티비티의 논리적 흐름을 정의하고 개선기회를 발견하여 최적화 시킴\n\n\n\n\nBPMN 표기법\n\n\n\n1. 대상 프로세스의 목표와 고객 확인\n\n프로세스 모델링 목표와 프로세스 자체의 수행 목표, 고객을 확인한 이후 다이어그램 작성\n\n모델링 목표:\n\n프로세스 발견\nAs-Is 모델링\nTo-Be 모델링\n자동화 실행 모델 구축\n\n\n\n\n\n2. 프로세스 정보 취합과 태스크 정의\n\n인터뷰, 자료 조사 등을 통해 관련 정보를 취합\n관련 액티비티(태스크) 들을 개략적으로 정의\n취합 정보:\n\n프로세스 수행 조직 및 참여자\n기동되는 이벤트\n다른 프로세스와의 연관관계\n관계되는 역할들\n관련 전문 용어\n\n프로세스 책임자를 통해 수집된 정보의 1차 검증 실시\n\n\n\n3. 프로세스 맵 작성\n\n프로세스 내에 존재하는 액티비티들 간 업무 흐름을 작성\n\n\n\n\n프로세스 맵 예시\n\n\n\nAs-Is 모델링\n\n프로세스의 논리적 흐름을 정의\n현재의 업무 프로세스를 분석하여 문제점을 도출\n\nCheck List:\n\n목표를 가지고 있는가?\nInput과 Output이 있는가?\n1회 이상 반복되고 있는가?\n효과를 측정할 수 있는가?\n시작과 끝이 존재하나?\n각 단계를 수행하는 작업자가 명확한가?\n성과 측정과 개선을 책임지는 인력이 존재하나?\n\n\n\n\n4. 프로세스 분석\n\n작성된 프로세스 맵을 대상으로 개선기회를 모색\n\n개선기회:\n\n품질 향상\n리드타임 단축\n\n병렬화나 파레토 법칙 적용\n\n생산성 향상\n원가 절감\n\n\nTo-Be 모델링\n\n\n\n5. 프로세스 맵 검증 및 워크숍 등을 통한 확정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-자동화",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-자동화",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 자동화",
    "text": "프로세스 자동화\n\n자동화 이전에 프로세스 표준화와 개선과정이 선행되어야 함\n\n\n1. BPM Suite 및 HW 장비 휙득\n\n사내 전산 부서에서 개발 혹은 전문 솔루션 업체로부터 구매 혹은 오픈소스 솔루션\nBPM Suite 구성 요소\n\nBPE(Business Process Execution): Camunda8\n\n\n\n\n2. 자동화 대상 프로세스 선정\n\n\n3. 요구사항 분석 및 설계\n\n실행 관점에서 비즈니스 프로세스의 상세 분석 및 설계를 수행함\n분석 설계 대상\n\n프로세스: BPM 엔진에 의해 직접 실행 가능하도록 설계해야 함\n어플리케이션: 액티비티를 수행하는 작업자가 활용하는 IT 솔루션(form)\n인터페이스: 액티비티 간 데이터 교환 프로토콜\n\n\n\n\n4. 구현 및 이행",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-성과관리",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-성과관리",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 성과관리",
    "text": "프로세스 성과관리\n\n개별 기능 부서의 관점이 아닌 프로세스의 End-to-End 관점에서 진단/분석/대책 수립\n결과뿐만 아니라 과정에 대한 지표를 포함\n\n\n\n\nKPI 계층 구조\n\n\n\n1. 전사 성과관리 체계 수립\n\n\n2. 성과관리 대상 프로세스의 선정\n\n\n3. 성과관리 지표 및 성과측정 모형의 설계\n\n\n\n성과관리 지표 설계 원칙\n\n\n\n\n4. 성과지표 모니터링\n\n실시간 성과지표 모니터링\nBAM(Business Activity Monitoring) 도구 활용\n\n\n\n5. 지속적 성과창출을 위한 전략 수립\n\n환경의 변화에 따라 대상 프로세스의 재선정, 성과지표의 재설계 등 수행",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-개선",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#프로세스-개선",
    "title": "프로세스 경영 구축 방법론",
    "section": "프로세스 개선",
    "text": "프로세스 개선\n\n\n\nOMG의 BPMM(Business Process Maturity Model)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#조직-변화관리",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/프로세스경영/03.html#조직-변화관리",
    "title": "프로세스 경영 구축 방법론",
    "section": "조직 변화관리",
    "text": "조직 변화관리\n\n관리 원칙:\n\n최고 경영진이 변화관리의 주체가 되어야 함\n쳬계적인 방법론을 가지고 접근해야 함\n시간을 필요로 함\n외부 변화를 고려해야 함\n객관적인 평가와 보상이 뒷받침 되어야 함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "프로세스경영",
      "프로세스 경영 구축 방법론"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/01.html#멱급수",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/01.html#멱급수",
    "title": "테일러 급수",
    "section": "멱급수",
    "text": "멱급수\n\n변수 x가 붙은 무한급수\n\\(f(x) = \\Sigma a_n (x-a)^n\\) 꼴\n\n급수의 수렴 공식을 이용해 해당 멱급수가 수렴하는 구간을 구할 수 있다.\n적어도 한 점 x=a에서 수렴한다.\n일반적으로 절대 비판정법을 많이 사용하고, 1, -1에서의 수렴 여부도 확인을 해줘야 한다.\n\n멱급수 \\(\\Sigma a_nx^n\\)이 \\(x_0\\)에서 수렴하면, \\(|x| &lt; |x_0|\\)인 모든 x에 대하여 절대수렴한다.\n멱급수 \\(\\Sigma a_nx^n\\)이 \\(x_0\\)에서 발산하면, \\(|x| &gt; |x_0|\\)인 모든 x에 대하여 발산한다.\n수렴반경이 R &gt; 0인 멱급수 \\(\\Sigma a_n (x-a)^n\\)에 대하여, \\(f(x) = \\Sigma a_n (x-a)^n, |x-a| &lt; R\\)로 정의된 f(x)는 구간 (a - R, a + R)에서 미분가능하고, 부정적분을 가지며 다음이 성립한다.\n\n\\(f'(x) = \\Sigma_{n=1}^{\\infty} n a_n (x-a)^{n-1}\\)\n\\(\\int f(x) dx = C + \\Sigma_{n=0}^{\\infty} \\frac{a_n (x-a)^{n+1}}{n+1}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "테일러 급수"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/01.html#테일러-급수",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/01.html#테일러-급수",
    "title": "테일러 급수",
    "section": "테일러 급수",
    "text": "테일러 급수\n\n\\(f(x) = \\Sigma_{n=0}^{\\infty} \\frac{f^{n}(a)}{n!}(x-a)^n, |x - a| &lt; R\\)\nx = 0 에서의 테일러 급수를 맥클로린 급수라고 한다.\np가 실수이고, \\(|x| &lt; 1\\)이면, \\((1 + x)^p = \\Sigma{n=0}^{\\infty} \\binom{p}{n} x^n\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "테일러 급수"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/01.html#멱급수의-연산",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/01.html#멱급수의-연산",
    "title": "테일러 급수",
    "section": "멱급수의 연산",
    "text": "멱급수의 연산",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "테일러 급수"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/기공수/01.html#테일러의-정리",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/기공수/01.html#테일러의-정리",
    "title": "테일러 급수",
    "section": "테일러의 정리",
    "text": "테일러의 정리",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "기공수",
      "테일러 급수"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질이란",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질이란",
    "title": "품질관리의 기본개념",
    "section": "품질이란",
    "text": "품질이란\n\n상대적으로 뛰어난 정도 / 구분되는 속성\n품질 관리자 입장에서 더 나은 품질을 만들기 위해 노력해야 함\n품질에 대한 다양한 관점이 존재한다.\n\n사용자 기반: 나한테 오는 재품이 얼마나 좋은가\n제조(시스템 관리자) 기반: 제품이 설계된 대로 잘 만들어졌는가\n제품 기반: 품질을 정확하고 측정 가능한 변수로 보는 것\n\n\n\n좋은 품질의 요건\n\n우수성(성능, 기능, 외관)\n일관성 (단품별, 사용환경, 내구성 등)\n둘을 만족할 때 고객 만족이 증가하고 조직의 이익이 증가함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질관리의-정의",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질관리의-정의",
    "title": "품질관리의 기본개념",
    "section": "품질관리의 정의",
    "text": "품질관리의 정의\n\n\n\n전통적인 관점에서의 품질관리\n\n\n\n\n\n현대적인 관점에서의 품질관리\n\n\n\n시장성이 높은 제품 및 서비스를 경제적으로 생산하기 위한 일련의 체계적 관리\n전통적 관점(QC):\n\n표준을 설정할 뿐 지키기 위한 노력 경시\n불량품을 솎어내는 검사에만 의존\n불량의 발생 원인제거 및 개선 노력 부족\n\n현대적 관점 / 종합적 품질관리(TQC):\n\n품질변동의 원인을 파악하여, 변동을 지속적으로 감소시키고 성과 수준을 높이는 과정\nPDSA cycle: plan, do, study, act",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질의-분류",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질의-분류",
    "title": "품질관리의 기본개념",
    "section": "품질의 분류",
    "text": "품질의 분류\n\n\n\n품질의 분류\n\n\n\n요구품질: 시장에서 요구되는 품질. 정성적(시장조사, 경쟁제품 분석)인 방법으로 파악\n설계품질: 파악한 고객요구를 설계 언어로 잘 변환했는지\n제조품질: 설계된 사양을 실제로 잘 구현했는지 정량적인 방법으로 파악\n사용품질: 고객이 실제로 느끼는 품질. 만족도",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질-관리의-유명-지도자",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질-관리의-유명-지도자",
    "title": "품질관리의 기본개념",
    "section": "품질 관리의 유명 지도자",
    "text": "품질 관리의 유명 지도자\n\nF. W. Taylor: 품질에 대한 과학적(정량적) 관리법 제시\nW. A. Shewhart: 통계적 품질관리(SQC)의 시조. 품질 변동의 원인을 우연원인과 이상원인으로 구분\nW. E. Deming: PDMA 사이클",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질관리의-역사",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질관리의-역사",
    "title": "품질관리의 기본개념",
    "section": "품질관리의 역사",
    "text": "품질관리의 역사\n\n작업자에 의한 품질관리: 한 명이 장인정신으로 다 함.\n작업반장에 의한 품질관리: 산업혁명 이후 작업자를 그룹으로 묶고 이들을 통제하는 작업반장이 품질 책임. 공식적인 품질검사 도입\n검사에 의한 품질관리: 컨베이어 시스템 도입으로 작업자 수 증가. 작업이 전문화, 규격화 되면서 검사만 전문적으로 담당하는 품질부서 등장.\n통계적 품질관리(SQC)\nTQC(Total Quality Control): 품질 관리. 품질 책임을 전사적으로 담당\nTQM(Total Quality Management): 품질 경영. 품질을 전체적으로 관리하는 전략\n\n\n품질 관리와 품질 경영의 차이\n\n적용 범위에 차이가 있다:\n\nPDSA를 기존에는 단순히 계획을 수립 및 설계하고 시스템을 운영하고 그 결과를 계획과 비교하여 통제하는데 집중함.\n생산자중심 시대에서는 시스템 운영상의 효율이 강조됨\n고객 중심 시대로 넘어옴에 따라 운영효율 뿐만 아니라 고객의 요구를 제대로 파악하고 이를 설계단계에 제대로 반영하는 계획 및 설계 단계의 중요성이 강조\n즉 품질 경영(선행형)은 과거 품질관리(대응형) 영역을 기본적으로 포함하며 계획 및 설계 단계로 확대한 개념\n\n\n\n\n\n전통적 품질 관리 vs 현대적 품질 관리",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질-비용",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질-비용",
    "title": "품질관리의 기본개념",
    "section": "품질 비용",
    "text": "품질 비용\n\n통제비용: 물품, 서비스의 품질과 관련해서 발생.\n\n예방비용: 계획, 교육, 훈련, 프로세스 개선 등 불량을 예방하는데 드는 비용\n평가비용: 불량을 검사하는데 드는 비용\n\n실패비용\n\n내적 실패비용: 생산 과정에서 발생하는 불량에 대한 비용\n외적 실패비용: 고객에게 전달된 후에 발생하는 불량에 대한 비용. 정량화가 매우 어려움\n\n\n\n\n\n품질 개선과 비용에 대한 견해\n\n\n\n전통적 견해: 최적 수준을 위해 일정량의 불량은 허용해야 한다고 봄\n무결점 견해: 통제비용이 올라가는 속도보다 실패비용이 감소하는 속도가 더 빠르기 때문에 품질 수준을 계속 높여야 한다고 봄",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질과-전략",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/01.html#품질과-전략",
    "title": "품질관리의 기본개념",
    "section": "품질과 전략",
    "text": "품질과 전략\n\n품질의 향상은 차별화와 비용 절감, 대응의 성공적인 전략을 구축하는데 도움이 됨\n좋은 품질 향상을 위해 전사적인 품질 경영(TQM)의 달성이 필요.\n\n\n\n\nTQM을 달성하기 위한 흐름\n\n\n\nTQM의 3요소\n\n고객 중심: 고객과 시장의 요구에 민첩하게 대응(TQM)하고, 고객의 목소리(VOC)를 설계 규격에 반영\n전사적 참여\n지속적 개선",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질관리의 기본개념"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/02.html#품질특성",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/02.html#품질특성",
    "title": "품질변동과 공정능력",
    "section": "품질특성",
    "text": "품질특성\n\n시장에서 고객의 요구사항을 충족시키는 정도를 평가하는 요소\n품질 특성치: 품질의 특성을 정량적 수치로 표현한 것\n\n물리적 특성: 길이, 무게, 온도, 압력 등\n감각적 특성: 색상, 냄새, 맛, 촉감 등\n추상적 특성: 신뢰성, 내구성, 유지보수성 등\n\n좋은 품질: 고객이 만족하는 우수한 품질을 균일하게 달성\n품질은 변동이 존재.\n\n품질특성의 분포에서 설계품질 허용수준 이하의 값이 실현되면 고객은 이를 불량이라 인식함.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질변동과 공정능력"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/02.html#설계규격과-불량",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/02.html#설계규격과-불량",
    "title": "품질변동과 공정능력",
    "section": "설계규격과 불량",
    "text": "설계규격과 불량\n\n\n설계규격: 기준치 + 허용차\n\n공차: 허용차의 범위. 규격한계\n\n치수공차: 길이, 너비, 두께, 등\n\nuni directional tolerance: 한쪽으로만 허용차가 있는 경우 (한 쪽의 제약이 +.00 or -.00)\nbi directional tolerance: 양쪽으로 허용차가 있는 경우\n\n기하공차: 형상의 정확성에 대한 허용 오차. 평면도, 직진도, 원형도 등",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질변동과 공정능력"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/02.html#규격한계-공정능력-한계",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/02.html#규격한계-공정능력-한계",
    "title": "품질변동과 공정능력",
    "section": "규격한계, 공정능력 한계",
    "text": "규격한계, 공정능력 한계\n\n\n규격한계: 설계규격에서 엄격하게 정한 범위 (규격 상한 USL, 규격 하한 LSL)\n공정능력 한계: 실제 공정에서 만들어지는 제품의 특성치가 분포하는 6σ 범위 (공정능력 상한 UPCL, 공정능력 하한 LPCL)\n\n공정이 얼마나 정밀하게 품질을 유지하는지 나타내는 기준\n\n공정능력: 설계 사양을 충족하는 능력\n\n공정의 자연적 변동과 설계 사양간의 관계를 측정한 것\n공정능력 한계가 규격한계 내에 들어와야 한다\n\n이론 공정능력지수(Cp): 불량을 적게 생산하는 능력\n\n하한 규격 = (μ - LSL) / 3σ\n상한 규격 = (USL - μ) / 3σ\n양쪽 규격 = (USL - LSL) / 6σ\n공정의 산포만 고려함. 프로세스가 얼마나 규격 범위에 잘 집중되어 있는지 확인 못함\n\n\n\n\n\n이론 공정능력지수\n\n\n\n실제 공정능력지수(Cpk): 평균이 중심에 있는지 고려\n\nCpk = min(\\(\\frac{USL - μ}{3σ} , \\frac{μ - LSL}{3σ}\\))\n평균은 시간이 지남에 따라 변동될 수 있음. 평균이 늘 공차의 중심에 있지 않을 수 있다.\n\n\n\n\n\n실제 공정능력지수\n\n\n\n\n\n시그마 품질 수준\n\n\n\n\\(Cp_k\\)는 장기적 품질 이동 가능성을 고려하여 1.5σ인 0.5를 뺀 값으로 계산\n6σ 품질을 추구하려면 Cp = 2.0, Cpk = 1.5(1.5σ의 평균 이동 가정)가 되어야 함\n\n100만개당 3.4개 미만의 불량품을 목적으로 한다면 CPk=2.0이 되어야 함\n\n이론 공정능력지수가 1이 되면 기대불량률은 0.27%\n\n하지만 실제로는 평균이 이동할 수 있으므로 기대불량률이 더 높아짐",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질변동과 공정능력"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/02.html#시그마-프로그램",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/02.html#시그마-프로그램",
    "title": "품질변동과 공정능력",
    "section": "6 시그마 프로그램",
    "text": "6 시그마 프로그램\n\nDMAIC 프로세스 개선 모델:\n\nDefine: 핵심품질특성(CTQ) 규명\nMeasure: CTQ에 영향을 주는 독립변수와 값에 대한 데이터 수집\nAnalyze: 통계적 기법을 활용하여 분석\nImprove: 원인이 규명되면 개선할 아이디어를 찾아 실행\nControl: 개선사항을 표준화하여 절차 수립 및 작업자 교육\n\n적합품질 결정요소\n\n5M:\n\nmethod: 간단한 절차가 좋다.\nman: 품질관리에 숙련된 인력이 필요하다.\nmachine: 설비의 주기적인 점검과 고정이 필요\nmaterial: 원재료의 품질이 중요\nmeasurement: 올바른 측정이 중요\n\n\n\n\n\n\n공정 관리 준비 순서",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "품질변동과 공정능력"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#프로세스-품질관리란",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#프로세스-품질관리란",
    "title": "관리도",
    "section": "프로세스 품질관리란?",
    "text": "프로세스 품질관리란?\n\n시간에 따른 추이를 통해 사전에 불량을 방지하는 것\n목표: 제조변동의 감소",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "관리도"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#제조-변동의-원인",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#제조-변동의-원인",
    "title": "관리도",
    "section": "제조 변동의 원인",
    "text": "제조 변동의 원인\n\n우연 변동\n\n일상적 변동\n작업환경 변화(지진같은 재해나 미세한 바람의 차이 등)\n작업자간 숙련도 차이\n설비 간의 기능 차이\n원자재간의 미세한 차이\n피할 수 없는 변동.\n\n이상 변동(assignable variable): 품질에 확연한 차이를 가져오며, 제거 가능한 원인\n\n피할 수 있는 변동\n\n관리상태: 분포가 지정된 한계 내로 유지되는 것.\n\n관리상태에서의 우연 변동은 허용 된다.\n우연적 원인만 있는 경우, 변동은 안정적이고 예측 가능한 분포를 형성\n이상변동이 있는 경우 중심 경향, 표준편차, 모양에 따라 달라지는 예상 불가능한 분포를 생성\n\n\n\n\n\n우연 변동\n\n\n\n\n\n이상변동 포함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "관리도"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#관리도",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#관리도",
    "title": "관리도",
    "section": "관리도",
    "text": "관리도\n\n\n\n관리도\n\n\n\n이상원인에 의한 품질변동의 발생 여부를 조사하기 위해 개발된 도구\n관리한계를 벗어날 확률: 0.0027, 하나라도 밖으로 나가면 공정 이상\n규격 상하한 != 관리 상하한\n불량을 탐지하는 것이 아니라, 공정이 안정상태에 있는지 없는지를 선행적으로 판단하여 불량 발생을 예방하는 것\nP(불량 | only 우연 변동) &lt; P(불량 | 우연 변동 + 이상 변동)\n\n관리한계를 벗어났을 때 다시 중심으로 되돌려 줘야함. 즉 예방 필요\n\n\n\n합리적 표본군\n\n\n\n표본군\n\n\n\nn: 표본의 갯수, k: 단위 시간 당 표본 추출 횟수\n같은 표본군 안에는 우연변동만 존재(균일한 표본군)이 합리적임.\n표본의 크기가 커질수록, 추출 빈도가 높을 수록 관리 규격이 좁아지며 작은 변동에 더 민감해짐\n\n비용과 trade-off\n또한 표본의 크기가 커질수록 균일하지 않은 표본군이 생성될 확률이 높아짐(그림 참조)\n적당히 작은 표본을 여러번 뽑는게 좋다.\n\n\n\n\n패턴 이상\n\n공정이 관리상태에 있다:\n\n관리도의 점들이 관리한계선을 벗어나지 않는다.\n관리도의 점들에 특정한 패턴이 나타나지 않는다.\n\n관리 한계를 벗어날 징조\n\n한계선 가까이 여러 개가 나타난다.\n경향성이 나타난다.\n\n변동의 크기를 줄일 수 있는 단서\n\n런의 길이가 너무 길거나 짧다.\n\n긴 길이: 평균 이동 가능성\n짧은 길이: 표본군 간 변동 요인 존재(층화 현상)\n\n주기성이 나타난다: 주기적 변동 요인(환경 변화, 작업자 능률 변화 가능성)\n층화현상이 나타난다. -&gt; 관리한계선이 잘못 설정되었다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "관리도"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#관리도의-종류",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#관리도의-종류",
    "title": "관리도",
    "section": "관리도의 종류",
    "text": "관리도의 종류\n\n\n\n관리도 종류\n\n\n\n\\(R_m\\)은 시계열 moving average\n부적합품 수: 불량품이 얼마나 있는지\n부적합 수: 하나의 제품(크기가 일정한)에서 몇개의 결함이 있는지\n\n\n변량(계량형) 관리도(연속형)\n\n\\(\\bar{X} - R\\) 관리도\n\n\n\n\nσ를 알 때\n\n\n\n\n\nσ를 모를 때\n\n\n\n정규성 가정 시 관리도용 계수표 사용 가능\nOC 곡선: 추정치 이동에 따른 β 값의 변화. n과 변화를 알면 B값을 바로 구할 수 있다.\n\n\\(θ ≠ θ_0\\)일 때, \\(θ_0\\)와 가까울 수록 헷갈리고, 멀 수록 명확하다.\n\n\n\n\n\n\\(\\bar{X}\\) 관리도 OC 곡선. n이 클수록 민감하다\n\n\n\nB 계산 시 분산은 \\(H_0\\)를 기준으로 계산.\n\n\n\n\nR 관리도의 OC커브\n\n\n\n\\(x - R_m\\) 관리도\n\n\n일반적으로 평균의 변화를 탐지하는데는 \\(\\bar{x}\\) 관리도가 효율적\n하지만 x 관리도는 매 관측값을 감시하므로 변화 발생시 빠른 조치 가능\n\n둘의 장점을 섞은 \\(\\bar{x} - x - R\\) 관리도 사용 가능\n\n\n\n\n속성(계수형) 관리도(이산형)\n\n연속적인 양으로 측정 가능한(계량형) 품질 특성을 계수형으로 관리하는 것은 부적합 -&gt; 정보의 손실\n\n\nnp 관리도\n\n불량개수 관리도\n\n\n\n\n\nnp 관리도\n\n\n\nOC 곡선\n분산은 \\(P_0\\) 기준. LCL은 -무한으로 설정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "관리도"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#군내변동-군간변동",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/품질경영/03.html#군내변동-군간변동",
    "title": "관리도",
    "section": "군내변동 군간변동",
    "text": "군내변동 군간변동\n\n공정이 안정상태에 있다면 군간변동 = 0, 제조변동 = 군내 변동 이여야 한다.\n제조 변동과 군내 변동의 차이가 크면 공정이 불안정상태에 있다고 추정할 수 있음. 하지만 추정값이기 때문에 어느정도 차이는 존재할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "품질경영",
      "관리도"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송",
    "title": "운송 및 창고관리",
    "section": "운송",
    "text": "운송\n\n물건의 장소 이동을 통해 얻어지는 만족도가 크면 운송\n지역이나 거리를 극복함으로써 유통의 간소화 및 가격하락 기여\n생산지와 소비지를 확대시킴으로써 시장 활성화 기여\n운송 관리의 기본 방향\n\n규모의 경제, 거리의 경제: 한번에 많이, 멀리 옮길수록 운송단가는 낮아짐\n라드너의 규칙\n\n수송거리의 2배 확장으로 인한 시장의 확장은 4배\n수송비용의 절감은 공급사슬의 효율을 제곱 수준으로 확장\n\n\n\n\n\n\n운송 체계\n\n\n\n간선 수송은 규모의 경제 적용 및 수송모드 선정 중요.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "운송 및 창고관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송-활동-관련-주체",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송-활동-관련-주체",
    "title": "운송 및 창고관리",
    "section": "운송 활동 관련 주체",
    "text": "운송 활동 관련 주체\n\n\n수송의 효율화를 위해 운송 주선인과 수송전문회사가 담당\n하나 또는 복수의 운송주선인이 통합 분배 과정을 거침\n대형주문에서 개별 포장 또는 택배형식으로 물량 변환 수송",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "운송 및 창고관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송-서비스의-성능평가-척도",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송-서비스의-성능평가-척도",
    "title": "운송 및 창고관리",
    "section": "운송 서비스의 성능평가 척도",
    "text": "운송 서비스의 성능평가 척도\n\n운송비용\n\n고정비: 터미널 설비비, 보험료 등 운송활동 강도에 비례하지 않는 것\n변동비: 유류비, 운반 처리비 등 운송활동 강도에 비례\n\n운송시간\n\n시간도 중요하지만 변동성 관리도 중요\n운송시간의 변동성: 높아질수록 안전재고 수준 향상, 대규모 창고 필요, 재고비용 증가\n\n운송 서비스 수준\n\n긴급도에 대한 대응도, 연계 수송의 편리함과 융통성\n납기에 대한 신뢰성, 지연의 가능성, 파손 및 분실의 위험성\n예) 고부가 가치 제품이나 응급 제품의 경우 항공수송은 서비스 수준 측면에서 유리",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "운송 및 창고관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#기업의-의사결정에-미치는-운송의-영향",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#기업의-의사결정에-미치는-운송의-영향",
    "title": "운송 및 창고관리",
    "section": "기업의 의사결정에 미치는 운송의 영향",
    "text": "기업의 의사결정에 미치는 운송의 영향\n\n가격 결정: 운송비용이 제품 원가의 상당 부분 차지\n입지 결정: 철강 / 정유 산업은 원자재가 해상 운송되므로 대부분의 공장이 해안에 위치\n시장 지역 결정: 부패가 쉬운 제품은 산간 및 오지에 배송 안함\n구매 지역 결정: 원자재 부품 구매 시 운송의 이용 가능성, 적합성, 비용 등을 고려",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "운송 및 창고관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송수단-및-특성",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송수단-및-특성",
    "title": "운송 및 창고관리",
    "section": "운송수단 및 특성",
    "text": "운송수단 및 특성\n\n항공운송\n\n기본시설의 높은 고정비 + 상대적으로 낮은 변동비(물량 대비)\n신속한 운송이 가능한 고가의 운송수단\n\n긴급 화물(공장 부품, 혈액 등)\n소규모 고부가가치 제품(반도체, 시계 등)\n\n고려 사항\n\n항공기 할당 및 유지보수 계획\n운임 관리 및 가격산정\n\n\n소화물 운송\n\n소형화물 및 소포 취급\n높은 운송비(물량 대비)\n신속하며 신뢰성 높은 운송서비스 제공\n빠른 재고흐름 및 주문상태 추적 등의 서비스\n고려사항\n\n운송수요의 집화지점의 위치 및 처리능력 결정\n소화물 추적, 조회를 위한 정보시스템 구축\n배달차량의 운송 스케쥴 및 순회 경로\n\n\n화물트럭 운송\n\n내륙 운송에서 많이 사용\n철도와 비교하여 운송비가 비싸지만, 출발지부터 도착지까지 배달\n상대적으로 짧은 운송시간\n만차수송(FTL)\n부분적재 수송(LTL)\n\n철도화물 운송\n\n저가의 운임: 저가치, 대 중량 화물의 장거리, 대규모 운송에 적합\n낮은 접근성\n서비스 빈도의 한계 및 긴급 운송 탄력성 부족\n고려사항\n\n차량과 직원 스케쥴\n터미널에서의 지연\n화물 추적: 중간에 잃어버림\n\n\n해상화물 운송\n\n벌크 화물의 원거리 운송\n수량 및 이동거리 고려 시 가장 저렴한 운송수단\n컨테이너 활용\n속도가 느리고, 터미널에서 지연이 많이 발생\n항만시설, 하역기기가 필요하며, 하역비가 비쌈\n\n파이프라인\n\n원유, 정제된 석유, 천연가스 등의 운송\n초기 방대한 자본 투자가 필요 (높은 고정비, 낮은 변동비)\n운송수단 중 최저가이자, 높은 신뢰성을 가진 운송수단\n\n기상조건, 도로 사정 등에 무관\n\n\n복합 운송\n\n두 개 이상의 상이한 운송수단을 공동화\n내륙운송의 경우, 철도 + 트럭의 복합운송을 사용\n상이한 운송수단 간의 환적이 용이하게 이루어질 수 있도록 하는 정보 교환이 필수적",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "운송 및 창고관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송-네트워크",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#운송-네트워크",
    "title": "운송 및 창고관리",
    "section": "운송 네트워크",
    "text": "운송 네트워크\n\n인바운드 네트워크\n\n하나 또는 소수의 노드를 향하여 다수의 공급점이 서비스를 제공하는 형태(수렴형)\n\n\n\n직접 배송: 지리적으로 가깝거나 공급 물량이 커서 직접 공급\n통합허브 배송: 배송 비용이나 수요처의 방문회수의 제한으로 중간 허브 통합\n밀크런 배송: 하나의 차량이 공급처를 순차적으로 방문 배송\n크로스도킹 배송: 중간 허브에서 공급 물량과 시간에 적절히 배송될 수 있도록 재분류하여 배송\n\n\n\n\n크로스 도킹\n\n\n\n\n아웃바운드 네트워크\n\n하나 또는 소수의 공급처에서 많은 수의 고객에게 제품을 공급하는 형태(확산형)\n\n\n\n공급바 분배센터 배송: 제조에서 담당하여 제품의 품질 유지가 핵심\n수요자 분배센터 배송: 다양한 제품의 효율적인 분배가 핵심\n유통 배송: 제 3의 유통전문회사가 담당하여 판매촉진 또는 분배 효율화\n제3자 물류배송: 분배과정을 전담하는 제3자 물류서비스 이용\n\n\n\nHub & Spoke 시스템\n\n\n허브끼리는 대량 운송, 최종 고객에게는 소량 운송\n점끼리 연결되지 않아 복잡성이 줄어듦",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "운송 및 창고관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#창고",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/09.html#창고",
    "title": "운송 및 창고관리",
    "section": "창고",
    "text": "창고\n\n주요 기능\n\n보관\n저장\n생산 과정의 일부 (와인)\n집화: 다른 곳에서 주문한 것들 묶어서 배송\n소분: 제조사에서 벌크 형식으로 받은 화물을 적은 화물들로 나누어 배송\n지연: 완제품 직전 단계로 가지고 있다가 주문이 들어오면 최종 조립 및 포장\n크로스도킹\n\n창고 내 자재 흐름 단계\n\n입하\n검수\n보관장소로의 이동 준비(라벨링 등)\n보관\n오더 피킹\n출하장으로 이동\n출하\n\n\n\n\n\n오더 피킹에는 다양한 방식이 있다\n\n\n\npicker의 이동 경로, 최적 order 묶음, storage assignment, picker-order assignment 고려 필요\nconveyor로 물건을 받아서(혹은 AMR로) put walls에 넣는 방식도 있음(pick and sort)\n\n\n\n\nput walls\n\n\n\n여러 zone에서의 picking이 synchronized 되어야 함\n창고 보관 방식\n\n무작위 보관 방식: 제품의 저장장소에 대한 제약 x, 공간 활용을 최대화 할 수 있으나 picker의 이동 시간이 증가\n전용 보관 방식: 제품이 창고 내 지정장소에 저장되어 picker가 제품 위치를 숙지해 효율적인 피킹 가능\n그룹기반 보관 방식: 제품들을 몇 개의 그룹을 나누고, 각 그룹 별 지정된 구역에 보관\n수요 기반\n제품 상관관계 기반\n\n\n\nSmart Warehouse\n\nRMF system 도입 -&gt; 설계 및 운영의 측면에서 다양한 의사결정 문제 등장\n\n보관 위치 결정\n작업 - 로봇 할당\n작업의 순서를 결정하는 문제\n로봇의 최단 경로 문제(충돌 회피, 실시간 경로 수정 등을 고려)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "운송 및 창고관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/01.html#의사결정-level",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/01.html#의사결정-level",
    "title": "SCM 의사결정",
    "section": "의사결정 level",
    "text": "의사결정 level\n\n\nsupply chain (strategic) design\n\n한 번 결정하면 오래 가고, 영향을 많이 미치는 장기적 의사결정\n기업의 우선경쟁역량1이나 경영전략을 종합적으로 고려\n설비의 수, 위치 선정, 제품설계 등\n수년 ~ 수십년 단위\n최고경영자 및 임원 수준에서 결정\n\nsupply chain (Tactical) planning\n\ninput: 공급사슬 설계, 예측 수요\n생산, 재고, 운송 등의 통합적 계획 수립\n수요-공급 계획, 생산 하청 계획, 판촉 규모와 시기 결정 등\n수주, 수개월 ~ 수년 단위 계획\n\nsupply chain operation\n\nSCO\nSCE(execution)\n시간, 수일 ~ 수주 단위 계획\n\n\n\n아래로 갈수록 범위가 좁아지고, 디테일해짐",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "SCM 의사결정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/01.html#공급사슬의-프로세스-구조",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/01.html#공급사슬의-프로세스-구조",
    "title": "SCM 의사결정",
    "section": "공급사슬의 프로세스 구조",
    "text": "공급사슬의 프로세스 구조\n\n주기(cycle) 프로세스\n\n\n\n주기\n\n\n\n\n\n주기 별 일반적인 프로세스\n\n\n\n인접한 단계간 활동이 주기적으로 반복된다고 보는 관점\n서로 다른 주기가 잘 synchornized 되어야 함\n\n\n\nPush-Pull 프로세스\n\npush: 예측 수요에 의해 공급사슬 활동이 촉발\npull: 실제 수요에 의해 공급사슬 활동이 촉발\n고객 주문을 접수하는 접점이 push-pull 경계\n접점을 어느 위치에 두느냐에 따라 cost-quality trade-off가 달라짐",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "SCM 의사결정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/01.html#difficulties-in-scm",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/01.html#difficulties-in-scm",
    "title": "SCM 의사결정",
    "section": "Difficulties in SCM",
    "text": "Difficulties in SCM\n\n제품개발 프로세스와의 통합\n\n공급사슬 전략은 SCM뿐만 아니라 제품개발 프로세스에 의해서도 영향을 받음\n각 단계에서 서로 다른 사람들이 책임을 져서, misalignment이 발생할 수 있음\n\n\n\n전역최적화\n\ncentralized decision making이 어려움\n\n공급사슬의 조정이 필요\n\nbuy back contract: 재고를 공급자가 다시 사들이는 계약\nquantity discount: 대량 구매시 할인\n\n\n\n\n\n불확실성\n\n위험과 불확실성에 노출되어 있음\n채찍효과: 공급사슬의 상류로 갈수록 수요 변동 폭이 확대되는 현상",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "SCM 의사결정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/01.html#footnotes",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/01.html#footnotes",
    "title": "SCM 의사결정",
    "section": "각주",
    "text": "각주\n\n\ncost, quality(성능, 일관성), time(time to market, lead-time, on time delivery), flexibility↩︎",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "SCM 의사결정"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#공급사슬에서-재고의-의미",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#공급사슬에서-재고의-의미",
    "title": "재고 관리",
    "section": "공급사슬에서 재고의 의미",
    "text": "공급사슬에서 재고의 의미\n\n미래 사용에 대비하여 보관중인 재화\n\n다양한 형태로 다양한 위치에 존재\n\n종류\n\n원자재 및 구매 부품\n재공품(WIP)\n완제품(FGI)\n운송중 재고(GIT)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#재고-유지비용",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#재고-유지비용",
    "title": "재고 관리",
    "section": "재고 유지비용",
    "text": "재고 유지비용\n\n자본비용: 기회비용\n보관 및 취급 비용: 창고 임대료\n세금\n보험료\n가치상실\n물리적 파손, 도난, 분실",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#재고의-필요성",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#재고의-필요성",
    "title": "재고 관리",
    "section": "재고의 필요성",
    "text": "재고의 필요성\n\n재고비용은 늘어나지만, 공급사실의 반응성에 영향을 준다.\nex:\n\n주기재고: 고정비용 / 주문비용 절감\n안전재고: 수요 / 공급 불확실성 대비\n가격 변동 대비 재고 확보",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#재고-관리",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#재고-관리",
    "title": "재고 관리",
    "section": "재고 관리",
    "text": "재고 관리\n\n재고 비용을 낮추면서 고객의 서비스 수준을 유지하는 것이 목표\n\n\n고객 서비스 수준 지표\n\n재고 충족률(In-stock rate)\n\n고객 주문에 대해 재고로부터 즉시 출고할 수 있는 비율\n\n백오더(Backorders)\n\n재고 부족으로 인해 나중에 처리하는 주문 경우의 수량/횟수\n\n재고 회전율(Inventory Turnover)\n\n\n\n재고 비용\n\n주문 비용\n보관 비용\n부족 비용",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#재고관리-모형",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/04.html#재고관리-모형",
    "title": "재고 관리",
    "section": "재고관리 모형",
    "text": "재고관리 모형\n\n\n\n재고 관리 모형\n\n\n\n경제적 주문량(EOQ: Economic Order Quantity)\n\n한 번에 많이 주문하기 vs 적게 여러 번 주문하기\n결정변수: 주문량(Q)\n단위 시간 당 수요: D\n재고 보유 기간: T = Q/D\n단위 제품 당 구매 비용: c\n\n주문량 Q는 단위 재품 당 구매비용에 영향을 안 줌\n\n단위 제품, 시간 당 재고 유지 비용: h\n고정 주문 비용: k\n목적 함수: \\(C(Q) = \\frac{KD}{Q} + \\frac{hQ}{2}\\)\n\n단위 시간당 셋업 비용(고정 주문 비용) + 단위 시간 당 평균 재고 유지 비용(재고 비용)\n\\(Q✶ = \\sqrt{\\frac{2KD}{h}}\\)\n\\(C(Q✶) = \\sqrt{2KDh}\\)\n\\(\\frac{C(Q)}{C(Q✶)} = \\frac{1}{2}(\\frac{Q}{Q✶} + \\frac{Q✶}{Q})\\)\n\n\n - 아래로 갈 수록 가파르게 변화한다. - \\(C(\\frac{Q✶}{k}) = C(kQ✶)\\) - power of two policy: \\(\\frac{T✶}{\\sqrt{2}} \\leq T_{base}2^k \\leq \\sqrt{2}T✶\\) - 위를 만족하는 제일 작은 k를 찾음 - 하위 부품드르이 주문 주기도 고려하여 위와 같이 주문 주기를 설정 - 그럼 최대 1.06%의 차이밖에 안 남.\n\n\n재주문점 모형\n\nEOQ에서는 zero lead time을 가정\n또한 수요는 불확실성이 있음.\n제품 인도기간 중 재고량 부족으로 수요가 만족되지 않는 경우 재고 고갈비용 발생\n결정변수\n\n재주문점(R): 몇 개 남았을 때 주문할지? (안전재고 수준)\n주문 량(Q): 몇 개 주문할 지? (주기재고 수준)\n\nROP: 주문 인도기간 동안의 수요 예측값 + 안전재고\n\nROP가 높을 수록 재고 고갈 확률은 감소하지만, 재고 보유 비용이 증가\n\nservice level: 주문인도기간 동안 고객의 수요가 만족될 가능성\n주문 인도기간 L이 불확실할 경우\n\n\\(D_L ≈ N(μ_Lμ_D, μ_Lσ_D^2 + μ_D^2σ_L^2)\\))",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/05.html#신문판매원-모형",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/05.html#신문판매원-모형",
    "title": "재고 관리 2",
    "section": "신문판매원 모형",
    "text": "신문판매원 모형\n\n구매비용\n판매가격\n잔존가치\n\n0이거나 음수일 수 있음\n\n\\(C_o\\): 과다재고비용 = 구매비용 - 잔존가치\n\n필요한 양보다 더 주문했을 때 발생하는 비용\n\n\\(C_u\\): 과소재고비용 = 판매가격 - 구매비용\n\n필요한 양보다 덜 주문했을 때 발생하는 기회비용\n\n최적의 주문량 F(Q) = \\(\\frac{C_u}{C_o + C_u}\\)\n\ncritical ratio가 두 값 사이일 경우 큰 값 선택\n\n기대 비용 함수: \\(TC(Q) = C_o E[max(Q - D, 0)] + C_u E[max(D - Q, 0)]\\)\n\\(TC(Q✶) = (C_o + C_u) ϕ(Z_α) σ\\) 정규분포일 경우",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리 2"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/05.html#통합-전략",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/05.html#통합-전략",
    "title": "재고 관리 2",
    "section": "통합 전략",
    "text": "통합 전략\n\n위치 또는 제품의 통합(pooling)으로 수요의 불확실성을 줄일 수 있다.\n\n수요변동성이 클 수록 이점 증가\n\npooling시 보통 재고 비용, 간접 비용, inbound는 감소하지만, 운송 비용(outbound, leadtime)은 증가\n\nnewsvendor 모형에서 주문량 절감 효과\n만약 수요 간 상관관계가 있다면\n\n상관관계가 1이면 통합 안한거랑 같고, 작아질 수록 이점 증가",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "재고 관리 2"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/03.html#sop-프로세스",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/03.html#sop-프로세스",
    "title": "S&OP: Sales and Operations Planning",
    "section": "S&OP 프로세스",
    "text": "S&OP 프로세스\n\n전략적 접근\n수익성을 극대화하기 위해 수요와 공급을 균형있게 맞추는 것이 목표\nCross-Functional 팀이 실행\n개별 제품이 아닌 제품군으로 접근\n\n\n\n\n프로세스\n\n\n\nExample\n\n일정하지 않은 수요의 상황에서 높은 수요를 충족하기 위한 방법\n\n\n최대 수요기간의 수요를 충족하기 위한 제조 능력 유지:\n\n낮은 재고비용, 높은 생산능력 비용\n\n비수기 동안 재고 축적:\n\n낮은 생산능력, 높은 재고 비용\n\n소매 파트와 협력을 바탕으로 비수기 가격 프로모션:\n\n수요 분산 효과\n\n\n\nS&OP 프로세스를 통해 어떤 방법이 회사의 이윤을 최대화 하는지 결정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "S&OP: Sales and Operations Planning"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/03.html#공급-관리",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/03.html#공급-관리",
    "title": "S&OP: Sales and Operations Planning",
    "section": "공급 관리",
    "text": "공급 관리\n\n생산 능력\n\n유연한 근무 시간\n계절적 인력 활용\nDual facilities - specialized, flexible\n\nspecialized: 일정한 수량을 효율적으로 생산\nflexible: 다양한 제품과 변동하는 양을 생산할 수 있는 공장\n\n하청\n생산 과정에 제품 유연성 설계\n\n\n\n재고\n\n다양한 제품에 공통 부품 사용\n\nex) 여름에 수요가 많은 잔디깎기와 겨울에 수요가 많은 제설기의 공통 부품은 수요가 안정적\n하지만 대부분의 제품이 동일한 피크 시즌을 가지면 불가능\n\n수요가 높은 제품 또는 예측 가능한 제품의 재고 구축\n\n수요가 불확실한 제품은 판매 시즌에 가까워질수록 예측할 수 있으므로 이때 생산",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "S&OP: Sales and Operations Planning"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/03.html#수요-관리",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/03.html#수요-관리",
    "title": "S&OP: Sales and Operations Planning",
    "section": "수요 관리",
    "text": "수요 관리\n\n다양한 가격 정책 및 프로모션 활용\n\n\n프로모션이 수요에 미치는 요인\n\n시장 성장\n점유율 증가\n선구매",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "S&OP: Sales and Operations Planning"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/03.html#총괄생산계획",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/공급사슬관리/03.html#총괄생산계획",
    "title": "S&OP: Sales and Operations Planning",
    "section": "총괄생산계획",
    "text": "총괄생산계획\n\n\n\n총괄생산계획\n\n\n\n목적: 비용 최소화, 이윤 최대화 등\n결정사항: 고용수준, 시간당 생산량, 재고 수준, 외주 생산량 등\n\n\n\n\n생산 단위\n\n\n\nConstant Workforce Plan\n\n일정한 인력 유지\n현실적으로 불가능한 경우가 많음\n\n노동력을 일정하게 유지한다고 해도, 월별 작업일 수가 다를 수 있음\n\n전략:\n\n누적순수요 = 누적생산량이 되는 일정한 월생산량을 유지 (\\(\\frac{총 누적 순수요}{총 기간} = 월별 생산량\\))\n\n\n\n수요를 못 맞출 경우가 있음\n\n\n누적 수요가 최고점을 도달하는 수요량 까지를 만족하는 생산량 (\\(\\frac{최고점까지의 누적 수요량}{최고점까지의 기간} = 월별 생산량\\))\n\n\n\n재고 비용이 늘어날 수 있음\n\n\n\n\n\n\nK Factor (인당 일 평균생산량)\n\n작업 일수에 따른 정확한 월생산량 계산하여 필요 작업자수 산출\n\n\n\n\n예시\n\n\n\nOR 기법으로 최적화 가능",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "공급사슬관리",
      "S&OP: Sales and Operations Planning"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/14_machine_learning.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/14_machine_learning.html",
    "title": "Amazon Rekognition",
    "section": "",
    "text": "Amazon Rekognition\n\nfind objects, people, text, scenes in image and video analysis ## content moderation\ndetect explicit and suggestive content\na2i for human review\n\n\n\nAmazon Transcribe\n: speech-to-text service\n\n\nAmazon Polly\n: text-to-speech service\n\n\nAmazon Translate\n: language translation service\n\n\nAmazon Comprehend\n: natural language processing service\n\n\nAmazon Lex\n: chatbot service\n\n\nAmazon SageMaker\n: machine learning service\n\n\nAmazon Forecast\n: time series forecasting service\n\n\nkendra\n: document search service\n\n\nAmazon Personalize\n: personalized recommendation service\n\n\nTextract\n: OCR service\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Amazon Rekognition"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/02_ec2.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/02_ec2.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EC2 stands for Elastic Compute Cloud. It is a web service provided by Amazon Web Services (AWS) that allows users to rent virtual servers in the cloud.\nEC2 instances can be easily scaled up or down based on the user’s needs, providing flexibility and cost efficiency.\nThese instances can be used to run applications, host websites, process large amounts of data, and perform various other computing tasks.\nEC2 offers a wide range of instance types to cater to different workloads, and users have full control over the configuration and management of their instances.\n\n\n\nlaunch virtual servers\nmanage storage (EBS, EFS, S3)\nscale up or down based on demand (ASG)\ndistribute traffic across multiple instances (ELB)\n\n\n\n\n\nos\nCPU\nmemory\nstorage\nnetwork\nsecurity (IAM, security groups, key pairs)\nbootstrap scripts (user data)\n\n\n\n\n\ngeneral purpose (t2, m5)\ncompute optimized (c5)\nmemory optimized (r5)\nstorage optimized (i3)\naccelerated computing (p3, g4)\n\n\n\n\n\nact as a virtual firewall for your EC2 instances\ncontrol inbound and outbound traffic\ncan be associated with multiple instances\nlocked down to a region/VPC combination\ndoes live outside the EC2 - if traffic is blocked, the EC2 instance won’t see it (time out)\ncan reference other security groups\n\n\n\n\n\non-demand: pay for what you use\nreserved: capacity reservation for 1 or 3 years\n\n\nreserved\nconvertible reserved instances\ngood for steady-state usage application(db)\nreserve a specific instance attributes (instance type, region, tenancy, os)\nyou can buy and sell in marketplace\n\n\nspot: bid for unused capacity\nsavings plan: commit to a consistent amount of usage for a discount\nif beyond pay, converted to on-demand\nlocked to a specific instance family, aws region\nflexible: instance size, os, tenancy\ndedicated hosts: physical server dedicated for your use\ndedicated instances: instance running on a dedicated host\ncapacity reservation: reserve capacity for specific instance type in a specific AZ\n\n\n\n\n\ncluster: low latency, high throughput\npartition: multiple EC2 instances within a single AZ\nspread: EC2 instances on distinct hardware, maximum 7",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "what is EC2"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/02_ec2.html#what-is-ec2",
    "href": "posts/03_archives/completed_project/aws_saa/notes/02_ec2.html#what-is-ec2",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EC2 stands for Elastic Compute Cloud. It is a web service provided by Amazon Web Services (AWS) that allows users to rent virtual servers in the cloud.\nEC2 instances can be easily scaled up or down based on the user’s needs, providing flexibility and cost efficiency.\nThese instances can be used to run applications, host websites, process large amounts of data, and perform various other computing tasks.\nEC2 offers a wide range of instance types to cater to different workloads, and users have full control over the configuration and management of their instances.\n\n\n\nlaunch virtual servers\nmanage storage (EBS, EFS, S3)\nscale up or down based on demand (ASG)\ndistribute traffic across multiple instances (ELB)\n\n\n\n\n\nos\nCPU\nmemory\nstorage\nnetwork\nsecurity (IAM, security groups, key pairs)\nbootstrap scripts (user data)\n\n\n\n\n\ngeneral purpose (t2, m5)\ncompute optimized (c5)\nmemory optimized (r5)\nstorage optimized (i3)\naccelerated computing (p3, g4)\n\n\n\n\n\nact as a virtual firewall for your EC2 instances\ncontrol inbound and outbound traffic\ncan be associated with multiple instances\nlocked down to a region/VPC combination\ndoes live outside the EC2 - if traffic is blocked, the EC2 instance won’t see it (time out)\ncan reference other security groups\n\n\n\n\n\non-demand: pay for what you use\nreserved: capacity reservation for 1 or 3 years\n\n\nreserved\nconvertible reserved instances\ngood for steady-state usage application(db)\nreserve a specific instance attributes (instance type, region, tenancy, os)\nyou can buy and sell in marketplace\n\n\nspot: bid for unused capacity\nsavings plan: commit to a consistent amount of usage for a discount\nif beyond pay, converted to on-demand\nlocked to a specific instance family, aws region\nflexible: instance size, os, tenancy\ndedicated hosts: physical server dedicated for your use\ndedicated instances: instance running on a dedicated host\ncapacity reservation: reserve capacity for specific instance type in a specific AZ\n\n\n\n\n\ncluster: low latency, high throughput\npartition: multiple EC2 instances within a single AZ\nspread: EC2 instances on distinct hardware, maximum 7",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "what is EC2"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/07_S3.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/07_S3.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Simple Storage Service\nobject storage service\nunlimited storage\ndefined at region level\nmax object size: 5TB (larger objects than 5GB can be stored using multipart upload)\nkey: full path\nvalue: body\nversion ID: enabled at the bucket level\nmetadata\ntags\n\n\n\n\nuser-based\n\nIAM policies\n\nresource-based\n\nbucket policies: allows cross account\nbucket ACL(Access Control List)\nobject ACL\n\nblock public access\n\n\n\n\n\nCRR: Cross-Region Replication\nSRR: Same-Region Replication\nversioning must be enabled on both source and destination buckets\nreplication is asynchronous\nreplication is cross-account\n\n\n\n\ndelete marker is replicated\noptional setting\n\n\n\n\n\ncan replicate existing objects and failed replication\n\n\n\n\n\n\nS3 Standard: 99.99% availability, general purpose #### infrequent access : for data that is less frequently accessed but requires rapid access when needed\nS3 Standard-IA: 99.9% availability\nS3 One Zone-IA: 99.5% availability #### Glacier : lower cost, for archive / backup\nS3 Glacier instant retrieval: milliseconds retrieval, 90 days minimum storage\nS3 Glacier flexible retireval: minutes to hours retrieval, 90 days minimum storage\nS3 Glacier Deep Archive: 12 hours to 48 hours retrieval, 180 days minimum storage\nS3 Intelligent-Tiering: auto pricing, auto move between IA and Standard\n\n\n\n\n: automate moving objects between storage classes - transition action - expriation action\n\n\n\n\nS3 event notification: SNS, SQS, Lambda\n\n\n\n\n\n3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket\n\n\n\n\n\nCloudFront edge locations\nmultipart upload is compatible\n\n\n\n\n\n\n\nS3 byte-range fetches\n\n\n\n\n\n\nS3 select: SQL query on S3 objects\nGlacier select: SQL query on Glacier objects\n\n\n\n\n\nS3 Batch Operations: S3 operations on large number of objects use cases: encrypt unencrypted objects, copy objects, … \n\n\n\n\n\nmulti-account, multi-region analyze dashboard\n\n\n\n\n\nSSE\n\nSSE-S3: S3 managed keys, enabled by default, must set request header x-amz-server-side-encryption: AES256\nSSE-KMS: KMS managed, must set request header x-amz-server-side-encryption: aws:kms, request limits\nSSE-C: customer managed, must set request header x-amz-server-side-encryption-customer-algorithm: AES256, must provide encryption key\n\nCSE\n\nclient-side encryption\n\n\n\n\n\n\npermanently delete objects\nsuspend versioning on bucket\nto enable, must enable versioning on bucket and only the bucket owner(root account) can enable MFA\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nbucket level lock\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nblock object deletion for a specified retention period\nmust set versioning\ncompliance and governance mode\nlegal hold: protect object from deletion indefinitely",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS S3"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/07_S3.html#aws-s3",
    "href": "posts/03_archives/completed_project/aws_saa/notes/07_S3.html#aws-s3",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Simple Storage Service\nobject storage service\nunlimited storage\ndefined at region level\nmax object size: 5TB (larger objects than 5GB can be stored using multipart upload)\nkey: full path\nvalue: body\nversion ID: enabled at the bucket level\nmetadata\ntags\n\n\n\n\nuser-based\n\nIAM policies\n\nresource-based\n\nbucket policies: allows cross account\nbucket ACL(Access Control List)\nobject ACL\n\nblock public access\n\n\n\n\n\nCRR: Cross-Region Replication\nSRR: Same-Region Replication\nversioning must be enabled on both source and destination buckets\nreplication is asynchronous\nreplication is cross-account\n\n\n\n\ndelete marker is replicated\noptional setting\n\n\n\n\n\ncan replicate existing objects and failed replication\n\n\n\n\n\n\nS3 Standard: 99.99% availability, general purpose #### infrequent access : for data that is less frequently accessed but requires rapid access when needed\nS3 Standard-IA: 99.9% availability\nS3 One Zone-IA: 99.5% availability #### Glacier : lower cost, for archive / backup\nS3 Glacier instant retrieval: milliseconds retrieval, 90 days minimum storage\nS3 Glacier flexible retireval: minutes to hours retrieval, 90 days minimum storage\nS3 Glacier Deep Archive: 12 hours to 48 hours retrieval, 180 days minimum storage\nS3 Intelligent-Tiering: auto pricing, auto move between IA and Standard\n\n\n\n\n: automate moving objects between storage classes - transition action - expriation action\n\n\n\n\nS3 event notification: SNS, SQS, Lambda\n\n\n\n\n\n3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket\n\n\n\n\n\nCloudFront edge locations\nmultipart upload is compatible\n\n\n\n\n\n\n\nS3 byte-range fetches\n\n\n\n\n\n\nS3 select: SQL query on S3 objects\nGlacier select: SQL query on Glacier objects\n\n\n\n\n\nS3 Batch Operations: S3 operations on large number of objects use cases: encrypt unencrypted objects, copy objects, … \n\n\n\n\n\nmulti-account, multi-region analyze dashboard\n\n\n\n\n\nSSE\n\nSSE-S3: S3 managed keys, enabled by default, must set request header x-amz-server-side-encryption: AES256\nSSE-KMS: KMS managed, must set request header x-amz-server-side-encryption: aws:kms, request limits\nSSE-C: customer managed, must set request header x-amz-server-side-encryption-customer-algorithm: AES256, must provide encryption key\n\nCSE\n\nclient-side encryption\n\n\n\n\n\n\npermanently delete objects\nsuspend versioning on bucket\nto enable, must enable versioning on bucket and only the bucket owner(root account) can enable MFA\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nbucket level lock\n\n\n\n\n\ncompliance and WORM (Write Once Read Many) model\nblock object deletion for a specified retention period\nmust set versioning\ncompliance and governance mode\nlegal hold: protect object from deletion indefinitely",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS S3"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html",
    "title": "Amazon Redshift",
    "section": "",
    "text": "SQLqueries -S3as data source - supportsCSV,JSON,Parquet,ORCdata formats - 5$ per TB scanned ## Performance Improvement - usecolumnardata formats (less scan) =&gt;Parquet,ORCby usingAWS Glue- compress data =&gt;GZIP,Snappy,LZO-partition datasetsin S3 for easy querying on virtual columns (path) -Use larger files` (&gt; 128MB) to minimize overhead",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#federated-query",
    "href": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#federated-query",
    "title": "Amazon Redshift",
    "section": "Federated Query",
    "text": "Federated Query\nallows you to query data in relational, non-relational, object, … in a single query on AWS or on-premises",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#cluster",
    "href": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#cluster",
    "title": "Amazon Redshift",
    "section": "cluster",
    "text": "cluster\n\nleader node\ncompute node",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#snapshots-and-dr",
    "href": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#snapshots-and-dr",
    "title": "Amazon Redshift",
    "section": "snapshots and DR",
    "text": "snapshots and DR\n\nMulti-AZ for some cluster\nsnapshots are point-in-time backups in S3\nchange is saved\nautomate snapshot, manual snapshote",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#loading-data-into-redshift",
    "href": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#loading-data-into-redshift",
    "title": "Amazon Redshift",
    "section": "loading data into redshift",
    "text": "loading data into redshift\n\nAmazon Kinesis Data Firehose\nAmazon S3 copy\nEC2 instance JDBC driver",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#redshift-spectrum",
    "href": "posts/03_archives/completed_project/aws_saa/notes/13_data_analytics.html#redshift-spectrum",
    "title": "Amazon Redshift",
    "section": "Redshift Spectrum",
    "text": "Redshift Spectrum\n: query data directly in S3 without loading it into Redshift",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Performance Improvement"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/10_message_queue.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/10_message_queue.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.\nunlimited throughput, no limit on the number of messages\nmessage retention period: default 4 days, maximum 14 days.\nlimit on message size 256kb\ncan have duplicate messages, out of order messages =&gt; need to handle in application or use FIFO queue\nSQS Access Policy\n\n\n\n\nthe amount of time that the message is invisible in the queue after a reader picks up the message.\ncan increase timeout by calling ChangeMessageVisibility API\n\n\n\n\n\nif no message in queue, the request will wait for a message to arrive for a certain amount of time.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/10_message_queue.html#aws-sqs",
    "href": "posts/03_archives/completed_project/aws_saa/notes/10_message_queue.html#aws-sqs",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "Amazon Simple Queue Service (SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.\nunlimited throughput, no limit on the number of messages\nmessage retention period: default 4 days, maximum 14 days.\nlimit on message size 256kb\ncan have duplicate messages, out of order messages =&gt; need to handle in application or use FIFO queue\nSQS Access Policy\n\n\n\n\nthe amount of time that the message is invisible in the queue after a reader picks up the message.\ncan increase timeout by calling ChangeMessageVisibility API\n\n\n\n\n\nif no message in queue, the request will wait for a message to arrive for a certain amount of time.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/10_message_queue.html#aws-sns",
    "href": "posts/03_archives/completed_project/aws_saa/notes/10_message_queue.html#aws-sns",
    "title": "김형훈의 학습 블로그",
    "section": "AWS SNS",
    "text": "AWS SNS\n\npublish/subscribe messaging service\nSNS FIFO (only SQS can subscribe)\nmessage filtering\n\n\nFanout\nSNS + multiple SQS",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/10_message_queue.html#amazon-kinesis",
    "href": "posts/03_archives/completed_project/aws_saa/notes/10_message_queue.html#amazon-kinesis",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Kinesis",
    "text": "Amazon Kinesis\n\nKinesis Data Streams\n\nreal-time data streaming service\ndata retention: default 24 hours, maximum 365 days\nonce data inserted, cannot be deleted\nprovisioned mode, on-demand mode\nVPC endpoint available \n\n\n\nKinensis Data Firehose\n\ndata transformation, compression, encryption\nbatch data delivery\nserverless  \n\n\n\nKinesis Data Analytics\n\n\nKinesis Video Streams",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS SQS"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers.\nLambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running. ### limitation (per region)\nmemory: 128MB - 10GB (more memory, need more vCPU)\nmax execution time: 15 minutes\nenvironment variables: 4KB\n/tmp directory storage: 512MB to 10GB\nconcurrent executions: 1000\ndeployment package: 50MB (zipped)\ndeployment package: 250MB (unzipped)\n\n\n\n\n\n\n\n\n\nLambda@Edge",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#aws-lambda",
    "href": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#aws-lambda",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers.\nLambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running. ### limitation (per region)\nmemory: 128MB - 10GB (more memory, need more vCPU)\nmax execution time: 15 minutes\nenvironment variables: 4KB\n/tmp directory storage: 512MB to 10GB\nconcurrent executions: 1000\ndeployment package: 50MB (zipped)\ndeployment package: 250MB (unzipped)\n\n\n\n\n\n\n\n\n\nLambda@Edge",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#dynamodb",
    "href": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#dynamodb",
    "title": "김형훈의 학습 블로그",
    "section": "DynamoDB",
    "text": "DynamoDB\n\nAmazon DynamoDB is a fully managed, serverless, key-value and document database that delivers single-digit millisecond performance at any scale.\nstandard table: high availability, durability, and performance\nIA table: infrequently accessed data\nmax item size: 400KB\nprovisioned mode, on-demand mode\n\n\nDynamoDB Accelerator (DAX)\n\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement - from milliseconds to microseconds - even at millions of requests per second.\nmicroseconds latency\nno need to modify application \n\n\n\nDynamoDB Streams\n\nDynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables.\ncan trigger lambda function, SQS, Kinesis\n24 hours retention period\nlimit: 5 active streams per table\n\n\n\nGlobal Tables\n\nAmazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution.\nautomatic replication\nmust enable DynamoDB Streams\n\n\n\nbackup and restore\n\nPoint-in-Time Recovery\n\nPoint-in-time recovery helps protect your DynamoDB tables from accidental write or delete operations.\nrestore to any point in time within 35 days\nthe recovery process creates a new table\n\non-demand backup\n\nrestore to any point\nthe recovery process creates a new table",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#api-gateway",
    "href": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#api-gateway",
    "title": "김형훈의 학습 블로그",
    "section": "API Gateway",
    "text": "API Gateway\n\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\nEdge-optimized API: global, use CloudFront\nRegional API: regional, use API Gateway\nPrivate API: VPC endpoint",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#step-functions",
    "href": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#step-functions",
    "title": "김형훈의 학습 블로그",
    "section": "Step Functions",
    "text": "Step Functions\n\nAWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#amazon-cognito",
    "href": "posts/03_archives/completed_project/aws_saa/notes/11_serverless.html#amazon-cognito",
    "title": "김형훈의 학습 블로그",
    "section": "Amazon Cognito",
    "text": "Amazon Cognito\n\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily.\nUser Pools: user directory\nIdentity Pools: federated identity, temporary access AWS resources",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Lambda"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/03_ebs.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/03_ebs.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EBS is a network device designed to work with AWS EC2 instances.\nEBS volumes are placed in a specific AZ and are automatically replicated to protect you from component failure.\nEBS volumes attached only to one instance at a time.\nEBS volumes can be detached from one instance and attached to another.\nEBS volumes can be used as a boot volume.\nEBS volumes have a provisioned size and IOPS can be resized.\nEBS volumes can exist independently of an EC2 instance.\nmultiple EBS volumes can be attached to a single EC2 instance.\n\n\n\n\nBy default, the root EBS volume is deleted when the EC2 instance is terminated.\nAdditional EBS volumes are not deleted when the EC2 instance is terminated.\n\n\n\n\n\nEBS snapshots archive tier:\n75% cheaper than the general-purpose tier.\ntakes 24 to 72 hours to restore.\nrecycle bin:\nset up rule to retain deleted snapshots\ncan specify period\nFast Snapshot Restore(FSR):\nforce full initialization of the EBS volume to have no latency\ntakes money\n\n\n\n\nAmazon Machine Image - AMI is a template that contains a software configuration (OS, application server, and applications) required to launch an EC2 instance. - AMI is built for a specific region. - can be copied to other regions.\n\n\n\n\nbetter IO performance than EBS volumes\ndata is lost when the instance is stopped or terminated.\ncan’t be resized.\n\n\n\n\n\nGeneral Purpose SSD (gp2, gp3): can be used for boot volumes\n\ngenerally used for system boot volumes, virtual desktops, low-latency interactive apps, development, and test environments\ngp2: 1GiB - 16TiB, burst up to 3000 IOPS linked to volume size\ngp3: 1GiB - 16TiB, 3000 IOPS, 125MiB/s, burst up to 16000 IOPS, 1000MiB/s independently\n\nHigh Performance SSD (io1, io2): can be used for boot volumes\n\ncritical business applications that require sustained IOPS performance\nmore than 16000 IOPS\ngenerally used for databases\n4GiB - 16TiB\nMax PIOPS: 64000 for Nitro EC2, 32000 for other EC2\nCan increase PIOPS independently from volume size\nio2 have more durability and more IOPS per GiB\nio2 Block Express: 4GiB - 64TiB, 256000 IOPS\nsupport Multi-Attach\n\nbound in AZ\nup to 16 EC2 instances\nMust use a file system that is cluster-aware (GFS, OCFS2, NTFS)\n\n\nLow cost, designed for frequently accessed HDD (st1)\n\n125MiB - 16TiB\n500MiB/s - 500MiB/s\nused for big data, data warehouses, log processing\n\nLow cost, designed for less frequently accessed HDD (sc1)\n\n125MiB - 16TiB\n250MiB/s - 250MiB/s\nused for file servers, infrequently accessed workloads\n\n\n\n\n\n\nElastic File System\nscalable storage solution for EC2 instances\ncan be shared across multiple instances in multi-AZ\ncan be accessed by multiple instances simultaneously\nexpensive than EBS\ncan be used for `content management, web serving\nuse NFSv4.1 protocol\nuse security group to control access\ncompatible with Linux-based AMI\nPerformance Mode:\n\nGeneral Purpose: latency-sensitive use cases\nMax I/O: higher latency, higher throughput\n\nThroughput Mode:\n\nBursting: burstable throughput\nProvisioned: provisioned throughput\nElastic: elastic throughput\n\nstorage classes:\n\nStandard: frequently accessed\nInfrequent Access: infrequently accessed\nOne Zone: infrequently accessed, stored in a single AZ. 90% cheaper than Regional",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "what is ebs"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/03_ebs.html#what-is-ebs",
    "href": "posts/03_archives/completed_project/aws_saa/notes/03_ebs.html#what-is-ebs",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "EBS is a network device designed to work with AWS EC2 instances.\nEBS volumes are placed in a specific AZ and are automatically replicated to protect you from component failure.\nEBS volumes attached only to one instance at a time.\nEBS volumes can be detached from one instance and attached to another.\nEBS volumes can be used as a boot volume.\nEBS volumes have a provisioned size and IOPS can be resized.\nEBS volumes can exist independently of an EC2 instance.\nmultiple EBS volumes can be attached to a single EC2 instance.\n\n\n\n\nBy default, the root EBS volume is deleted when the EC2 instance is terminated.\nAdditional EBS volumes are not deleted when the EC2 instance is terminated.\n\n\n\n\n\nEBS snapshots archive tier:\n75% cheaper than the general-purpose tier.\ntakes 24 to 72 hours to restore.\nrecycle bin:\nset up rule to retain deleted snapshots\ncan specify period\nFast Snapshot Restore(FSR):\nforce full initialization of the EBS volume to have no latency\ntakes money\n\n\n\n\nAmazon Machine Image - AMI is a template that contains a software configuration (OS, application server, and applications) required to launch an EC2 instance. - AMI is built for a specific region. - can be copied to other regions.\n\n\n\n\nbetter IO performance than EBS volumes\ndata is lost when the instance is stopped or terminated.\ncan’t be resized.\n\n\n\n\n\nGeneral Purpose SSD (gp2, gp3): can be used for boot volumes\n\ngenerally used for system boot volumes, virtual desktops, low-latency interactive apps, development, and test environments\ngp2: 1GiB - 16TiB, burst up to 3000 IOPS linked to volume size\ngp3: 1GiB - 16TiB, 3000 IOPS, 125MiB/s, burst up to 16000 IOPS, 1000MiB/s independently\n\nHigh Performance SSD (io1, io2): can be used for boot volumes\n\ncritical business applications that require sustained IOPS performance\nmore than 16000 IOPS\ngenerally used for databases\n4GiB - 16TiB\nMax PIOPS: 64000 for Nitro EC2, 32000 for other EC2\nCan increase PIOPS independently from volume size\nio2 have more durability and more IOPS per GiB\nio2 Block Express: 4GiB - 64TiB, 256000 IOPS\nsupport Multi-Attach\n\nbound in AZ\nup to 16 EC2 instances\nMust use a file system that is cluster-aware (GFS, OCFS2, NTFS)\n\n\nLow cost, designed for frequently accessed HDD (st1)\n\n125MiB - 16TiB\n500MiB/s - 500MiB/s\nused for big data, data warehouses, log processing\n\nLow cost, designed for less frequently accessed HDD (sc1)\n\n125MiB - 16TiB\n250MiB/s - 250MiB/s\nused for file servers, infrequently accessed workloads\n\n\n\n\n\n\nElastic File System\nscalable storage solution for EC2 instances\ncan be shared across multiple instances in multi-AZ\ncan be accessed by multiple instances simultaneously\nexpensive than EBS\ncan be used for `content management, web serving\nuse NFSv4.1 protocol\nuse security group to control access\ncompatible with Linux-based AMI\nPerformance Mode:\n\nGeneral Purpose: latency-sensitive use cases\nMax I/O: higher latency, higher throughput\n\nThroughput Mode:\n\nBursting: burstable throughput\nProvisioned: provisioned throughput\nElastic: elastic throughput\n\nstorage classes:\n\nStandard: frequently accessed\nInfrequent Access: infrequently accessed\nOne Zone: infrequently accessed, stored in a single AZ. 90% cheaper than Regional",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "what is ebs"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/06_route53.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/06_route53.html",
    "title": "Route53",
    "section": "",
    "text": "Domain/subdomain\nRecord type (A, AAAA, CNAME, NS)\n\nA: IPv4\nAAAA: IPv6\nCNAME: Canonical name. cant be used for root domain\nNS: Name server (another DNS server)\nalias: Route53 specific. can be used for root domain. free. health check. no TTL, can’t be used for ec2 instance\n\nValue\nRouting policy\n\nSimple: one record with multiple values, choose randomly by client, no health check, if alias then specify only one\nWeighted: split traffic based on weight\nLatency based: split traffic based on latency\nFailover: primary and secondary\nGeolocation\nMultivalue answer: multiple values, health check, choose randomly by client\nGeo-proximity\n\nTTL\n\n\n\n\nEndpoint\nCalculated\nCloudWatch alarm",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Route53"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/06_route53.html#health-check",
    "href": "posts/03_archives/completed_project/aws_saa/notes/06_route53.html#health-check",
    "title": "Route53",
    "section": "",
    "text": "Endpoint\nCalculated\nCloudWatch alarm",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Route53"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/19_DR.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/19_DR.html",
    "title": "Disaster Recovery(DR)",
    "section": "",
    "text": "Disaster Recovery(DR)\n\nRPO: Recovery Point Objective\n\nthe maximum acceptable amount of data loss measured in time\n\nRTO: Recovery Time Objective\n\nthe maximum acceptable amount of time to recover the system  ## DR Strategies\n\nBackup and Restore: Simple but RPO and RTO are high\nPilot Light: Minimal version of the environment is always running\nWarm Standby: A scaled-down version of a fully functional environment is always running\nHot-site / Multi-Site Approach: Fully functional environment is always running\n\n\n\nDatabase Migration Service(DMS)\n\nmigrate data from one database to another\nsource is available during migration\nHomogeneous Migration: same database engine\nHeterogeneous Migration: different database engine. must use Schema Conversion Tool(SCT)\nContinuous Data Replication using CDC\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "Disaster Recovery(DR)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html",
    "href": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Snow Family is a collection of physical devices designed for use in edge locations, data centers, and in disconnected environments.\nData Migration: snowcone, snowball edge, snowmobile\nEdge Computing: snowcone, snowball edge  ### use process\n\n\nOrder: Order a Snow device from the AWS Management Console.\ninstall: Install the Snow client / AWS ops hub on your server\nTransfer: Transfer data to the Snow device using the Snow client.\nShip: Ship the Snow device back to AWS.\nLoad: Load the data into your S3 bucket.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html#aws-snow-family",
    "href": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html#aws-snow-family",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "AWS Snow Family is a collection of physical devices designed for use in edge locations, data centers, and in disconnected environments.\nData Migration: snowcone, snowball edge, snowmobile\nEdge Computing: snowcone, snowball edge  ### use process\n\n\nOrder: Order a Snow device from the AWS Management Console.\ninstall: Install the Snow client / AWS ops hub on your server\nTransfer: Transfer data to the Snow device using the Snow client.\nShip: Ship the Snow device back to AWS.\nLoad: Load the data into your S3 bucket.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html#aws-fsx",
    "href": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html#aws-fsx",
    "title": "김형훈의 학습 블로그",
    "section": "AWS FSx",
    "text": "AWS FSx\n\nAmazon FSx for Windows File Server: fully managed Windows file system\nAmazon FSx for Lustre: fully managed Lustre file system, seamlessly integrated with S3 (can read and write data directly to S3)\nAmazon FSx for NetApp ONTAP: fully managed NetApp ONTAP file system, point-in-time snapshots, data deduplication, and data compression\nAmazon FSx for OpenZFS: fully managed OpenZFS file system, point-in-time snapshots, data deduplication, and data compression\n\n\nFile System Deployment Options\n\nScratch File System: temporary storage for data processing. no replication.\nPersistent File System: long-term storage for data processing. replicate data across multiple Availability Zones.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html#aws-storage-gateway",
    "href": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html#aws-storage-gateway",
    "title": "김형훈의 학습 블로그",
    "section": "AWS Storage Gateway",
    "text": "AWS Storage Gateway\n\nAWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.\nS3 File Gateway: store and retrieve objects in Amazon S3 using file protocols (NFS, SMB), cache data locally, not glacier.\nFSx File Gateway: store and retrieve objects in Amazon FSx using file protocols (NFS, SMB) for window file server.\nVolume Gateway: store and retrieve objects in Amazon S3, EBS Snapshot using iSCSI protocol.\n\ncached volume: cache frequently accessed data locally.\nstored volume: entire dataset stored locally, asynchronously backed up to S3.\n\nTape Gateway: store and retrieve objects in Amazon S3 using virtual tape library (VTL) interface",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html#aws-datasync",
    "href": "posts/03_archives/completed_project/aws_saa/notes/09_aws_storage.html#aws-datasync",
    "title": "김형훈의 학습 블로그",
    "section": "AWS DataSync",
    "text": "AWS DataSync\n\nAWS DataSync is a data transfer service that makes it easy for you to automate moving data between on-premises storage and Amazon S3, Amazon Elastic File System (Amazon EFS), or Amazon FSx for Windows File Server.\nFile permissions and metadata are preserved during transfer.\nif not aws to aws, need agent to transfer data.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비",
      "Notes",
      "AWS Snow Family"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/index.html",
    "href": "posts/03_archives/completed_project/aws_saa/index.html",
    "title": "AWS SAA 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2024-04-15\n        종료일: 2024-05-22\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증cloud",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/index.html#details",
    "href": "posts/03_archives/completed_project/aws_saa/index.html#details",
    "title": "AWS SAA 준비",
    "section": "Details",
    "text": "Details\nAWS Solution Architect Associate 자격증을 취득하였습니다.\n자격증 링크",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/index.html#tasks",
    "href": "posts/03_archives/completed_project/aws_saa/index.html#tasks",
    "title": "AWS SAA 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/aws_saa/index.html#참고-자료",
    "title": "AWS SAA 준비",
    "section": "참고 자료",
    "text": "참고 자료\n\nAWS Udemy 강의",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/aws_saa/index.html#related-posts",
    "href": "posts/03_archives/completed_project/aws_saa/index.html#related-posts",
    "title": "AWS SAA 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "AWS SAA 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#눈의-구조",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#눈의-구조",
    "title": "Sensor System (Visual)",
    "section": "눈의 구조",
    "text": "눈의 구조\n\n결막 (conjunctiva): 눈을 보호\n각막 (cornea): 빛을 굴절\n홍채(iris): 빛의 양을 조절\n동공 (pupil): 빛이 들어오는 곳\n수정체(lens): 빛을 집중하는 역할\n공막 (sclera): 눈을 보호\n유리체 (vitreous humor): 눈을 유지\n망막(retina): 빛을 감지\n망막의 세포\n\nNerve cell: 빛을 감지\nPhotoreceptor: 빛을 감지\n\n간상세포(cone): 세부적인 정보, 색상 인식, photopic conditions. fovea에 몰려있음. 짧은 파장의 색에 더 민감함\n막대세포(rod): 어두운 곳에서 활동, 주변 시야 빛을 받으면 rhodopsin이 분해됨, scotopic conditions. 긴 파장의 색에 더 민감함.\n\nChoroid: 영양 공급\n\n시신경(optic nerve): 망막에서 뇌로 정보 전달\n맹점(optic disk)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#light-adaption",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#light-adaption",
    "title": "Sensor System (Visual)",
    "section": "light adaption",
    "text": "light adaption\n\n눈이 어두운 곳에서 밝은 곳으로 이동할 때, 시간이 걸림\n명순응동안 rod sensitivity가 감소하고 cone sensitivity가 증가\n어두운 곳에서 밝은 곳으로 이동할 때, 눈이 눈부실 수 있음\n암순응동안 cone sensitivity가 감소하고 rod sensitivity가 증가",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#color-vision",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#color-vision",
    "title": "Sensor System (Visual)",
    "section": "color vision",
    "text": "color vision\n\ncone cell의 photo-pigment(RGB 64:32:2)로 색상을 인식\n망막 중앙에는 파란색이 없음\nsharpness는 brightness와 color difference에 영향을 받음\n사람은 7백만가지 색상을 인식할 수 있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#design-with-color",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#design-with-color",
    "title": "Sensor System (Visual)",
    "section": "design with color",
    "text": "design with color\n\nMono-chromatic: 단색\nAnalogous: 비슷한 색\nComplementary: 반대 색\n\n\nbefore design\n\n굳이 흑백을 안쓰고 color를 사용해야하는 이유가 있는지\ncolor가 텍스트나 object에 적합한지\ncolor가 이해나 관습에 도움이 되는지\n노안 / 색맹 고려",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#depth-perception",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/3_sensory_system.html#depth-perception",
    "title": "Sensor System (Visual)",
    "section": "depth perception",
    "text": "depth perception\n\ndepth judgment\n\nobject-centered cues\n\n\nlinear perspective: 두 평행선이 좁을 수록 더 멀리 있는 것으로 인식\ninterposition(occlusion): 물체가 다른 물체를 가리면 가려진 물체가 더 멀리 있는 것으로 인식\nheight in the plane: 물체가 높이 있을수록 더 멀리 있는 것으로 인식\nlight and shadow: 빛과 그림자로 물체의 거리를 인식\nrelative size: 물체가 작을수록 더 멀리 있는 것으로 인식\ntexture gradient: 물체가 멀어질수록 세부적인 텍스처가 사라짐\nbrightness: 물체가 밝을수록 더 가까이 있는 것으로 인식\naerial perspective: 물체가 먼발에서 가까워질수록 색이 흐려짐\nmotion parallax: 물체가 빠르게 움직일수록 더 가까이 있는 것으로 인식 fixation point\n\n\nobserver-centered cues\n\n\nbinocular disparity: 두 눈의 시각적 차이\nconvergence: 눈이 물체를 바라볼 때 발생하는 각도\naccommodation: 눈의 렌즈가 물체를 바라볼 때 발생하는 조절. 가까운 물체일수록 렌즈가 더 둥글어짐",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Sensor System (Visual)"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#display-purpose",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#display-purpose",
    "title": "Display",
    "section": "Display Purpose",
    "text": "Display Purpose\n\n사람의 인지와 시스템의 실제 정보 사이의 커뮤니케이션을 위한 중간 다리 역할.\n시스템이 무엇을 하는 중이고, 무엇을 해야 하고, 어떻게 작동하는지 오퍼레이터에게 전달하기 위한 목적(mental model을 만들기 위함)\n설계된 sensory input을 통해서 파악하게 해야함\n다른 sensory input과 구별이 되야함.\n사용자가 이해할 수 있어야함(Compatible)\n\nConceptual Compatibility\nex) 플로피 디스크 심볼은 저장 용도로 사용됨\nmovement compatibility(pictorial realism): 실제와 유사한 모양을 사용하면 이해하기 쉬움\nex) 엘리베이터가 위 아래로 움직이니까 스케일을 linear로 맞춤",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#display-rules",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#display-rules",
    "title": "Display",
    "section": "Display Rules",
    "text": "Display Rules\n\nFour Cardinal Rules\n\n꼭 필요한 정보만 제공해라\n필요한 수준의 정확도만 제공하라 (ex. 소숫점 3자리까지만. 굳이 다 보여주지 않아도 됨)\n가장 direct, simple, understandable, and usable하게 정보를 제공하라\nex) 지하철 디스플레이에서 열차가 언제 도착하는지 알려줘야 하는데 이상한걸 보여줘서 멘탈 워크로드가 높아진다.\n고장이나 작업 실패의 경우 명확히 어디서 문제가 발생했는지 바로 알아차리게 제공하라\n첫번째 원칙을 위반할 수도 있다(alarm flooding)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#types-of-displays",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#types-of-displays",
    "title": "Display",
    "section": "Types of Displays",
    "text": "Types of Displays\n\nAuditory\n\nDetectable\nDiscrmination\nMeaningful\nMain problem: hearing ability depends on environments / background noise (auditory spatial coding 인지하기 힘듦)\n\nTactual(Haptic)\n\nDetectable: 손처럼 민감한 부분은 가능하지만, 둔감한 부분은 어렵다\nDiscrimination: nomal job이랑 구분되어야 한다\nMeaningful: tactual display에서는 어려운 부분. convention이 없음\nMain problem: 잘 안쓰이고, 손 이외에는 사용하기 어렵다\n\nOlfactory Displays - smell\n\nDetectable\nDiscrimination\nMeaningful\nMain problem: 냄새에 대한 민감도가 사람마다 다르다. regenerate 하기가 어렵다. 후각이 금방 마비된다. 전쟁에서 후각을 이용한 의사전달을 시도하기도 함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#visual-displays",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#visual-displays",
    "title": "Display",
    "section": "Visual Displays",
    "text": "Visual Displays\n\nAppropriate if:\n\nNoisy environment\n한 자리에 머무는 경우(traditional. 옛날엔 들고 다니기 어려웠음)\nmessage가 길거나 복잡한 경우, spatial coding이 필요한 경우\n\nGuiding principles for design:\n\n눈에 잘 띄어야함(배경과 전경의 차이가 구분되어야함)\nLegible, 쉽게 보고 읽을 수 있어야함(ex. 엠뷸런스의 글씨)\nUnderstandable\nmain problem: 시각이 overload됨. 정보가 너무 많음\n\n\n\nDynamic Information\n변화하는 정보를 보여주는 것에는 4가지 원칙이 있다.\n\nSituation awareness\n가까운 미래에 무슨 일이 일어날 지 예측할 수 있어야함\n상황 인식 3단계:\n\nPerception: 무엇이 일어나고 있는지 인지(check readings)\nComprehension: 그것이 무엇을 의미하는지 이해\nProjection: 미래에 무슨 일이 일어날지 예측\n\n\n\nQuantitative readings\n정확한 값을 보여주는 용도로 사용됨\n\n고정 스케일의 움직이는 초점 (generally best)\n움직이는 스케일의 고정 초점\ndigital display (변동성이 큰 경우 그냥 숫자만 보여주는것보다 스케일, 초점을 사용하는게 더 효과적임)\nDesign of Analog Scales\n\n일반적으로 fixed scale, moving pointer가 좋다\n숫자의 증가는, linear 스케일에 움직이는 포인터가 자연스럽다.\nex) 온도계, 엘리베이터\n같은 작업을 하는 여러개의 pointer, scale indicator를 섞어 쓰지 말아라.\ncontrol, display가 혼합된경우 control로 pointer로 움직여라\n작은 변화 감지가 중요한 경우는 moving pointer가 더 좋다\n범위가 너무 큰 경우는 moving scale이 더 좋다\n\n\n\n\nQualitative readings\n대략적인 값, 트렌드, 변화의 비율, 변화의 방향을 보여주는 용도로 주로 사용됨.\n\ncontinuous data converted to range\n의미를 강조하고 싶을 때 color를 보조 도구로써 보여주기도 함\nShape coding\nex) 교통 표지판 8각형은 stop을 의미\nZone coding\nex) 신호등의 위치가 고정되어 있음\n\nRedundancy gain: 시각 청각 촉각, 혹은 칼라코드, 위치코드 같이 여러가지 정보(multi-modal)를 제공하면 정확히 해석할 수 있다.\n\n\nCheck readings\n시스템의 상태를 확인할 수 있어야함\n\nqualitaative reading의 특별한 case\n정상인 상태는 명확히 보여줘야함\n정상적인 것은 align해서, 비정상적인 것은 삐뚤어지게 설계해서 pre-attentive processing을 유도하라\n시각 정보를 보완하기 위해 청각 시그널을 제공하라",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#signal-and-warning-lights",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/7_display.html#signal-and-warning-lights",
    "title": "Display",
    "section": "Signal and Warning Lights",
    "text": "Signal and Warning Lights\n실질적인, 잠재적인 위험 상황을 알리는 용도\n일반적으로 하나의 라이트만 사용함\n\nsteady-state light: 지속적인 싱태를 나타냄\nflashing light: 위급 상황 (flash 비율은 3-10 per seconds)\n배경에 비해 최소 두 배 이상 밝아야함\n유효 시야 30도 안쪽에 배치해야함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Display"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/2_human_information_processing_model.html#인간-정보-처리-과정",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/2_human_information_processing_model.html#인간-정보-처리-과정",
    "title": "Human Information Processing Model",
    "section": "인간 정보 처리 과정",
    "text": "인간 정보 처리 과정\n\n\n1. 감각 처리 (Sensory Processing)\n\n주요 감각: 시각, 청각, 운동 감각(proprioception)\n운동 감각: 생리적 신호, 중력과 가속도에 따른 몸의 위치 감각\nSTSS (Short Term Sensory Store)\n\n각 감각기관별 정보 저장 공간\n주의 집중 불필요, 정보 그대로 저장\n빠르게 소멸 (예: 시각 - Iconic Memory 200-300ms, 청각 - Echoic Memory 2-8s)\n\n\n\n\n2. 지각 (Perception)\n\n정보 해석 과정\nTop-Down Processing: 과거 기억으로 정보 해석 (Long Term Memory 활용)\nBottom-Up Processing: 새로운/익숙하지 않은 정보 해석\nPreattentive Processing: 주의 집중 없이 자동적 정보 해석 (예: Stroop Effect)\n\n\n\n3. 주의 자원 (Attention Resources)\n\n전반적인 정보 처리 과정에 영향\nSearch Light Metaphor: 주의 집중의 비유적 설명\n주의 실패 유형:\n\nSelective Attention: 잘못된 곳에 집중\nFocused Attention: 주의 집중 부족\nDivided Attention: 다중 작업 시 주의 분산\nSustained Attention: 장시간 주의 유지 실패\n\n\n\n\n4. 장기 기억 (Long Term Memory)\n\n학습을 통한 정보 저장\n유형:\n\nDeclarative Memory (What): 사실적 지식 (Episodic, Semantic)\nProcedural Memory (How): 절차적 지식\n\n기억 실패: Encoding, Storage, Retrieval 단계에서 발생 가능\n\n\n\n5. 인지 (Cognition)\n\nWorking Memory:\n\n단기적 정보 처리 및 조작\n제한된 용량 (7±2 items)\nLong Term Memory와 Perception으로부터 정보 획득\n\n\n\n\n6. 반응 선택 (Response Selection)\n\n의사결정 이론:\n\nSignal Detection Theory\nExpected Value\nBayesian Decision Theory\nMulti-attribute Theory\n\nInformation Processing\n\nAttention and working memory\nHeuristics and biases: 인간은 합리적이지 않음\n\nNaturalistic Decision Making: Recognition-Primed Decision Model (직감)\n\n\n\n7. 반응 실행 (Response Execution)\n\n\n8. 시스템 환경 (System Environment)\n\nclosed-loop 형태의 피드백 제공\ndelay가 발생시 성능 저하",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Human Information Processing Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#귀의-구조",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#귀의-구조",
    "title": "Auditory Haptic",
    "section": "귀의 구조",
    "text": "귀의 구조",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#sound-waves",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#sound-waves",
    "title": "Auditory Haptic",
    "section": "Sound Waves",
    "text": "Sound Waves\n\n소리는 압력이 변하는 것\n소리는 vibrating object에 의해 발생한다\n연못에 돌을 던지면 생기는 wave와 비슷함\n어떤 분자든 움직이고 압력을 만들 수 있는건 전달 가능함\n물속에서도 소리가 전달됨. (밀도가 높아서 더 빨리 전달됨)\n고체(층간소음, 철로), gas\n진공에서는 매질이 없어서 소리가 안들림(우주 공간) &lt;-&gt; 빛은 매질이 없어도 이동됨\n파동이 전기신호로 바뀌어서 들림\n\n\n물리적 특성\n\n\namplitude: 진폭의 크기 -&gt; volume\nwavelength: 진폭의 넓이 -&gt; pitch, 1초 안에 몇 번 진동하는지(주파수 10Hz = 10번 진동)\n\n\n\n사람이 느끼는 perception\n\nPitch(소리의 높낮이)\n사람이 들을 수 있는 주파수는 20Hz ~ 15kHz\n어릴 때는 고주파를 잘 들음 (고주파에 고막이 반응을 못해서)\n사람은 고주파에 반응을 잘 못함\n절대 음감이랑 관련\nTimbre(음색)\n음악에서는 악기마다 다른 음색이 있음\n음색은 여러 주파수의 하모니(complex set of resonance공명)로 결정됨\nAmplitude and loudness\n소리의 물리적 강도가 2배 증가할 때 우리가 느끼는 소리의 크기(loudness)가 배로 느껴짐(찾아보니까 2배는 아니긴 함) 160db까지 들을 수 있음 130db부터는 고통스러움\nSpatialisation\n소리는 어느 방향에서 소리가 나도 들을 수 있음 (omnidirectional).\n시각은 볼 수 있는 방향만 볼 수 있음\n\n\n\nSound intensity (dB)\n\n데시벨(dB)은 기준점에서 로그 스케일만큼 증가. (선형적 x)\nthreshold: 주변의 소음에 비해 소리가 들리는 정도. 주파수에 따라 다른 특성을 가짐.\n\n\n\n85 dB에 장시간 노출되면 청력 손상\n\n신경의 손상: 장시간 센 자극에 hair cell이 손상됨\nconduction damage: 소리의 세기가 너무 커서 고막이나 뼈에서 손상이 생김",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#masking-effect",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#masking-effect",
    "title": "Auditory Haptic",
    "section": "Masking Effect",
    "text": "Masking Effect\n\n청각에서만 주로 나타남.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#equal-loudness-contour",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#equal-loudness-contour",
    "title": "Auditory Haptic",
    "section": "Equal Loudness Contour",
    "text": "Equal Loudness Contour\n\n곡선은 사람들이 소리가 같은 크기라고 느끼는 지점을 나타냄\n인간 청력의 threshold가 주파수마다 다르다.\n저주파수와 고주파수는 중간 주파수에 비해 같은 강도에서 상대적으로 작게 들립니다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#locating-sounds",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#locating-sounds",
    "title": "Auditory Haptic",
    "section": "Locating Sounds",
    "text": "Locating Sounds\n왼쪽 귀와 오른쪽 귀에서 들리는 소리의 차이\n\ndifference in phase(위상): 소리의 파장이 오목한 phase, 볼록한 phase 차이\ndifference in loudness: 가까운게 더 크게 들림\ndifference in onset: 가까운게 더 빨리 도달함\n\n\n여기까지가 소리의 mechanical한 특성이고, 이후는 이 소리를 인간이 어떻게 perception하는지에 관한 내용",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#hearing-without-awareness",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#hearing-without-awareness",
    "title": "Auditory Haptic",
    "section": "Hearing Without Awareness",
    "text": "Hearing Without Awareness\n\nCocktail Party Effect: 주변 소음에서도 특정 소리를 들을 수 있음\nEx) 친구 이름을 듣고 반응하는 것, 한국인들이 한국말을 잘 듣는것\nDichotic Listening: 두 귀에 다른 소리를 들려주고 정보를 인식했는지 확인\nignored 귀에서 여전히 정보를 인식할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#alarms",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#alarms",
    "title": "Auditory Haptic",
    "section": "Alarms",
    "text": "Alarms\n\nOverview\nomnidirectional한 특성때문에 visual alarm에 비해 자주 사용된다\nEx) 소방차 사이렌 소리\n\n주변 소음에 비해 충분히 db이 커야함 주변 소음과의 차이가 15dB minimal, 30dB required\n하지만 소리가 너무 크면 청각에 손상을 일으킬 수 있음\n안전상의 이유로 소리는 85 ~ 90dB 이하로 유지되어야 함\nmasking 위협때문에 여러 주파수를 혼합해서 냄\n다른 signal과 헷갈리지 않아야함\nEx) 병원의 환자실에 여러 장비가 있는데 장비마다 알람이 구분이 안되면 안됨.\nInformative and distinctive\n각각의 physical dimension(pitch(4), duration(4), amplitude(4))은 4개를 넘게 쓰지 마라\nEx) 컴퓨터 메인보드, 장비 고장 시 비프음 기준이 있음\nEx) 자동차\n\nstereotypic: 어디서 소리가 오는지\n\npitch\n\n\n\n\nNon-speech Alarm\n\nlanguage independent\n글로벌하게 가고 싶다면\nNSA가 유용하다는 증거\n클릭을할 때 딸깍 소리가 나면 실수가 덜 함\n비디오 게이머들은 소리가 없으면 게임을 못함\n일시적이고 부수적인 상태 정보 전달에 효과적\n예시) 게임에서:\n\nHP가 부족할 때 주기적인 경고음\n\n아이템 획득 시 짧은 효과음\n\n배경에서 지속적으로 재생되는 상태 알림음\n\nstereo sound로 방향을 알려줄 수 있음\n비쥬얼로는 3d 표현하기 어려움\n\n\n\nVoice Alarm\n\n자연스러운 방법으로 기기와 통신할 수 있음\nSymbolic alarm에 비해 더 많은 정보를 전달할 수 있음\nSymbolic alarm은 학습을 해야한다는 단점이 있음\nNon-Speech에 비한 한계\n\n소리가 섞이면 헷갈림\n\nmore susceptible to frequency-specific masking\n사람의 voice는 정해진 주파수가 있다.\n그 주파수에 소리가 섞이면 소리를 못들을 수 있음\n다국어 환경을 고려해야함\n\nSound Transmission Problem\n말하고자 하는 바가 전달이 잘 안될 수 있음\nEx) 파일럿 안내. 라디오에서 사용할 수 있는 대역폭이 제한되있음",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#voice-recognition",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#voice-recognition",
    "title": "Auditory Haptic",
    "section": "Voice Recognition",
    "text": "Voice Recognition\nSound Transmission Problem을 해결하기 위해 사용됨\n\nArticulation index: pure bottom-up(signal의 특성에 의존) approach\nsignal이 얼마나 명확하게 잘 들리는지 평가\n1.0: 주변소음에 상관없이 잘 들리는 상태\n0.0: 주변소음에 묻혀서 소리가 들리지 않는 상태\nSpeech intelligibility measure\npoor signal quality is compensated by top-down processing =&gt; 어떻게 top-down processing을 잘 할까?를 테스트 해 보았다.\n전달하는 정보의 양을 제한, 문장의 형태로 전달하는게 좋음\n긍정이나 부정이 잘 나타나는 단어를 사용(Ok는 애매함)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-perception",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-perception",
    "title": "Auditory Haptic",
    "section": "Tactile Perception",
    "text": "Tactile Perception\n\nTouch is complex\nOnly bi-directional communication channel: 접촉하거나 움직이거나 한 다음에 반응을 얻는 등, input과 output이 동시에 일어남\n환경에 대한 정보를 포괄해서 전달함\n온도, 표면의 거칠기, 등등\nfeedback을 제공함\n수용체가 피부 변형을 감지함\n민감도는 단위 면적당 촉점이 얼마나 분포되어 있는지에 따라 결정됨",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-information",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_human/4_Auditory_Haptic.html#tactile-information",
    "title": "Auditory Haptic",
    "section": "Tactile information",
    "text": "Tactile information\n\n운동 감각을 통해 받아들이는 것 (force feedback)\n\n\n\n큰 물체는 움직이기 어렵게 함 =&gt; 더 세밀하게 조종 가능\n\n게임에서 많이 사용됨\n\n\n촉감을 통해 받아들이는 것 (vibration feedback)\n\n\n\nusing vibration for information transfer\nsimilar physical characteristics to auditory signal\n\namplitude, frequency, duration, wave pattern\n\nused for navigation aid\n중요한 정보는 시각, 나머지 feedback은 촉각, 청각으로 받아들임.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Human",
      "Auditory Haptic"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/6-정규분포.html#정규-분포의-합",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/6-정규분포.html#정규-분포의-합",
    "title": "정규 분포",
    "section": "정규 분포의 합",
    "text": "정규 분포의 합\n두 분포의 합이 같은 분포가 되는 경우는 흔치 않다 (uniform distribution도 같지 않다)\n두 정규분포의 합은 정규분포가 된다\n\\(X + Y \\sim N(μ_1 + μ_2, σ_1^2 + σ_2^2)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "정규 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/6-정규분포.html#chi-square-분포",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/6-정규분포.html#chi-square-분포",
    "title": "정규 분포",
    "section": "Chi-square 분포",
    "text": "Chi-square 분포\nα = ν/2, θ = 2인 감마분포\n\\(Z \\sim N(0,1)\\)일 때, \\(Z^2 \\sim χ^2(1)\\)\n\\(Z_i \\sim N(0,1)\\)일 때, \\(Z_1^2 + Z_2^2 + ...  + Z_n^2 \\sim χ^2(n)\\)\n\\(X_i\\)가 서로 독립이고, 자유도가 \\(ν_i\\)인 카이제곱분포를 따른다면, \\(X_1 + X_2 + ... + X_n \\sim x^2(ν_1 + ν_2 + ... + ν_n)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "정규 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#통계학의-정의",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#통계학의-정의",
    "title": "확률과 통계의 정의",
    "section": "통계학의 정의",
    "text": "통계학의 정의\n\n불확실한 상황에서 데이터에 근거하여 과학적인 의사결정을 도출하기 위한 이론과 방법 체계\n모집단으로부터 수집된 데이터(sample)를 기반으로 모집단의 특성을 추론하는 것을 목표로 함\n\n\n\n모집단: 통계분석의 대상이 되는 모든 개체들의 집합\n표본: 모집단으로부터 일정한 규칙에 의해 추출한 부분집합",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-개념",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-개념",
    "title": "확률과 통계의 정의",
    "section": "확률의 개념",
    "text": "확률의 개념\n\n모집단에서 특정 사건(event)의 상대도수의 극한\n\n\nLaw of Large Numbers\n무수히 많은 시행이 반복되면 상대도수에 의해 계산되는 확률(통계적 확률)이 이론적 확률로 수렴한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#sample-space-and-events",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#sample-space-and-events",
    "title": "확률과 통계의 정의",
    "section": "Sample Space and Events",
    "text": "Sample Space and Events\n\nExperiment(확률실험): 동일한 조건에서 독립적으로 반복할 수 있는 실험이나 관측\nSample space(표본공간): 모든 simple event의 집합\nEvent(사건): 실험에서 발생하는 결과 (부분 집합)\nSimple event(단순사건): 원소가 하나인 사건\n\n\n\n\nevent는 여러 원소를 가질 수 있다",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-정의",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#확률의-정의",
    "title": "확률과 통계의 정의",
    "section": "확률의 정의",
    "text": "확률의 정의\n\n고전적 확률: 모든 simple event가 동일한 확률을 가질 때 P(A)는 sample space가 n개의 원소로 이루어져 있을 때 k개의 원소를 가지는 event A의 확률\n통계적 확률: simple event가 동일한 확률을 가지지 않아도 된다. 표본의 수가 무한대로 갈 때, 표본의 확률이 수렴하는 값\n\n\n확률의 성질\n\n모든x에 대하여 P(x) &gt;= 0\nP(sample space) = 1\nA와 B가 배반사건이면 P(A or B) = P(A) + P(B)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#조건부-확률",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#조건부-확률",
    "title": "확률과 통계의 정의",
    "section": "조건부 확률",
    "text": "조건부 확률\n\nEvent B가 발생했을 때 Event A의 확률 \\[P(A|B) = \\frac{P(A∩B)}{P(B)}\\]\n결합확률 (joint probability): P(A∩B)\n주변확률 (marginal probability): P(A), P(B), …\n\n\nMultiplication Law\n\\[P(A∩B) = P(A|B)P(B)\\]",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#independent-events",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#independent-events",
    "title": "확률과 통계의 정의",
    "section": "Independent Events",
    "text": "Independent Events\n\n두 사건 A와 B가 독립일 때, P(A|B) = P(A), P(B|A) = P(B)\nsample space는 임의의 event와 독립이다.\n공집합은 임의의 event와 독립이다. (P(∅∩A) = P(∅) * P(A) = 0 * P(A) = 0 = P(∅))",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#베이즈-정리",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/1-통계학의-개념.html#베이즈-정리",
    "title": "확률과 통계의 정의",
    "section": "베이즈 정리",
    "text": "베이즈 정리\n\n\nsample space를 상호 배반인 {B1, B2, …, Bn}으로 분할 (partition)\n\\(P(A) = P(A∩B_1) + P(A∩B_2) + ... + P(A∩B_n)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률과 통계의 정의"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#확률변수의-기댓값",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#확률변수의-기댓값",
    "title": "확률변수의 기댓값",
    "section": "확률변수의 기댓값",
    "text": "확률변수의 기댓값\n\n\\(μ = E(x)\\)로 가정. (모집단)\ncovariance는 선형관계를 보여준다.\nx와 y는 독립이다 -&gt; cov(x, y) = 0",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수의 기댓값"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#moment-generationg-functions",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/3-확률변수의-기댓값.html#moment-generationg-functions",
    "title": "확률변수의 기댓값",
    "section": "Moment Generationg Functions",
    "text": "Moment Generationg Functions\n\n평균과 분산만으로 확률분포를 설명하기에는 부족하다.\n\nmoment: \\(μ_k′ = E(X^k), k ∈ ℤ+\\)\nVar(x) = \\(μ_2′ - μ_1′^2\\)\nE(x) = \\(μ_1′\\)\n모든 k에 대해 검증 불가 =&gt; mgf(moment generating function)\nmgf: \\(M_X(t) = E(e^{tx})\\)\n\n\n\\(M_X(t) = 1 + tμ_1′ + \\frac{t^2}{2!}μ_2′ + \\frac{t^3}{3!}μ_3′ + ...\\)\n\\(M′(t) = μ_1′ + \\frac{2t}{2!}μ_2′ + \\frac{3t^2}{3!}μ_3′ + ...\\)\n\\(M′(0) = μ_1′\\)\n\\(M′′(0) = μ_2′\\)\n\\(M^{(k)}(0) = μ_k′\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "확률변수의 기댓값"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#좋은-추정량이-되기-위한-조건",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#좋은-추정량이-되기-위한-조건",
    "title": "표본의 분포",
    "section": "좋은 추정량이 되기 위한 조건",
    "text": "좋은 추정량이 되기 위한 조건\n\n불편성 (Unbiasedness) - 기본 조건\n추정량의 기대값이 추정하려는 모수와 같아야 함\n\\(E(\\hat{X}) = μ\\)\n\\(E(X_1) = μ\\)\n최소분산 (Minimum Variance)\n추정량의 분산이 가능한 작아야 함.\n표본의 갯수를 늘릴수록 분산이 줄어들어서 더 좋은 추정량이 됨\n\\(Var(\\hat{X}) = \\frac{σ^2}{n}\\)\n\\(Var(X_1) = \\sigma^2\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본평균의-분포",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본평균의-분포",
    "title": "표본의 분포",
    "section": "표본평균의 분포",
    "text": "표본평균의 분포",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본분산의-분포",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#표본분산의-분포",
    "title": "표본의 분포",
    "section": "표본분산의 분포",
    "text": "표본분산의 분포\n정규분포로 부터 추출된 표본의 \\(\\sum_{i=1}^{n} Z^2\\)은 자유도가 n인 카이제곱분포를 따름\n정규분포로 부터 추출된 표본의 \\(\\frac{(n-1)s^2}{\\sigma^2}\\)은 자유도가 n-1인 카이제곱분포를 따름",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#평균",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#평균",
    "title": "표본의 분포",
    "section": "평균",
    "text": "평균\n\\(\\frac{\\hat{X} - μ}{s/\\sqrt{n}}\\) t-분포를 따름\nT분포: 표준 정규분포 Z, 자유도가 n인 카이제곱분포가 서로 독립일 때 \\(T=\\frac{Z}{\\sqrt{Y/n}}\\)\nT분포는 정규분포와 비슷하지만, 표본의 크기가 작을 때 정규분포보다 두꺼운 꼬리를 가짐\nT분포가 값이 더 작고, 신뢰도가 감소함",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#분산",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_statistics/7-표본의-분포.html#분산",
    "title": "표본의 분포",
    "section": "분산",
    "text": "분산\n확률변수 U와 V가 자유도가 n1, n2인 카이제곱분포를 따르고 서로 독립이면, \\(F=\\frac{U/n1}{V/n2}\\)는 F분포를 따름",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Statistics",
      "표본의 분포"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/04-2.html#ddl-data-definition-language",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/04-2.html#ddl-data-definition-language",
    "title": "SQL",
    "section": "DDL (Data Definition Language)",
    "text": "DDL (Data Definition Language)\n\nCREATE (database, tables, views, indexes)\nALTER: modify columns / constraints\nDROP (database, tables, views, indexes)\nTRUNCATE: delete table data while keeping structure.\nMS Access에서는 지원하지 않음 =&gt; DELETE FROM table\n\nCREATE TABLE student (\n    id INT NOT NULL,\n    CourseID INT NOT NULL,\n    Name VARCHAR(100) UNIQUE, # unique는 자동으로 index 생성\n    Age INT,\n    CONSTRAINT STUDENT_PK PRIMARY KEY (id),\n    CONSTRAINT \n    COURSE_FK FOREIGN KEY (CourseID) \n    REFERENCES Course(CourseID) \n    ON UPDATE CASACADE \n    ON DELETE NO ACTION\n);\nALTER TABLE student ADD COLUMN major VARCHAR(100);\nALTER TABLE student ADD CONSTRAINT STUDENT_FK FOREIGN KEY (CourseID) REFERENCES Course(CourseID) ON DELETE CASCADE;\nALTER TABLE student ADD CONSTRAINT AGE_CHECK CHECK (Age &gt; 0);\nALTER TABLE student DROP CONSTRAINT AGE_CHECK;\nDROP TABLE student;\nTRUNCATE TABLE student;\n\nCREATE VIEW [view name] AS SELECT * FROM student;\n\nDML (Data Manipulation Language)\nINSERT INTO student VALUES (1, 'Alice', 20);\nUPDATE student SET age = 21, Name = 'babo' WHERE id = 1;\nDELETE FROM student WHERE id = 1;\n\n\nDQL (Data Query Language)\nA query create temporarily a new table.\nthis allows a query to create a new relation and feed information to another query as a subquery\nSELECT * FROM student;\nSELECT name \nFROM student \nWHERE age &gt; 20\nORDER BY name DESC, age ASC;\nSELECT DISTINCT name FROM student;\nSELECT name, age FROM student WHERE Age &gt; (SELECT AVG(Age) FROM student);\n\n\nJOIN\n\ninner join(equijoin)\n\nexplicit join: FROM table1 INNER JOIN table2 ON table1.id = table2.id\n(MS Access에서는 INNER를 명시해야됨)\nimplicit join: FROM table1, table2 WHERE table1.id = table2.id\n\nouter join\n\nleft outer join: FROM table1 LEFT JOIN table2 ON table1.id = table2.id\nright outer join: FROM table1 RIGHT JOIN table2 ON table1.id = table2.id",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "SQL"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#entity",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#entity",
    "title": "The Relational Model",
    "section": "entity",
    "text": "entity\na formal name for a thing that is being tracked one theme or topic (just single table)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#relation",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#relation",
    "title": "The Relational Model",
    "section": "Relation",
    "text": "Relation\n\na two-dimensional table that has specific charateristics\nCell of the table hold single value\nAll entries in a column are of the same kind\nNo two rows in a table are identical",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#domain-cartesian-product",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#domain-cartesian-product",
    "title": "The Relational Model",
    "section": "domain & cartesian product",
    "text": "domain & cartesian product\n\ndomain: set of possible values for a column\ncartesian product: set of all possible combinations of rows from two tables",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#presenting-relation-structures",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#presenting-relation-structures",
    "title": "The Relational Model",
    "section": "Presenting Relation Structures",
    "text": "Presenting Relation Structures\nRELATION_NAME(PrimaryKey, ForeignKey, ColumnName, …)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#key",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#key",
    "title": "The Relational Model",
    "section": "key",
    "text": "key\n\nidentify a row\nUnique Key(Primary Key)\nNonUnique Key(Foreign Key)\nComposite Key: Primary key가 두개 이상. Surrogate Key로 대체되곤 함.\nCandidate Key: unique한 columns\nSurrogate Key: 자동으로 할당되는 일련번호\nIDENTITY (start, increment)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#referential-integrity-constraint",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#referential-integrity-constraint",
    "title": "The Relational Model",
    "section": "Referential Integrity Constraint",
    "text": "Referential Integrity Constraint\n\n모든 foriegn key는 존재하는 primary key와 매칭되야한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#null-values",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/03.html#null-values",
    "title": "The Relational Model",
    "section": "Null values",
    "text": "Null values\n\nrequired, allow nulls 설정으로 null값을 허용할지 결정",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "The Relational Model"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/04-1.html#normalization",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/04-1.html#normalization",
    "title": "Database Normalization",
    "section": "Normalization",
    "text": "Normalization\n\nprocess of organizing a database to reduce redundancy problem and improve data integrity",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/04-1.html#functional-dependency",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/04-1.html#functional-dependency",
    "title": "Database Normalization",
    "section": "Functional Dependency",
    "text": "Functional Dependency\n\n하나의 atrribute가 다른 attribute의 value를 결정하는지 여부를 판단\nwell formed인지 판별할 수 있는 기준\nA(Determinant) -&gt; B(dependent): A가 결정되면 B도 결정된다면 B는 A에 함수적 종속\nEvery determinant must be a Candidate Key",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/04-1.html#normalization-process",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/04-1.html#normalization-process",
    "title": "Database Normalization",
    "section": "Normalization Process",
    "text": "Normalization Process\n\nBCFNF: Boyce-Codd Normal Form =&gt; Each relation has only one theme\n\n\nIdentify all the Candidate Keys.\nIdentify all the Functional Dependencies.\nExamine the determinants of the functional dependencies\n\nplace the columns of the functional dependency in a new relation of their own\nmake the determinant of the functianl dependency the primary key of the new relation\nLeabe a copy of the determinant as a foreign key in the original relation\ncreate a referential integrity constraint between the original and new relation\n\nRepeat the process until every determinant of every relation is a candidate key",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Normalization"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/11.html#concurrency-control",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/11.html#concurrency-control",
    "title": "Database Administration",
    "section": "Concurrency control",
    "text": "Concurrency control\nEnsuring that one user’s work does not inappropriately influence another user’s work\n\nStrict concurrency control requires locking the database, 다른 사용자의 동시 사용 허가 x\nLower concurrency control allows more throughput\n\n\nTransactions\nUsers submit Transactions(LUWs)\n\nAtmomic Transaction: 데이터베이스에서 일련의 작업들이 모두 성공적으로 수행되거나, 그렇지 않을 경우 작업이 전혀 수행되지 않아 데이터베이스가 변경되지 않는 상태를 유지하는 트랜잭션\n→ Before committed, all LUWs must be successfully completed, or rollback\nConcurrent Transactions: 여러 트랜잭션이 동시에 실행되는 것\n\nLost update problem: 두 트랜잭션이 동시에 같은 데이터를 수정할 때, 하나의 트렌잭션이 다른 트랜잭션의 변경을 덮어쓰는 문제\nInconsistent read problem: 한 트랜잭션이 데이터를 읽는 도중 다른 트랜잭션이 데이터를 수정하는 문제\n\nDirty read: commit 되기 이전에 수정된 데이터를 읽는 것. 만약 rollback이 될 경우 문제가 발생.\nNonrepeatable read: 데이터를 두 번 읽었는데 commit된 transaction 때문에 값이 다른 경우\nPhantom read: 데이터를 두 번 읽었는데 commit된 transaction 때문에 새로운 row가 추가된 경우\n\nResource locking\n\nImplicit locks: DBMS가 자동으로 수행하는 lock\nExplicit locks\nLOCK TABLES table_name READ -- or WRITE\nUNLOCK TABLES\nExclusive locks: 다른 트랜잭션에서 읽기/쓰기 불가\nShared locks: 다른 트랜잭션에서 읽기 가능, 쓰기 불가\nrock granularity: row-level vs table-level vs database-level\n\n\nSerializable Transactions: 가장 강력한 격리 수준 보장\n\nTwo-pase locking(2PL): growing phase와 shrinking phase로 나뉨\n\nACID Transaction\n\nAtomic: 성공한 transaction만 저장되어야 한다\nConsistent: 현재의 transaction이 마무리 되기 전 까지 record를 저장할 수 없다\n→ 트랜잭션의 살향 결과로 데이터베이스 상태가 모순되지 않음\nIsolated\n\nread uncommitted: 다른 트랜잭션에서 commit되지 않은 데이터도 읽을 수 있음\nread committed: 다른 트랜잭션이 commit된 데이터만 읽을 수 있음\nrepeatable read: 한 트랜잭션에서 하나의 스냅션만 사용\nserializable: 가장 강력한 격리 수준 보장\n\nDurable: 트랜잭션이 성공적으로 완료되면, 그 결과는 영구적으로 저장되어야 한다\n\n\n\n\n\nDeadlock / deadly embrace\n두 개 이상의 트랜잭션이 서로 unlock을 무한히 기다리는 상태\n\n\nlock\n\noptimistic locking\n\nassumption: No conflict will occur\nif no conflict occurs, the transaction is committed else it is rolled back and repeated\n\npessimistic locking\n\nassumption: Conflict will occur\nlock the data before the transaction starts\n\n\n\n\nCursor\nA cursor is a pointer into a set of rows that are the result set from an SQL SELECT statement\nDECLARE cursor_name CURSOR FOR SELECT column_name FROM table_name",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/11.html#backup-and-recovery",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/11.html#backup-and-recovery",
    "title": "Database Administration",
    "section": "Backup and recovery",
    "text": "Backup and recovery\n\nRecovery\n\nvia Reprocessing\nvia Rollback and Rollforward\n\nlog file transaction을 undo할 때, before-images가 존재해함. (rollback) transaction을 redo할 때, after-images가 존재해함.(rollforward)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/11.html#security",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/11.html#security",
    "title": "Database Administration",
    "section": "Security",
    "text": "Security\nonly authenticated users perform authorized activities\n\nAuthentication: User ID와 password를 사용하여 사용자를 인증\nAuthorization: user groups(roles): dbcreator, public, … sql  GRANT SELECT, INSERT, UPDATE, DELETE ON table_name TO user_name",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/11.html#database-performance",
    "href": "posts/03_archives/completed_project/bs_2_2/notes/bs_database/11.html#database-performance",
    "title": "Database Administration",
    "section": "Database Performance",
    "text": "Database Performance\n\nindex\ndisk mirroring: 데이터 복제 말씀하신 듯\nRAID\nSANs\nDistributed database: service cluster partitioned replicated\n\n\nDBA Responsibilities\n\nuser reported errors를 모아서 system이 잘 돌아갈 수 있게 해야함\ndatabase 설정을 잘 관리해야함\n문서화 잘 해야함\ncloud로 db 관리(service level agreement)",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리",
      "Notes",
      "Bs Database",
      "Database Administration"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/index.html",
    "href": "posts/03_archives/completed_project/bs_2_2/index.html",
    "title": "2학년 2학기 학부 정리",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2024-09-02\n        종료일: 2024-12-20\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        산업공학 학부",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/index.html#details",
    "href": "posts/03_archives/completed_project/bs_2_2/index.html#details",
    "title": "2학년 2학기 학부 정리",
    "section": "Details",
    "text": "Details\n산업정보시스템공학과 2학년 2학기 수강 과목들에 대한 개념 정리 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/index.html#tasks",
    "href": "posts/03_archives/completed_project/bs_2_2/index.html#tasks",
    "title": "2학년 2학기 학부 정리",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_2_2/index.html#related-posts",
    "href": "posts/03_archives/completed_project/bs_2_2/index.html#related-posts",
    "title": "2학년 2학기 학부 정리",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "2학년 2학기 학부 정리"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/index.html",
    "href": "posts/03_archives/completed_project/toeic_speaking/index.html",
    "title": "토익 스피킹 준비",
    "section": "",
    "text": "COMPLETED\n    \n    \n        시작일: 2025-07-19\n        종료일: 2025-07-26\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/index.html#details",
    "href": "posts/03_archives/completed_project/toeic_speaking/index.html#details",
    "title": "토익 스피킹 준비",
    "section": "Details",
    "text": "Details\n아 하기 싫다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/index.html#tasks",
    "href": "posts/03_archives/completed_project/toeic_speaking/index.html#tasks",
    "title": "토익 스피킹 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/toeic_speaking/index.html#참고-자료",
    "title": "토익 스피킹 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/toeic_speaking/index.html#related-posts",
    "href": "posts/03_archives/completed_project/toeic_speaking/index.html#related-posts",
    "title": "토익 스피킹 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "토익 스피킹 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/notes/1.html#second-brain의-핵심-기능",
    "href": "posts/03_archives/stored_categories/blog/notes/1.html#second-brain의-핵심-기능",
    "title": "Second Brain - 티아고 포르테",
    "section": "Second Brain의 핵심 기능",
    "text": "Second Brain의 핵심 기능\n\n아이디어를 구체화한다\n머릿속에서 아이디어를 분리하여 구체적인 형태로 만들어야 한다.\n아이디어 사이의 연관성을 새롭게 밝혀낸다.\n다양한 자료를 한곳에 보관하면 자료간 연결 작업이 촉진되며, 생각지 못한 연관성을 찾아낼 가능성을 높일 수 있다.\n시간을 두고 아이디어를 발전시킨다.\n사람들은 줄곧 아이디어를 떠올릴 때 최신 정보에 중요성을 더 부여하는 경향이 있다.\n몇년 간 축적된 아이디어를 마음껏 이용할 수 있다면 더 좋을 것이다.\n나만의 독특한 관점을 정교하게 다듬는다.\n작가의 벽에 부딪히는 것은 적절한 단어를 떠올릴 수 없다는 것이 아니라, 글을 쓸 탄약이 부족하다는 것이다.\n자신의 견해를 지지할 수 있는 자료를 지속적으로 모아야한다.\n\n\n머리는 아이디어를 생각하는 곳이지 보관하는 곳이어선 안된다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/notes/1.html#중요한-것을-기억하는-4-단계-code",
    "href": "posts/03_archives/stored_categories/blog/notes/1.html#중요한-것을-기억하는-4-단계-code",
    "title": "Second Brain - 티아고 포르테",
    "section": "중요한 것을 기억하는 4 단계 (CODE)",
    "text": "중요한 것을 기억하는 4 단계 (CODE)\n\nCapture: 공명하는 내용을 수집하라\n당신과 마음에 닿는 내용을 분별하여 보관하고 나머지는 버려라\nOrganazie: 실행을 목표로 정리하라\n실행을 염두에 두고 정리하라.\nDistill: 핵심을 찾아 추출하라\n메모의 요점을 정리하라.\n메모를 저장한 이유, 생각하던 내용, 무엇이 당신의 관심을 끌었는지에 대한 설명\nExpress: 작업한 결과물을 표현하라\n개인적이고 구체적이며 검증된 정보는 실제로 사용할 때에 비로소 지식이 된다.\n당신이 아는 내용을 다른 사람과 공유하기 전까지는 그저 이론에 불과하다.\n\n\nCapture\n미래에 어떻게 될지 전혀 모르는데 무엇을 저장할지 어떻게 결정할 수 있을까? 어떤 정보가 보관할 가치가 있는지 정확히 알아내도록 통찰력을 키우기 위해 리처드 파인만의 좋아하는 12가지 문제 방법을 제시한다. 자신에게 흥미를 불러일으키는 열린 질문들을 자유롭게 적어보자. 그후 해당 질문들을 학습의 방향을 제시하는 북극성으로 삼아 활용한다.\n\n\n\n\n\n\n직접 적어본 질문들\n\n\n\n\n쇠퇴하지 않는 사람이 되기 위해 꾸준히 해야하는 활동에는 어떤게 있을까?\n운에 좌절하지 않기 위해 어떤걸 준비해야 할까?\n학점을 잘 받으려면 어떻게 공부해야 할까?\n소중한 인연은 무엇인가?\n무엇을 위해 발전해야 하는가?\n시간이 지나도 가치있는건 무엇일까?\n공명하는 지식이 매번 진실일까?\n돈을 잘 벌려면 어떻게 해야할까?\n\n\n\n그런 다음, 해당 주제와 관련된 자료에서 아래의 기준에 해당하는 내용들을 선별한다. 수집하는 자료는 외부에 존재하는 자료뿐만 아니라 자료를 수집하면서 얻은 내면 세계의 아이디어 역시 그 대상이될 수 있다.\n\n영감을 불러일으키는가\n나와 내 일에 유용한가\n개인적인 정보인가\n가족이나 친구들과 나눈 문자 메세지들도 수집의 대상이 될 수 있다.\n놀랄 만한 사실인가\n기존의 알고있는 자료만 수집하면 확증편향의 위험이 있다.\nsecond brain은 이미 알고 있는 내용을 또 확인하는 방법이 되어서는 안 된다.\n\n다음과 같은 유형들은 보관하기에 적합하지 않다.\n\n민감한 정보\n포토샵 파일이나 비디오 영상처럼 전용 앱이 필요한 경우\n대용량 파일\n공동 편집이 필요한 경우\n\n\n\nOrganize\n수집한 자료를 정리할 때, 종류별로 나누지 않고, 얼마나 실행 가능한지에 따라 정리할 수 있다. 주제와 하위 항목으로 연달아 이루어진 복잡한 계층 체계에 따라 메모를 정리하는 대신, 이것은 어떤 프로젝트에 가장 도움이 될까?라는 간단한 질문 하나에만 답하면 된다.\n\nPARA\n\nProject: 일이나 생활에서 현재 진행 중이며 단기간 노력이 필요한 일\n시작과 끝이 존재. 완성, 승인, 착수, 발표처럼 구체적이고 확실한 결과가 있어야 한다.\nArea: 오랫동안 관리하고 싶고 장기적으로 책임지는 일\n정해진 종료 날짜와 최종 목표가 없음.\nResource: 향후 도움이 될 수 있는 주제 혹은 관심사\n현재 진행하는 프로젝트 혹은 영역과 관련 없는 자료, 당분간 실행할 수 없는 메모나 파일 등을 보관할 수 있다.\nArchive: 전에는 위의 세 가지 유형에 속했지만, 지금은 비활성화된 항목\n완료하거나 취소된 프로젝트, 이제는 관리하지 않는 책임 영역, 흥미를 잃은 자원 등을 보관할 수 있다.\n\nPARA 정리 방식은 부엌 정리 방식과 유사하다. 부엌에 있는 물건들은 전부 식사를 준비하도록 설계되고 정리된다. 각각의 상위 폴더들을 비유하면, archive는 냉동고, resource는 식료품 저장고, 영역은 냉장고, 프로젝트는 불 위에서 끓고 있는 냄비나 팬과 같다.\n부엌을 음식 종류에 따라 정리하면 얼마나 터무니없을지 상상해보라. 신선한 과일과 말린 과일, 과일 주스와 냉동 과일은 모두 과일로 만들었다는 이유로 같은 장소에 보관될 것이다. 그런데 이것이 바로 대부분의 사람들이 파일과 메모를 정리하는 방식이다. 책을 읽으며 메모했다는 이유만으로 책 메모는 책 메모끼리, 다른 사람의 말을 인용했다는 이유만으로 인용문은 인용문끼리 보관한다.\n\n\n\nDistill\n메모는 단순한 수집을 넘어 실제 활용이 가능한 형태로 정제되어야 한다. 이를 위해 다음과 같은 단계별 요약 과정을 거친다.\n\n메모 수집: 먼저 빠르게 수집, 정리 이후 정제는 나중에 진행\n굵게 처리: 중요한 문장이나 구절을 표시\n하이라이트 처리: 굵게 처리된 내용 중 핵심을 강조\n핵심 요약: 최종적으로 메모의 핵심을 추출\n\n이 과정에서 주의할 점들\n\n과다 하이라이트 처리: 이전 단계 내용의 10-20% 정도만 선별\n목적 없는 하이라이트 처리: 무작정 시작하지 않고, 메모를 어떻게 사용할지 알게 될 때까지 기다린 후, 필요에 따라 하이라이트 한다.\n어려운 방식의 하이라이트 처리: 본인의 직관에 맞게 흥미로운 구절들을 하이라이트 한다.\n\n\n\n\n\n\n\n메모의 생존 여부는 ’얼마나 쉽게 찾을 수 있는가’에 달려있다.\n\n\n\n\n\nExpress\n맡은일을 중간 단계로 나누어서 최대한 빠르게 결과물을 도출하라 도출한 작업물들을 중간단계로써 다른 프로젝트에 사용할 때, 도움을 얻을 수 있다. 도출한 결과물들을 다른사람들과 공유를 해서 피드백을 받아라",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/notes/1.html#창조력을-완성하는-과정",
    "href": "posts/03_archives/stored_categories/blog/notes/1.html#창조력을-완성하는-과정",
    "title": "Second Brain - 티아고 포르테",
    "section": "창조력을 완성하는 과정",
    "text": "창조력을 완성하는 과정\n언제든 참신한 아이디어를 떠올릴 수 있다고 기대해서는 안된다. 혁신과 문제 해결은 흥미로운 아이디어를 체계적으로 불러일으켜 우리가 인식하게 하는 일상에 달려 있다.\n세컨드브레인은 창의적인 과정들을 아이디어 수집, 정리, 핵심 추출, 조립의 단계로 표준화하여 우리의 뇌 활동을 돕는다.\n\n창의적인 프로젝트를 완료할 때 도움이 되는 전략\n\n아이디어 군도: 프로젝트 수행에 필요한 모든 문서를 모은다. 그리고 해당 문서들을 연결하라.\n헤밍웨이 다리: 현재 진행중인 프로젝트에서 다음과 같은 사항들을 메모에 기록하라.\n\n다음 단계에는 어떤 이야기를 쓸 지\n현재 상황\n잊어버리기 쉬운 세부 사항\n다음 작업 시간의 목표\n\n범위 조금씩 축소하기: 프로젝트의 복잡한 문제가 드러나면 과감하게 범위를 축소하라\n\n\n\n효율적인 실행을 위한 세 가지 습관\n\n\n\n\n\n\n정리정돈은 타고난 특성이 아닌 습관이다.\n\n\n\n\n체크리스트 습관\n\n수집: 프로젝트에 대한 내 생각을 수집하라\n\n이 프로젝트에 대해 이미 알고 있는 것은 무엇인가?\n알아내야 하지만 아직 모르는 것은 무엇인가?\n목표나 목적은 무엇인가?\n통찰력을 얻으려면 누구와 대화해야 하는가?\n아이디어를 얻으려면 어떤 것을 읽거나 들어야 하는가?\n\n검토: 관련 메모가 있을 만한 폴더나 태그를 검토하라\n검색: 모든 폴더에서 관련 용어를 검색하라\n이동: 관련 메모를 프로젝트 폴더로 이동하거나 태그를 설정하라\n작성: 수집한 메모로 개요를 작성하고 프로젝트를 계획하라\n\n\n\n리뷰 습관\n\n주간 리뷰\n일주일 동안 작업한 모든 메체의 메모를 검토하라 그리고 이번 주 할 과제를 정하라\n월간 리뷰\n솔직히 이런건 잘 안할거 같다.\n\n\n\n알아차리는 습관",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog",
      "Notes",
      "Second Brain - 티아고 포르테"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/hadoop/notes/00.html",
    "href": "posts/03_archives/stored_categories/hadoop/notes/00.html",
    "title": "Hadoop Ecosystem",
    "section": "",
    "text": "HDFS: Hadoop Distributed File System\n\n클러스터의 하드 드라이브를 하나의 거대한 파일 시스템으로 통합\n자동 복제 및 장애 조치 기능 제공\n\nYARN: Yet Another Resource Negotiator\n\n클러스터의 리소스를 관리하고 작업을 스케줄링\n\nMapReduce: 데이터 처리 모델\n\n데이터를 분할하고 병렬로 처리하는 프레임워크\nMap 단계에서 데이터를 필터링하고 정렬, Reduce 단계에서 집계 및 요약\n\nPig: 데이터 흐름 언어\n\nSQL과 유사한 스크립트 언어로 MapReduce / TEZ를 위한 데이터 처리 작업을 작성\n\nHive: SQL과 유사한 쿼리 언어\n\n대규모 데이터 세트에 대한 쿼리 및 분석을 위한 SQL 인터페이스 제공\n\nAmbari: 데이터 시각화 도구\n\nHadoop 클러스터에서 데이터를 시각화하고 대시보드를 생성\n\nHBase: NoSQL 데이터베이스\n\n대규모 데이터 세트를 실시간으로 읽고 쓸 수 있는 분산형 데이터베이스\n\nstorm: 실시간 데이터 처리\n\n스트림 데이터를 실시간으로 처리하고 분석하는 프레임워크\n\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Hadoop",
      "Notes",
      "Hadoop Ecosystem"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/vault/notes/0_overview.html#how-vault-encrypt-data",
    "href": "posts/03_archives/stored_categories/vault/notes/0_overview.html#how-vault-encrypt-data",
    "title": "Overview",
    "section": "how vault encrypt data",
    "text": "how vault encrypt data",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "vault",
      "Notes",
      "Overview"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/kaggle/notes/titanic/00.html#data-이해",
    "href": "posts/03_archives/stored_categories/kaggle/notes/titanic/00.html#data-이해",
    "title": "titanic",
    "section": "Data 이해",
    "text": "Data 이해\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('_data/train.csv')\ntest = pd.read_csv('_data/test.csv')\n\n\ntrain.describe()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\nAge 결측치 177개\n\n\ntest.describe()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n418.000000\n418.000000\n332.000000\n418.000000\n418.000000\n417.000000\n\n\nmean\n1100.500000\n2.265550\n30.272590\n0.447368\n0.392344\n35.627188\n\n\nstd\n120.810458\n0.841838\n14.181209\n0.896760\n0.981429\n55.907576\n\n\nmin\n892.000000\n1.000000\n0.170000\n0.000000\n0.000000\n0.000000\n\n\n25%\n996.250000\n1.000000\n21.000000\n0.000000\n0.000000\n7.895800\n\n\n50%\n1100.500000\n3.000000\n27.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n1204.750000\n3.000000\n39.000000\n1.000000\n0.000000\n31.500000\n\n\nmax\n1309.000000\n3.000000\n76.000000\n8.000000\n9.000000\n512.329200\n\n\n\n\n\n\n\n\nAge 결측치 86개\nFare 결측치 1개: 이 정도는 그냥 삭제해도 될듯\n\n\nplt.figure(figsize=(15, 10))\n\n# Age 분포 확인\nplt.subplot(2, 2, 1)\nsns.boxplot(x='Survived', y='Age', data=train)\nplt.title('Age 분포 (생존 여부별)')\n\n# Fare 분포 확인\nplt.subplot(2, 2, 2)\nsns.boxplot(x='Survived', y='Fare', data=train)\nplt.title('Fare 분포 (생존 여부별)')\n\n# Pclass에 따른 Age 분포\nplt.subplot(2, 2, 3)\nsns.boxplot(x='Pclass', y='Age', data=train)\nplt.title('Age 분포 (객실 등급별)')\n\n# Pclass에 따른 Fare 분포\nplt.subplot(2, 2, 4)\nsns.boxplot(x='Pclass', y='Fare', data=train)\nplt.title('Fare 분포 (객실 등급별)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain_x = train.drop('Survived', axis=1).values\ntrain_y = train['Survived'].values\ntest_x = test.values",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Kaggle",
      "Notes",
      "Titanic",
      "titanic"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/08.html#dot-product",
    "href": "posts/03_archives/stored_categories/선형대수/notes/08.html#dot-product",
    "title": "vector dot product, cross product",
    "section": "Dot Product",
    "text": "Dot Product\n\n\\(v ⋅ w = \\sum_{i=1}^{n} v_i w_i\\)\n\n\nProperties\n\n\\(v ⋅ w = w ⋅ v\\)\n\\(v ⋅ (w + u) = v ⋅ w + v ⋅ u\\)\n\\(v ⋅ (c w) = c (v ⋅ w)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/08.html#length-of-vector",
    "href": "posts/03_archives/stored_categories/선형대수/notes/08.html#length-of-vector",
    "title": "vector dot product, cross product",
    "section": "Length of vector",
    "text": "Length of vector\n\n\\(||v|| = \\sqrt{v ⋅ v}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/08.html#some-properties",
    "href": "posts/03_archives/stored_categories/선형대수/notes/08.html#some-properties",
    "title": "vector dot product, cross product",
    "section": "Some properties",
    "text": "Some properties\nfor non-zero vectors \\(v\\) and \\(w\\):\n\ncauchy-schwarz inequality\n\n\\(|v ⋅ w| ≤ ||v|| ||w||\\)\n\\(|v ⋅ w| = ||v|| ||w||\\) ⟺ \\(v = cw\\)\n\n\n\nTriangle inequality\n\n\\(||v + w|| ≤ ||v|| + ||w||\\)\n\n\n\nAngle between vectors\n\n\\(cosθ = \\frac{v ⋅ w}{||v|| ||w||}\\)\nif \\(v\\) and \\(w\\) are orthogonal, then \\(v ⋅ w = 0\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/08.html#cross-product",
    "href": "posts/03_archives/stored_categories/선형대수/notes/08.html#cross-product",
    "title": "vector dot product, cross product",
    "section": "Cross Product",
    "text": "Cross Product\n\nonly for 3D vectors\nget a vector that is orthogonal to both \\(v\\) and \\(w\\)\n\\(v × w = (v_2 w_3 - v_3 w_2, v_3 w_1 - v_1 w_3, v_1 w_2 - v_2 w_1)\\)\n\\(sinθ = \\frac{||v × w||}{||v|| ||w||}\\)\n\\(v × w = 0\\) ⟺ \\(v\\) and \\(w\\) are parallel\nv와 w로 이루어진 평행사변형의 넓이는 \\(||v × w||\\)이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/08.html#triple-productlagrange-identity",
    "href": "posts/03_archives/stored_categories/선형대수/notes/08.html#triple-productlagrange-identity",
    "title": "vector dot product, cross product",
    "section": "Triple product(lagrange identity)",
    "text": "Triple product(lagrange identity)\n\n\\(a x (b x c) = b(a ⋅ c) - c(a ⋅ b)\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/08.html#차원의-평면에서-직선의-방정식",
    "href": "posts/03_archives/stored_categories/선형대수/notes/08.html#차원의-평면에서-직선의-방정식",
    "title": "vector dot product, cross product",
    "section": "3차원의 평면에서 직선의 방정식",
    "text": "3차원의 평면에서 직선의 방정식\n\n평면에 대한 법선벡터 \\((a, b, c)\\)와 평면 위의 한 점 \\((x_p, y_p, z_p)\\)이 주어졌을 때, 평면의 방정식은 다음과 같다.\n\\(ax + by + cz = D\\), \\(D = ax_p + by_p + cz_p\\)\n평행한 평면은 a, b, c의 계수가 같은 평면이다.\n\\(\\frac{Ax_0 + By_0 + Cz_0 + D}{\\sqrt{A^2 + B^2 + C^2}}\\)은 평면과 점 \\((x_0, y_0, z_0)\\) 사이의 거리이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "vector dot product, cross product"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/11.html#prerequest-for-linear-transformation",
    "href": "posts/03_archives/stored_categories/선형대수/notes/11.html#prerequest-for-linear-transformation",
    "title": "linear transformations",
    "section": "PreRequest for linear transformation",
    "text": "PreRequest for linear transformation\n\nT(a + b) = T(a) + T(b)\nT(ca) = cT(a)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/11.html#transformation",
    "href": "posts/03_archives/stored_categories/선형대수/notes/11.html#transformation",
    "title": "linear transformations",
    "section": "Transformation",
    "text": "Transformation\n\n표준 기저벡터가 어디로 이동하는지에 따라 변환을 정의한다.\nθ만큼 회전하는 A in \\(R^2\\)::\n\n\\(A = \\begin{bmatrix} \\cos θ & -\\sin θ \\\\ \\sin θ & \\cos θ \\end{bmatrix}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/11.html#projection",
    "href": "posts/03_archives/stored_categories/선형대수/notes/11.html#projection",
    "title": "linear transformations",
    "section": "Projection",
    "text": "Projection\n\n\\(proj_L(x) = \\frac{x ⋅ v}{u ⋅ v} v\\)\n\nif v is unit vecotr, then \\(proj_L(x) = (x ⋅ v) v\\)\n\\(A = \\begin{bmatrix} μ_1^2 & μ_2μ_1 \\\\ μ_1μ_2 & μ_2^2 \\end{bmatrix}\\)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/11.html#합성곱",
    "href": "posts/03_archives/stored_categories/선형대수/notes/11.html#합성곱",
    "title": "linear transformations",
    "section": "합성곱",
    "text": "합성곱\n\n선형 변환의 합성곱 역시 선형 변환: 합성곱을 Ax로 표현 가능.\n행렬의 곱은 결합법칙이 성립한다.\n행렬의 곱은 교환법칙이 성립하지 않는다.\n행렬의 곱은 분배법칙이 성립한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "linear transformations"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/00.html#what-is-linear-algebra",
    "href": "posts/03_archives/stored_categories/선형대수/notes/00.html#what-is-linear-algebra",
    "title": "what is linear algebra",
    "section": "what is linear algebra",
    "text": "what is linear algebra\n선형 방정식을 matrix와 vector로 표현해서 다루는 수학\n\\(ax^2 + bx + c = 0\\) (x)\n\\(ax_1 + bx_2 + c = 0\\) (0)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/00.html#what-is-vector",
    "href": "posts/03_archives/stored_categories/선형대수/notes/00.html#what-is-vector",
    "title": "what is linear algebra",
    "section": "what is vector",
    "text": "what is vector\nvector는 크기(magnitude)와 방향(direction)을 가지고 있다.\n2, 3, 4 차원 너머를 수학적으로 표현할 수 있다.\nvector는 수학적으로, 아래와 같이 표현할 수 있다.\n\\[\n\\vec{v} =\n\\begin{bmatrix}\n3 \\\\\n4\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/00.html#example",
    "href": "posts/03_archives/stored_categories/선형대수/notes/00.html#example",
    "title": "what is linear algebra",
    "section": "Example",
    "text": "Example\n\\[\\begin{aligned}\nx + 2y \\quad  &= 4 \\\\\n2x + 5y \\quad &= 9\n\\end{aligned}\\]\n위의 연립 1차 방정식을 matrix와 vector로 표현해보자\n\\[\n\\underset{A}{\\begin{bmatrix}\n1 & 2 \\\\\n2 & 5\n\\end{bmatrix}}\n\\underset{x}{\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}} =\n\\begin{bmatrix}\n1x + 2y \\\\\n2x + 5y\n\\end{bmatrix} =\n\\underset{b}{\\begin{bmatrix}\n4 \\\\\n9\n\\end{bmatrix}}\n\\]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "what is linear algebra"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/12.html#함수의-역",
    "href": "posts/03_archives/stored_categories/선형대수/notes/12.html#함수의-역",
    "title": "역함수와 역변환",
    "section": "함수의 역",
    "text": "함수의 역\n\n\\(f\\) is invertible ⟺ ∀y ∈ Y, ∃!x ∈ X such that \\(f(x) = y\\), 즉 f는 전단사 함수이다.\n\n전사: \\(f(R^n) -&gt; R^m, f(x) = Ax, C(A) = R^M\\), 즉 rank(A) = m\n단사: N(A) = {0}, 즉 Ax = 0의 해는 유일하다. = C(A)가 linearly independent이다.\n즉, A는 정방행렬이다.\n즉, A의 기약행렬은 I이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/12.html#함수의-해집합",
    "href": "posts/03_archives/stored_categories/선형대수/notes/12.html#함수의-해집합",
    "title": "역함수와 역변환",
    "section": "함수의 해집합",
    "text": "함수의 해집합\n\nAx = b의 해집합은 Ax = 0의 해집합(Null space)을 특수해 만큼 평행 이동한 것과 같다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/12.html#행렬식",
    "href": "posts/03_archives/stored_categories/선형대수/notes/12.html#행렬식",
    "title": "역함수와 역변환",
    "section": "행렬식",
    "text": "행렬식\n\n선형 변환 시 부피의 변화율과 방향을 나타낸다. (기존 부피에 행렬식의 절댓값을 곱한 것이 새로운 부피)\nn x n: row를 정해서, 그 row를 기준으로, \\(a_{row 1}A_{row 1} - a_{row 2}A_{row 2} + ... + a_{row n}A_{row n}\\)의 형태로 나타낼 수 있다.\n부호는 checkerboard pattern을 따른다.\n행렬의 scalar 곱은, 행렬식의 \\(\\text{scalar}^n\\)\n행렬의 특정 row끼리 더한 행렬의 행렬식은 두 행렬식의 합과 같다.\nduplicated row는 행렬식이 0이 된다. 즉, 역행렬이 존재하지 않는다.\n\n따라서 한 행에 상수배를 해서 다른 행에 더해도 행렬식은 변하지 않는다.\n\n상삼각행렬의 행렬식은 대각선 원소의 곱이다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "역함수와 역변환"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/13.html",
    "href": "posts/03_archives/stored_categories/선형대수/notes/13.html",
    "title": "전치행렬",
    "section": "",
    "text": "전치행렬의 행렬식은 원래 행렬의 행렬식과 같다.\n\\((ABC)^T = C^T B^T A^T\\)\n\\((A + B)^T = A^T + B^T\\)\n\\((A^{-1})^T = (A^T)^{-1}\\)\n\\(C(A^T) = N(A)^⟂\\)\n\\(N(A^T) = C(A)^⟂\\)\nrank(A) + nullity(Aᵀ) = n\n\\(P=A(A^TA)^{−1}A^T\\): projection matrix\n\\(Px\\)는 x를 A의 column space로 projection한 것이다.\n\\(A^TAx = A^Tb\\): || Ax = b ||의 최소제곱해를 구하는 식\n닮음 변환: \\(D=C^{-1}AC\\) (C는 invertible matrix)\n\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "전치행렬"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/05.html#선형결합",
    "href": "posts/03_archives/stored_categories/선형대수/notes/05.html#선형결합",
    "title": "선형결합과 생성",
    "section": "선형결합",
    "text": "선형결합\n벡터들의 상수배 합으로 만들 수 있는 벡터의 집합",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "선형결합과 생성"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/notes/06.html#linear-independence",
    "href": "posts/03_archives/stored_categories/선형대수/notes/06.html#linear-independence",
    "title": "linear independence",
    "section": "Linear independence",
    "text": "Linear independence\n\nDefinition\n\nDependence: one of the vectors in the set can be written as a linear combination of the others.\nIndependence: ⫬ dependence\n\n\n\nTheorem\nS = \\({v_1, v_2, ..., v_n}\\)\n\\(S\\) is linearly dependent ⟺ ∃(\\(c_i\\) is not 0) \\(c_1v_1 + c_2v_2 + ... + c_nv_n = 0\\) is \\(c_1 = c_2 = ... = c_n = 0\\).\n\nif \\(c_1 = c_2 = ... = c_n = 0\\), then \\(S\\) is linearly independent.\n벡터의 원소 수가 \\(n\\)인 경우, span(\\(S\\))의 차원은 \\(n\\) 이하이다. (나머지 벡터는 선형 결합으로 표현 가능)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수",
      "Notes",
      "linear independence"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/index.html",
    "href": "posts/03_archives/stored_categories/선형대수/index.html",
    "title": "선형대수",
    "section": "",
    "text": "선형대수를 공부해봅시다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/index.html#details",
    "href": "posts/03_archives/stored_categories/선형대수/index.html#details",
    "title": "선형대수",
    "section": "",
    "text": "선형대수를 공부해봅시다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/index.html#tasks",
    "href": "posts/03_archives/stored_categories/선형대수/index.html#tasks",
    "title": "선형대수",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/선형대수/index.html#참고-자료",
    "title": "선형대수",
    "section": "참고 자료",
    "text": "참고 자료\n\nKhan Academy 강의\n3Blue1Brown 강의",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/선형대수/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/선형대수/index.html#related-posts",
    "title": "선형대수",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "선형대수"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/helm/notes/00.html",
    "href": "posts/03_archives/stored_categories/helm/notes/00.html",
    "title": "개요",
    "section": "",
    "text": "Chart.yml 파일을 통해 Helm 차트의 메타데이터를 정의할 수 있습니다. 이 파일은 Helm 차트의 이름, 버전, 설명, 라이선스 등의 정보를 포함합니다. 또한, 차트가 의존하는 다른 차트나 리소스에 대한 정보도 포함할 수 있습니다.\ntemplate/ 디렉토리는 Helm 차트의 템플릿 파일을 포함합니다. 이 템플릿 파일은 Kubernetes 리소스를 생성하는 데 사용됩니다. Helm은 이 템플릿 파일을 렌더링하여 실제 Kubernetes 리소스를 생성합니다.\nvalues.yml 파일은 Helm 차트의 기본값을 정의합니다. 이 파일은 Helm 차트를 설치할 때 사용자가 제공할 수 있는 값들을 포함합니다. 사용자는 이 파일을 수정하여 Helm 차트의 동작을 사용자 정의할 수 있습니다.\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Helm",
      "Notes",
      "개요"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/terraform/notes/tfc/00.html#what-is-terraform-cloud",
    "href": "posts/03_archives/stored_categories/terraform/notes/tfc/00.html#what-is-terraform-cloud",
    "title": "Terraform Cloud",
    "section": "What is Terraform Cloud?",
    "text": "What is Terraform Cloud?\n\nTerraform Open-Source를 확장해주는 서비스\n\n\n\n\nTerraform Open-Source의 한계\n\n\n\n기존의 terraform을 대규모 팀 단위에서 사용하기엔 무리가 있음 → TFC\non-premise 환경을 위한 Terraform Enterpise 서비스도 존재함.\nTACOS: Terraform Automation & Collaboration Software",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/terraform/notes/tfc/00.html#what-is-organization",
    "href": "posts/03_archives/stored_categories/terraform/notes/tfc/00.html#what-is-organization",
    "title": "Terraform Cloud",
    "section": "What is Organization?",
    "text": "What is Organization?\n\nworkspaces, policies, terraform modules를 공유하는 공간\n\n\n\n\nOrganization level에서 모든 setting이 이루어짐\n\n\n\n하나의 조직을 운용하는 것이 일반적이나, 조직 구조에 따라 여러 조직을 생성해서 운용할 수 있다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/terraform/notes/tfc/00.html#authenticating-to-tfc",
    "href": "posts/03_archives/stored_categories/terraform/notes/tfc/00.html#authenticating-to-tfc",
    "title": "Terraform Cloud",
    "section": "Authenticating to TFC",
    "text": "Authenticating to TFC\n\nweb interface\nCLI\n\n\nToken\n\nUser Tokens\nTeam Tokens: CI/CD pipeline에 주로 사용됨\nOrganization Tokens",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Terraform",
      "Notes",
      "Tfc",
      "Terraform Cloud"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html",
    "href": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "- master nodes: manage the worker nodes and the pods in the cluster - etcd: key-value store for all cluster data - kube-scheduler: schedules pods to worker nodes - kube-controller-manager: runs controller processes - replication controller: ensures that the correct number of pods are running - node controller: monitors the nodes - worker nodes: host the pods that are the components of the application - kubelet: communicates with the master node - kube-proxy: forwards requests to the correct pod\n\n\n  - initially, k8s was built on top of docker - gradually, k8s started supporting other container runtimes like containerd, cri-o, etc. and built a container runtime interface (CRI) to support multiple container runtimes - docker was not designed to be a container runtime, it was designed to be a container engine so it has a lot of features that are not needed by k8s and removed.\n\n\n\n\nkey-value store for all cluster data\nstores nodes, pods, configs, secrets, accounts, roles, bindings, etc.\n\n\n\n\n\n\n\nkube-api-server\n\n\n\n\n\n\n\n\n\n\n\n\nmust be installed on every node in the cluster manually ## kube-proxy\nkubeadm automatically installs kube-proxy on every node using daemonset\nwhen a service is created, kube-proxy creates a set of iptables rules to forward traffic to the correct pod",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html#docker-vs-containerd",
    "href": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html#docker-vs-containerd",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "- initially, k8s was built on top of docker - gradually, k8s started supporting other container runtimes like containerd, cri-o, etc. and built a container runtime interface (CRI) to support multiple container runtimes - docker was not designed to be a container runtime, it was designed to be a container engine so it has a lot of features that are not needed by k8s and removed.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html#etcd",
    "href": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html#etcd",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "key-value store for all cluster data\nstores nodes, pods, configs, secrets, accounts, roles, bindings, etc.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html#kube-api-server",
    "href": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html#kube-api-server",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "kube-api-server",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html#kubelet",
    "href": "posts/03_archives/stored_categories/k8s/notes/0_core_concept.html#kubelet",
    "title": "k8s cluster architecture",
    "section": "",
    "text": "must be installed on every node in the cluster manually ## kube-proxy\nkubeadm automatically installs kube-proxy on every node using daemonset\nwhen a service is created, kube-proxy creates a set of iptables rules to forward traffic to the correct pod",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "k8s cluster architecture"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/2_logging_monitoring.html",
    "href": "posts/03_archives/stored_categories/k8s/notes/2_logging_monitoring.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "in-memmory solution.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/2_logging_monitoring.html#metrics-server",
    "href": "posts/03_archives/stored_categories/k8s/notes/2_logging_monitoring.html#metrics-server",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "in-memmory solution.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/2_logging_monitoring.html#cadvisor",
    "href": "posts/03_archives/stored_categories/k8s/notes/2_logging_monitoring.html#cadvisor",
    "title": "김형훈의 학습 블로그",
    "section": "cAdvisor",
    "text": "cAdvisor\n\ncontainer advisor\nsub-component of kubelet\ncollects, aggregates, processes, and exports information about running containers",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "metrics server"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/6_network.html",
    "href": "posts/03_archives/stored_categories/k8s/notes/6_network.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s uses coreDNS to provide DNS service",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/6_network.html#core-dns",
    "href": "posts/03_archives/stored_categories/k8s/notes/6_network.html#core-dns",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s uses coreDNS to provide DNS service",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/6_network.html#network-plugin",
    "href": "posts/03_archives/stored_categories/k8s/notes/6_network.html#network-plugin",
    "title": "김형훈의 학습 블로그",
    "section": "network plugin",
    "text": "network plugin\n\nbridge type network\n - all container runtime solutions use same bridge script - and you can use third party plugins like flannel, calico, weave, etc.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "core DNS"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/4_security.html",
    "href": "posts/03_archives/stored_categories/k8s/notes/4_security.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s does not support user authentication by default (except service accounts)\n\n\n\n\nk8s uses TLS to secure communication between components \nuser can grouped by certificate’s Common Name or Organization field\nnode’s group name is system:nodes\n\n\n\n\n\n~/.kube/config file is used to store k8s cluster information\nkubectl uses this file to connect to the cluster\nclusters, users, context",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/4_security.html#authentication",
    "href": "posts/03_archives/stored_categories/k8s/notes/4_security.html#authentication",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "k8s does not support user authentication by default (except service accounts)\n\n\n\n\nk8s uses TLS to secure communication between components \nuser can grouped by certificate’s Common Name or Organization field\nnode’s group name is system:nodes\n\n\n\n\n\n~/.kube/config file is used to store k8s cluster information\nkubectl uses this file to connect to the cluster\nclusters, users, context",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/4_security.html#authorization",
    "href": "posts/03_archives/stored_categories/k8s/notes/4_security.html#authorization",
    "title": "김형훈의 학습 블로그",
    "section": "Authorization",
    "text": "Authorization\n\nAPI groups\n\nk8s API is divided into groups\ncore group is the default group\ngroup has its own set of resources and verbs \n\n\n\nRBAC\n\ncreate Role object (namespace scoped resources)\ncreate RoleBinding object\n\nor\n\ncreate ClusterRole object (cluster scoped resources)\ncreate ClusterRoleBinding object\n\n\n\nservice account\n\ncreate ServiceAccount object\nthen it create token\nthen create secret object with the token\nthen secret object is linked to the service account\nand the token is automatically mounted to the pod\n\n=&gt; but this is not secure, and scalable =&gt; TokenRequest API is used",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/notes/4_security.html#image-security",
    "href": "posts/03_archives/stored_categories/k8s/notes/4_security.html#image-security",
    "title": "김형훈의 학습 블로그",
    "section": "image security",
    "text": "image security\nif you use private image registry, you need to create secret object 1. create docker-registry type secret 2. add imagePullSecrets field in the pod spec",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s",
      "Notes",
      "Authentication"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/index.html",
    "href": "posts/03_archives/stored_categories/k8s/index.html",
    "title": "k8s",
    "section": "",
    "text": "k8s 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/index.html#details",
    "href": "posts/03_archives/stored_categories/k8s/index.html#details",
    "title": "k8s",
    "section": "",
    "text": "k8s 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/index.html#tasks",
    "href": "posts/03_archives/stored_categories/k8s/index.html#tasks",
    "title": "k8s",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/k8s/index.html#참고-자료",
    "title": "k8s",
    "section": "참고 자료",
    "text": "참고 자료\n\nCKA Udemy 강의",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/k8s/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/k8s/index.html#related-posts",
    "title": "k8s",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "k8s"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/01.html#dag-skeleton",
    "href": "posts/03_archives/stored_categories/air_flow/notes/01.html#dag-skeleton",
    "title": "Coding pipeline",
    "section": "DAG skeleton",
    "text": "DAG skeleton\n\nfrom airflow import DAG\nfrom datetime import datetime\n\nwith DAG(\n    dag_id='example_dag',\n    schedule='@daily',\n    start_date=datetime(2022, 4, 5),\n    catchup=False,\n) as dag:\n    pass\n\n\nDAG는 start_date / last_execution time + schedule_interval에 실행된다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/01.html#operator",
    "href": "posts/03_archives/stored_categories/air_flow/notes/01.html#operator",
    "title": "Coding pipeline",
    "section": "Operator",
    "text": "Operator\n\noperator 하나 당 하나의 task만 실행하는게 좋다.\n\n\noperator type\n\nAction operators\n\nBashOperator\nPythonOperator\n\nTransfer operators\nSensor operators",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/01.html#providers",
    "href": "posts/03_archives/stored_categories/air_flow/notes/01.html#providers",
    "title": "Coding pipeline",
    "section": "Providers",
    "text": "Providers\n\nAirflow providers are a set of packages that contain operators, sensors, hooks, and other utilities to interact with external platforms and services.\nProviders are installed separately from Airflow and can be added to your environment as needed.\nIn Airflow core, Bash and Python operators, … are included\n\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\nwith DAG(\n    dag_id='example_db',\n    schedule='@daily',\n    start_date=datetime(2022, 4, 5),\n    catchup=False,\n) as dag:\n    create_table = PostgresOperator(\n        task_id='create_table',\n        postgres_conn_id='postgres',\n        sql=\"\"\"\n            CREATE TABLE IF NOT EXISTS example_table (\n                id SERIAL PRIMARY KEY,\n                name VARCHAR(50)\n            );\n        \"\"\",\n    )\n\nDB에 접속하기 위해서 connection을 설정해야 한다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/notes/01.html#hook",
    "href": "posts/03_archives/stored_categories/air_flow/notes/01.html#hook",
    "title": "Coding pipeline",
    "section": "Hook",
    "text": "Hook",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow",
      "Notes",
      "Coding pipeline"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/index.html",
    "href": "posts/03_archives/stored_categories/air_flow/index.html",
    "title": "AirFlow",
    "section": "",
    "text": "Air Flow 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/index.html#details",
    "href": "posts/03_archives/stored_categories/air_flow/index.html#details",
    "title": "AirFlow",
    "section": "",
    "text": "Air Flow 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/index.html#tasks",
    "href": "posts/03_archives/stored_categories/air_flow/index.html#tasks",
    "title": "AirFlow",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/air_flow/index.html#참고-자료",
    "title": "AirFlow",
    "section": "참고 자료",
    "text": "참고 자료\n\nUdemy 강의",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/air_flow/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/air_flow/index.html#related-posts",
    "title": "AirFlow",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "AirFlow"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/smart_contract/index.html",
    "href": "posts/03_archives/stored_categories/smart_contract/index.html",
    "title": "Smart Contract",
    "section": "",
    "text": "smart contract 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/smart_contract/index.html#details",
    "href": "posts/03_archives/stored_categories/smart_contract/index.html#details",
    "title": "Smart Contract",
    "section": "",
    "text": "smart contract 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/smart_contract/index.html#tasks",
    "href": "posts/03_archives/stored_categories/smart_contract/index.html#tasks",
    "title": "Smart Contract",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/smart_contract/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/smart_contract/index.html#참고-자료",
    "title": "Smart Contract",
    "section": "참고 자료",
    "text": "참고 자료\n\nblock chain 강의 사이트",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/smart_contract/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/smart_contract/index.html#related-posts",
    "title": "Smart Contract",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Smart Contract"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/index.html",
    "href": "posts/03_archives/stored_categories/금융/index.html",
    "title": "금융",
    "section": "",
    "text": "금융 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/index.html#details",
    "href": "posts/03_archives/stored_categories/금융/index.html#details",
    "title": "금융",
    "section": "",
    "text": "금융 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/index.html#tasks",
    "href": "posts/03_archives/stored_categories/금융/index.html#tasks",
    "title": "금융",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/금융/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/금융/index.html#related-posts",
    "title": "금융",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "금융"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/ros/index.html",
    "href": "posts/03_archives/stored_categories/ros/index.html",
    "title": "ROS",
    "section": "",
    "text": "ROS 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "ROS"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/ros/index.html#details",
    "href": "posts/03_archives/stored_categories/ros/index.html#details",
    "title": "ROS",
    "section": "",
    "text": "ROS 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "ROS"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/ros/index.html#tasks",
    "href": "posts/03_archives/stored_categories/ros/index.html#tasks",
    "title": "ROS",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "ROS"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/ros/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/ros/index.html#참고-자료",
    "title": "ROS",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "ROS"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/ros/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/ros/index.html#related-posts",
    "title": "ROS",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "ROS"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-sdk",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-sdk",
    "title": "Qiskit",
    "section": "Qiskit SDK",
    "text": "Qiskit SDK\nQiskit SDK(패키지 이름: qiskit)는 확장된(정적, 동적, 스케줄된) 양자 회로, 연산자, 프리미티브 수준에서 양자 컴퓨터를 다루기 위한 오픈소스 소프트웨어 개발 키트입니다. 이 라이브러리는 Qiskit의 핵심 구성 요소로, 양자 계산을 위한 가장 광범위한 도구 모음을 제공하며, 다른 많은 구성 요소가 여기에 연결됩니다.\nQiskit SDK의 가장 유용한 기능은 다음과 같습니다:\n\n회로 구축 도구(qiskit.circuit): 레지스터, 회로, 명령, 게이트, 매개변수, 제어 흐름 객체를 초기화하고 조작하기 위한 도구.\n회로 라이브러리(qiskit.circuit.library): 회로, 명령, 게이트의 방대한 범위 - 회로 기반 양자 계산의 핵심 구성 요소.\n양자 정보 라이브러리(qiskit.quantum_info): 샘플링 노이즈 없이 정확한 계산을 통해 양자 상태, 연산자, 채널을 다루는 툴킷. 입력 관측 가능 항목을 지정하고 프리미티브 쿼리의 출력 충실도를 분석하는 데 사용.\n트랜스파일러(qiskit.transpiler): 특정 장치 토폴로지에 맞게 양자 회로를 변환 및 적응시키고, 실제 양자 처리 장치(QPU)에서 실행을 최적화.\n프리미티브(qiskit.primitives): Sampler와 Estimator 프리미티브의 기본 정의와 참조 구현을 포함하는 모듈로, 다양한 양자 하드웨어 제공자가 이를 기반으로 자체 구현을 파생할 수 있음. Qiskit Runtime 프리미티브에 대한 자세한 내용은 문서에서 확인 가능.\n\n\n설치\nQiskit SDK 설치에 대한 자세한 소개는 설치 페이지를 확인하세요. 지금 설치할 준비가 되었다면 다음 명령어를 실행하세요:\npip install qiskit",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#벤치마킹과-benchpress-패키지",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#벤치마킹과-benchpress-패키지",
    "title": "Qiskit",
    "section": "벤치마킹과 Benchpress 패키지",
    "text": "벤치마킹과 Benchpress 패키지\n벤치마킹은 개발 워크플로우의 여러 단계에서 양자 소프트웨어의 상대적 성능을 비교하는 데 중요합니다. 예를 들어, 양자 소프트웨어 벤치마킹 테스트는 회로 구축, 조작, 트랜스파일링의 속도와 품질을 평가할 수 있습니다. IBM Quantum은 가능한 한 성능이 뛰어난 SDK를 제공하기 위해 노력하며, 이를 위해 Qiskit SDK는 주요 대학, 국립 연구소, IBM 연구원들이 개발한 1,000개 이상의 테스트를 통해 벤치마킹됩니다. 이러한 테스트에 사용되는 벤치마킹 스위트는 Benchpress라는 이름으로 오픈소스 패키지로 제공됩니다. 이제 Benchpress 패키지를 사용해 양자 SDK 성능을 직접 분석할 수 있습니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-runtime",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-runtime",
    "title": "Qiskit",
    "section": "Qiskit Runtime",
    "text": "Qiskit Runtime\nQiskit Runtime은 IBM Quantum® 하드웨어에서 양자 계산을 실행하기 위한 클라우드 기반 서비스입니다. qiskit-ibm-runtime 패키지는 이 서비스의 클라이언트로, Qiskit IBM Provider의 후속 버전입니다. Qiskit Runtime 서비스는 양자 계산을 간소화하고 IBM Quantum 하드웨어에 최적화된 Qiskit 프리미티브 구현을 제공합니다. Qiskit Runtime 프리미티브를 시작하려면 문서를 방문하세요.\nQiskit Runtime은 추가적인 고전 및 양자 컴퓨팅 자원을 활용하도록 설계되었으며, 오류 억제(error suppression)와 오류 완화(error mitigation) 같은 기술을 사용해 양자 회로 실행에서 더 높은 품질의 결과를 반환합니다. 예로는 오류 억제를 위한 동적 디커플링(dynamical decoupling), 오류 완화를 위한 판독 완화(readout mitigation)와 제로 노이즈 외삽(ZNE)이 있습니다. 이러한 옵션 설정 방법은 오류 완화 설정 페이지에서 확인할 수 있습니다.\nQiskit Runtime은 IBM 하드웨어에서 양자 프로그램을 실행하기 위해 세 가지 실행 모드(Job, Session, Batch)를 제공하며, 각각은 서로 다른 사용 사례와 양자 작업 큐에 대한 영향을 가집니다: - Job: 지정된 샷 수로 실행되는 단일 프리미티브 쿼리. - Session: 양자 컴퓨터에서 반복 작업 부하를 여러 작업으로 효율적으로 실행. - Batch: 모든 작업을 한 번에 제출해 병렬 처리. 참고: Open Plan 사용자는 세션 작업을 제출할 수 없습니다.\nQiskit Runtime을 빠르게 설치하려면 다음 명령어를 실행하세요:\npip install qiskit-ibm-runtime\n개발 환경 설정에 대한 자세한 내용은 설치 페이지에서 확인할 수 있습니다.\n\nQiskit Runtime은 오픈소스인가요?\n간단히 답하면, 전부는 아닙니다. IBM Quantum 장치에서 양자 프로그램을 실행하는 기술적 세부 사항(오류 완화 및 억제 포함)을 처리하는 Qiskit Runtime 서비스 소프트웨어는 오픈소스가 아닙니다. 그러나 Qiskit Runtime 클라이언트(사용자가 Qiskit Runtime 서비스에 접근하는 인터페이스), 서버 측에서 실행되는 Qiskit SDK, 오류 완화에 사용되는 일부 소프트웨어는 오픈소스입니다. Qiskit 오픈소스 활동에 참여하려면 GitHub 조직(github.com/Qiskit 및 github.com/Qiskit-Extensions)을 방문하세요.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-serverless",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-serverless",
    "title": "Qiskit",
    "section": "Qiskit Serverless",
    "text": "Qiskit Serverless\n유틸리티 규모의 양자 응용 프로그램을 만들려면 일반적으로 다양한 컴퓨팅 자원 요구 사항이 필요합니다. Qiskit Serverless(qiskit-ibm-catalog.QiskitServerless)는 양자-고전 자원 전반에 걸쳐 작업 부하를 실행하기 위한 간단한 인터페이스를 제공합니다. 여기에는 IBM Quantum Platform에 프로그램 배포, 원격 작업 부하 실행, 멀티 클라우드 및 양자 중심 슈퍼컴퓨팅 사용 사례를 위한 쉬운 자원 관리가 포함됩니다. 자세한 내용은 Qiskit Serverless 문서에서 확인할 수 있습니다: - 고전 작업(전처리, 후처리 등) 병렬화. - 노트북이 꺼져 있어도 클라우드에서 장기 작업 유지. - 클라우드에 재사용 가능한 프로그램 배포.\nQiskit Serverless를 바로 사용하려면 다음 명령어로 설치하세요:\npip install qiskit_serverless",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-functions",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-functions",
    "title": "Qiskit",
    "section": "Qiskit Functions",
    "text": "Qiskit Functions\nQiskit Functions(qiskit-ibm-catalog.QiskitFunctionsCatalog)는 알고리즘 발견과 응용 프로토타이핑을 가속화하도록 설계된 추상화된 서비스입니다. Qiskit Functions Catalog를 탐색해보세요: - Circuit Functions: 트랜스파일링, 오류 억제, 오류 완화, 후처리 기술을 포함하며, 추상 회로와 원하는 측정 관측 가능 항목을 입력으로 받는 서비스. 사용자는 하드웨어 성능 관리를 신경 쓰지 않고 새로운 알고리즘과 응용을 탐구 가능. - Application Functions: 고전에서 양자로의 매핑, 하드웨어 최적화, 하드웨어 실행, 후처리를 포함한 전체 양자 워크플로우를 제공. 사용자는 산업별 친숙한 입력과 출력으로 응용 프로토타이핑 가능.\nPremium Plan 회원은 IBM 제공 함수에 즉시 접근하거나 파트너가 제공하는 함수를 파트너로부터 직접 라이선스 구매 가능합니다. 카탈로그는 다음 명령어로 설치 가능:\npip install qiskit-ibm-catalog",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-transpiler-as-a-service",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-transpiler-as-a-service",
    "title": "Qiskit",
    "section": "Qiskit Transpiler as a Service",
    "text": "Qiskit Transpiler as a Service\nQiskit Transpiler Service(패키지 이름: qiskit-ibm-transpiler)는 IBM Quantum Premium Plan 사용자에게 클라우드에서 원격 트랜스파일링 기능을 제공하는 새로운 실험적 서비스입니다. 로컬 Qiskit SDK 트랜스파일러 기능 외에도, 이 서비스를 통해 트랜스파일링 작업은 IBM Quantum 클라우드 자원과 AI 기반 트랜스파일러 패스를 활용할 수 있습니다. 클라우드 기반 트랜스파일링을 Qiskit 워크플로우에 통합하는 방법은 문서에서 확인하세요.\n트랜스파일러 서비스는 다음 명령어로 설치 가능:\npip install qiskit-ibm-transpiler",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-addons",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-addons",
    "title": "Qiskit",
    "section": "Qiskit Addons",
    "text": "Qiskit Addons\nQiskit Addons는 유틸리티 규모 알고리즘 발견을 위한 연구 기능 모음입니다. 이러한 기능은 Qiskit의 고성능 기반 위에 양자 알고리즘 생성 및 실행 도구를 구축합니다. Addons는 워크플로우에 연결되어 새로운 양자 알고리즘을 확장하거나 설계하는 모듈형 소프트웨어 구성 요소입니다. 사용 가능한 Qiskit Addons 세트와 시작 방법은 문서에서 확인하세요.\n관심 있는 연구 기능에 따라 여러 Addons가 있으며, 각기 pip로 설치 가능: - Sample-based Quantum Diagonalization (SQD): pip install qiskit-addon-sqd - Approximate Quantum Compilation (AQC): pip install qiskit-addon-aqc-tensor[quimb-jax] - Operator Backpropagation (OBP): pip install qiskit-addon-obp - Multi-product Formulas (MPF): pip install qiskit-addon-mpf",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-생태계",
    "href": "posts/03_archives/stored_categories/quantum_programming/notes/01.html#qiskit-생태계",
    "title": "Qiskit",
    "section": "Qiskit 생태계",
    "text": "Qiskit 생태계\nQiskit 외에도 “Qiskit” 이름을 사용하는 많은 오픈소스 프로젝트가 있으며, 이는 Qiskit 자체의 일부는 아니지만 Qiskit과 인터페이스를 이루며 핵심 Qiskit 워크플로우를 보완하는 유용한 추가 기능을 제공합니다. 일부 프로젝트는 IBM Quantum 팀이 유지 관리하며, 다른 일부는 더 넓은 오픈소스 커뮤니티에서 지원됩니다. Qiskit SDK는 모듈화되고 확장 가능한 방식으로 설계되어 개발자들이 이를 확장하는 프로젝트를 쉽게 만들 수 있습니다.\nQiskit 생태계의 인기 있는 프로젝트: - Qiskit Aer(qiskit-aer): 현실적인 노이즈 모델을 포함한 양자 컴퓨팅 시뮬레이터 패키지. 여러 시뮬레이션 방법으로 노이즈 유무에 따라 양자 회로 실행 가능. IBM Quantum 유지 관리. - qBraid SDK(qbraid): 양자 소프트웨어 및 하드웨어 제공자를 위한 플랫폼 독립적 양자 런타임 프레임워크로, 프로그램 사양 정의부터 작업 제출, 후처리 및 결과 시각화까지 양자 작업의 전체 수명 주기 관리를 간소화. qBraid 유지 관리. - mthree: M3(Matrix-free Measurement Mitigation) 측정 완화 기술을 구현하는 패키지로, 차원 축소 후 직접 LU 분해 또는 O(1) 단계로 수렴하는 전처리 반복 방법을 사용해 수정된 측정 확률을 계산하며 병렬 처리 가능. IBM Quantum 유지 관리.\nQiskit 생태계 페이지에서 프로젝트 카탈로그와 자신의 프로젝트를 추천하는 방법에 대한 정보를 확인할 수 있습니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Quantum Programming",
      "Notes",
      "Qiskit"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/독서/notes/01.html#사람을-대하는-기본-기술",
    "href": "posts/03_archives/stored_categories/독서/notes/01.html#사람을-대하는-기본-기술",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람을 대하는 기본 기술",
    "text": "사람을 대하는 기본 기술\n\n꿀을 얻으려면 벌집을 걷어차지 마라\n남을 비난하고 원망하며 불평하는 것은 어떤 바보라도 할 수 있다. 실제로 바보들은 그렇게 한다. 하지만 남을 이해하고 용서하려면 인격과 자제력이 필요하다.\n사람을 비난하는 대신 그들을 이해하려고 노력해 보자. 그들이 왜 그런 행동을 하는지 곰곰이 생각해 보자. 그편이 비난하는 것보다 훨씬 이롭고 흥미롭다.\n\n\n사람을 대하는 핵심 비결\n누군가에게 어떤 일을 하게 만드는 방법은 상대방이 그 일을 하고 싶게 만드는 것 뿐이다. 강제적인 방법들은 반드시 역효과를 일으킨다.\n타인에게 어떤 일을 하게 하려면 그 사람이 원하는 것을 주는 방법밖에 없다. 인간의 본성이 지닌 가장 깊은 충동이 바로 중요한 사람이 되고자 하는 욕망이다.\n이러한 갈망을 제대로 충족시켜 주는 사람은 다른 사람의 마음을 사로잡을 수 있다. 타인을 진실된 마음으로 칭찬하자. 이는 이기적이고 거짓인 아첨과는 다르다.\n\n\n이 일을 해내는 사람은 세상을 얻을 것이고, 그렇지 못한 사람은 외로운 길을 걸을 것이다.\n상대방에게 영향을 미치는  방법은 그사람이 원하는 것을 이야기하고, 이를 얻는 방법을 보여 주는 것이다.\n\n\n\n\n\n\n…이 부분 예시로 드는 것들이 조금 오버스럽다고 느껴진다.\n문화가 달라서 그런가? 아니면 번역 이슈인가?\n무슨 말을 하는진 알겠는데, 몇몇 부분은 별로 공감이 안 된다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "독서",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/독서/notes/01.html#사람들에게-호감을-얻는-6가지-방법",
    "href": "posts/03_archives/stored_categories/독서/notes/01.html#사람들에게-호감을-얻는-6가지-방법",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람들에게 호감을 얻는 6가지 방법",
    "text": "사람들에게 호감을 얻는 6가지 방법\n\n이렇게 하면 어디서든 환영받을 것이다.\n다른 사람에게 관심이 없는 사람은 인생에서 가장 큰 어려움을 겪고, 다른 사람에게 가장 큰 상처를 준다. 상대방에게 진심으로 관심을 가져라\n\n\n좋은 첫인상을 남기는 간단한 방법\n미소를 지어라\n\n\n이렇게 하지 않으면 문제가 생길 것이다.\n사람의 이름을 기억하라\n\n\n좋은 대화 상대가 되는 쉬운 방법\n상대의 이야기를 경청하고, 상대가 자신에 관해 이야기하도록 격려하라\n\n\n사람들의 관심을 얻는 방법\n상대방의 관심사에 대해 이야기하라\n\n\n사람들에게 즉시 호감을 얻는 방법\n모든 사람은 자신이 상대보다 우월하다고 생각한다. 항상 상대방이 자신을 중요하다고 느끼게 하라",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "독서",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/독서/notes/01.html#사람들의-마음을-사로잡는-12가지-방법",
    "href": "posts/03_archives/stored_categories/독서/notes/01.html#사람들의-마음을-사로잡는-12가지-방법",
    "title": "인간 관계론 - 데일 카네기",
    "section": "사람들의 마음을 사로잡는 12가지 방법",
    "text": "사람들의 마음을 사로잡는 12가지 방법\n\n논쟁으로는 이길 수 없다\n자기 의사에 반하여 설득당한 사람은 여전히 자기 생각을 바꾸지 않는 법이다. 논쟁에서 최선의 결과를 얻는 유일한 방법은 논쟁을 피하는 것뿐이다.\n\n\n적을 만드는 확실한 방법과 이를 피하는 방법\n되도록 남들보다 지혜로운 사람이 되거라. 하지만 남들에게 그렇다고 말하지 않도록 해라.\n상대가 틀린 말을 해도 굳이 지적하지 마라\n\n\n틀렸다면, 인정하라\n조금 잘못했는데, 상대가 비난할거 같으면 오바해서 자기 잘못을 시인하라\n\n\n이성에 호소하는 확실한 방법\n우호적인 방식으로 시작하라\n\n\n소크라테스의 비결\n상대방이 동의할 수 밖에 없는 질문을 유도하라.\n\n\n불만을 잠재우는 안전밸브\n여기부터 읽어야함",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "독서",
      "Notes",
      "인간 관계론 - 데일 카네기"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/time_series/01.html#data-window",
    "href": "posts/02_categories/deep_learning/notes/time_series/01.html#data-window",
    "title": "data windowing & baseline modeling",
    "section": "Data window",
    "text": "Data window\n\nwindow: 입력, 레이블 쌍을 생성하는데 사용되는 데이터의 청크\n\n입력: 모델에 공급되는 과거 관측치\n레이블: 모델이 예측하도록 학습되는 미래 관측치\n\nbatch: 모델에 공급되는 window의 모음\n\n[t=0, t=1, …], [t=1, t=2, …], [t=2, t=3, …], … 이런식으로 구성\n각 window가 독립적이라면 batch level에서 shuffle 가능.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Time Series",
      "data windowing & baseline modeling"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/pytorch/00.html#특징",
    "href": "posts/02_categories/deep_learning/notes/pytorch/00.html#특징",
    "title": "pytorch 기초",
    "section": "특징",
    "text": "특징\n\n동적 계산 그래프(Dynamic Computation Graph)\n\n파이토치는 동적 계산 그래프를 사용하여 모델을 정의하고 수정할 수 있다. 모델의 구조를 실행 시점에 변경할 수 있다.\n\nGPU 가속 지원\n직관적 인터페이스\n\n실행모드를 지원하고, 계산 그래프를 빌드하지 않고 코드를 실행할 수 있다.\n\n제한된 프로덕션 지원\n\n주로 연구 목적이다.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Pytorch",
      "pytorch 기초"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/pytorch/00.html#텐서",
    "href": "posts/02_categories/deep_learning/notes/pytorch/00.html#텐서",
    "title": "pytorch 기초",
    "section": "텐서",
    "text": "텐서\n\nimport torch\n\ntensor = torch.rand(1, 2)\n\nprint(tensor)\nprint(tensor.shape) # 크기\nprint(tensor.dtype) # 자료형\nprint(tensor.device) # GPU 가속 여부\n\ntensor([[0.1930, 0.8300]])\ntorch.Size([1, 2])\ntorch.float32\ncpu\n\n\n\n장치 설정\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncpu = torch.FloatTensor([1, 2, 3])\ngpu = torch.cuda.FloatTensor([1, 2, 3]) # GPU 가속 지정 방법 1. MAC에서는 지원이 안될 수도 있다.\ntensor = torch.rand((1, 1), device=device) # GPU 가속 지정 방법 2\nprint(device)\nprint(cpu)\nprint(gpu)\nprint(tensor)\n\ncuda\ntensor([1., 2., 3.])\ntensor([1., 2., 3.], device='cuda:0')\ntensor([[0.9628]], device='cuda:0')\n\n\n\ncpu 텐서와 gpu 텐서는 상호 간 연산이 불가능하다.\nnumpy 배열은 cpu 텐서와의 연산만 가능\n\n\n\n장치 변환\n\ncpu = torch.FloatTensor([1, 2, 3])\ngpu = cpu.cuda() # cpu -&gt; gpu\ncpu2 = gpu.cpu() # gpu -&gt; cpu\ngpu2 = cpu.to(\"cuda\") # cpu -&gt; gpu 2 MAC에서도 지원이 되니까 이 방법으로 사용하자\nprint(cpu)\nprint(cpu2)\nprint(gpu)\nprint(gpu2)\n\ntensor([1., 2., 3.])\ntensor([1., 2., 3.])\ntensor([1., 2., 3.], device='cuda:0')\ntensor([1., 2., 3.], device='cuda:0')\n\n\n\nimport numpy as np\n\nndarray = np.array([1, 2, 3], dtype=np.uint8)\nyo = torch.from_numpy(ndarray)\nprint(yo)\nprint(yo.to(\"cuda\"))\n\ntensor([1, 2, 3], dtype=torch.uint8)\ntensor([1, 2, 3], device='cuda:0', dtype=torch.uint8)\n\n\n\nndarray = yo.detach().cpu().numpy() # detach: graph에서 분리된 새로운 텐서 반환\nprint(ndarray)\nprint(type(ndarray))\n\n[1 2 3]\n&lt;class 'numpy.ndarray'&gt;",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Pytorch",
      "pytorch 기초"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/pytorch/00.html#단순-선형회귀",
    "href": "posts/02_categories/deep_learning/notes/pytorch/00.html#단순-선형회귀",
    "title": "pytorch 기초",
    "section": "단순 선형회귀",
    "text": "단순 선형회귀\n\nfrom torch import optim\n\nx = torch.FloatTensor([\n    [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],\n    [11], [12], [13], [14], [15], [16], [17], [18], [19], [20],\n    [21], [22], [23], [24], [25], [26], [27], [28], [29], [30]\n])\ny = torch.FloatTensor([\n    [0.94], [2.05], [2.87], [4.10], [5.01], [6.15], [6.95], [8.12], [9.05], [10.11],\n    [11.03], [12.20], [12.89], [14.15], [15.02], [16.18], [16.95], [18.22], [19.10], [20.05],\n    [21.01], [22.20], [22.89], [24.10], [25.05], [26.15], [26.95], [28.12], [29.05], [30.10]\n])\n\n\nweight = torch.zeros(1, requires_grad=True)\nbias = torch.zeros(1, requires_grad=True)\nlearning_rate = 0.001\n\n\noptimizer = optim.SGD([weight, bias], lr=learning_rate)\n\nfor epoch in range(1000):\n    hypothesis = x * weight + bias\n    cost = torch.mean((hypothesis - y) ** 2)\n\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost.item():.4f} | Weight: {weight.item():.4f} | Bias: {bias.item():.4f}\")\n\nEpoch 100/1000 | Cost: 0.0089 | Weight: 1.0010 | Bias: 0.0482\nEpoch 200/1000 | Cost: 0.0089 | Weight: 1.0010 | Bias: 0.0473\nEpoch 300/1000 | Cost: 0.0089 | Weight: 1.0011 | Bias: 0.0463\nEpoch 400/1000 | Cost: 0.0088 | Weight: 1.0011 | Bias: 0.0455\nEpoch 500/1000 | Cost: 0.0088 | Weight: 1.0012 | Bias: 0.0446\nEpoch 600/1000 | Cost: 0.0088 | Weight: 1.0012 | Bias: 0.0438\nEpoch 700/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0430\nEpoch 800/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0423\nEpoch 900/1000 | Cost: 0.0088 | Weight: 1.0013 | Bias: 0.0416\nEpoch 1000/1000 | Cost: 0.0088 | Weight: 1.0014 | Bias: 0.0409\n\n\n\nfrom torch import nn\n\nmodel = nn.Linear(1, 1, bias=True)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(1000):\n    hypothesis = model(x)\n    cost = criterion(hypothesis, y)\n\n    optimizer.zero_grad()\n    cost.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost:.4f} | Model: {list(model.parameters())}\")\n\nEpoch 100/1000 | Cost: 0.0136 | Model: [Parameter containing:\ntensor([[0.9950]], requires_grad=True), Parameter containing:\ntensor([0.1702], requires_grad=True)]\nEpoch 200/1000 | Cost: 0.0132 | Model: [Parameter containing:\ntensor([[0.9953]], requires_grad=True), Parameter containing:\ntensor([0.1636], requires_grad=True)]\nEpoch 300/1000 | Cost: 0.0128 | Model: [Parameter containing:\ntensor([[0.9956]], requires_grad=True), Parameter containing:\ntensor([0.1573], requires_grad=True)]\nEpoch 400/1000 | Cost: 0.0124 | Model: [Parameter containing:\ntensor([[0.9959]], requires_grad=True), Parameter containing:\ntensor([0.1513], requires_grad=True)]\nEpoch 500/1000 | Cost: 0.0121 | Model: [Parameter containing:\ntensor([[0.9962]], requires_grad=True), Parameter containing:\ntensor([0.1455], requires_grad=True)]\nEpoch 600/1000 | Cost: 0.0118 | Model: [Parameter containing:\ntensor([[0.9965]], requires_grad=True), Parameter containing:\ntensor([0.1400], requires_grad=True)]\nEpoch 700/1000 | Cost: 0.0115 | Model: [Parameter containing:\ntensor([[0.9967]], requires_grad=True), Parameter containing:\ntensor([0.1348], requires_grad=True)]\nEpoch 800/1000 | Cost: 0.0113 | Model: [Parameter containing:\ntensor([[0.9970]], requires_grad=True), Parameter containing:\ntensor([0.1298], requires_grad=True)]\nEpoch 900/1000 | Cost: 0.0110 | Model: [Parameter containing:\ntensor([[0.9972]], requires_grad=True), Parameter containing:\ntensor([0.1251], requires_grad=True)]\nEpoch 1000/1000 | Cost: 0.0108 | Model: [Parameter containing:\ntensor([[0.9974]], requires_grad=True), Parameter containing:\ntensor([0.1205], requires_grad=True)]",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Pytorch",
      "pytorch 기초"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/pytorch/00.html#데이터-로드",
    "href": "posts/02_categories/deep_learning/notes/pytorch/00.html#데이터-로드",
    "title": "pytorch 기초",
    "section": "데이터 로드",
    "text": "데이터 로드\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_x = torch.FloatTensor([\n    [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]\n])\ntrain_y = torch.FloatTensor([\n    [0.1, 1.5], [1, 2.8], [1.9, 4.1], [2.8, 5.4], [3.7, 6.7], [4.6, 8]\n])\ntrain_dataset = TensorDataset(train_x, train_y)\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=True)\n\n\nmodel = nn.Linear(2, 2, bias=True)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(1000):\n    cost = 0\n    for batch in train_dataloader:\n        x, y = batch\n        output = model(x)\n        loss = criterion(output, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        cost += loss\n\n    cost = cost / len(train_dataloader)\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch {epoch+1}/1000 | Cost: {cost:.4f} | Model: {list(model.parameters())}\")\n\nEpoch 100/1000 | Cost: 0.1810 | Model: [Parameter containing:\ntensor([[0.3308, 0.2977],\n        [0.8800, 0.2466]], requires_grad=True), Parameter containing:\ntensor([0.0446, 0.6826], requires_grad=True)]\nEpoch 200/1000 | Cost: 0.1563 | Model: [Parameter containing:\ntensor([[0.3674, 0.2787],\n        [0.9034, 0.2345]], requires_grad=True), Parameter containing:\ntensor([-0.0109,  0.6471], requires_grad=True)]\nEpoch 300/1000 | Cost: 0.1366 | Model: [Parameter containing:\ntensor([[0.4018, 0.2612],\n        [0.9254, 0.2234]], requires_grad=True), Parameter containing:\ntensor([-0.0628,  0.6140], requires_grad=True)]\nEpoch 400/1000 | Cost: 0.1195 | Model: [Parameter containing:\ntensor([[0.4339, 0.2448],\n        [0.9458, 0.2129]], requires_grad=True), Parameter containing:\ntensor([-0.1114,  0.5830], requires_grad=True)]\nEpoch 500/1000 | Cost: 0.1056 | Model: [Parameter containing:\ntensor([[0.4636, 0.2291],\n        [0.9649, 0.2029]], requires_grad=True), Parameter containing:\ntensor([-0.1568,  0.5540], requires_grad=True)]\nEpoch 600/1000 | Cost: 0.0913 | Model: [Parameter containing:\ntensor([[0.4916, 0.2147],\n        [0.9827, 0.1936]], requires_grad=True), Parameter containing:\ntensor([-0.1992,  0.5269], requires_grad=True)]\nEpoch 700/1000 | Cost: 0.0799 | Model: [Parameter containing:\ntensor([[0.5178, 0.2012],\n        [0.9994, 0.1851]], requires_grad=True), Parameter containing:\ntensor([-0.2389,  0.5016], requires_grad=True)]\nEpoch 800/1000 | Cost: 0.0704 | Model: [Parameter containing:\ntensor([[0.5421, 0.1884],\n        [1.0150, 0.1769]], requires_grad=True), Parameter containing:\ntensor([-0.2760,  0.4779], requires_grad=True)]\nEpoch 900/1000 | Cost: 0.0606 | Model: [Parameter containing:\ntensor([[0.5650, 0.1766],\n        [1.0296, 0.1694]], requires_grad=True), Parameter containing:\ntensor([-0.3107,  0.4558], requires_grad=True)]\nEpoch 1000/1000 | Cost: 0.0531 | Model: [Parameter containing:\ntensor([[0.5864, 0.1657],\n        [1.0432, 0.1624]], requires_grad=True), Parameter containing:\ntensor([-0.3431,  0.4351], requires_grad=True)]\n\n\n\n모듈 클래스\n\nfrom torch.utils.data import Dataset\nimport pandas as pd\n\nclass CustomDataset(Dataset):\n    def __init__(self, file_path):\n        df = pd.read_csv(file_path)\n        self.x = df.iloc[:, 0].values\n        self.y = df.iloc[:, 1].values\n        self.length = len(df)\n\n    def __getitem__(self, index):\n        x = torch.FloatTensor([self.x[index] ** 2, self.x[index]])\n        y = torch.FloatTensor([self.y[index]])\n        return x, y\n\n    def __len__(self):\n        return self.length\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        return x",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Pytorch",
      "pytorch 기초"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/03.html#계산-그래프",
    "href": "posts/02_categories/deep_learning/notes/basic/03.html#계산-그래프",
    "title": "오차역전법",
    "section": "계산 그래프",
    "text": "계산 그래프\n\n특징\n\n국소적 계산: 자신과 관계된 정보만 결과로 출력\n중간 결과를 모두 저장할 수 있다.\n역전파를 통해 특정 단계에서의 최정 결과에 대한 미분을 효율적으로 계산할 수 있다.\n\n\n\n예시 - 순전파\n\n철수는 슈퍼에서 사과를 2개 귤을 3개 샀다. 사과는 1개 100원, 귤은 1개 150원이다. 소비세가 10%일 때 지불 금액은?\n\n\n\n\n\n\nflowchart LR\n    apple_num((사과 갯수)) -- 2 --&gt; apple_x\n    apple_price((사과 가격)) -- 100 --&gt; apple_x\n    orange_num((귤 갯수)) -- 3 --&gt; orange_x\n    orange_price((귤 가격)) -- 150 --&gt; orange_x\n    tax((소비세)) -- 1.1 --&gt; total_x\n    apple_x[x] -- 200 --&gt; fruit_plus\n    orange_x[x] -- 450 --&gt; fruit_plus\n    fruit_plus(+) -- 650 --&gt; total_x\n    total_x[x] -- 715 --&gt; output((지불 금액))",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/03.html#오차역전법",
    "href": "posts/02_categories/deep_learning/notes/basic/03.html#오차역전법",
    "title": "오차역전법",
    "section": "오차역전법",
    "text": "오차역전법\n\n연쇄법칙을 생각하면 됨.\n\n\n덧셈 노드 역전파\n\n덧셈 노드의 역전파는 입력값을 그대로 전달한다.\n\n\n\n곱셈 노드 역전파\n\n곱셈 노드의 역전파는 입력값을 그대로 전달하되, 다른 입력값을 곱해준다.\n\n\n\n예시 - 역전파\n\n철수는 슈퍼에서 사과를 2개 귤을 3개 샀다. 사과는 1개 100원, 귤은 1개 150원이다. 소비세가 10%일 때 지불 금액은?\n\n\n\n\n\n\nflowchart LR\n    apple_num((사과 갯수)) -- 2 --&gt; apple_x\n    apple_price((사과 가격)) -- 100 --&gt; apple_x\n    tax((소비세)) -- 1.1 --&gt; total_x\n    apple_x[x] -- 200 --&gt; total_x\n    total_x[x] -- 220 --&gt; output((지불 금액))\n\n    apple_x -. 110 .-&gt; apple_num\n    apple_x -. 2.2 .-&gt; apple_price\n    total_x -. 200 .-&gt; tax\n    total_x -. 1.1 .-&gt; apple_x\n    output -. 1 .-&gt; total_x",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/03.html#python-구현",
    "href": "posts/02_categories/deep_learning/notes/basic/03.html#python-구현",
    "title": "오차역전법",
    "section": "python 구현",
    "text": "python 구현\n\nclass MulLayer:\n\n    def __init__(self):\n        self.x = None\n        self.y = None\n\n    def forward(self, x, y):\n        self.x = x\n        self.y = y\n        out = x * y\n\n        return out\n\n    def backward(self, dout):\n        dx = dout * self.y\n        dy = dout * self.x\n\n        return dx, dy\n\n\nclass AddLayer:\n    def __init__(self):\n        pass\n\n    def forward(self, x, y):\n        out = x + y\n        return out\n\n    def backward(self, dout):\n        dx = dout * 1\n        dy = dout * 1\n        return dx, dy",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/03.html#활성화-함수-계층",
    "href": "posts/02_categories/deep_learning/notes/basic/03.html#활성화-함수-계층",
    "title": "오차역전법",
    "section": "활성화 함수 계층",
    "text": "활성화 함수 계층\n\nReLU\n\n입력이 0보다 크면 그대로, 작으면 0\n\n\nclass Relu:\n    def __init__(self):\n        self.mask = None\n\n    def forward(self, x):\n        self.mask = (x &lt;= 0)\n        out = x.copy()\n        out[self.mask] = 0\n        return out\n\n    def backward(self, dout):\n        dx = dout.copy()\n        dx[self.mask] = 0\n        return dx\n\n\n\nSigmoid\n\n\\(\\frac{hL}{hy}y^2exp(-x) = \\frac{hL}{hy}y(1-y)\\) (계산 생략)\n\n\nimport numpy as np\n\nclass Sigmoid:\n    def __init__(self):\n        self.out = None\n\n    def forward(self, x):\n        out = 1 / (1 + np.exp(-x))\n        self.out = out\n\n        return out\n\n    def backward(self, dout):\n        dx = dout * (1 - self.out) * self.out\n        return dx\n\n\n\nAffine\n\nWX + B 계산 node\n\\(\\frac{dL}{dX} = \\frac{dL}{dY} ⋅ W^T\\)\n\\(\\frac{dL}{dW} = X^T ⋅ \\frac{dL}{dY}\\)\n\\(\\frac{dL}{dB} = \\sum \\frac{dL}{dY}\\)\n\n\nclass Affine:\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n        self.x = None\n        self.dW = None\n        self.db = None\n\n    def forward(self, x):\n        self.x = x\n        out = np.dot(x, self.W) + self.b\n        return out\n\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis=0)\n\n        return dx\n\n\n\nSoftmax\n\nclass SoftmaxWithLoss:\n    def __init__(self):\n        self.loss = None\n        self.y = None\n        self.t = None\n\n    def forward(self, x, t):\n      self.t = t\n      self.y = softmax(x)\n      self.loss = cross_entropy_error(self.y, self.t)\n\n      return self.loss\n\n    def backward(self, dout=1):\n        batch_size = self.t.shape[0]\n        dx = (self.y - self.t) / batch_size\n\n        return dx",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/03.html#오차역전법-신경망",
    "href": "posts/02_categories/deep_learning/notes/basic/03.html#오차역전법-신경망",
    "title": "오차역전법",
    "section": "오차역전법 신경망",
    "text": "오차역전법 신경망\n\nfrom dl_common.layers import *\nfrom dl_common.gradient import numerical_gradient\nfrom collections import OrderedDict\n\nclass TwoLayerNet:\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] =  weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b1'] = np.zeros(output_size)\n      \n        self.layers = OrderedDict()\n        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n        self.layers['Relu1'] = Relu()\n        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n        self.last_layer = SoftmaxWithLoss()\n\n    def predict(self, x):\n        for layer in self.layers.values():\n            x = layer.forward(x)\n        return x\n\n    def loss(self, x , t):\n        y = self.predict(x)\n        return self.last_layer.forward(y, t)\n\n    def auuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        if t.ndim != 1:\n            t = np.argmax(t, axis=1)\n        acc = np.mean(y == t)\n\n        return acc\n\n    def gradient(self, x, t):\n        self.loss(x, t)\n\n        dout = 1\n        dout = self.last_layer.backward(dout)\n        \n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n        \n        grads = {}\n\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        \n        return grads\n\n\nfrom dl_dataset.mnist import load_mnist\n\n(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\niters_num = 10000\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\niter_per_epoch = max(train_size / batch_size, 1)\n\nfor i in range(iters_num):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    grads = network.gradient(x_batch, t_batch)\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grads[key]\n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n\n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "오차역전법"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/05.html#합성곱-계층",
    "href": "posts/02_categories/deep_learning/notes/basic/05.html#합성곱-계층",
    "title": "합성곱 신경망",
    "section": "합성곱 계층",
    "text": "합성곱 계층\n\n완전연결 계층의 문제점\n\n데이터의 형상이 무시된다: n차원으로 된 데이터도, 1차원 데이터로 평탄화해서 입력해야 한다.\n\n\n\n연산\n\n입력 데이터 * 필터(커널) + 편향\n\n필터가 완전연결 신경망에서의 가중치 역할을 함.\n단일 곱셈 누산: 이 연산을 윈도우를 일정 간격으로 이동해 가며 입력 데이터에 적용함.\n3차원 이미지(R, G, B)의 경우, 입력과 필터의 채널 크기가 같아야 한다.\n\n패딩: 입력 데이터 주변을 특정 값으로 채움. 크기가 커지면 출력 크기가 커짐.\n스트라이드: 필터를 정용하는 위치의 간격. 크기가 커지면 출력 크기가 작아짐.\n출력 크기\n\n\\(OH = \\frac{H + 2P - FH}{S} + 1\\)\n\\(OW = \\frac{W + 2P - FW}{S} + 1\\)\n\n\\((H, W)\\): 입력 크기, \\((FH, FW)\\): 필터 크기, \\((OH, OW)\\): 출력 크기, \\(P\\): 패딩, \\(S\\): 스트라이드",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "합성곱 신경망"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/05.html#풀링-계층",
    "href": "posts/02_categories/deep_learning/notes/basic/05.html#풀링-계층",
    "title": "합성곱 신경망",
    "section": "풀링 계층",
    "text": "풀링 계층\n\n세로, 가로 방향의 공간을 줄이는 연산 (차원 축소 계층)\n풀링의 윈도우 크기와 스트라이드 값은 같게 설정하는 것이 일반적\n대상 영역에서 최대 / 평균 값을 산출\n\n\n특징\n\n학습해야할 매개변수가 없음\n채널 수가 변하지 않음\n입력의 변화에 영향을 적게 받는다",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "합성곱 신경망"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/05.html#전체-구조",
    "href": "posts/02_categories/deep_learning/notes/basic/05.html#전체-구조",
    "title": "합성곱 신경망",
    "section": "전체 구조",
    "text": "전체 구조\n\nfrom dl_common.util import im2col\nimport numpy as np\n\nx1 = np.random.rand(1, 3, 7, 7) # 1개 배치, 3 채널, 7 x 7 크기 이미지\ncol1 = im2col(x1, 5, 5, stride=1, pad=0)\n\nx2 = np.random.rand(10, 3, 7, 7)\ncol2 = im2col(x2, 5, 5, stride=1, pad=0)\n\n\nclass Convolution:\n    def __init__(self, W, b, stride=1, pad=0):\n        self.W = W\n        self.b = b\n        self.stride = stride\n        self.pad = pad\n\n    def forward(self, x):\n        FN, C, FH, FW =  self.W.shape\n        N, C, H, W = x.shape\n        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n        \n        col = im2col(x, FH, FW, self.stride, self.pad)\n        col_W = self.W.reshape(FN, -1).T\n        out = np.dot(col, col_W) + self.b\n\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n        return out",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "합성곱 신경망"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/07.html#overview",
    "href": "posts/02_categories/deep_learning/notes/basic/07.html#overview",
    "title": "word2vec",
    "section": "Overview",
    "text": "Overview\n\n이전에 봤던 방법은 통계 기반 기법, 모든 학습 데이터를 한꺼번에 처리하는 방식\nword2vec은 신경망 기반 기법, 미니배치 학습\n\n\nimport numpy as np\n\nc = np.array([[1, 0, 0, 0, 0, 0, 0]])\nW = np.random.randn(7, 3)\nh = np.dot(c, W)\nh\n\narray([[0.81425065, 0.98171133, 0.93446329]])",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "word2vec"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/07.html#cbow-모델",
    "href": "posts/02_categories/deep_learning/notes/basic/07.html#cbow-모델",
    "title": "word2vec",
    "section": "CBOW 모델",
    "text": "CBOW 모델\n\n딥러닝 학습을 진행\n\n말뭉치로부터 목표하는 단어를 타깃으로, 그 주변 단어를 맥락으로 뽑아냄.\n맥락을 one hot 인코딩 해서 입력으로 사용, 타깃을 정답 레이블로 사용\n\n입력 측의 가중치(단어의 분산 표현)를 이용해서 예측을 진행",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "word2vec"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/06.html#자연어-처리nlp",
    "href": "posts/02_categories/deep_learning/notes/basic/06.html#자연어-처리nlp",
    "title": "자연어와 단어의 분산 표현",
    "section": "자연어 처리(NLP)",
    "text": "자연어 처리(NLP)\n\n우리 말을 컴퓨터에게 이해시키기 위한 기술\n\n기계 번역, 검색, 등",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "자연어와 단어의 분산 표현"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/06.html#시소러스",
    "href": "posts/02_categories/deep_learning/notes/basic/06.html#시소러스",
    "title": "자연어와 단어의 분산 표현",
    "section": "시소러스",
    "text": "시소러스\n\n사람이 직접 레이블링 한 유의어 사전\n대표적으로 WordNet이 있다.\n\n\n한계\n\n시대 변화에 따라 단어의 의미가 바뀌는 경우가 있다.\n사람 쓰는 비용이 크다.\n단어의 미묘한 차이를 표현할 수 없다.",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "자연어와 단어의 분산 표현"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/06.html#통계-기반-기법",
    "href": "posts/02_categories/deep_learning/notes/basic/06.html#통계-기반-기법",
    "title": "자연어와 단어의 분산 표현",
    "section": "통계 기반 기법",
    "text": "통계 기반 기법\n\n말뭉치: 언어의 실제 사용 예시를 모은 텍스트 데이터\n위키백과, 뉴스 기사, 블로그 글 등\n\n\nimport numpy as np\n\ndef preprocess(text):\n    text = text.lower()\n    text = text.replace(\".\", \" .\")\n    words = text.split(' ')\n\n    word_to_id = {}\n    id_to_word = {}\n\n    for word in words:\n        if word not in word_to_id:\n            new_id = len(word_to_id)\n            word_to_id[word] = new_id\n            id_to_word[new_id] = word\n    corpus = np.array([word_to_id[w] for w in words])\n    return corpus, word_to_id, id_to_word\n\n\ntext = \"The quick brown fox jumps over the lazy dog.\"\ncorpus, word_to_id, id_to_word = preprocess(text)\n\n\n단어의 분산 표현\n\n단어의 의미를 벡터로 표현하는 방법\n분포 가설: 단어의 의미는 그 단어의 주변 단어들(맥락)에 의해 결정된다.\n\n\ndef create_co_matrix(corpus, vocab_size, window_size=1):\n    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n    corpus_size = len(corpus)\n\n    for idx, word_id in enumerate(corpus):\n        left = max(0, idx - window_size)\n        right = min(corpus_size, idx + window_size + 1)\n        for i in range(left, right):\n            if i == idx:\n                continue\n            co_matrix[word_id, corpus[i]] += 1\n    return co_matrix\n\n\ndef cos_similarity(x, y, eps=1e-8):\n    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n    return np.dot(nx, ny)\n\n\nvocab_size = len(word_to_id)\nC = create_co_matrix(corpus, vocab_size, window_size=1)\n\nC0 = C[word_to_id['the']]\nC1 = C[word_to_id['dog']]\nsimilarity = cos_similarity(C0, C1)\nsimilarity\n\nnp.float64(0.4082482852200891)\n\n\n\n\n유사 단어 랭킹\n\ndef most_similar(query, word_to_id, id_to_word, C, top=5):\n    if query not in word_to_id:\n        print(f\"{query}는 사전에 없습니다.\")\n        return None\n\n    query_id = word_to_id[query]\n    query_vec = C[query_id]\n\n    vocab_size = C.shape[0]\n    similarity = np.zeros(vocab_size)\n\n    for i in range(vocab_size):\n        similarity[i] = cos_similarity(query_vec, C[i])\n\n    # 유사도 순으로 정렬\n    count = 0\n    for i in (-1 * similarity).argsort():\n        if id_to_word[i] == query:\n            continue\n        print(f\"{id_to_word[i]}: {similarity[i]:.4f}\")\n        count += 1\n        if count &gt;= top:\n            return",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "자연어와 단어의 분산 표현"
    ]
  },
  {
    "objectID": "posts/02_categories/deep_learning/notes/basic/06.html#통계-기반-기법-개선",
    "href": "posts/02_categories/deep_learning/notes/basic/06.html#통계-기반-기법-개선",
    "title": "자연어와 단어의 분산 표현",
    "section": "통계 기반 기법 개선",
    "text": "통계 기반 기법 개선\n\n이전 방법의 한계\n\n단어의 의미를 벡터로 표현하는 방법이 단순히 주변 단어의 빈도수에 의존한다.\n\n점별 상호정보량(PMI)\n\n\n\ndef ppmi(C, eps=1e-8):\n    M = np.zeros_like(C, dtype=np.float32)\n    N = np.sum(C)\n    S = np.sum(C, axis=0)\n\n    for i in range(C.shape[0]):\n        for j in range(C.shape[1]):\n            pmi = np.log2((C[i, j] * N) / (S[i] * S[j]) + eps)\n            M[i, j] = max(0, pmi)\n    return M",
    "crumbs": [
      "Home",
      "Categories",
      "Deep Learning",
      "Notes",
      "Basic",
      "자연어와 단어의 분산 표현"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/08.html#intro",
    "href": "posts/02_categories/42_seoul/notes/08.html#intro",
    "title": "cloud-1 개념 설명",
    "section": "intro",
    "text": "intro\n\n\n\n42 seoul outer 과제\n\n\n다음 학기 시작 전까지 개념공부만 하면서 시간을 보내려고 하니까 프로젝트가 하고 싶어졌습니다. 원래는 python 과제를 하려고 했는데, 이전에 cloud 과제를 진행하다가 말았던게 기억나서 이어서 해보면 괜찮겠다 생각했습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/08.html#프로젝트-및-구현-설명",
    "href": "posts/02_categories/42_seoul/notes/08.html#프로젝트-및-구현-설명",
    "title": "cloud-1 개념 설명",
    "section": "프로젝트 및 구현 설명",
    "text": "프로젝트 및 구현 설명\n\n개요\n과제 명세서\n참고한 자료는 다음과 같습니다:\n\nAWS SAA Udemy 강의\nansible terraform Udemy 강의\n\n이 강의들도 본 지 1년이 다되어가긴 하지만..과제할 때 사용한 제 배경지식이 여기서 나온거니까요. 과제를 진행하실 분들은 한번 수강해보시면 도움이 될 것 같습니다.\n\n\n\n\n\n\n이 포스팅에서 docker와 nginx, wordpress, mysql 구조에 대한 설명은 생략하겠습니다.\n전체 코드는 github repo에서 확인하실 수 있습니다.\n\n\n\n\n\nWhat is IaC?\n이 프로젝트의 목표는 IaC(Infrastructure as Code) tool을 이용하여 wordpress 사이트를 cloud에 자동으로 배포하는 것입니다.\nIaC는 인프라 구성을 코드로 관리하는 방식으로, 수동으로 리소스를 생성하고 설정하는 방식에 비해 버전 관리가 간편하고, 동일한 환경을 쉽게 재현하거나, 코드 리뷰 등의 방식으로 휴먼 에러를 줄이는 데 용이하게 사용할 수 있습니다.\n이번 프로젝트에서는 Packer, Terraform, Ansible 세 가지 IaC tool을 조합해 사용했습니다\n\n\nPacker: 인프라 생성 전, 상세 설정이 되어있는 image를 build할 수 있는 tool 입니다.\nTerraform: cloud 인프라를 생성하는 tool입니다. packer에서 생성한 ami를 사용할 수 있습니다.\nAnsible: 서버 내부의 상세 설정을 자동화합니다. 일반적인 bash script와는 다르게 멱등성 있는 설정이 가능하다는 점이 큰 장점입니다. 이때, 서버는 python이 설치되어 있어야 하고, ssh로 접근 가능해야 합니다.\n\n위의 이미지 처럼, packer로 필요한 설정이 완료된 image를 생성한 뒤, 그 이미지를 기반으로 cloud infra를 terraform으로 생성하고, 생성된 infra의 상세 설정을 ansible을 이용해서 구현해줄 것입니다.\nPacker와 Ansible은 서버 설정 자동화라는 동일한 기능을 수행하는 도구입니다. 두 도구는 각각 다양한 특징과 장단점이 있지만, 이 과제에서 알아야 하는 차이점은 아래와 같습니다.\nPacker는 임시 EC2 인스턴스를 생성하여 그 위에서 필요한 설정을 완료한 후, 해당 인스턴스를 AMI로 변환하는 방식으로 동작합니다. 이렇게 생성된 AMI는 이후 실제 인프라 구축 시 그대로 사용할 수 있습니다. 따라서 최종 목적지 서버가 SSH 접근이 제한되는 환경이더라도, 미리 필요한 모든 설정이 완료된 이미지를 사용할 수 있다는 장점이 있습니다.\n반면에 Ansible은 SSH 접근이 가능한 서버에서만 동작하지만, Packer와 달리 인프라 구축 후에 얻을 수 있는 정보(예: EC2의 IP)를 활용할 수 있습니다.\n이러한 특성을 고려하여 이 프로젝트에서는 두 도구를 상황에 맞게 조합하여 사용했습니다.\n\n\n전체적인 구조\n\n\n\n구현 aws 구조\n\n\nPublic subnet의 EC2들에 대한 ssh 접근은 관리용 컴퓨터에서만(terraform, ansible 코드가 실행되는 컴퓨터) 접근이 가능하도록 제한했고, MySQL의 데이터는 Private subnet의 EC2에 저장한 뒤 Public subnet의 EC2만 접근할 수 있도록 설정했습니다. Public subnet의 EC2는 사용자가 원하는 갯수를 설정할 수 있고, 그 갯수에 맞춰서 private subnet의 dbms EC2가 생성되도록 설계했습니다.\n실제 프로덕션 환경이라면 위와 같은 구조로는 설계하지 않습니다. 일단 EC2 머신들을 Auto Scaling Group으로 묶고, 그 앞에 Network Load Balancer를 두어 단일 엔드포인트로 관리하는 것이 좋습니다. 또한 Database는 AWS RDS를 이용하고, WordPress의 파일 시스템은 EFS나 S3를 활용해 Stateless하게 구현하는게 좋습니다.\n\n\n\n조금 더 일반적인 구조(물론 docker compose는 잘 안쓸것 같긴 합니다)\n\n\n제 구현에서는 각 서버가 독립적인 상태와 엔드포인트를 가지고 있습니다.\n그렇게 한 이유는 일단 aws free tier 서비스만으로 과제를 구현하려고 했던게 제일 크고요..(NLB는 사용할 수 없었습니다.) 나머지는 과제 제약사항 때문인데,\n\n\n\n과제 제약사항\n\n\n모든 프로세스는 컨테이너 안에서 동작해야 한다는 제약때문에, aws RDS는 사용할 수 없었습니다. 그리고 database는 public internet에서 접근할 수 없다고 해서, db는 private subnet의 ec2에서 돌아가게 설계했습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/08.html#outro",
    "href": "posts/02_categories/42_seoul/notes/08.html#outro",
    "title": "cloud-1 개념 설명",
    "section": "outro",
    "text": "outro\n여기서 구현된 infra 구조는 사실 별로 근본있는 구조는 아니니까, 이것보다는 IaC 툴을 얼마나 편리하게 사용할 수 있는지에 초점을 맞춰서 봐주시길 바라고 있습니다.\n이어서 코드에 대한 설명은 다음 게시글에 포스팅하겠습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "cloud-1 개념 설명"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/04.html#intro",
    "href": "posts/02_categories/42_seoul/notes/04.html#intro",
    "title": "inception-of-things part 1",
    "section": "Intro",
    "text": "Intro\n\n\n\n42 seoul outer 과제\n\n\n42 Seoul의 공통 과정을 마무리하면, 원하는 분야를 선택하여 심화 과제를 수행할 수 있습니다. 그중에서도 ’Inception-of-Things’는 인프라 관련 심화 과제로, 가장 많은 경험치를 얻을 수 있는 과제입니다.\n얼핏 보면 매우 어려운 과제처럼 느껴질 수 있지만, 개념을 확실히 이해하고 공부한다면 누구나 빠르게 완료할 수 있다고 생각합니다. 저의 경우, CKA 자격증 취득을 목표로 k8s를 공부하던 중 우연히 팀원을 구하게 되어 이 과제를 수행하게 되었습니다. 배경지식이 어느 정도 있는 상태에서 진행하다 보니, 크게 어렵지 않게 잘 마무리할 수 있었던 것 같습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/04.html#개요",
    "href": "posts/02_categories/42_seoul/notes/04.html#개요",
    "title": "inception-of-things part 1",
    "section": "개요",
    "text": "개요\n과제 명세서\n참고한 자료는 다음과 같습니다:\n\nCKA Udemy 강의\nArgoCD Udemy 강의\ngitlab helm 베포 Docs\nVagrant Docs\n\n\n\n\n\n\n\n전체 코드는 비공개 되어있는 상태입니다",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/04.html#개념-설명",
    "href": "posts/02_categories/42_seoul/notes/04.html#개념-설명",
    "title": "inception-of-things part 1",
    "section": "개념 설명",
    "text": "개념 설명\ncluster는 노드(컴퓨터)들의 논리적인 집합을 의미합니다. 일반적으로, 하나의 컴퓨터로 처리하기 어려운 방대한 양의 작업을 처리하기 위해 도입을 합니다.\n클러스터는 특정한 목적을 가지고 있고, 그 안의 노드들을 각자 맡은 역할을 수행합니다. (보통 클러스터 내부의 노드들을 관리하는 master, 작업을 수행하는 worker로 구분할 수 있습니다.) 이때, k8s는 분산된 노드(컴퓨터)들을 하나의 클러스터로 묶어주고, 관리해주는 도구로써 사용할 수 있습니다.\n\n\n\n\n\n\n노트\n\n\n\n컴퓨팅 능력을 확장할 목적으로 수직적 확장과 수평적 확장을 고려할 수 있습니다.\n수직적 확장은 cpu나 memmory 성능을 높여서 단일 노드의 성능을 향상시키는 것을 의미하고, 수평적 확장은 작업을 분산시킬 수 있는 여러 노드를 추가하는 것을 의미합니다.\n클러스터링은 수평적으로 확장된 컴퓨팅 리소스들을 그룹화 해주는 것을 의미합니다.\n\n\n한 가지 주의해야 하는 것은, k8s 자체는 노드를 생성(provision)해주는 도구가 아니라는 것입니다. 즉, provision 단계는 k8s clustering 이전에 진행되어야 합니다.\n\n\n\nPart 1 구조\n\n\nPart 1에서는 vagrant tool을 이용해서 master, agent 역할을 하는 두 대의 가상 머신을 local에서 provision하고, k3s를 이용해서 clustering 하는 것을 요구합니다. 참고로 k3s는 k8s의 경량화 버전입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/04.html#코드-설명",
    "href": "posts/02_categories/42_seoul/notes/04.html#코드-설명",
    "title": "inception-of-things part 1",
    "section": "코드 설명",
    "text": "코드 설명\n파일 구조는 아래와 같습니다.\np1/\n├── scripts/\n│   ├── agent.yml\n│   └── server.yml\n└── Vagrantfile\nvagrant는 local에서 가상 머신을 생성하고, provision을 할 수 있는 도구입니다. 사용자가 원하는 스펙을 Vagrantfile 이름의 파일에 정의하면, vagrant up 명령어를 통해 간단하게 가상머신을 생성할 수 있습니다.\n과제 요구사항에 맞게 spec을 정의해줍니다.\n\n\nVagrantfile\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"bento/ubuntu-24.04\"\n  config.vm.box_version = \"202404.26.0\"\n\n  config.vm.define \"hyunghkiS\" do |control|\n    control.vm.hostname = \"hyunghkiS\"\n    control.vm.network \"private_network\", ip: \"192.168.56.110\"\n    control.vm.provider \"virtualbox\" do |v|\n      v.customize [\"modifyvm\", :id, \"--name\", \"hyunghkiS\"]\n      v.memory = \"1024\"\n      v.cpus = \"1\"\n    end\n    # just for evaluation\n    control.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n      sudo apt-get update\n      sudo apt-get install -y net-tools\n    SHELL\n    control.vm.provision \"shell\", path: \"scripts/server.sh\"\n  end\n  config.vm.define \"hyunghkiSW\" do |control|\n    control.vm.hostname = \"hyunghkiSW\"\n    control.vm.network \"private_network\", ip: \"192.168.56.111\"\n    control.vm.provider \"virtualbox\" do |v|\n      v.customize [\"modifyvm\", :id, \"--name\", \"hyunghkiSW\"]\n      v.memory = \"1024\"\n      v.cpus = \"1\"\n    end\n    # just for evaluation\n    control.vm.provision \"shell\", inline: &lt;&lt;-SHELL\n      sudo apt-get update\n      sudo apt-get install -y net-tools\n    SHELL\n    control.vm.provision \"shell\", path: \"scripts/agent.sh\"\n  end\nend\n\n저 just for evaluation 부분은 아마 과제 명세서에 ifconfig 명령어를 입력해보는 부분 때문에 추가한 것 같습니다. (사실 이 글을 쓰는 시점은 과제를 수행하고 1년이 지난 시점이라 기억이 가물가물 합니다.)\n\n\nserver.sh\n\n#!/bin/bash\n\necho 'alias k=kubectl' &gt;&gt; /home/vagrant/.bashrc\nsource /home/vagrant/.bashrc\n\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\"644\" sh -s - server --node-ip 192.168.56.110\nK3S_TOKEN=$(sudo cat /var/lib/rancher/k3s/server/node-token)\necho $K3S_TOKEN &gt; /vagrant/k3s_token # vagrant 공유 폴더에 master token 정보를 저장해주었습니다.\n\n\n\nagent.sh\n\n#!/bin/bash\n\necho 'alias k=kubectl' &gt;&gt; /home/vagrant/.bashrc\nsource /home/vagrant/.bashrc\n\nK3S_TOKEN=$(cat /vagrant/k3s_token) # vagrant 공유 폴더에 저장된 master token 정보를 읽어옵니다.\ncurl -sfL https://get.k3s.io | K3S_URL=https://192.168.56.110:6443 K3S_TOKEN=$K3S_TOKEN sh -s - --node-ip 192.168.56.111\n\nk3s 공식 문서를 참고해서 master와 agent를 clustering 해주는 스크립트를 작성해주었습니다. 각각 노드 안에서 로직이 실행되어, 하나는 master로, 하나는 agent로 역할을 수행하게 됩니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/04.html#outro",
    "href": "posts/02_categories/42_seoul/notes/04.html#outro",
    "title": "inception-of-things part 1",
    "section": "Outro",
    "text": "Outro\n오랜만에 해당 과제의 로직을 다시 보니까 기억이 잘 안납니다.\n남은 부분은 천천히 포스팅하겠습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 1"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/09.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/09.html#preprocessing",
    "title": "K Nearest Neighbors",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/09.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/09.html#modeling",
    "title": "K Nearest Neighbors",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier()\nclassifier.fit(x_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/09.html#predict",
    "href": "posts/02_categories/machine_learning/notes/09.html#predict",
    "title": "K Nearest Neighbors",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[58  8]\n [ 0 34]]\n\n\n0.92",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/16.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/16.html#preprocessing",
    "title": "Apriori",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/16.csv', header=None)\ntransactions = []\nfor i in range(0, len(dataset)):\n    transactions.append([str(dataset.values[i, j]) for j in range(0, len(dataset.columns))])\ntransactions\n\n[['shrimp',\n  'almonds',\n  'avocado',\n  'vegetables mix',\n  'green grapes',\n  'whole weat flour',\n  'yams',\n  'cottage cheese',\n  'energy drink',\n  'tomato juice',\n  'low fat yogurt',\n  'green tea',\n  'honey',\n  'salad',\n  'mineral water',\n  'salmon',\n  'antioxydant juice',\n  'frozen smoothie',\n  'spinach',\n  'olive oil'],\n ['burgers',\n  'meatballs',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chutney',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'energy bar',\n  'whole wheat rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'light cream',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'champagne',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'chicken',\n  'honey',\n  'oil',\n  'cooking oil',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'black tea',\n  'salmon',\n  'eggs',\n  'chicken',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['meatballs',\n  'milk',\n  'honey',\n  'french fries',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'pasta',\n  'pepper',\n  'eggs',\n  'chocolate',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['rice',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'ham',\n  'body spray',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'shrimp',\n  'pasta',\n  'avocado',\n  'honey',\n  'white wine',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'soup',\n  'avocado',\n  'milk',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'energy bar',\n  'black tea',\n  'salmon',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'tomato sauce',\n  'light cream',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'avocado',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'strong cheese',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'spaghetti',\n  'salmon',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'honey',\n  'whole wheat rice',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'hot dogs',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'avocado',\n  'french fries',\n  'hot dogs',\n  'brownies',\n  'body spray',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'cereals',\n  'clothes accessories',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'black tea',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'brownies',\n  'white wine',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'frozen vegetables',\n  'tomatoes',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'chicken',\n  'gums',\n  'soda',\n  'body spray',\n  'energy drink',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'mineral water',\n  'cider',\n  'cooking oil',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['clothes accessories',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'bug spray',\n  'shallot',\n  'protein bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'avocado',\n  'cider',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'tomatoes',\n  'tomato sauce',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'escalope',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'milk',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'energy bar',\n  'butter',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['asparagus',\n  'salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'french fries',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'shrimp',\n  'pasta',\n  'spaghetti',\n  'mineral water',\n  'meatballs',\n  'olive oil',\n  'energy bar',\n  'french wine',\n  'eggs',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'almonds',\n  'eggs',\n  'french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'soup',\n  'escalope',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ham',\n  'frozen vegetables',\n  'pepper',\n  'oil',\n  'extra dark chocolate',\n  'tea',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'barbecue sauce',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'energy bar',\n  'almonds',\n  'eggs',\n  'corn',\n  'mayonnaise',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'ground beef',\n  'chocolate',\n  'soup',\n  'almonds',\n  'eggs',\n  'hot dogs',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'chocolate',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'energy bar',\n  'pet food',\n  'carrots',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'tomato sauce',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'olive oil',\n  'gums',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'mineral water',\n  'soup',\n  'avocado',\n  'milk',\n  'olive oil',\n  'green grapes',\n  'eggs',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'soup',\n  'cake',\n  'cooking oil',\n  'chicken',\n  'light mayo',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'escalope',\n  'mushroom cream sauce',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'oil',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french wine',\n  'eggs',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'eggs',\n  'french fries',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'salmon',\n  'white wine',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'olive oil',\n  'eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'tomatoes',\n  'mineral water',\n  'soup',\n  'milk',\n  'almonds',\n  'eggs',\n  'chocolate',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'pasta',\n  'frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'eggs',\n  'chocolate',\n  'french fries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'antioxydant juice',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['carrots',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'chocolate',\n  'olive oil',\n  'eggs',\n  'cooking oil',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'pancakes',\n  'chicken',\n  'chili',\n  'tea',\n  'french fries',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'chocolate',\n  'frozen smoothie',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cooking oil',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'tomato sauce',\n  'mineral water',\n  'meatballs',\n  'olive oil',\n  'light cream',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'eggs',\n  'cake',\n  'gums',\n  'cooking oil',\n  'chocolate',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'avocado',\n  'milk',\n  'olive oil',\n  'eggs',\n  'rice',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'shrimp',\n  'pasta',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'nonfat milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'pasta',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'eggs',\n  'chicken',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'soup',\n  'milk',\n  'pancakes',\n  'whole wheat rice',\n  'barbecue sauce',\n  'carrots',\n  'chocolate',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'frozen vegetables',\n  'water spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'milk',\n  'eggs',\n  'whole wheat rice',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'cake',\n  'frozen smoothie',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'chicken',\n  'chocolate bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'spaghetti',\n  'eggs',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'olive oil',\n  'strong cheese',\n  'light cream',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'spaghetti',\n  'avocado',\n  'milk',\n  'oil',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'yams',\n  'milk',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'pasta',\n  'spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'ground beef',\n  'soup',\n  'avocado',\n  'milk',\n  'black tea',\n  'eggs',\n  'barbecue sauce',\n  'carrots',\n  'cookies',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'french fries',\n  'strawberries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'yams',\n  'chicken',\n  'honey',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'honey',\n  'cake',\n  'rice',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'milk',\n  'light mayo',\n  'asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'mineral water',\n  'corn',\n  'cottage cheese',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'french wine',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'whole wheat rice',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'french fries',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'pasta',\n  'milk',\n  'green tea',\n  'french fries',\n  'cookies',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'whole wheat rice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'milk',\n  'whole wheat rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'soup',\n  'meatballs',\n  'chicken',\n  'blueberries',\n  'cooking oil',\n  'champagne',\n  'yogurt cake',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cooking oil',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'milk',\n  'eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'spaghetti',\n  'olive oil',\n  'french wine',\n  'eggs',\n  'french fries',\n  'champagne',\n  'pancakes',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'honey',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'bacon',\n  'eggs',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'tomatoes',\n  'energy bar',\n  'french wine',\n  'antioxydant juice',\n  'french fries',\n  'frozen smoothie',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'spinach',\n  'soda',\n  'energy drink',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bug spray',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'energy bar',\n  'chicken',\n  'eggs',\n  'cake',\n  'french fries',\n  'body spray',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'meatballs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'fromage blanc',\n  'bacon',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'vegetables mix',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'tomatoes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'olive oil',\n  'cereals',\n  'brownies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'cookies',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'pancakes',\n  'cooking oil',\n  'gluten free bar',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'soup',\n  'black tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'grated cheese',\n  'tomatoes',\n  'chocolate',\n  'fromage blanc',\n  'honey',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'ground beef',\n  'milk',\n  'olive oil',\n  'cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'soup',\n  'chutney',\n  'cereals',\n  'energy drink',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'whole wheat rice',\n  'cake',\n  'green tea',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'yams',\n  'soup',\n  'avocado',\n  'salmon',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'meatballs',\n  'vegetables mix',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'shrimp',\n  'pasta',\n  'mineral water',\n  'olive oil',\n  'eggs',\n  'cake',\n  'brownies',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'green grapes',\n  'hot dogs',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'meatballs',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['meatballs',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'chocolate',\n  'olive oil',\n  'french wine',\n  'salmon',\n  'rice',\n  'light mayo',\n  'fresh bread',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'french wine',\n  'vegetables mix',\n  'rice',\n  'clothes accessories',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'dessert wine',\n  'spaghetti',\n  'chicken',\n  'cake',\n  'protein bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french wine',\n  'cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'flax seed',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'energy bar',\n  'green grapes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'eggs',\n  'cake',\n  'chocolate',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['almonds',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'salt',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salt',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'eggs',\n  'pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'chocolate',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'pepper',\n  'spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'red wine',\n  'mineral water',\n  'french fries',\n  'hand protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'energy drink',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strong cheese',\n  'salmon',\n  'green tea',\n  'french fries',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'pasta',\n  'tomatoes',\n  'milk',\n  'frozen smoothie',\n  'sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'olive oil',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'antioxydant juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'almonds',\n  'eggs',\n  'strawberries',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'antioxydant juice',\n  'body spray',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'oil',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'butter',\n  'chicken',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'spaghetti',\n  'cake',\n  'chocolate',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'oil',\n  'cereals',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'spinach',\n  'eggs',\n  'oil',\n  'cooking oil',\n  'green tea',\n  'shampoo',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'mint green tea',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'whole wheat rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'red wine',\n  'spaghetti',\n  'mineral water',\n  'salmon',\n  'eggs',\n  'cooking oil',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'shallot',\n  'tomato juice',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'spaghetti',\n  'muffins',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'eggs',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'muffins',\n  'tomato juice',\n  'mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'yams',\n  'mineral water',\n  'muffins',\n  'frozen smoothie',\n  'hot dogs',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'soup',\n  'milk',\n  'olive oil',\n  'salmon',\n  'eggs',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'champagne',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'french wine',\n  'green grapes',\n  'rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'honey',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'dessert wine',\n  'ground beef',\n  'soup',\n  'salmon',\n  'eggs',\n  'gums',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'carrots',\n  'french fries',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'cooking oil',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'pepper',\n  'mineral water',\n  'chocolate',\n  'eggs',\n  'cake',\n  'mashed potato',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'strong cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'mineral water',\n  'soup',\n  'rice',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'french fries',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'tomatoes',\n  'mineral water',\n  'soup',\n  'avocado',\n  'meatballs',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'turkey',\n  'herb & pepper',\n  'ground beef',\n  'pancakes',\n  'eggs',\n  'light cream',\n  'rice',\n  'champagne',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'french wine',\n  'vegetables mix',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'yams',\n  'butter',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'barbecue sauce',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'french fries',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['antioxydant juice',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'salmon',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'avocado',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'eggs',\n  'green tea',\n  'frozen smoothie',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'chicken',\n  'cider',\n  'green grapes',\n  'honey',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'milk',\n  'chocolate',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'chicken',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'protein bar',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'whole wheat pasta',\n  'yams',\n  'mineral water',\n  'bacon',\n  'nonfat milk',\n  'spinach',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'whole wheat pasta',\n  'spaghetti',\n  'meatballs',\n  'chicken',\n  'frozen smoothie',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'spinach',\n  'cooking oil',\n  'chocolate',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'cider',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'energy bar',\n  'honey',\n  'rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'chocolate',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'strawberries',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'salmon',\n  'honey',\n  'extra dark chocolate',\n  'green tea',\n  'mushroom cream sauce',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomatoes',\n  'mineral water',\n  'meatballs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'water spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'tomatoes',\n  'soup',\n  'butter',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'tomato juice',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'milk',\n  'whole wheat rice',\n  'cereals',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'soup',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'vegetables mix',\n  'spinach',\n  'french fries',\n  'escalope',\n  'cauliflower',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'olive oil',\n  'eggs',\n  'soda',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'pancakes',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'tomato sauce',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'almonds',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'escalope',\n  'shallot',\n  'strawberries',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'blueberries',\n  'oil',\n  'cooking oil',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'frozen smoothie',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'butter',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'soup',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'ground beef',\n  'salmon',\n  'pancakes',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'yams',\n  'chocolate',\n  'olive oil',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'salmon',\n  'eggs',\n  'cookies',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['magazines',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'extra dark chocolate',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'mineral water',\n  'soup',\n  'milk',\n  'french wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'salt',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'escalope',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'fromage blanc',\n  'whole wheat rice',\n  'cake',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'cake',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'eggs',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'honey',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'shrimp',\n  'mineral water',\n  'barbecue sauce',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'eggs',\n  'french fries',\n  'escalope',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'milk',\n  'eggs',\n  'whole wheat rice',\n  'chocolate',\n  'escalope',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'soup',\n  'eggs',\n  'oil',\n  'cooking oil',\n  'energy drink',\n  'protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'grated cheese',\n  'herb & pepper',\n  'shrimp',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'carrots',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'red wine',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'mushroom cream sauce',\n  'cottage cheese',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'cake',\n  'chocolate bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'almonds',\n  'cake',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'frozen vegetables',\n  'milk',\n  'butter',\n  'salmon',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'whole wheat pasta',\n  'whole wheat rice',\n  'chicken',\n  'green tea',\n  'cauliflower',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomato sauce',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'olive oil',\n  'salmon',\n  'green beans',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'tomato sauce',\n  'mineral water',\n  'green grapes',\n  'eggs',\n  'gums',\n  'light cream',\n  'oil',\n  'green tea',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'spaghetti',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'honey',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'mineral water',\n  'oil',\n  'ketchup',\n  'chili',\n  'pet food',\n  'eggplant',\n  'green tea',\n  'escalope',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'mineral water',\n  'pancakes',\n  'eggs',\n  'cake',\n  'blueberries',\n  'tea',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'spaghetti',\n  'salmon',\n  'carrots',\n  'frozen smoothie',\n  'pasta',\n  'mashed potato',\n  'shallot',\n  'light mayo',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'olive oil',\n  'whole wheat rice',\n  'eggplant',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'whole wheat rice',\n  'cake',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'eggs',\n  'ham',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'brownies',\n  'pancakes',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'spaghetti',\n  'milk',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'pancakes',\n  'french fries',\n  'strawberries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'pancakes',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'honey',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'red wine',\n  'tomato sauce',\n  'olive oil',\n  'chili',\n  'french fries',\n  'cookies',\n  'salt',\n  'fresh bread',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'bramble',\n  'frozen smoothie',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'meatballs',\n  'butter',\n  'eggs',\n  'salad',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'spaghetti',\n  'chocolate',\n  'olive oil',\n  'chicken',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'escalope',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'pepper',\n  'soup',\n  'milk',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'pancakes',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'escalope',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french wine',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'spaghetti',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'milk',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'eggs',\n  'bramble',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'frozen vegetables',\n  'tomatoes',\n  'cake',\n  'chicken',\n  'green tea',\n  'toothpaste',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'olive oil',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'almonds',\n  'cooking oil',\n  'cereals',\n  'protein bar',\n  'white wine',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'mineral water',\n  'milk',\n  'eggs',\n  'cake',\n  'burger sauce',\n  'chicken',\n  'french fries',\n  'frozen smoothie',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'mineral water',\n  'whole wheat rice',\n  'yogurt cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'meatballs',\n  'chicken',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'eggs',\n  'green tea',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'herb & pepper',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'salmon',\n  'nonfat milk',\n  'green grapes',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'bug spray',\n  'oatmeal',\n  'sandwich',\n  'asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'almonds',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'butter',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'pepper',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'whole wheat rice',\n  'rice',\n  'cooking oil',\n  'chocolate',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'butter',\n  'chocolate',\n  'french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'milk',\n  'chutney',\n  'cooking oil',\n  'asparagus',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'frozen smoothie',\n  'cookies',\n  'champagne',\n  'cottage cheese',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'mineral water',\n  'eggs',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'tomato juice',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'almonds',\n  'cereals',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'chocolate',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'whole wheat rice',\n  'champagne',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'pepper',\n  'mineral water',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'turkey',\n  'spaghetti',\n  'milk',\n  'whole wheat rice',\n  'oil',\n  'tomato juice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'french wine',\n  'french fries',\n  'light mayo',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'green grapes',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'soup',\n  'milk',\n  'almonds',\n  'eggs',\n  'oil',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'cake',\n  'green tea',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'tomatoes',\n  'ground beef',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cider',\n  'eggs',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cider',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'cookies',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'escalope',\n  'champagne',\n  'hot dogs',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'escalope',\n  'shallot',\n  'protein bar',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'parmesan cheese',\n  'spaghetti',\n  'meatballs',\n  'butter',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'escalope',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'pepper',\n  'tomato sauce',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'escalope',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'french fries',\n  'asparagus',\n  'low fat yogurt',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cake',\n  'light cream',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'barbecue sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'escalope',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'chicken',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'energy bar',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'eggs',\n  'gums',\n  'escalope',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'pancakes',\n  'chocolate',\n  'escalope',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'yams',\n  'flax seed',\n  'salmon',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burger sauce',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'soda',\n  'pet food',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'olive oil',\n  'chicken',\n  'eggs',\n  'extra dark chocolate',\n  'melons',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'ground beef',\n  'yams',\n  'milk',\n  'strong cheese',\n  'salmon',\n  'muffins',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'spaghetti',\n  'light cream',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chicken',\n  'tea',\n  'pancakes',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'turkey',\n  'herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'cooking oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'shallot',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'chocolate',\n  'shrimp',\n  'whole wheat pasta',\n  'pepper',\n  'mineral water',\n  'soup',\n  'milk',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'avocado',\n  'milk',\n  'butter',\n  'cooking oil',\n  'chocolate',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'eggs',\n  'french fries',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'butter',\n  'eggs',\n  'green tea',\n  'cottage cheese',\n  'mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['antioxydant juice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'pancakes',\n  'eggs',\n  'whole wheat rice',\n  'cooking oil',\n  'chocolate',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'parmesan cheese',\n  'mineral water',\n  'olive oil',\n  'bacon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green grapes',\n  'gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'milk',\n  'eggs',\n  'cooking oil',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'spaghetti',\n  'olive oil',\n  'butter',\n  'chocolate',\n  'french fries',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'eggs',\n  'gums',\n  'cooking oil',\n  'frozen smoothie',\n  'cottage cheese',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'grated cheese',\n  'energy bar',\n  'vegetables mix',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'nonfat milk',\n  'cooking oil',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'mineral water',\n  'salmon',\n  'whole wheat rice',\n  'cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'chocolate',\n  'low fat yogurt',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'light cream',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'parmesan cheese',\n  'butter',\n  'whole wheat rice',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'avocado',\n  'milk',\n  'chicken',\n  'rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'black tea',\n  'french wine',\n  'pancakes',\n  'rice',\n  'green tea',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'parmesan cheese',\n  'mineral water',\n  'eggs',\n  'cake',\n  'oil',\n  'cooking oil',\n  'toothpaste',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'olive oil',\n  'black tea',\n  'vegetables mix',\n  'cooking oil',\n  'green tea',\n  'frozen smoothie',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'cider',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'tomato sauce',\n  'mineral water',\n  'chocolate',\n  'escalope',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'mineral water',\n  'energy bar',\n  'eggs',\n  'cooking oil',\n  'pancakes',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'soup',\n  'escalope',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pepper',\n  'mineral water',\n  'soup',\n  'milk',\n  'eggs',\n  'rice',\n  'barbecue sauce',\n  'clothes accessories',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'chocolate',\n  'olive oil',\n  'light cream',\n  'chicken',\n  'green tea',\n  'french fries',\n  'sandwich',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'salmon',\n  'green tea',\n  'cookies',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'soup',\n  'salmon',\n  'pancakes',\n  'chicken',\n  'mint green tea',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'chocolate',\n  'milk',\n  'olive oil',\n  'almonds',\n  'french wine',\n  'yogurt cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'spaghetti',\n  'nonfat milk',\n  'eggs',\n  'gums',\n  'chocolate',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'ground beef',\n  'olive oil',\n  'energy bar',\n  'chicken',\n  'brownies',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'cooking oil',\n  'frozen smoothie',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['vegetables mix',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'escalope',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'tomato sauce',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'french wine',\n  'eggs',\n  'whole wheat rice',\n  'chocolate',\n  'escalope',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'carrots',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'herb & pepper',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'avocado',\n  'milk',\n  'almonds',\n  'muffins',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'salmon',\n  'antioxydant juice',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'shrimp',\n  'whole wheat pasta',\n  'ground beef',\n  'soup',\n  'energy bar',\n  ' asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'butter',\n  'pancakes',\n  'french fries',\n  'frozen smoothie',\n  'white wine',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'frozen vegetables',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'shrimp',\n  'frozen vegetables',\n  'whole wheat pasta',\n  'yams',\n  'mineral water',\n  'olive oil',\n  'almonds',\n  'cake',\n  'chicken',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'energy bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'milk',\n  'almonds',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'pet food',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'whole wheat pasta',\n  'olive oil',\n  'energy bar',\n  'light cream',\n  'oil',\n  'french fries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'mineral water',\n  'fromage blanc',\n  'eggs',\n  'honey',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bramble',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'pepper',\n  'milk',\n  'eggs',\n  'green tea',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'red wine',\n  'butter',\n  'french fries',\n  'cookies',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'vegetables mix',\n  'eggs',\n  'whole wheat rice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'mineral water',\n  'salmon',\n  'whole wheat rice',\n  'burger sauce',\n  'escalope',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'body spray',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'whole wheat pasta',\n  'cake',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'parmesan cheese',\n  'eggs',\n  'french fries',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'almonds',\n  'pancakes',\n  'chocolate',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'black tea',\n  'french wine',\n  'cider',\n  'chutney',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['almonds',\n  'cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'chicken',\n  'light cream',\n  'green tea',\n  'chocolate',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'blueberries',\n  'cooking oil',\n  'chocolate',\n  'cookies',\n  'champagne',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'frozen smoothie',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'salmon',\n  'muffins',\n  'chocolate',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'herb & pepper',\n  'parmesan cheese',\n  'milk',\n  'olive oil',\n  'pancakes',\n  'honey',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'butter',\n  'whole wheat rice',\n  'french fries',\n  'escalope',\n  'cookies',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'french fries',\n  'champagne',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'mineral water',\n  'milk',\n  'eggs',\n  'chocolate',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'milk',\n  'chicken',\n  'nonfat milk',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'milk',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'carrots',\n  'protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'grated cheese',\n  'honey',\n  'green beans',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'light cream',\n  'rice',\n  'oil',\n  'chicken',\n  'barbecue sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['nonfat milk',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'parmesan cheese',\n  'whole wheat pasta',\n  'mineral water',\n  'milk',\n  'eggs',\n  'low fat yogurt',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'tomato sauce',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'protein bar',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'honey',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'parmesan cheese',\n  'french wine',\n  'french fries',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'parmesan cheese',\n  'whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'avocado',\n  'milk',\n  'olive oil',\n  'muffins',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'shrimp',\n  'spaghetti',\n  'olive oil',\n  'honey',\n  'strawberries',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'olive oil',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'olive oil',\n  'pancakes',\n  'light cream',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'avocado',\n  'pancakes',\n  'eggs',\n  'honey',\n  'green tea',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'burgers',\n  'almonds',\n  'honey',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bug spray',\n  'clothes accessories',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'honey',\n  'whole wheat rice',\n  'green beans',\n  'french fries',\n  'frozen smoothie',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'oil',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'mineral water',\n  'milk',\n  'almonds',\n  'strong cheese',\n  'cake',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'rice',\n  'eggplant',\n  'hand protein bar',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'meatballs',\n  'nonfat milk',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cake',\n  'cookies',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'bug spray',\n  'french fries',\n  'yogurt cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'milk',\n  'cider',\n  'chicken',\n  'green beans',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'avocado',\n  'milk',\n  'salmon',\n  'frozen smoothie',\n  'escalope',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'whole wheat rice',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'olive oil',\n  'chicken',\n  'salmon',\n  'pancakes',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'chocolate',\n  'grated cheese',\n  'frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'milk',\n  'butter',\n  'bacon',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'pepper',\n  'chocolate',\n  'fromage blanc',\n  'eggs',\n  'green tea',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'ground beef',\n  'avocado',\n  'milk',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'green tea',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'avocado',\n  'bacon',\n  'oil',\n  'french fries',\n  'brownies',\n  'pancakes',\n  'zucchini',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'chicken',\n  'cake',\n  'burger sauce',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'cooking oil',\n  'tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'soup',\n  'vegetables mix',\n  'pancakes',\n  'body spray',\n  'melons',\n  'protein bar',\n  'asparagus',\n  'mayonnaise',\n  'mint',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'muffins',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'spaghetti',\n  'milk',\n  'gums',\n  'light cream',\n  'cooking oil',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'pancakes',\n  'french fries',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'parmesan cheese',\n  'ground beef',\n  'tomato sauce',\n  'milk',\n  'extra dark chocolate',\n  'melons',\n  'mint',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'chicken',\n  'chocolate',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'frozen smoothie',\n  'sandwich',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'frozen smoothie',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'rice',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'whole wheat pasta',\n  'meatballs',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'parmesan cheese',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'cream',\n  'black tea',\n  'almonds',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'parmesan cheese',\n  'ground beef',\n  'mineral water',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'pancakes',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'soda',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'chicken',\n  'rice',\n  'oil',\n  'cooking oil',\n  'hot dogs',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'chocolate',\n  'champagne',\n  'yogurt cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'yogurt cake',\n  'light mayo',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'olive oil',\n  'light cream',\n  'cooking oil',\n  'chicken',\n  'extra dark chocolate',\n  'cereals',\n  'french fries',\n  'frozen smoothie',\n  'pancakes',\n  'tomato juice',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'soup',\n  'avocado',\n  'milk',\n  'yogurt cake',\n  'energy drink',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'chocolate',\n  'ground beef',\n  'mineral water',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'champagne',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'olive oil',\n  'rice',\n  'antioxydant juice',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomato sauce',\n  'spaghetti',\n  'milk',\n  'pancakes',\n  'energy drink',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'strong cheese',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'cider',\n  'cooking oil',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'parmesan cheese',\n  'spaghetti',\n  'fromage blanc',\n  'vegetables mix',\n  'pancakes',\n  'honey',\n  'hot dogs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green beans',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'honey',\n  'whole wheat rice',\n  'champagne',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'milk',\n  'butter',\n  'black tea',\n  'eggs',\n  'frozen smoothie',\n  'light mayo',\n  'shampoo',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'milk',\n  'pancakes',\n  'whole wheat rice',\n  'cooking oil',\n  'frozen smoothie',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'eggs',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'muffins',\n  'french fries',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'blueberries',\n  'soda',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['nonfat milk',\n  'cookies',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'ground beef',\n  'soup',\n  'milk',\n  'butter',\n  'honey',\n  'cake',\n  'mint green tea',\n  'brownies',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'napkins',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'parmesan cheese',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'energy bar',\n  'butter',\n  'almonds',\n  'fromage blanc',\n  'eggs',\n  'cake',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'honey',\n  'pasta',\n  'energy drink',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'chocolate',\n  'milk',\n  'french wine',\n  'muffins',\n  'pancakes',\n  'champagne',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'tomatoes',\n  'parmesan cheese',\n  'ground beef',\n  'fromage blanc',\n  'eggs',\n  'honey',\n  'cake',\n  'rice',\n  'cereals',\n  'french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'strong cheese',\n  'cereals',\n  'escalope',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'fresh tuna',\n  'spaghetti',\n  'pancakes',\n  'eggs',\n  'cake',\n  'cottage cheese',\n  'energy drink',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'honey',\n  'extra dark chocolate',\n  'carrots',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'frozen smoothie',\n  'cottage cheese',\n  'strawberries',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strong cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'avocado',\n  'cake',\n  'light cream',\n  'cooking oil',\n  'chicken',\n  'chocolate bread',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'energy bar',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'champagne',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'chocolate',\n  'avocado',\n  'butter',\n  'zucchini',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'green tea',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomato sauce',\n  'milk',\n  'butter',\n  'bacon',\n  'salmon',\n  'cooking oil',\n  'chocolate',\n  'french fries',\n  'hot dogs',\n  'melons',\n  'protein bar',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'burger sauce',\n  'brownies',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'pancakes',\n  'whole wheat rice',\n  'green tea',\n  'french fries',\n  'cookies',\n  'shallot',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'butter',\n  'cooking oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'red wine',\n  'frozen vegetables',\n  'pepper',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'honey',\n  'whole wheat rice',\n  'cooking oil',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'eggs',\n  'cake',\n  'chili',\n  'pet food',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'milk',\n  'butter',\n  'chicken',\n  'salt',\n  'mayonnaise',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'fromage blanc',\n  'honey',\n  'gums',\n  'chocolate',\n  'french fries',\n  'frozen smoothie',\n  'sparkling water',\n  'strawberries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sparkling water',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'butter',\n  'vegetables mix',\n  'green grapes',\n  'pancakes',\n  'eggs',\n  'cake',\n  'barbecue sauce',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'yams',\n  'eggs',\n  'burger sauce',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'eggs',\n  'chocolate',\n  'champagne',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french fries',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cereals',\n  'salt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'chocolate',\n  'french fries',\n  'escalope',\n  'cookies',\n  'brownies',\n  'pancakes',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'yams',\n  'chicken',\n  'cooking oil',\n  'chocolate',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'olive oil',\n  'bug spray',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'olive oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'oil',\n  'chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'whole wheat rice',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'frozen vegetables',\n  'chicken',\n  'fromage blanc',\n  'honey',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'champagne',\n  'cookies',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'chocolate',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'spaghetti',\n  'french fries',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'oil',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'milk',\n  'butter',\n  'french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'chicken',\n  'eggs',\n  'frozen smoothie',\n  'cauliflower',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'tomatoes',\n  'milk',\n  'energy bar',\n  'almonds',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'milk',\n  'cake',\n  'cooking oil',\n  'french fries',\n  'escalope',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'olive oil',\n  'whole wheat rice',\n  'cake',\n  'frozen smoothie',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'red wine',\n  'pepper',\n  'spaghetti',\n  'fromage blanc',\n  'salmon',\n  'pancakes',\n  'eggs',\n  'honey',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'parmesan cheese',\n  'soup',\n  'chocolate',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'vegetables mix',\n  'nonfat milk',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'parmesan cheese',\n  'cider',\n  'muffins',\n  'spinach',\n  'honey',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'mushroom cream sauce',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'almonds',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'butter',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'french fries',\n  'light mayo',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'spaghetti',\n  'whole wheat rice',\n  'eggplant',\n  'chocolate',\n  'cookies',\n  'shallot',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'spaghetti',\n  'avocado',\n  'milk',\n  'blueberries',\n  'light cream',\n  'rice',\n  'green tea',\n  'chocolate',\n  'french fries',\n  'frozen smoothie',\n  'cookies',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'tomatoes',\n  'bacon',\n  'whole wheat rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'cake',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'fromage blanc',\n  'ketchup',\n  'chocolate',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'french fries',\n  'salt',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'yogurt cake',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'meatballs',\n  'milk',\n  'olive oil',\n  'chicken',\n  'honey',\n  'frozen smoothie',\n  'escalope',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'eggs',\n  'light cream',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'olive oil',\n  'blueberries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'red wine',\n  'mineral water',\n  'eggs',\n  'oil',\n  'carrots',\n  'hand protein bar',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'soup',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'mineral water',\n  'chicken',\n  'fromage blanc',\n  'salmon',\n  'eggs',\n  'cake',\n  'frozen smoothie',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'butter',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'mineral water',\n  'whole wheat rice',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'olive oil',\n  'butter',\n  'salmon',\n  'oil',\n  'cooking oil',\n  'frozen smoothie',\n  'cauliflower',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'eggs',\n  'escalope',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'milk',\n  'carrots',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'soup',\n  'meatballs',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'cooking oil',\n  'french fries',\n  'cookies',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'milk',\n  'olive oil',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'pancakes',\n  'cake',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'chicken',\n  'french fries',\n  'cottage cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'body spray',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'mineral water',\n  'muffins',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'spaghetti',\n  'milk',\n  'cider',\n  'eggs',\n  'honey',\n  'cake',\n  'green tea',\n  'french fries',\n  'brownies',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ...]",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Apriori"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/16.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/16.html#modeling",
    "title": "Apriori",
    "section": "Modeling",
    "text": "Modeling\n\nfrom apyori import apriori\n\nrules = apriori(transactions=transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2, max_length=2)\n\n\nresults = list(rules)\nresults\n\n[RelationRecord(items=frozenset({'light cream', 'chicken'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n RelationRecord(items=frozenset({'mushroom cream sauce', 'escalope'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n RelationRecord(items=frozenset({'honey', 'fromage blanc'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n RelationRecord(items=frozenset({'ground beef', 'herb & pepper'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n RelationRecord(items=frozenset({'ground beef', 'tomato sauce'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n RelationRecord(items=frozenset({'light cream', 'olive oil'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n RelationRecord(items=frozenset({'whole wheat pasta', 'olive oil'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n RelationRecord(items=frozenset({'pasta', 'shrimp'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]\n\n\n\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    confidences = [result[2][0][2] for result in results]\n    lifts       = [result[2][0][3] for result in results]\n    return list(zip(lhs, rhs, supports, confidences, lifts))\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])\nresultsinDataFrame\n\n\n\n\n\n\n\n\nLeft Hand Side\nRight Hand Side\nSupport\nConfidence\nLift\n\n\n\n\n0\nlight cream\nchicken\n0.004533\n0.290598\n4.843951\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n0.300699\n3.790833\n\n\n2\npasta\nescalope\n0.005866\n0.372881\n4.700812\n\n\n3\nfromage blanc\nhoney\n0.003333\n0.245098\n5.164271\n\n\n4\nherb & pepper\nground beef\n0.015998\n0.323450\n3.291994\n\n\n5\ntomato sauce\nground beef\n0.005333\n0.377358\n3.840659\n\n\n6\nlight cream\nolive oil\n0.003200\n0.205128\n3.114710\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n0.271493\n4.122410\n\n\n8\npasta\nshrimp\n0.005066\n0.322034\n4.506672\n\n\n\n\n\n\n\n\nresultsinDataFrame.nlargest(n=10, columns='Lift')\n\n\n\n\n\n\n\n\nLeft Hand Side\nRight Hand Side\nSupport\nConfidence\nLift\n\n\n\n\n3\nfromage blanc\nhoney\n0.003333\n0.245098\n5.164271\n\n\n0\nlight cream\nchicken\n0.004533\n0.290598\n4.843951\n\n\n2\npasta\nescalope\n0.005866\n0.372881\n4.700812\n\n\n8\npasta\nshrimp\n0.005066\n0.322034\n4.506672\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n0.271493\n4.122410\n\n\n5\ntomato sauce\nground beef\n0.005333\n0.377358\n3.840659\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n0.300699\n3.790833\n\n\n4\nherb & pepper\nground beef\n0.015998\n0.323450\n3.291994\n\n\n6\nlight cream\nolive oil\n0.003200\n0.205128\n3.114710",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Apriori"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/15.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/15.html#preprocessing",
    "title": "hierarchical clustering",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/14.csv')\nx = dataset.iloc[:, [3, 4]].values",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/15.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/15.html#modeling",
    "title": "hierarchical clustering",
    "section": "Modeling",
    "text": "Modeling\n\nimport scipy.cluster.hierarchy as sch\n\ndendogram = sch.dendrogram(sch.linkage(x, method='ward'))\nplt.title('dendogram')\nplt.xlabel('Customers')\nplt.ylabel('Distance')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')\nyh = hc.fit_predict(x)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/15.html#visualize",
    "href": "posts/02_categories/machine_learning/notes/15.html#visualize",
    "title": "hierarchical clustering",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(x[yh == 0, 0], x[yh == 0, 1], c='red', label='Cluster 1')\nplt.scatter(x[yh == 1, 0], x[yh == 1, 1], c='pink', label='Cluster 2')\nplt.scatter(x[yh == 2, 0], x[yh == 2, 1], c='blue', label='Cluster 3')\nplt.scatter(x[yh == 3, 0], x[yh == 3, 1], c='purple', label='Cluster 4')\nplt.scatter(x[yh == 4, 0], x[yh == 4, 1], c='cyan', label='Cluster 5')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/03.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/03.html#preprocessing",
    "title": "Multiple Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/03.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# model automatically avoid dummy variable trap\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.2)\n\n# in multiple linear regression, we don't need to apply feature scaling",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/03.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/03.html#modeling",
    "title": "Multiple Linear Regression",
    "section": "modeling",
    "text": "modeling\n\nfrom sklearn.linear_model import LinearRegression\n\n# model automatically choose best model (dont need to apply 후진제거법)\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/03.html#predict",
    "href": "posts/02_categories/machine_learning/notes/03.html#predict",
    "title": "Multiple Linear Regression",
    "section": "predict",
    "text": "predict\n\ny_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred), 1), \n                      y_test.reshape(len(y_test), 1)), 1))\n\n[[138322.22 146121.95]\n [190142.33 191792.06]\n [102290.15  99937.59]\n [102011.24  96479.51]\n [135465.24 125370.37]\n [158930.96 155752.6 ]\n [115700.53 110352.25]\n [181379.03 182901.99]\n [ 79850.61  78239.91]\n [ 92415.33  81005.76]\n [ 64929.35  35673.41]\n [112669.86 108733.99]\n [161132.02 156122.51]\n [123970.57 108552.04]\n [143319.49 144259.4 ]\n [131033.94 124266.9 ]\n [ 53952.84  14681.4 ]\n [156637.38 152211.77]\n [121726.18 118474.03]\n [176370.52 166187.94]\n [130091.21 141585.52]\n [121421.64 126992.93]\n [103873.65  97427.84]\n [160743.9  149759.96]\n [121516.75 111313.02]\n [ 76148.93  90708.19]\n [120893.91 105008.31]\n [ 50612.44  42559.73]\n [105553.8  107404.34]\n [ 83092.48  81229.06]\n [ 78030.09  71498.49]\n [154020.85 129917.04]\n [197983.97 192261.83]\n [ 66742.41  65200.33]\n [130734.81 134307.35]\n [ 89997.76  96712.8 ]\n [173214.1  156991.12]\n [101428.58  89949.14]\n [ 57465.13  49490.75]\n [147657.78 132602.65]]",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/03.html#evaluate",
    "href": "posts/02_categories/machine_learning/notes/03.html#evaluate",
    "title": "Multiple Linear Regression",
    "section": "evaluate",
    "text": "evaluate\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n0.9166557911386918",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/00.html#machine-learning-process",
    "href": "posts/02_categories/machine_learning/notes/00.html#machine-learning-process",
    "title": "overview",
    "section": "Machine Learning Process",
    "text": "Machine Learning Process\n\nData Pre-Processing\n\nimport data\nclean data\nsplit data trainig and testing\nfeature scailing\n\nnormalization: \\(\\frac{x - min(x)}{max(x) - min(x)}\\)\nstandardization: \\(\\frac{x - μ}{σ}\\)\n\n\nModeling\n\nbuild / train model\nmake predictions\n\nEvaluation\n\ncalculate performance metrix\nmake a verdict",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "overview"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/12.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/12.html#preprocessing",
    "title": "Decision Tree Classification",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/12.html#modeling---linear",
    "href": "posts/02_categories/machine_learning/notes/12.html#modeling---linear",
    "title": "Decision Tree Classification",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion='entropy')\nclassifier.fit(x_train, y_train)\n\nDecisionTreeClassifier(criterion='entropy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy')",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/12.html#predict",
    "href": "posts/02_categories/machine_learning/notes/12.html#predict",
    "title": "Decision Tree Classification",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[59  7]\n [ 7 27]]\n\n\n0.86",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/12.html#predict-1",
    "href": "posts/02_categories/machine_learning/notes/12.html#predict-1",
    "title": "Decision Tree Classification",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[59  7]\n [ 7 27]]\n\n\n0.86",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/10.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/10.html#preprocessing",
    "title": "Support Vector Machine",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/10.html#modeling---linear",
    "href": "posts/02_categories/machine_learning/notes/10.html#modeling---linear",
    "title": "Support Vector Machine",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.svm import SVC\n\nclassifier = SVC()\nclassifier.fit(x_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/10.html#predict",
    "href": "posts/02_categories/machine_learning/notes/10.html#predict",
    "title": "Support Vector Machine",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[55  6]\n [ 5 34]]\n\n\n0.89",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/10.html#modeling---non-linear",
    "href": "posts/02_categories/machine_learning/notes/10.html#modeling---non-linear",
    "title": "Support Vector Machine",
    "section": "Modeling - non-linear",
    "text": "Modeling - non-linear\n\nclassifier = SVC(kernel='rbf')\nclassifier.fit(x_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/10.html#predict-1",
    "href": "posts/02_categories/machine_learning/notes/10.html#predict-1",
    "title": "Support Vector Machine",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[55  6]\n [ 5 34]]\n\n\n0.89",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/05.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/05.html#preprocessing",
    "title": "Support Vector Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values\ny = y.reshape(len(y), 1)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_x = StandardScaler()\nsc_y = StandardScaler()\n\nx = sc_x.fit_transform(x)\ny = sc_y.fit_transform(y)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/05.html#train",
    "href": "posts/02_categories/machine_learning/notes/05.html#train",
    "title": "Support Vector Regression",
    "section": "Train",
    "text": "Train\n\nfrom sklearn.svm import SVR\n\nregressor = SVR(kernel='rbf')\nregressor.fit(x, y)\n\nSVR()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVR?Documentation for SVRiFittedSVR()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/05.html#visualize",
    "href": "posts/02_categories/machine_learning/notes/05.html#visualize",
    "title": "Support Vector Regression",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(sc_x.inverse_transform(x), sc_y.inverse_transform(y), color='red')\nplt.plot(sc_x.inverse_transform(x), sc_y.inverse_transform(regressor.predict(x).reshape(-1, 1)))\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/05.html#high-resolution",
    "href": "posts/02_categories/machine_learning/notes/05.html#high-resolution",
    "title": "Support Vector Regression",
    "section": "High resolution",
    "text": "High resolution\n\nx_grid = np.arange(min(sc_x.inverse_transform(x)), max(sc_x.inverse_transform(x)), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(sc_x.inverse_transform(x), sc_y.inverse_transform(y), color='red')\nplt.plot(x_grid, sc_y.inverse_transform(regressor.predict(sc_x.transform(x_grid)).reshape(-1, 1)))\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/07.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/07.html#preprocessing",
    "title": "random forest",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/07.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/07.html#modeling",
    "title": "random forest",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=10, random_state=0)\n\n# 모델 학습\nregressor.fit(x, y)\n\nRandomForestRegressor(n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(n_estimators=10, random_state=0)",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/07.html#visualization",
    "href": "posts/02_categories/machine_learning/notes/07.html#visualization",
    "title": "random forest",
    "section": "Visualization",
    "text": "Visualization\n\nx_grid = np.arange(min(x), max(x), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(x, y, color='red')\nplt.plot(x_grid, regressor.predict(x_grid), color='blue')\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/06.html#preprocessing",
    "href": "posts/02_categories/machine_learning/notes/06.html#preprocessing",
    "title": "Decision Tree Regression",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/06.html#modeling",
    "href": "posts/02_categories/machine_learning/notes/06.html#modeling",
    "title": "Decision Tree Regression",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor()\n\n# 모델 학습\nregressor.fit(x, y)\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/02_categories/machine_learning/notes/06.html#visualization",
    "href": "posts/02_categories/machine_learning/notes/06.html#visualization",
    "title": "Decision Tree Regression",
    "section": "Visualization",
    "text": "Visualization\n\nx_grid = np.arange(min(x), max(x), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(x, y, color='red')\nplt.plot(x_grid, regressor.predict(x_grid), color='blue')\nplt.show()",
    "crumbs": [
      "Home",
      "Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/notes/00.html#명동형부",
    "href": "posts/01_projects/toeic/notes/00.html#명동형부",
    "title": "Toeic 문법",
    "section": "명동형부",
    "text": "명동형부\n\n명사 / 대명사\n\n기본 명사 생김새\n\n명사 1초 공식 TOP 5\n\n관사(a/an/the) + 명사 + 전치사\n전치사 + 명사 + 전치사\n관사/소유격 + (부사) + (형용사) + 명사\n형용사(지시형용사, 수량형용사) + 명사\n명사 + 명사\n\n명사 자리\n\n주어\n타/목\n전/목\n보어\n\n가산명사 vs. 불가산명사 &gt; 관사 a/an는 가산 단수 명사 앞에만 쓸 수 있다. &gt; the는 가산 단수 복수, 불가산 명사 앞에 쓸 수 있다.\n\n불가산 명사: a/an, -s 안 붙음 information: 정보 advice: 조언 merchandise: 상품 access: 접근 assistance: 지원 equipment: 장비 luggage/ baggage: 수하물 stationery: 문구류 funding: 자금 제공 furniture: 가구 produce: 농산물 news: 소식 \n\n~ing는 대부분 불가산 명사\n\n\n사람명사 vs. 사물명사\n\n사람명사: 가산 명사\n사물명사: 가산/불가산 명사\n\n특이 어미 명사: -al(proposal, arrival, approval), -ive(initiative),\n\n기본 인칭대명사 표\n\n소유격-소유대명사\n\n소유격 뒤에만 명사가 올 수 있다.\nhis: 소유격이면서 소유대명사 \n\n목적격-재귀대명사\n\n재귀대명사는 재귀용법, 강조용법(부사 자리)으로 사용 가능\n\n지시대명사\n\nThis, That + 단수동사/ These, Those + 복수동사\nThose who + 복수동사\n\n부정대명사\n\none vs another\nsome vs any\n\nsome: 긍정문(not이 없음), 권유나 제안을 나타내는 의문문\nany: 부정문, 의문문, 조건문, 어떤 ~라도\n\n\n\n\n\n\n동사 (수, 태, 시제)\n\nS질량의 보존의 법칙: 단수 주어 + 단수 동사 / 복수 주어 + 복수 동사\n\n고유명사(대문자)는 단수\n\n\n주어 + 거품(부사 / 전치사구) + 동사\n무조건 동사원형 자리\n\n조동사(will, can, should, may) + 동사원형\nplease + 동사원형\n\n무조건 단수 동사\n\n단수 명사\n불가산 명사\nto 부정사구\n동명사구\nthat 절\n\n\n기본 동사 보는 방법\n\npp + be: 수동\n나머지: 능동\n\n\n\n\n\n12가지 시제\n\n\n\n\n형용사 / 부사\n\n형용사 위치와 공식\n\n주격 보어자리(s + 2v + sc), 2v: be, become, remain, seem, Stay\n목적격 보어자리(s + 5v + o + oc), 5v: Keep, Find, Consider, Make\n\n수량형용사 수일치\n\neach, every, another + 단수 명사\nmany, (a) few, a number of, several + 복수명사\n\na number of + 주어: 주어에 맞춘 동사\nthe number of + 주어: the number of에 맞춘 동사(단수)\n\nmuch, little, a little, less + 불가산 명사\n\na little(few) vs litte(few): a 가 붙으면 긍정, 아니면 부정\n\nall, some, most, a lot of + 복수 명사/ 불가산 명사",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/notes/00.html#전접부",
    "href": "posts/01_projects/toeic/notes/00.html#전접부",
    "title": "Toeic 문법",
    "section": "전접부",
    "text": "전접부\n\n전치사 / 등위접속사 / 부사절 접속사\n\n빈출 접속사\n\nat: 시각(at 7 a.m., at the end of this year), 정확한 지점 / 주소\non: 몇 월 몇 일, 일, 접촉(on the table, on the wall, on the ceiling)\nin: 연도, 월, 계절, 행정 구역, 3차원 공간\nuntil: (동사가) 지속성\nby: (동사가) 일회성\nduring: 특정 기간 명사\nfor: 숫자, 숙어, ~을 위해\nthrough: ~을 통해(관통해서), 수단 / 매개\nthroughout: 장소, 시간\nbetween / among: 복수명사가 온다. between은 2개, among은 3개 이상\nsince: 부사로도 쓸 수 있긴 함\nbecause of / due to / owing to / on account of: ~때문에\n\n\n등위접속사\n\nBoth A and B\n\n복수 동사\n\nEither A or B\nNeither A nor B\nNot only A but also B\nB As well as A\nNot A but B\n\nB에 맞춘 동사\n\n\n부사절 접속사\n\n조건: if, unless, provided (that), as long as, in case (that), in event (that)\n\n\n\n\n명사절 접속사 / 형용사절 접속사\n\nif 미래내용: 현재동사\n형용사절 접속사\n\n소유격 + 완전한 문장\n그 외 + 불완전한 문장\n, 다음에 that 안 씀\n\n\n\n\n접속부사 / 전치사 접속사 부사\n\nnotwithstanding: ~에도 불구하고",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/notes/00.html#준동사",
    "href": "posts/01_projects/toeic/notes/00.html#준동사",
    "title": "Toeic 문법",
    "section": "준동사",
    "text": "준동사\n\nTo부정사 / 동명사 / 분사\n\nTo부정사: 명사, 형용사, 부사(~하기 위해서 so as to, in order to) 역할. 미래 지향적인 느낌\n동명사: 명사 역할. 과거 지향적이고 부정적인 느낌\n\n\n\n\n분사: 형용사 역할\n\n자동사: ing",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/toeic/notes/00.html#기타",
    "href": "posts/01_projects/toeic/notes/00.html#기타",
    "title": "Toeic 문법",
    "section": "기타",
    "text": "기타\n\n분사 구문 / 비교 구문",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/index.html",
    "href": "posts/01_projects/정보처리기사/index.html",
    "title": "정보처리기사",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2025-12-19\n        종료일: 2026-06-12\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증정보처리기사",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/index.html#details",
    "href": "posts/01_projects/정보처리기사/index.html#details",
    "title": "정보처리기사",
    "section": "Details",
    "text": "Details\n정보처리기사 자격증 학습 관련 노트",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/index.html#tasks",
    "href": "posts/01_projects/정보처리기사/index.html#tasks",
    "title": "정보처리기사",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    필기 접수 (2026.01.13 10:00)\n                \n                \n            \n\n            \n            \n                \n                    \n                    필기 시험 (2026.01.30)\n                \n                \n            \n\n            \n            \n                \n                    \n                    필기 합격 발표 (2026.03.11 09:00)\n                \n                \n            \n\n            \n            \n                \n                    \n                    실기 접수 (2026.03.23 10:00)\n                \n                \n            \n\n            \n            \n                \n                    \n                    실기 시험 (2026.04.18)\n                \n                \n            \n\n            \n            \n                \n                    \n                    실기 합격 발표 (2026.06.12 09:00)",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/index.html#참고-자료",
    "href": "posts/01_projects/정보처리기사/index.html#참고-자료",
    "title": "정보처리기사",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/index.html#related-posts",
    "href": "posts/01_projects/정보처리기사/index.html#related-posts",
    "title": "정보처리기사",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/대학원/01.html",
    "href": "posts/01_projects/진로준비/notes/대학원/01.html",
    "title": "연구실",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "대학원",
      "연구실"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/index.html",
    "href": "posts/01_projects/진로준비/index.html",
    "title": "진로 준비",
    "section": "",
    "text": "진로 준비 관련 노트",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/index.html#details",
    "href": "posts/01_projects/진로준비/index.html#details",
    "title": "진로 준비",
    "section": "",
    "text": "진로 준비 관련 노트",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/index.html#tasks",
    "href": "posts/01_projects/진로준비/index.html#tasks",
    "title": "진로 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/index.html#related-posts",
    "href": "posts/01_projects/진로준비/index.html#related-posts",
    "title": "진로 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/blog/notes/2.html",
    "href": "posts/03_archives/stored_categories/blog/notes/2.html",
    "title": "PARA 폐기",
    "section": "",
    "text": "PARA 기법이 딱히 유명한것도 아니고, 1년간 써보니 굳이 이렇게 사용할 필요가 있나하는 생각이 들어서 그냥 폐기했습니다.\n구조는 간단하게\n\n현재 진행준인 프로젝트\n카테고리별로 정리된 노트들\n아카이브\n\n완료된 프로젝트\n잘 안보는 노트들\n\n\n이런 식으로 구성돼있습니다.\n이제 빨리 quarto를 버리고 제대로된 블로그를 만들고 싶은데 시간이 없네요\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Blog",
      "Notes",
      "PARA 폐기"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/인생/notes/03.html",
    "href": "posts/03_archives/stored_categories/인생/notes/03.html",
    "title": "나의 계획",
    "section": "",
    "text": "3학년 1학기가 끝난 여름방학. 나는 요즘 부쩍 생각이 많아졌다.\n내 나이 27 살. 전공은 산업공학. 아직 졸업은 하지 못했다.\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "인생",
      "Notes",
      "나의 계획"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/smart_contract/block_chain_basic/00.html#what-is-block-chain",
    "href": "posts/02_categories/block_chain/notes/smart_contract/block_chain_basic/00.html#what-is-block-chain",
    "title": "block chain basic",
    "section": "What is Block Chain?",
    "text": "What is Block Chain?\n\n우리가 돈을 관리하는 방식부터 집단적 합의를 이루는 방식에 이르기까지, 중앙집중식 시스템에 내재된 취약점과 한계에 대한 대응책\n중앙집중식 시스템: 종종 효율적이지만, 중앙 권력에 대한 의존은 통제, 허가, 그리고 신뢰와 관련된 심각한 문제들을 야기\n\n절대적인 통제권\n배제(검열) 가능성\n프라이버시 부재\n\n신뢰가 최소화된 시스템:\n\n공유된 탈중앙화 원장(The Shared, Decentralized Ledger)\n\n노드들은 탈중앙화된 P2P 네트워크를 형성하고, 중앙 서버나 단일 실패 지점이 없다.\n새로운 거래가 발생하면 네트워크에 전파되고, 마치 소문이 퍼지듯 노드에서 노드로 전달되어 결국 모든 참여자가 동일한 최신 원장 사본을 갖게 된다.\n\nBlocks\n\n거래들은 블록이라는 그룹으로 모이고 묶인다. 블록을 글로벌 기록 장부의 한 페이지라고 생각하면 됨.\n한 페이지가 검증된 거래 목록으로 채워지면, 장부에 추가될 준비가 된 것.\n\n해시: 블록이 생성될 때, 내부의 모든 거래 데이터는 암호화 알고리즘을 거쳐 해시라는 고유한 코드를 생성.\n이 해시는 해당 블록에 대한 디지털 지문이나 승인 도장 역할을 함. 거래 데이터의 글자 하나만 바뀌어도 해시는 완전히 달라짐.\n체인 연결: 각 블록은 이전 블록의 해시를 포함하여, 블록들이 시간 순서대로 연결되고 암호화 기술로 보안이 유지되어, 영구적이고 끊어지지 않는 기록을 만듦.\n\n\nChain\n\n블록들은 시간 순서대로 연결되고 암호화 기술로 보안이 유지되어, 영구적이고 끊어지지 않는 기록을 만듦.\n\n\n\n\nHistory\n\n이중지불문제: 디지털 화폐 한 단위가 한 번 이상 사용될 수 있는 내재적 위험.\n일부 노드가 악의적이고 거짓 정보(이중 지불 시도 등)를 퍼뜨리려 하더라도, 네트워크는 어떤 거래가 유효하고 어떤 순서로 발생했는지에 대해 집단적으로 합의해야 함.\n2008년, 사토시 나카모토는 비트코인 백서를 발표하여, 탈중앙화된 디지털 화폐 시스템을 제안.\n2015년, 돈 뿐만 아니라 합의에도 적용할 수 있는 스마트 계약 개념이 도입됨.(이더리움)\n\n\n\nChain in Block Chain\n\n거래 수요가 처리 능력을 훨씬 초과하면 네트워크는 혼잡해짐.\n혼잡한 블록체인에서는 거래 속도가 급격히 떨어지고, 거래 수수료 비싸짐\nLayer 1 (L1): 기초적이고 독립적인 블록체인\n\n각각이 독립적인 개발 커뮤니티, 생태계를 가지고 있음\n\nLayer 2 (L2): L1 블록체인 위에 구축된 확장 솔루션\n\n대량의 거래 묶음을 매우 저렴한 비용으로 오프체인에서 처리하고, 이를 압축된 요약본으로 묶은 뒤, 그 요약본을 다시 L1에 제출\n\n\n\n\nProblem\n\nOracale Problem: 블록체인 외부의 데이터를 신뢰할 수 있는 방식으로 블록체인에 제공하는 문제\n\n스마트 계약은 블록체인 내부의 데이터에만 접근할 수 있음\n외부 데이터를 필요로 하는 스마트 계약은 신뢰할 수 있는 제3자(오라클)를 필요로 함\n오라클이 악의적이거나 오류가 있을 경우, 스마트 계약이 잘못된 데이터를 기반으로 실행될 수 있음\n\nDON(Decentralized Oracle Network): 여러 오라클로부터 데이터를 수집하고, 이를 집계하여 스마트 계약에 제공하는 네트워크\n\n네트워크 내의 여러 독립적인 노드가 현실 세계에서 동일한 데이터를 가져온다. (예: 여러 프리미엄 데이터 소스에서 이더리움 가격을 수집)\n노드들은 서로 데이터를 교차 검증.\n합의 프로토콜을 실행하여 하나의 정확한 값에 동의.\n검증되고 합의된 이 값이 스마트 계약을 위해 온체인에 제출.\n\nChain link: 오라클 문제를 해결하기 위한 핵심 인프라를 제공하는 업계 표준의 모듈식 탈중앙화 오라클 네트워크\n\n\n\nSmart Contract\n\n모든 노드가 동일한 스마트 계약 코드를 실행하여, 동일한 출력을 생성\n\nEVM을 통해 deterministic하게 실행됨\n\n등가성: EVM 등가성을 가진 체인은 모든 면에서 이더리움 메인넷과 똑같이 작동하도록 설계\n호환성: 내부적으로 이더리움 스마트 계약과 작동하는 방식에서 차이가 있음",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "Smart Contract",
      "Block Chain Basic",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/smart_contract/block_chain_basic/00.html#architecture",
    "href": "posts/02_categories/block_chain/notes/smart_contract/block_chain_basic/00.html#architecture",
    "title": "block chain basic",
    "section": "Architecture",
    "text": "Architecture\n\nConsensus Mechanism\n\n하나의 공유된 ’진실의 원천’을 만들기 위해 아래 세 가지 주요 과제를 해결해야 함\n\n\n시빌 저항성(Sybil Resistance)으로 조작 방어하기\n\n각 노드가 거래 내역에 대해 하나의 “투표권”을 가진다고 볼 수 있는 블록체인에서, 시빌 공격은 치명적인 위협.\n네트워크를 위협할 만큼 큰 규모로 운영하는 것을 엄청나게 비싸게 만들어서 시빌 공격을 방어\n\n작업 증명(PoW): 네트워크 합의에 참여하려면 노드는 막대한 컴퓨팅 파워와 전기를 소모하는 복잡한 수학 퍼즐을 풀어야 한다.\n지분 증명(PoS): 검증자로 참여하려면 사용자는 네트워크의 기축 암호화폐 상당량을 담보로 예치해야 한다.\n\n검증자가 일정 양의 코인을 예치\n무작위로 선택된 검증자가 블록 제안\n다른 검증자들이 블록을 검증하고 투표\n\n\n\n완결성(Finality)으로 영구적 보장하기\n\n일단 거래가 확정되어 블록체인에 추가되면, 되돌릴 수 없으며 결코 변경되거나 취소되거나 삭제될 수 없다는 보장\n블록체인마다 이 보장을 달성하기 위한 규칙과 소요 시간이 다름\n비트코인에서는 6 컨펌 이후 완결성 보장\n이더리움에서는 네트워크 전체 예치된 지분의 최소 2/3이 블록을 포함하는 체인을 지지할 때 정당화 되고, 2/3의 검증자가 합의하면 블록이 최종 확정\n\n합의 문제(Consensus Problem) 해결하기\n\n거래의 올바른 순서, 어떤 거래가 유효하고 어떤 것이 사기인지, 블록체인의 어떤 버전이 유일한 진짜 정본(canonical) 체인인지\n\n보통 제일 긴 체인(longest chain)을 진짜 체인으로 간주\n\n\n\n\n\n블록체인의 취약점\n\nSybil Attack\n\n공격자가 네트워크에서 다수의 가짜 신원을 만들어, 네트워크의 합의를 조작하거나 방해하는 공격\n방어 방법: PoW, PoS 등 시빌 저항성 메커니즘 도입\n\n51% Attack\n\n공격자가 네트워크의 해시 파워 또는 스테이킹된 코인의 51% 이상을 통제하여, 거래를 되돌리거나 이중 지불을 수행하는 공격\n\nMEV와 샌드위치 공격\n\nMEV(Maximal Extractable Value): 블록 생산자가 거래 순서를 조작하여 얻을 수 있는 추가 수익\n샌드위치 공격: 공격자가 피해자의 거래 전후에 자신의 거래를 삽입하여 가격을 조작하고 이익을 얻는 전략\n\n클라이언트 소프트웨어 버그\n리플레이 공격\n\n공격자가 이전에 유효했던 거래 데이터를 다른 네트워크에서 재전송하여, 동일한 거래를 두 번 이상 실행하는 공격\n방어 방법: chain ID, nonce\n\n\n\n\nHard Fork\n\n이더리움에서 프로토콜 업그레이드를 구현하는 데 사용되는 기술적 과정\n하드 포크가 활성화되면 새로운 규칙이 이전 규칙과 너무 달라져서 더 이상 호환되지 않음.\n즉, 새로운 소프트웨어로 업데이트하지 않은 노드(블록체인 소프트웨어를 실행하는 컴퓨터)는 새로운 블록과 거래를 검증할 수 없게 됨.\nEIP(Ethereum Improvement Proposal) 작성 및 승인으로 업데이트가 이루어짐\n\nERC(Ethereum Request for Comments): 애플리케이션 레벨의 상호작용을 위한 규칙을 만드는 EIP의 필수적인 부분집합으로, 생태계가 번영할 수 있도록 하는 상호운용성을 촉진\n\n종류\n\n비경합적 하드 포크: 커뮤니티 전체가 제안된 변경 사항이 유익하다는 데 동의하고, 모두가 소프트웨어를 업그레이드하기로 협력한다.\n경합적 하드 포크: 커뮤니티 내에서 의견이 갈려 일부 노드가 업그레이드를 거부하고 이전 규칙을 계속 따르는 경우\n\n개발자가 코드를 작성하지만, 새 소프트웨어를 실행할지 말지 최종적으로 선택하는 것은 노드를 운영하는 수천 명의 개인과 조직. 그들이 업데이트하지 않으면 업그레이드는 실패.\n\n\n\nGas Fee\n\n이더리움에서 작업을 실행하는 데 필요한 연산 작업량을 측정하는 단위\n필요성:\n\n검증자에 대한 보상: 가스비는 필수적인 작업을 수행한 대가로 받는 보상이며, 정직하게 참여하고 네트워크를 계속 가동하게 만드는 직접적인 경제적 유인을 제공함.\n스팸 방지: 모든 연산에 수수료를 요구함으로써, 가스는 스팸 공격 비용을 엄두도 못 낼 만큼 비싸게 만드는 금전적 장벽을 형성하여 네트워크의 제한된 블록 공간을 보호함.\n\n각 블록의 공간은 유한함. 더 많은 사용자가 거래를 제출하려고 하면 이 제한된 공간에 대한 수요가 증가하고, 결과적으로 블록에 포함되기 위해 필요한 가격(가스비)이 올라감.\n변화:\n\n래거시 거래(type 0): 최고가 입찰 경매 모델\n\n사용자가 gasPrice, gasLimit 설정. 거래를 빨리 포함시키려면 사용자는 다른 사람들이 gasPrice로 얼마를 입찰하고 있는지 추측해야 하고, 이로 인해 비용을 과도하게 지불하거나, 너무 적게 지불하여 사용자 경험에 어려움을 줌.\n\nEIP-1559 거래(type 2): 기본 수수료 + 팁 모델\n\n기본 수수료: 네트워크 혼잡에 따라 자동으로 조정되는 수수료\n팁: 거래를 우선 처리하도록 채굴자에게 주는 추가 수수료\n기본 수수료는 소각되어 네트워크에서 영구적으로 제거됨\n\n\n\n\n\nAccount Abstraction\n\nEOA(Externally Owned Accounts): 개인 키로 제어되는 계정\n스마트 계약 계정(Smart Contract Accounts): 스마트 계약을 계정처럼 사용하는 것. smart contract 주소가 계정 주소 역할\n\ntransaction을 실행할 수 없으나, 프로그래밍 가능한 계정처럼 사용할 수 있다.\nauthentication이나 account recovery, gas fee automation 같은 기능을 스마트 계약 코드로 구현 가능\n\nAbstracted Account: EOA와 스마트 계약 계정의 기능을 결합한 계정\n\nsmart contract를 main account로 사용\n\n구현(EIP-4337):\n\nUser Operation: 사용자가 서명한 거래 요청\nEntry Point Contract: User Operation을 수신하고 처리하는 스마트 계약\nBundler: 여러 User Operation을 Alt Mempool에 모아서 Entry Point Contract에 제출하는 역할\nPaymaster: 사용자의 가스비를 대신 지불하는 역할",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "Smart Contract",
      "Block Chain Basic",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/smart_contract/block_chain_basic/00.html#blockchain-use-cases",
    "href": "posts/02_categories/block_chain/notes/smart_contract/block_chain_basic/00.html#blockchain-use-cases",
    "title": "block chain basic",
    "section": "Blockchain Use Cases",
    "text": "Blockchain Use Cases\n\nDeFi\n\n사용자가 탈중앙화된 방식으로 금융 서비스와 상호작용할 수 있게 해주는 모든 프로토콜, 서비스, 애플리케이션을 포괄\nDeFi 애플리케이션은 본질적으로 블록체인에 배포된 스마트 계약, 혹은 스마트 계약들의 시스템.\n결정론적 실행, 결합성\n사례:\n\n탈중앙화 거래소 (DEX): 용자가 중앙 중개인 없이 서로 직접 디지털 자산을 거래할 수 있게 함\n대출 및 차입: 사용자가 자산을 빌려주고 이자를 얻거나, 기존 암호화폐 보유량을 담보로 사용하여 자산을 빌릴 수 있게 함\n이자 농사(Yield Farming) 및 유동성 채굴: 많은 프로토콜이 사용자에게 플랫폼에 유동성(자금)을 제공한 대가로 추가 토큰을 보상하여 참여를 장려함\n\n토큰:\n\n네이티브 토큰: 블록체인 프로토콜의 기본 암호화폐 (예: 이더리움의 ETH, 비트코인의 BTC)\n대체 가능 토큰(ERC20): 상호 교환 가능하고 동일한 가치가 있는 토큰 (예: USDC, DAI)\n대체 불가능 토큰(ERC721): 고유한 디지털 자산을 나타내며, 예술 작품, 수집품, 게임 아이템 등에 사용됨\n준 대체 가능 토큰(ERC155): 여러 개의 동일한 복사본이 존재할 수 있지만, 각 복사본이 여전히 개별적으로 추적되고 소유되는 자산에 사용됨\n\n\n\n\n중앙화 거래소 vs 탈중앙화 거래소\n\n중앙화 거래소:\n\n자금을 입금할 때 자산의 통제권을 거래소로 넘기게 됨. 회사가 자신의 지갑에 코인을 보관하며, 이는 곧 그들이 개인키를 통제한다는 뜻.\n법정 화폐를 코인으로 바꾸게 해주고, 코인을 다시 법정 화폐로 바꿔 은행으로 출금할 수 있게 해줌.\n거래소의 지급 능력과 보안에 완전히 의존하게 됨\n\n탈중앙화 거래소(DEX):\n\n자동화된 시장 조성자 모델\n\n\n\n\nDAO\n\n참여자들이 자신의 금융 자산을 걸고 투표\n탈중앙화 금융(DeFi)에서 가장 큰 프로토콜들을 관리하는 데 실제로 사용됨.",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "Smart Contract",
      "Block Chain Basic",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/smart_contract/block_chain_basic/00.html#scalability",
    "href": "posts/02_categories/block_chain/notes/smart_contract/block_chain_basic/00.html#scalability",
    "title": "block chain basic",
    "section": "Scalability",
    "text": "Scalability\n\nRoll up: L2 체인에서 트랜잭션을 실행한 다음, 수백 개의 트랜잭션을 하나의 압축된 트랜잭션 배치로 묶거나 말아 올려서, L1 네트워크에 게시하는 방식으로 작동\n\nOptimistic Rollup: 트랜잭션이 유효하다고 가정. L2 운영자가 L1에 배치를 게시하면, 일반적으로 약 일주일 동안 지속되는 이의 제기 기간이 시작\nZK Rollups: 모든 트랜잭션 배치의 유효성을 선제적으로 증명. 증명이 유효하다면, 배치는 즉시 수락되고 확정됨.\n\nsequencer: L2 네트워크에서 트랜잭션의 순서를 정하고, 이를 롤업 배치로 묶어 L1에 제출하는 역할.\n\n현재 대부분의 시퀀서가 중앙화되어 있음.\n평판이 좋은 프로젝트라면 장기 로드맵에 점진적 탈중앙화 전략을 반드시 포함하고 있음.\n롤업 시퀀서의 상태는 그 프로젝트의 성숙도, 보안, 그리고 Web3 핵심 원칙에 대한 의지를 보여주는 가장 중요한 지표 중 하나\n\n\n\nRollup Stage\n\n완전한 보조 바퀴\n\n신뢰할 수 있는 운영자 팀과 보안 위원회에 크게 의존\n오픈 소스 데이터 재구성: 롤업 소프트웨어는 오픈 소스여야 하며, 누구나 L1 체인에 게시된 데이터를 사용하여 롤업의 상태를 재구성할 수 있어야 함.\n제한된 사용자 탈출: 원치 않는 업그레이드나 시스템 장애 발생 시, 운영자의 협조와 함께 사용자가 시스템을 빠져나가 자금을 출금할 수 있는 짧은 기간이 주어짐.\n\n강화된 롤업 거버넌스\n\n보안 위원회는 여전히 존재하지만, 그 역할은 종종 긴급 버그 수정 등으로 축소\n운영 증명 시스템: 완전히 기능하는 탈중앙화된 사기 증명 또는 유효성 증명 시스템을 갖추고 있음.\n무허가 사용자 탈출: 롤업 운영자의 허가나 조정 없이 출금할 수 있는 더 긴 기간이 주어짐\n\n보조 바퀴 제거\n\n완전한 스마트 계약 거버넌스: 롤업은 중앙화된 운영자에 대한 의존 없이 온체인 스마트 계약에 의해 완전히 관리됨.\n무허가 증명 시스템: 사기 또는 유효성 증명 시스템은 완전히 무허가형이며, 누구나 체인 검증에 참여할 수 있음.\n제한된 보안 위원회: 보안 위원회가 여전히 존재한다면, 그 권한은 온체인에서 판결된 오류를 해결하는 것으로 엄격히 제한됨.\n충분한 탈출 기간: 제안된 업그레이드가 실행되기 전에 사용자에게 시스템을 빠져나갈 수 있는 상당한 시간이 제공되어, 변경 사항에 동의하지 않을 경우 자산을 안전하게 출금할 수 있도록 보장",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "Smart Contract",
      "Block Chain Basic",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/06.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/06.html#preprocessing",
    "title": "Decision Tree Regression",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/06.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/06.html#modeling",
    "title": "Decision Tree Regression",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor()\n\n# 모델 학습\nregressor.fit(x, y)\n\nDecisionTreeRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/06.html#visualization",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/06.html#visualization",
    "title": "Decision Tree Regression",
    "section": "Visualization",
    "text": "Visualization\n\nx_grid = np.arange(min(x), max(x), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(x, y, color='red')\nplt.plot(x_grid, regressor.predict(x_grid), color='blue')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/07.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/07.html#preprocessing",
    "title": "random forest",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/07.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/07.html#modeling",
    "title": "random forest",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=10, random_state=0)\n\n# 모델 학습\nregressor.fit(x, y)\n\nRandomForestRegressor(n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(n_estimators=10, random_state=0)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/07.html#visualization",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/07.html#visualization",
    "title": "random forest",
    "section": "Visualization",
    "text": "Visualization\n\nx_grid = np.arange(min(x), max(x), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(x, y, color='red')\nplt.plot(x_grid, regressor.predict(x_grid), color='blue')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "random forest"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/05.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/05.html#preprocessing",
    "title": "Support Vector Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values\ny = y.reshape(len(y), 1)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc_x = StandardScaler()\nsc_y = StandardScaler()\n\nx = sc_x.fit_transform(x)\ny = sc_y.fit_transform(y)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/05.html#train",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/05.html#train",
    "title": "Support Vector Regression",
    "section": "Train",
    "text": "Train\n\nfrom sklearn.svm import SVR\n\nregressor = SVR(kernel='rbf')\nregressor.fit(x, y)\n\nSVR()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVR?Documentation for SVRiFittedSVR()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/05.html#visualize",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/05.html#visualize",
    "title": "Support Vector Regression",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(sc_x.inverse_transform(x), sc_y.inverse_transform(y), color='red')\nplt.plot(sc_x.inverse_transform(x), sc_y.inverse_transform(regressor.predict(x).reshape(-1, 1)))\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/05.html#high-resolution",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/05.html#high-resolution",
    "title": "Support Vector Regression",
    "section": "High resolution",
    "text": "High resolution\n\nx_grid = np.arange(min(sc_x.inverse_transform(x)), max(sc_x.inverse_transform(x)), 0.1)\nx_grid = x_grid.reshape((len(x_grid), 1))\nplt.scatter(sc_x.inverse_transform(x), sc_y.inverse_transform(y), color='red')\nplt.plot(x_grid, sc_y.inverse_transform(regressor.predict(sc_x.transform(x_grid)).reshape(-1, 1)))\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/10.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/10.html#preprocessing",
    "title": "Support Vector Machine",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/10.html#modeling---linear",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/10.html#modeling---linear",
    "title": "Support Vector Machine",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.svm import SVC\n\nclassifier = SVC()\nclassifier.fit(x_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/10.html#predict",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/10.html#predict",
    "title": "Support Vector Machine",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[57  8]\n [ 2 33]]\n\n\n0.9",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/10.html#modeling---non-linear",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/10.html#modeling---non-linear",
    "title": "Support Vector Machine",
    "section": "Modeling - non-linear",
    "text": "Modeling - non-linear\n\nclassifier = SVC(kernel='rbf')\nclassifier.fit(x_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/10.html#predict-1",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/10.html#predict-1",
    "title": "Support Vector Machine",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[57  8]\n [ 2 33]]\n\n\n0.9",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Support Vector Machine"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/12.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/12.html#preprocessing",
    "title": "Decision Tree Classification",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/12.html#modeling---linear",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/12.html#modeling---linear",
    "title": "Decision Tree Classification",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(criterion='entropy')\nclassifier.fit(x_train, y_train)\n\nDecisionTreeClassifier(criterion='entropy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(criterion='entropy')",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/12.html#predict",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/12.html#predict",
    "title": "Decision Tree Classification",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[67  4]\n [ 7 22]]\n\n\n0.89",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/12.html#predict-1",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/12.html#predict-1",
    "title": "Decision Tree Classification",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[67  4]\n [ 7 22]]\n\n\n0.89",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Decision Tree Classification"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/00.html#machine-learning-process",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/00.html#machine-learning-process",
    "title": "overview",
    "section": "Machine Learning Process",
    "text": "Machine Learning Process\n\nData Pre-Processing\n\nimport data\nclean data\nsplit data trainig and testing\nfeature scailing\n\nnormalization: \\(\\frac{x - min(x)}{max(x) - min(x)}\\)\nstandardization: \\(\\frac{x - μ}{σ}\\)\n\n\nModeling\n\nbuild / train model\nmake predictions\n\nEvaluation\n\ncalculate performance metrix\nmake a verdict",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "overview"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/03.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/03.html#preprocessing",
    "title": "Multiple Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/03.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# model automatically avoid dummy variable trap\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.2)\n\n# in multiple linear regression, we don't need to apply feature scaling",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/03.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/03.html#modeling",
    "title": "Multiple Linear Regression",
    "section": "modeling",
    "text": "modeling\n\nfrom sklearn.linear_model import LinearRegression\n\n# model automatically choose best model (dont need to apply 후진제거법)\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/03.html#predict",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/03.html#predict",
    "title": "Multiple Linear Regression",
    "section": "predict",
    "text": "predict\n\ny_pred = regressor.predict(X_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred), 1), \n                      y_test.reshape(len(y_test), 1)), 1))\n\n[[117338.6  105008.31]\n [117811.86 110352.25]\n [187746.78 192261.83]\n [116928.28 118474.03]\n [ 66193.2   81229.06]\n [111280.09 103282.38]\n [ 82591.27  90708.19]\n [ 53559.86  42559.73]\n [166958.27 182901.99]\n [117500.08 126992.93]\n [128527.92 144259.4 ]\n [ 63720.28  69758.98]\n [ 98767.43  99937.59]\n [ 74991.19  77798.83]\n [174260.15 191050.39]\n [187473.17 191792.06]\n [ 98520.92  96778.92]\n [128460.77 124266.9 ]\n [122403.56 111313.02]\n [ 97974.29  96712.8 ]\n [151122.4  152211.77]\n [ 71684.6   71498.49]\n [ 53343.71  14681.4 ]\n [ 79167.8   78239.91]\n [100569.02  97483.56]\n [128262.09 134307.35]\n [153010.88 156122.51]\n [ 81471.74  81005.76]\n [111310.31 108733.99]\n [ 98259.09  97427.84]\n [151284.47 132602.65]\n [131871.33 146121.95]\n [131614.68 125370.37]\n [163325.32 166187.94]\n [148736.87 149759.96]\n [ 51825.02  64926.08]\n [104729.02 107404.34]\n [ 72491.83  65200.33]\n [ 66980.93  49490.75]\n [ 82260.41  89949.14]]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/03.html#evaluate",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/03.html#evaluate",
    "title": "Multiple Linear Regression",
    "section": "evaluate",
    "text": "evaluate\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n0.9332006974984312",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/15.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/15.html#preprocessing",
    "title": "hierarchical clustering",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/14.csv')\nx = dataset.iloc[:, [3, 4]].values",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/15.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/15.html#modeling",
    "title": "hierarchical clustering",
    "section": "Modeling",
    "text": "Modeling\n\nimport scipy.cluster.hierarchy as sch\n\ndendogram = sch.dendrogram(sch.linkage(x, method='ward'))\nplt.title('dendogram')\nplt.xlabel('Customers')\nplt.ylabel('Distance')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')\nyh = hc.fit_predict(x)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/15.html#visualize",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/15.html#visualize",
    "title": "hierarchical clustering",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(x[yh == 0, 0], x[yh == 0, 1], c='red', label='Cluster 1')\nplt.scatter(x[yh == 1, 0], x[yh == 1, 1], c='pink', label='Cluster 2')\nplt.scatter(x[yh == 2, 0], x[yh == 2, 1], c='blue', label='Cluster 3')\nplt.scatter(x[yh == 3, 0], x[yh == 3, 1], c='purple', label='Cluster 4')\nplt.scatter(x[yh == 4, 0], x[yh == 4, 1], c='cyan', label='Cluster 5')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "hierarchical clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/16.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/16.html#preprocessing",
    "title": "Apriori",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/16.csv', header=None)\ntransactions = []\nfor i in range(0, len(dataset)):\n    transactions.append([str(dataset.values[i, j]) for j in range(0, len(dataset.columns))])\ntransactions\n\n[['shrimp',\n  'almonds',\n  'avocado',\n  'vegetables mix',\n  'green grapes',\n  'whole weat flour',\n  'yams',\n  'cottage cheese',\n  'energy drink',\n  'tomato juice',\n  'low fat yogurt',\n  'green tea',\n  'honey',\n  'salad',\n  'mineral water',\n  'salmon',\n  'antioxydant juice',\n  'frozen smoothie',\n  'spinach',\n  'olive oil'],\n ['burgers',\n  'meatballs',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chutney',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'energy bar',\n  'whole wheat rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'light cream',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'champagne',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'chicken',\n  'honey',\n  'oil',\n  'cooking oil',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'black tea',\n  'salmon',\n  'eggs',\n  'chicken',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['meatballs',\n  'milk',\n  'honey',\n  'french fries',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'pasta',\n  'pepper',\n  'eggs',\n  'chocolate',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['rice',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'ham',\n  'body spray',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'shrimp',\n  'pasta',\n  'avocado',\n  'honey',\n  'white wine',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'soup',\n  'avocado',\n  'milk',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'energy bar',\n  'black tea',\n  'salmon',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'tomato sauce',\n  'light cream',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'avocado',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'strong cheese',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'spaghetti',\n  'salmon',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'honey',\n  'whole wheat rice',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'hot dogs',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'avocado',\n  'french fries',\n  'hot dogs',\n  'brownies',\n  'body spray',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'cereals',\n  'clothes accessories',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'black tea',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'brownies',\n  'white wine',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'frozen vegetables',\n  'tomatoes',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'chicken',\n  'gums',\n  'soda',\n  'body spray',\n  'energy drink',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'mineral water',\n  'cider',\n  'cooking oil',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['clothes accessories',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'bug spray',\n  'shallot',\n  'protein bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'avocado',\n  'cider',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'tomatoes',\n  'tomato sauce',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'escalope',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'milk',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'energy bar',\n  'butter',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['asparagus',\n  'salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'french fries',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'shrimp',\n  'pasta',\n  'spaghetti',\n  'mineral water',\n  'meatballs',\n  'olive oil',\n  'energy bar',\n  'french wine',\n  'eggs',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'almonds',\n  'eggs',\n  'french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'soup',\n  'escalope',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ham',\n  'frozen vegetables',\n  'pepper',\n  'oil',\n  'extra dark chocolate',\n  'tea',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'barbecue sauce',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'energy bar',\n  'almonds',\n  'eggs',\n  'corn',\n  'mayonnaise',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'ground beef',\n  'chocolate',\n  'soup',\n  'almonds',\n  'eggs',\n  'hot dogs',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'chocolate',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'energy bar',\n  'pet food',\n  'carrots',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'tomato sauce',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'olive oil',\n  'gums',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'mineral water',\n  'soup',\n  'avocado',\n  'milk',\n  'olive oil',\n  'green grapes',\n  'eggs',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'soup',\n  'cake',\n  'cooking oil',\n  'chicken',\n  'light mayo',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'escalope',\n  'mushroom cream sauce',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'oil',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french wine',\n  'eggs',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'eggs',\n  'french fries',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'salmon',\n  'white wine',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'olive oil',\n  'eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'tomatoes',\n  'mineral water',\n  'soup',\n  'milk',\n  'almonds',\n  'eggs',\n  'chocolate',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'pasta',\n  'frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'eggs',\n  'chocolate',\n  'french fries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'antioxydant juice',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['carrots',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'chocolate',\n  'olive oil',\n  'eggs',\n  'cooking oil',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'pancakes',\n  'chicken',\n  'chili',\n  'tea',\n  'french fries',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'chocolate',\n  'frozen smoothie',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cooking oil',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'tomato sauce',\n  'mineral water',\n  'meatballs',\n  'olive oil',\n  'light cream',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'eggs',\n  'cake',\n  'gums',\n  'cooking oil',\n  'chocolate',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'avocado',\n  'milk',\n  'olive oil',\n  'eggs',\n  'rice',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'shrimp',\n  'pasta',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'nonfat milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'pasta',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'eggs',\n  'chicken',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'soup',\n  'milk',\n  'pancakes',\n  'whole wheat rice',\n  'barbecue sauce',\n  'carrots',\n  'chocolate',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'frozen vegetables',\n  'water spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'milk',\n  'eggs',\n  'whole wheat rice',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'cake',\n  'frozen smoothie',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'chicken',\n  'chocolate bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'spaghetti',\n  'eggs',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'olive oil',\n  'strong cheese',\n  'light cream',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'spaghetti',\n  'avocado',\n  'milk',\n  'oil',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'yams',\n  'milk',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'pasta',\n  'spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'ground beef',\n  'soup',\n  'avocado',\n  'milk',\n  'black tea',\n  'eggs',\n  'barbecue sauce',\n  'carrots',\n  'cookies',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'french fries',\n  'strawberries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'yams',\n  'chicken',\n  'honey',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'honey',\n  'cake',\n  'rice',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'milk',\n  'light mayo',\n  'asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'mineral water',\n  'corn',\n  'cottage cheese',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'french wine',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'whole wheat rice',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'french fries',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'pasta',\n  'milk',\n  'green tea',\n  'french fries',\n  'cookies',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'whole wheat rice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'milk',\n  'whole wheat rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'soup',\n  'meatballs',\n  'chicken',\n  'blueberries',\n  'cooking oil',\n  'champagne',\n  'yogurt cake',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cooking oil',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'milk',\n  'eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'spaghetti',\n  'olive oil',\n  'french wine',\n  'eggs',\n  'french fries',\n  'champagne',\n  'pancakes',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'honey',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'bacon',\n  'eggs',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pasta',\n  'tomatoes',\n  'energy bar',\n  'french wine',\n  'antioxydant juice',\n  'french fries',\n  'frozen smoothie',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'spinach',\n  'soda',\n  'energy drink',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bug spray',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'energy bar',\n  'chicken',\n  'eggs',\n  'cake',\n  'french fries',\n  'body spray',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'meatballs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'fromage blanc',\n  'bacon',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'vegetables mix',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'tomatoes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'olive oil',\n  'cereals',\n  'brownies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'cookies',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'pancakes',\n  'cooking oil',\n  'gluten free bar',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'soup',\n  'black tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'grated cheese',\n  'tomatoes',\n  'chocolate',\n  'fromage blanc',\n  'honey',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'ground beef',\n  'milk',\n  'olive oil',\n  'cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'soup',\n  'chutney',\n  'cereals',\n  'energy drink',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'whole wheat rice',\n  'cake',\n  'green tea',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'yams',\n  'soup',\n  'avocado',\n  'salmon',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'meatballs',\n  'vegetables mix',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'shrimp',\n  'pasta',\n  'mineral water',\n  'olive oil',\n  'eggs',\n  'cake',\n  'brownies',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'green grapes',\n  'hot dogs',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'meatballs',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['meatballs',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'chocolate',\n  'olive oil',\n  'french wine',\n  'salmon',\n  'rice',\n  'light mayo',\n  'fresh bread',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'french wine',\n  'vegetables mix',\n  'rice',\n  'clothes accessories',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'dessert wine',\n  'spaghetti',\n  'chicken',\n  'cake',\n  'protein bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french wine',\n  'cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'flax seed',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'energy bar',\n  'green grapes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'eggs',\n  'cake',\n  'chocolate',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['almonds',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'salt',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salt',\n  'tomato juice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'eggs',\n  'pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'chocolate',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'pepper',\n  'spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'red wine',\n  'mineral water',\n  'french fries',\n  'hand protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'energy drink',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strong cheese',\n  'salmon',\n  'green tea',\n  'french fries',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'pasta',\n  'tomatoes',\n  'milk',\n  'frozen smoothie',\n  'sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'olive oil',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'antioxydant juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'milk',\n  'almonds',\n  'eggs',\n  'strawberries',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'antioxydant juice',\n  'body spray',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'oil',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'butter',\n  'chicken',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'spaghetti',\n  'cake',\n  'chocolate',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'oil',\n  'cereals',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'spinach',\n  'eggs',\n  'oil',\n  'cooking oil',\n  'green tea',\n  'shampoo',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'mint green tea',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'whole wheat rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'red wine',\n  'spaghetti',\n  'mineral water',\n  'salmon',\n  'eggs',\n  'cooking oil',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'shallot',\n  'tomato juice',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'spaghetti',\n  'muffins',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'eggs',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'muffins',\n  'tomato juice',\n  'mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'yams',\n  'mineral water',\n  'muffins',\n  'frozen smoothie',\n  'hot dogs',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'soup',\n  'milk',\n  'olive oil',\n  'salmon',\n  'eggs',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'champagne',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'french wine',\n  'green grapes',\n  'rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pet food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'honey',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'dessert wine',\n  'ground beef',\n  'soup',\n  'salmon',\n  'eggs',\n  'gums',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'carrots',\n  'french fries',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'cooking oil',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'pepper',\n  'mineral water',\n  'chocolate',\n  'eggs',\n  'cake',\n  'mashed potato',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'strong cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'mineral water',\n  'soup',\n  'rice',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'french fries',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'tomatoes',\n  'mineral water',\n  'soup',\n  'avocado',\n  'meatballs',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'turkey',\n  'herb & pepper',\n  'ground beef',\n  'pancakes',\n  'eggs',\n  'light cream',\n  'rice',\n  'champagne',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'french wine',\n  'vegetables mix',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'yams',\n  'butter',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'barbecue sauce',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'french fries',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['antioxydant juice',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'salmon',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'avocado',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'eggs',\n  'green tea',\n  'frozen smoothie',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'chicken',\n  'cider',\n  'green grapes',\n  'honey',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'frozen smoothie',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'milk',\n  'chocolate',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'chicken',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'protein bar',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'whole wheat pasta',\n  'yams',\n  'mineral water',\n  'bacon',\n  'nonfat milk',\n  'spinach',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'whole wheat pasta',\n  'spaghetti',\n  'meatballs',\n  'chicken',\n  'frozen smoothie',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'spinach',\n  'cooking oil',\n  'chocolate',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato sauce',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'cider',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'energy bar',\n  'honey',\n  'rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'salad',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'chocolate',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'strawberries',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'salmon',\n  'honey',\n  'extra dark chocolate',\n  'green tea',\n  'mushroom cream sauce',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomatoes',\n  'mineral water',\n  'meatballs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'water spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'tomatoes',\n  'soup',\n  'butter',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'tomato juice',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'milk',\n  'whole wheat rice',\n  'cereals',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'soup',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'vegetables mix',\n  'spinach',\n  'french fries',\n  'escalope',\n  'cauliflower',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'olive oil',\n  'eggs',\n  'soda',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'pancakes',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'tomato sauce',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'almonds',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'escalope',\n  'shallot',\n  'strawberries',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'blueberries',\n  'oil',\n  'cooking oil',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'frozen smoothie',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['salmon',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'butter',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'soup',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'ground beef',\n  'salmon',\n  'pancakes',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'yams',\n  'chocolate',\n  'olive oil',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'salmon',\n  'eggs',\n  'cookies',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['magazines',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'eggs',\n  'cooking oil',\n  'extra dark chocolate',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'mineral water',\n  'soup',\n  'milk',\n  'french wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'salt',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'cake',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'escalope',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'fromage blanc',\n  'whole wheat rice',\n  'cake',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'frozen vegetables',\n  'ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'cake',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'eggs',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'mineral water',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomatoes',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'honey',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'shrimp',\n  'mineral water',\n  'barbecue sauce',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'eggs',\n  'french fries',\n  'escalope',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'milk',\n  'eggs',\n  'whole wheat rice',\n  'chocolate',\n  'escalope',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'soup',\n  'eggs',\n  'oil',\n  'cooking oil',\n  'energy drink',\n  'protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'grated cheese',\n  'herb & pepper',\n  'shrimp',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'carrots',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'red wine',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'mushroom cream sauce',\n  'cottage cheese',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'cake',\n  'chocolate bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'almonds',\n  'cake',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'frozen vegetables',\n  'milk',\n  'butter',\n  'salmon',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'whole wheat pasta',\n  'whole wheat rice',\n  'chicken',\n  'green tea',\n  'cauliflower',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'corn',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomato sauce',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'olive oil',\n  'salmon',\n  'green beans',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'tomato sauce',\n  'mineral water',\n  'green grapes',\n  'eggs',\n  'gums',\n  'light cream',\n  'oil',\n  'green tea',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'spaghetti',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'honey',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'mineral water',\n  'oil',\n  'ketchup',\n  'chili',\n  'pet food',\n  'eggplant',\n  'green tea',\n  'escalope',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'shrimp',\n  'mineral water',\n  'pancakes',\n  'eggs',\n  'cake',\n  'blueberries',\n  'tea',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'spaghetti',\n  'salmon',\n  'carrots',\n  'frozen smoothie',\n  'pasta',\n  'mashed potato',\n  'shallot',\n  'light mayo',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'olive oil',\n  'whole wheat rice',\n  'eggplant',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'whole wheat rice',\n  'cake',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'eggs',\n  'ham',\n  'chocolate',\n  'french fries',\n  'champagne',\n  'brownies',\n  'pancakes',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'spaghetti',\n  'milk',\n  'eggs',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'pancakes',\n  'french fries',\n  'strawberries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'pancakes',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'honey',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'red wine',\n  'tomato sauce',\n  'olive oil',\n  'chili',\n  'french fries',\n  'cookies',\n  'salt',\n  'fresh bread',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'bramble',\n  'frozen smoothie',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'meatballs',\n  'butter',\n  'eggs',\n  'salad',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'spaghetti',\n  'chocolate',\n  'olive oil',\n  'chicken',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'escalope',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'pepper',\n  'soup',\n  'milk',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'pancakes',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'escalope',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french wine',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'spaghetti',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'milk',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'eggs',\n  'bramble',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'frozen vegetables',\n  'tomatoes',\n  'cake',\n  'chicken',\n  'green tea',\n  'toothpaste',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'olive oil',\n  'energy bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'almonds',\n  'cooking oil',\n  'cereals',\n  'protein bar',\n  'white wine',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'fresh tuna',\n  'mineral water',\n  'milk',\n  'eggs',\n  'cake',\n  'burger sauce',\n  'chicken',\n  'french fries',\n  'frozen smoothie',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'mineral water',\n  'whole wheat rice',\n  'yogurt cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'meatballs',\n  'chicken',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'eggs',\n  'green tea',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'herb & pepper',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'salmon',\n  'nonfat milk',\n  'green grapes',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'bug spray',\n  'oatmeal',\n  'sandwich',\n  'asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'almonds',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'butter',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'whole wheat pasta',\n  'pepper',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'whole wheat rice',\n  'rice',\n  'cooking oil',\n  'chocolate',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'butter',\n  'chocolate',\n  'french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'milk',\n  'chutney',\n  'cooking oil',\n  'asparagus',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'frozen smoothie',\n  'cookies',\n  'champagne',\n  'cottage cheese',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'mineral water',\n  'eggs',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'tomato juice',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'almonds',\n  'cereals',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'chocolate',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'pepper',\n  'spaghetti',\n  'mineral water',\n  'whole wheat rice',\n  'champagne',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'pepper',\n  'mineral water',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'turkey',\n  'spaghetti',\n  'milk',\n  'whole wheat rice',\n  'oil',\n  'tomato juice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'french wine',\n  'french fries',\n  'light mayo',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'green grapes',\n  'frozen smoothie',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'soup',\n  'milk',\n  'almonds',\n  'eggs',\n  'oil',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'eggs',\n  'cake',\n  'green tea',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'tomatoes',\n  'ground beef',\n  'eggs',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cider',\n  'eggs',\n  'chocolate',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cider',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'cookies',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'escalope',\n  'champagne',\n  'hot dogs',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'escalope',\n  'shallot',\n  'protein bar',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'parmesan cheese',\n  'spaghetti',\n  'meatballs',\n  'butter',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'escalope',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'pepper',\n  'tomato sauce',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'escalope',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'french fries',\n  'asparagus',\n  'low fat yogurt',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cake',\n  'light cream',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'barbecue sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'escalope',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'chicken',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'energy bar',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'eggs',\n  'gums',\n  'escalope',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'pancakes',\n  'chocolate',\n  'escalope',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'yams',\n  'flax seed',\n  'salmon',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burger sauce',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'ground beef',\n  'mineral water',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'soda',\n  'pet food',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'olive oil',\n  'chicken',\n  'eggs',\n  'extra dark chocolate',\n  'melons',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'ground beef',\n  'yams',\n  'milk',\n  'strong cheese',\n  'salmon',\n  'muffins',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'spaghetti',\n  'light cream',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chicken',\n  'tea',\n  'pancakes',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'turkey',\n  'herb & pepper',\n  'spaghetti',\n  'mineral water',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'cooking oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'herb & pepper',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'shallot',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'chocolate',\n  'shrimp',\n  'whole wheat pasta',\n  'pepper',\n  'mineral water',\n  'soup',\n  'milk',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'avocado',\n  'milk',\n  'butter',\n  'cooking oil',\n  'chocolate',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'eggs',\n  'french fries',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'butter',\n  'eggs',\n  'green tea',\n  'cottage cheese',\n  'mayonnaise',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['antioxydant juice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'pancakes',\n  'eggs',\n  'whole wheat rice',\n  'cooking oil',\n  'chocolate',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'parmesan cheese',\n  'mineral water',\n  'olive oil',\n  'bacon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green grapes',\n  'gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'milk',\n  'eggs',\n  'cooking oil',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'spaghetti',\n  'olive oil',\n  'butter',\n  'chocolate',\n  'french fries',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'eggs',\n  'gums',\n  'cooking oil',\n  'frozen smoothie',\n  'cottage cheese',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'grated cheese',\n  'energy bar',\n  'vegetables mix',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'nonfat milk',\n  'cooking oil',\n  'energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'mineral water',\n  'salmon',\n  'whole wheat rice',\n  'cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'chocolate',\n  'low fat yogurt',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'light cream',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'parmesan cheese',\n  'butter',\n  'whole wheat rice',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'avocado',\n  'milk',\n  'chicken',\n  'rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'soup',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'black tea',\n  'french wine',\n  'pancakes',\n  'rice',\n  'green tea',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'parmesan cheese',\n  'mineral water',\n  'eggs',\n  'cake',\n  'oil',\n  'cooking oil',\n  'toothpaste',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'herb & pepper',\n  'frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'olive oil',\n  'black tea',\n  'vegetables mix',\n  'cooking oil',\n  'green tea',\n  'frozen smoothie',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'cider',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'tomato sauce',\n  'mineral water',\n  'chocolate',\n  'escalope',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy drink',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'mineral water',\n  'energy bar',\n  'eggs',\n  'cooking oil',\n  'pancakes',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'soup',\n  'escalope',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pepper',\n  'mineral water',\n  'soup',\n  'milk',\n  'eggs',\n  'rice',\n  'barbecue sauce',\n  'clothes accessories',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mineral water',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'yams',\n  'mineral water',\n  'chocolate',\n  'olive oil',\n  'light cream',\n  'chicken',\n  'green tea',\n  'french fries',\n  'sandwich',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'milk',\n  'salmon',\n  'green tea',\n  'cookies',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'soup',\n  'salmon',\n  'pancakes',\n  'chicken',\n  'mint green tea',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'whole wheat pasta',\n  'ground beef',\n  'mineral water',\n  'chocolate',\n  'milk',\n  'olive oil',\n  'almonds',\n  'french wine',\n  'yogurt cake',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'grated cheese',\n  'spaghetti',\n  'nonfat milk',\n  'eggs',\n  'gums',\n  'chocolate',\n  'tomato juice',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'ground beef',\n  'olive oil',\n  'energy bar',\n  'chicken',\n  'brownies',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'cooking oil',\n  'frozen smoothie',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['vegetables mix',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'escalope',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'ground beef',\n  'tomato sauce',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'french wine',\n  'eggs',\n  'whole wheat rice',\n  'chocolate',\n  'escalope',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'carrots',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'herb & pepper',\n  'ground beef',\n  'yams',\n  'mineral water',\n  'avocado',\n  'milk',\n  'almonds',\n  'muffins',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'salmon',\n  'antioxydant juice',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'shrimp',\n  'whole wheat pasta',\n  'ground beef',\n  'soup',\n  'energy bar',\n  ' asparagus',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'butter',\n  'pancakes',\n  'french fries',\n  'frozen smoothie',\n  'white wine',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'frozen vegetables',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'fresh tuna',\n  'shrimp',\n  'frozen vegetables',\n  'whole wheat pasta',\n  'yams',\n  'mineral water',\n  'olive oil',\n  'almonds',\n  'cake',\n  'chicken',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'energy bar',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'meatballs',\n  'milk',\n  'almonds',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'pet food',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'whole wheat pasta',\n  'olive oil',\n  'energy bar',\n  'light cream',\n  'oil',\n  'french fries',\n  'pancakes',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'mineral water',\n  'fromage blanc',\n  'eggs',\n  'honey',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bramble',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'pepper',\n  'milk',\n  'eggs',\n  'green tea',\n  'chocolate',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'red wine',\n  'butter',\n  'french fries',\n  'cookies',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'vegetables mix',\n  'eggs',\n  'whole wheat rice',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'mineral water',\n  'salmon',\n  'whole wheat rice',\n  'burger sauce',\n  'escalope',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'body spray',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'red wine',\n  'whole wheat pasta',\n  'cake',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'parmesan cheese',\n  'eggs',\n  'french fries',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'almonds',\n  'pancakes',\n  'chocolate',\n  'french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'black tea',\n  'french wine',\n  'cider',\n  'chutney',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['almonds',\n  'cake',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'soup',\n  'chicken',\n  'light cream',\n  'green tea',\n  'chocolate',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'blueberries',\n  'cooking oil',\n  'chocolate',\n  'cookies',\n  'champagne',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'frozen smoothie',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'salmon',\n  'muffins',\n  'chocolate',\n  'magazines',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'herb & pepper',\n  'parmesan cheese',\n  'milk',\n  'olive oil',\n  'pancakes',\n  'honey',\n  'cake',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'butter',\n  'whole wheat rice',\n  'french fries',\n  'escalope',\n  'cookies',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['energy bar',\n  'eggs',\n  'french fries',\n  'champagne',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'eggs',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'mineral water',\n  'milk',\n  'eggs',\n  'chocolate',\n  'french fries',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'chocolate',\n  'milk',\n  'chicken',\n  'nonfat milk',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'milk',\n  'eggs',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'sparkling water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'mineral water',\n  'olive oil',\n  'carrots',\n  'protein bar',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'grated cheese',\n  'honey',\n  'green beans',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'red wine',\n  'mineral water',\n  'chocolate',\n  'soup',\n  'light cream',\n  'rice',\n  'oil',\n  'chicken',\n  'barbecue sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['nonfat milk',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'parmesan cheese',\n  'whole wheat pasta',\n  'mineral water',\n  'milk',\n  'eggs',\n  'low fat yogurt',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'tomato sauce',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'protein bar',\n  'toothpaste',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'honey',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'parmesan cheese',\n  'french wine',\n  'french fries',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'parmesan cheese',\n  'whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'avocado',\n  'milk',\n  'olive oil',\n  'muffins',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'shrimp',\n  'spaghetti',\n  'olive oil',\n  'honey',\n  'strawberries',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'chocolate',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'olive oil',\n  'extra dark chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'olive oil',\n  'pancakes',\n  'light cream',\n  'champagne',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'vegetables mix',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'mineral water',\n  'avocado',\n  'pancakes',\n  'eggs',\n  'honey',\n  'green tea',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'burgers',\n  'almonds',\n  'honey',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['bug spray',\n  'clothes accessories',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'honey',\n  'whole wheat rice',\n  'green beans',\n  'french fries',\n  'frozen smoothie',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'oil',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'mineral water',\n  'milk',\n  'almonds',\n  'strong cheese',\n  'cake',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'rice',\n  'eggplant',\n  'hand protein bar',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'meatballs',\n  'nonfat milk',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'cake',\n  'cookies',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'bug spray',\n  'french fries',\n  'yogurt cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['soup',\n  'milk',\n  'cider',\n  'chicken',\n  'green beans',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'avocado',\n  'milk',\n  'salmon',\n  'frozen smoothie',\n  'escalope',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'whole wheat rice',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'whole wheat pasta',\n  'spaghetti',\n  'mineral water',\n  'milk',\n  'olive oil',\n  'chicken',\n  'salmon',\n  'pancakes',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'chocolate',\n  'grated cheese',\n  'frozen vegetables',\n  'ground beef',\n  'spaghetti',\n  'milk',\n  'butter',\n  'bacon',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'pepper',\n  'chocolate',\n  'fromage blanc',\n  'eggs',\n  'green tea',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'ground beef',\n  'avocado',\n  'milk',\n  'pancakes',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'green tea',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'tomato juice',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yams',\n  'avocado',\n  'bacon',\n  'oil',\n  'french fries',\n  'brownies',\n  'pancakes',\n  'zucchini',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'milk',\n  'olive oil',\n  'chicken',\n  'cake',\n  'burger sauce',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chicken',\n  'cooking oil',\n  'tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'soup',\n  'vegetables mix',\n  'pancakes',\n  'body spray',\n  'melons',\n  'protein bar',\n  'asparagus',\n  'mayonnaise',\n  'mint',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'olive oil',\n  'blueberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'herb & pepper',\n  'muffins',\n  'eggs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'spaghetti',\n  'milk',\n  'gums',\n  'light cream',\n  'cooking oil',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'pancakes',\n  'french fries',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'parmesan cheese',\n  'ground beef',\n  'tomato sauce',\n  'milk',\n  'extra dark chocolate',\n  'melons',\n  'mint',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'chicken',\n  'chocolate',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'frozen smoothie',\n  'sandwich',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'frozen smoothie',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'chocolate',\n  'rice',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'whole wheat pasta',\n  'meatballs',\n  'milk',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'parmesan cheese',\n  'ground beef',\n  'pepper',\n  'spaghetti',\n  'cream',\n  'black tea',\n  'almonds',\n  'french wine',\n  'pancakes',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'shrimp',\n  'parmesan cheese',\n  'ground beef',\n  'mineral water',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'pancakes',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'soda',\n  'french fries',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'chicken',\n  'rice',\n  'oil',\n  'cooking oil',\n  'hot dogs',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'chocolate',\n  'champagne',\n  'yogurt cake',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'eggs',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'yogurt cake',\n  'light mayo',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'cooking oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'olive oil',\n  'light cream',\n  'cooking oil',\n  'chicken',\n  'extra dark chocolate',\n  'cereals',\n  'french fries',\n  'frozen smoothie',\n  'pancakes',\n  'tomato juice',\n  'fresh bread',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'soup',\n  'avocado',\n  'milk',\n  'yogurt cake',\n  'energy drink',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'chocolate',\n  'ground beef',\n  'mineral water',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'champagne',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['escalope',\n  'pasta',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'olive oil',\n  'rice',\n  'antioxydant juice',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'tomato sauce',\n  'spaghetti',\n  'milk',\n  'pancakes',\n  'energy drink',\n  'white wine',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'spaghetti',\n  'strong cheese',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'spaghetti',\n  'cider',\n  'cooking oil',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'frozen vegetables',\n  'parmesan cheese',\n  'spaghetti',\n  'fromage blanc',\n  'vegetables mix',\n  'pancakes',\n  'honey',\n  'hot dogs',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green beans',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'honey',\n  'whole wheat rice',\n  'champagne',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'milk',\n  'butter',\n  'black tea',\n  'eggs',\n  'frozen smoothie',\n  'light mayo',\n  'shampoo',\n  'low fat yogurt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'milk',\n  'pancakes',\n  'whole wheat rice',\n  'cooking oil',\n  'frozen smoothie',\n  'cookies',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'eggs',\n  'bug spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'muffins',\n  'french fries',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'eggs',\n  'blueberries',\n  'soda',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['nonfat milk',\n  'cookies',\n  'mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'salmon',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'ground beef',\n  'soup',\n  'milk',\n  'butter',\n  'honey',\n  'cake',\n  'mint green tea',\n  'brownies',\n  'salt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'spaghetti',\n  'mineral water',\n  'chocolate',\n  'napkins',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'parmesan cheese',\n  'chocolate',\n  'soup',\n  'milk',\n  'olive oil',\n  'energy bar',\n  'butter',\n  'almonds',\n  'fromage blanc',\n  'eggs',\n  'cake',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'honey',\n  'pasta',\n  'energy drink',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'chocolate',\n  'milk',\n  'french wine',\n  'muffins',\n  'pancakes',\n  'champagne',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'tomatoes',\n  'parmesan cheese',\n  'ground beef',\n  'fromage blanc',\n  'eggs',\n  'honey',\n  'cake',\n  'rice',\n  'cereals',\n  'french fries',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'mineral water',\n  'milk',\n  'strong cheese',\n  'cereals',\n  'escalope',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'fresh tuna',\n  'spaghetti',\n  'pancakes',\n  'eggs',\n  'cake',\n  'cottage cheese',\n  'energy drink',\n  'gluten free bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggplant',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'spaghetti',\n  'mineral water',\n  'almonds',\n  'honey',\n  'extra dark chocolate',\n  'carrots',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'frozen smoothie',\n  'cottage cheese',\n  'strawberries',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strong cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'avocado',\n  'cake',\n  'light cream',\n  'cooking oil',\n  'chicken',\n  'chocolate bread',\n  'mashed potato',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['olive oil',\n  'energy bar',\n  'shallot',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'shrimp',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'spaghetti',\n  'champagne',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'mineral water',\n  'chocolate',\n  'avocado',\n  'butter',\n  'zucchini',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'green tea',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomato sauce',\n  'milk',\n  'butter',\n  'bacon',\n  'salmon',\n  'cooking oil',\n  'chocolate',\n  'french fries',\n  'hot dogs',\n  'melons',\n  'protein bar',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'burger sauce',\n  'brownies',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'pancakes',\n  'whole wheat rice',\n  'green tea',\n  'french fries',\n  'cookies',\n  'shallot',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'butter',\n  'cooking oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'red wine',\n  'frozen vegetables',\n  'pepper',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'honey',\n  'whole wheat rice',\n  'cooking oil',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'ground beef',\n  'spaghetti',\n  'pancakes',\n  'eggs',\n  'cake',\n  'chili',\n  'pet food',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'milk',\n  'butter',\n  'chicken',\n  'salt',\n  'mayonnaise',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'mineral water',\n  'fromage blanc',\n  'honey',\n  'gums',\n  'chocolate',\n  'french fries',\n  'frozen smoothie',\n  'sparkling water',\n  'strawberries',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sparkling water',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'tomatoes',\n  'spaghetti',\n  'chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'burgers',\n  'butter',\n  'vegetables mix',\n  'green grapes',\n  'pancakes',\n  'eggs',\n  'cake',\n  'barbecue sauce',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['red wine',\n  'shrimp',\n  'yams',\n  'eggs',\n  'burger sauce',\n  'yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ground beef',\n  'eggs',\n  'chocolate',\n  'champagne',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mushroom cream sauce',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'mint green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['gums',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'french fries',\n  'champagne',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cereals',\n  'salt',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'spaghetti',\n  'chocolate',\n  'french fries',\n  'escalope',\n  'cookies',\n  'brownies',\n  'pancakes',\n  'melons',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ham',\n  'spaghetti',\n  'yams',\n  'chicken',\n  'cooking oil',\n  'chocolate',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'olive oil',\n  'bug spray',\n  'frozen smoothie',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'mineral water',\n  'soup',\n  'olive oil',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'oil',\n  'chicken',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'whole wheat rice',\n  'escalope',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'frozen vegetables',\n  'chicken',\n  'fromage blanc',\n  'honey',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['muffins',\n  'champagne',\n  'cookies',\n  'fresh bread',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'chocolate',\n  'escalope',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['pickles',\n  'spaghetti',\n  'french fries',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['sandwich',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'oil',\n  'shampoo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'milk',\n  'butter',\n  'french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'frozen vegetables',\n  'chicken',\n  'eggs',\n  'frozen smoothie',\n  'cauliflower',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'tomatoes',\n  'milk',\n  'energy bar',\n  'almonds',\n  'chicken',\n  'chocolate',\n  'french fries',\n  'body spray',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'milk',\n  'cake',\n  'cooking oil',\n  'french fries',\n  'escalope',\n  'zucchini',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['tomatoes',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'olive oil',\n  'whole wheat rice',\n  'cake',\n  'frozen smoothie',\n  'protein bar',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'red wine',\n  'pepper',\n  'spaghetti',\n  'fromage blanc',\n  'salmon',\n  'pancakes',\n  'eggs',\n  'honey',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'parmesan cheese',\n  'soup',\n  'chocolate',\n  'brownies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'vegetables mix',\n  'nonfat milk',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'parmesan cheese',\n  'cider',\n  'muffins',\n  'spinach',\n  'honey',\n  'oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'chocolate',\n  'mushroom cream sauce',\n  'candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['candy bars',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['avocado',\n  'honey',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'tomatoes',\n  'cottage cheese',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'almonds',\n  'french fries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'spaghetti',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'mineral water',\n  'butter',\n  'frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'french fries',\n  'light mayo',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'ham',\n  'spaghetti',\n  'whole wheat rice',\n  'eggplant',\n  'chocolate',\n  'cookies',\n  'shallot',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'frozen vegetables',\n  'spaghetti',\n  'avocado',\n  'milk',\n  'blueberries',\n  'light cream',\n  'rice',\n  'green tea',\n  'chocolate',\n  'french fries',\n  'frozen smoothie',\n  'cookies',\n  'hot dogs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'tomatoes',\n  'bacon',\n  'whole wheat rice',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['parmesan cheese',\n  'cake',\n  'hot dogs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'fromage blanc',\n  'ketchup',\n  'chocolate',\n  'babies food',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['butter',\n  'french fries',\n  'salt',\n  'fresh bread',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'french fries',\n  'strawberries',\n  'yogurt cake',\n  'light mayo',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'eggplant',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['yogurt cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'spaghetti',\n  'meatballs',\n  'milk',\n  'olive oil',\n  'chicken',\n  'honey',\n  'frozen smoothie',\n  'escalope',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'rice',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'ground beef',\n  'spaghetti',\n  'chocolate',\n  'milk',\n  'eggs',\n  'light cream',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'champagne',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat pasta',\n  'ground beef',\n  'olive oil',\n  'blueberries',\n  'escalope',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'red wine',\n  'mineral water',\n  'eggs',\n  'oil',\n  'carrots',\n  'hand protein bar',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['chocolate',\n  'grated cheese',\n  'herb & pepper',\n  'mineral water',\n  'soup',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'shrimp',\n  'mineral water',\n  'chicken',\n  'fromage blanc',\n  'salmon',\n  'eggs',\n  'cake',\n  'frozen smoothie',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['french fries',\n  'cookies',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['milk',\n  'butter',\n  'eggs',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'mushroom cream sauce',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['eggs',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['mineral water',\n  'whole wheat rice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'frozen vegetables',\n  'parmesan cheese',\n  'mineral water',\n  'whole wheat rice',\n  'cake',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['whole wheat rice',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'spaghetti',\n  'olive oil',\n  'butter',\n  'salmon',\n  'oil',\n  'cooking oil',\n  'frozen smoothie',\n  'cauliflower',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['fresh tuna',\n  'eggs',\n  'escalope',\n  'strawberries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'soup',\n  'milk',\n  'carrots',\n  'chocolate',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['burgers',\n  'mineral water',\n  'soup',\n  'meatballs',\n  'olive oil',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'cooking oil',\n  'french fries',\n  'cookies',\n  'low fat yogurt',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'milk',\n  'olive oil',\n  'french fries',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen vegetables',\n  'mineral water',\n  'pancakes',\n  'cake',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['spaghetti',\n  'milk',\n  'chicken',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['grated cheese',\n  'mineral water',\n  'chicken',\n  'french fries',\n  'cottage cheese',\n  'pancakes',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['ground beef',\n  'mineral water',\n  'milk',\n  'eggs',\n  'mint',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['shrimp',\n  'body spray',\n  'green tea',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['frozen smoothie',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['herb & pepper',\n  'frozen vegetables',\n  'mineral water',\n  'muffins',\n  'cereals',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ['turkey',\n  'tomatoes',\n  'spaghetti',\n  'milk',\n  'cider',\n  'eggs',\n  'honey',\n  'cake',\n  'green tea',\n  'french fries',\n  'brownies',\n  'tomato juice',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan',\n  'nan'],\n ...]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Apriori"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/16.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/16.html#modeling",
    "title": "Apriori",
    "section": "Modeling",
    "text": "Modeling\n\nfrom apyori import apriori\n\nrules = apriori(transactions=transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2, max_length=2)\n\n\nresults = list(rules)\nresults\n\n[RelationRecord(items=frozenset({'chicken', 'light cream'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n RelationRecord(items=frozenset({'mushroom cream sauce', 'escalope'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n RelationRecord(items=frozenset({'fromage blanc', 'honey'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n RelationRecord(items=frozenset({'herb & pepper', 'ground beef'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n RelationRecord(items=frozenset({'ground beef', 'tomato sauce'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n RelationRecord(items=frozenset({'olive oil', 'light cream'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n RelationRecord(items=frozenset({'whole wheat pasta', 'olive oil'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n RelationRecord(items=frozenset({'shrimp', 'pasta'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]\n\n\n\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    confidences = [result[2][0][2] for result in results]\n    lifts       = [result[2][0][3] for result in results]\n    return list(zip(lhs, rhs, supports, confidences, lifts))\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])\nresultsinDataFrame\n\n\n\n\n\n\n\n\nLeft Hand Side\nRight Hand Side\nSupport\nConfidence\nLift\n\n\n\n\n0\nlight cream\nchicken\n0.004533\n0.290598\n4.843951\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n0.300699\n3.790833\n\n\n2\npasta\nescalope\n0.005866\n0.372881\n4.700812\n\n\n3\nfromage blanc\nhoney\n0.003333\n0.245098\n5.164271\n\n\n4\nherb & pepper\nground beef\n0.015998\n0.323450\n3.291994\n\n\n5\ntomato sauce\nground beef\n0.005333\n0.377358\n3.840659\n\n\n6\nlight cream\nolive oil\n0.003200\n0.205128\n3.114710\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n0.271493\n4.122410\n\n\n8\npasta\nshrimp\n0.005066\n0.322034\n4.506672\n\n\n\n\n\n\n\n\nresultsinDataFrame.nlargest(n=10, columns='Lift')\n\n\n\n\n\n\n\n\nLeft Hand Side\nRight Hand Side\nSupport\nConfidence\nLift\n\n\n\n\n3\nfromage blanc\nhoney\n0.003333\n0.245098\n5.164271\n\n\n0\nlight cream\nchicken\n0.004533\n0.290598\n4.843951\n\n\n2\npasta\nescalope\n0.005866\n0.372881\n4.700812\n\n\n8\npasta\nshrimp\n0.005066\n0.322034\n4.506672\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n0.271493\n4.122410\n\n\n5\ntomato sauce\nground beef\n0.005333\n0.377358\n3.840659\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n0.300699\n3.790833\n\n\n4\nherb & pepper\nground beef\n0.015998\n0.323450\n3.291994\n\n\n6\nlight cream\nolive oil\n0.003200\n0.205128\n3.114710",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Apriori"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/09.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/09.html#preprocessing",
    "title": "K Nearest Neighbors",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/09.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/09.html#modeling",
    "title": "K Nearest Neighbors",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclassifier = KNeighborsClassifier()\nclassifier.fit(x_train, y_train)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/09.html#predict",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/09.html#predict",
    "title": "K Nearest Neighbors",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[63  9]\n [ 2 26]]\n\n\n0.89",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "K Nearest Neighbors"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/14.html#k-means-algorithm",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/14.html#k-means-algorithm",
    "title": "k-means clustering",
    "section": "K-means++ algorithm",
    "text": "K-means++ algorithm\n\n시작점을 잘 선택하여 수렴 속도를 높이는 알고리즘\n초기 중심점을 선택할 때, 멀리 떨어진 중심점을 선택하도록 함\n\n첫 번째 중심점을 랜덤하게 선택\n나머지 중심점을 선택할 때, 각 데이터 포인트와 가장 먼 중심점을 선택\nk개의 중심점을 선택할 때까지 반복",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/14.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/14.html#preprocessing",
    "title": "k-means clustering",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/14.csv')\nx = dataset.iloc[:, [3, 4]].values",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/14.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/14.html#modeling",
    "title": "k-means clustering",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n  cluster = KMeans(n_clusters=i, init='k-means++')\n  cluster.fit(x)\n  wcss.append(cluster.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of Cluster')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster = KMeans(n_clusters=5, init='k-means++')\ny_kmeans = cluster.fit_predict(x)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/14.html#visualize",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/14.html#visualize",
    "title": "k-means clustering",
    "section": "Visualize",
    "text": "Visualize\n\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], c='red', label='Cluster 1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], c='pink', label='Cluster 2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], c='blue', label='Cluster 3')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], c='purple', label='Cluster 4')\nplt.scatter(x[y_kmeans == 4, 0], x[y_kmeans == 4, 1], c='cyan', label='Cluster 5')\nplt.scatter(cluster.cluster_centers_[:, 0], cluster.cluster_centers_[:, 1], s=100, c='black', label='Centroids')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "k-means clustering"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/02.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/02.html#preprocessing",
    "title": "Simple Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/02-data.csv')\n\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/02.html#train",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/02.html#train",
    "title": "Simple Linear Regression",
    "section": "train",
    "text": "train\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/02.html#predict",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/02.html#predict",
    "title": "Simple Linear Regression",
    "section": "predict",
    "text": "predict\n\ny_pred = regressor.predict(X_test)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/02.html#visualize",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/02.html#visualize",
    "title": "Simple Linear Regression",
    "section": "visualize",
    "text": "visualize\n\nplt.scatter(X_train, y_train, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (training set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.scatter(X_test, y_test, color='red')\nplt.plot(X_train, regressor.predict(X_train), color='blue')\nplt.title('Salary vs Experience (test set)')\nplt.xlabel('Years of Experience')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/02.html#evaluate",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/02.html#evaluate",
    "title": "Simple Linear Regression",
    "section": "evaluate",
    "text": "evaluate\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n0.9694654760447262",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/08.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/08.html#preprocessing",
    "title": "Logistic Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/08.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/08.html#modeling",
    "title": "Logistic Regression",
    "section": "Modeling",
    "text": "Modeling\n\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(x_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/08.html#predict",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/08.html#predict",
    "title": "Logistic Regression",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[58  7]\n [ 7 28]]\n\n\n0.86",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/13.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/13.html#preprocessing",
    "title": "Random Forest",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/13.html#modeling---linear",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/13.html#modeling---linear",
    "title": "Random Forest",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators=10, criterion='entropy')\nclassifier.fit(x_train, y_train)\n\nRandomForestClassifier(criterion='entropy', n_estimators=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(criterion='entropy', n_estimators=10)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/13.html#predict",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/13.html#predict",
    "title": "Random Forest",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[58  3]\n [ 2 37]]\n\n\n0.95",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/13.html#predict-1",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/13.html#predict-1",
    "title": "Random Forest",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[58  3]\n [ 2 37]]\n\n\n0.95",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Random Forest"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/04.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/04.html#preprocessing",
    "title": "Polynorminal Linear Regression",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/04.csv')\nx = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/04.html#linear-regression-model",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/04.html#linear-regression-model",
    "title": "Polynorminal Linear Regression",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\n\nfrom sklearn.linear_model import LinearRegression\n\nregressor = LinearRegression()\nregressor.fit(x, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/04.html#polynorminal-linear-regression",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/04.html#polynorminal-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Polynorminal Linear Regression",
    "text": "Polynorminal Linear Regression\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=4)\nx_poly = poly.fit_transform(x)\nregressor2 = LinearRegression()\nregressor2.fit(x_poly, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/04.html#visualize-linear-regression",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/04.html#visualize-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Visualize Linear Regression",
    "text": "Visualize Linear Regression\n\nplt.scatter(x, y, color='red')\nplt.plot(x, regressor.predict(x), color='blue')\nplt.title('Linear Regression Model')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/04.html#visualize-poly-linear-regression",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/04.html#visualize-poly-linear-regression",
    "title": "Polynorminal Linear Regression",
    "section": "Visualize Poly Linear Regression",
    "text": "Visualize Poly Linear Regression\n\nplt.scatter(x, y, color='red')\nplt.plot(x, regressor2.predict(x_poly), color='blue')\nplt.title('Poly Linear Regression Model')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\nplt.show()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Polynorminal Linear Regression"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/18.html",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/18.html",
    "title": "Upper Confidence Bound",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Upper Confidence Bound"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/11.html#독립성-가정이-필요한-수학적-이유",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/11.html#독립성-가정이-필요한-수학적-이유",
    "title": "Naive Bayes",
    "section": "독립성 가정이 필요한 수학적 이유",
    "text": "독립성 가정이 필요한 수학적 이유\nNaive Bayes는 베이즈 정리를 기반으로 합니다. 클래스 \\(C\\)와 특성 벡터 \\(X = (x_1, x_2, ..., x_n)\\)이 있을 때, 베이즈 정리는 다음과 같습니다:\n\\[P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\\]\n여기서 \\(P(C|X)\\)는 특성 \\(X\\)가 주어졌을 때 클래스 \\(C\\)일 확률입니다. 문제는 \\(P(X|C)\\)를 계산하기가 어렵다는 점입니다. 특성이 많을수록 가능한 \\(X\\) 조합의 수가 기하급수적으로 증가하기 때문입니다.\n이 문제를 해결하기 위해 Naive Bayes는 모든 특성이 서로 조건부 독립이라고 가정합니다. 즉:\n\\[P(x_i|C, x_1, x_2, ..., x_{i-1}, x_{i+1}, ..., x_n) = P(x_i|C)\\]\n이 독립성 가정을 통해 \\(P(X|C)\\)를 다음과 같이 단순화할 수 있습니다:\n\\[P(X|C) = P(x_1, x_2, ..., x_n|C) = P(x_1|C) \\cdot P(x_2|C) \\cdot ... \\cdot P(x_n|C) = \\prod_{i=1}^{n} P(x_i|C)\\]\n이렇게 특성 간 독립성을 가정함으로써 복잡한 결합 확률을 개별 특성의 확률들의 곱으로 계산할 수 있게 되어 계산이 매우 단순해집니다. 이것이 바로 Naive Bayes에서 “naive(순진한)” 독립성 가정이 반드시 필요한 이유입니다.\n이제 Naive Bayes에서 독립성 가정이 필요한 이유가 수학적으로 명확하게 설명되었습니다. 이 설명을 통해 알 수 있듯이:\n\nNaive Bayes는 베이즈 정리를 사용하여 P(C|X)를 계산합니다.\n문제는 P(X|C)를 계산하는 것이 복잡하다는 점입니다.\n독립성 가정을 통해 P(X|C)를 개별 특성들의 조건부 확률 곱으로 단순화할 수 있습니다.\n이 단순화가 없다면, 특성의 조합이 많아질수록 계산이 기하급수적으로 복잡해집니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/11.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/11.html#preprocessing",
    "title": "Naive Bayes",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/08.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/11.html#modeling---linear",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/11.html#modeling---linear",
    "title": "Naive Bayes",
    "section": "Modeling - linear",
    "text": "Modeling - linear\n\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GaussianNB?Documentation for GaussianNBiFittedGaussianNB()",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/11.html#predict",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/11.html#predict",
    "title": "Naive Bayes",
    "section": "Predict",
    "text": "Predict\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[63  8]\n [ 4 25]]\n\n\n0.88",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/11.html#predict-1",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/11.html#predict-1",
    "title": "Naive Bayes",
    "section": "Predict",
    "text": "Predict\n\ny_pred = classifier.predict(x_test)\nprint(confusion_matrix(y_test, y_pred))\naccuracy_score(y_test, y_pred)\n\n[[63  8]\n [ 4 25]]\n\n\n0.88",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/01.html#load-library-and-data",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/01.html#load-library-and-data",
    "title": "data preprocessing",
    "section": "Load Library and data",
    "text": "Load Library and data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('_data/00-data.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n\nx\n\narray([['France', 44.0, 72000.0],\n       ['Spain', 27.0, 48000.0],\n       ['Germany', 30.0, 54000.0],\n       ['Spain', 38.0, 61000.0],\n       ['Germany', 40.0, nan],\n       ['France', 35.0, 58000.0],\n       ['Spain', nan, 52000.0],\n       ['France', 48.0, 79000.0],\n       ['Germany', 50.0, 83000.0],\n       ['France', 37.0, 67000.0]], dtype=object)\n\n\n\ny\n\narray(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n      dtype=object)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/01.html#taking-care-of-missing-data",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/01.html#taking-care-of-missing-data",
    "title": "data preprocessing",
    "section": "Taking care of Missing data",
    "text": "Taking care of Missing data\n\ndelete\nreplace\n\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(x[:, 1:3])\nx[:, 1:3] = imputer.transform(x[:, 1:3])\nprint(x)\n\n[['France' 44.0 72000.0]\n ['Spain' 27.0 48000.0]\n ['Germany' 30.0 54000.0]\n ['Spain' 38.0 61000.0]\n ['Germany' 40.0 63777.77777777778]\n ['France' 35.0 58000.0]\n ['Spain' 38.77777777777778 52000.0]\n ['France' 48.0 79000.0]\n ['Germany' 50.0 83000.0]\n ['France' 37.0 67000.0]]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/01.html#encoding-cagegorical-data",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/01.html#encoding-cagegorical-data",
    "title": "data preprocessing",
    "section": "Encoding Cagegorical data",
    "text": "Encoding Cagegorical data\n\n단순히 categorical 변수를 1, 2, 3으로 변형하면 순서가 고려된 것으로 간주될 수 있다.\n그래서 [0, 0, 1], [1, 0, 1] 이런 식으로 one hot encoding을 진행한다.\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))\nprint(x)\n\n[[1.0 0.0 0.0 44.0 72000.0]\n [0.0 0.0 1.0 27.0 48000.0]\n [0.0 1.0 0.0 30.0 54000.0]\n [0.0 0.0 1.0 38.0 61000.0]\n [0.0 1.0 0.0 40.0 63777.77777777778]\n [1.0 0.0 0.0 35.0 58000.0]\n [0.0 0.0 1.0 38.77777777777778 52000.0]\n [1.0 0.0 0.0 48.0 79000.0]\n [0.0 1.0 0.0 50.0 83000.0]\n [1.0 0.0 0.0 37.0 67000.0]]\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(y)\nprint(y)\n\n[0 1 0 0 1 1 0 1 0 1]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/01.html#split-dataset-into-training-set-and-test-set",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/01.html#split-dataset-into-training-set-and-test-set",
    "title": "data preprocessing",
    "section": "Split dataset into training set and test set",
    "text": "Split dataset into training set and test set\n\nfeature scaling 이전에 진행되어야함. (test set은 모델이 모르는 정보가 되야하기 때문)\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/01.html#feature-scaling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/01.html#feature-scaling",
    "title": "data preprocessing",
    "section": "feature scaling",
    "text": "feature scaling\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\nX_test[:, 3:] = sc.transform(X_test[:, 3:])\n\n\nprint(X_train)\n\n[[0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n [0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]\n [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n [1.0 0.0 0.0 0.566708506533324 0.633562432710455]]\n\n\n\nprint(X_test)\n\n[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "data preprocessing"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/17.html#preprocessing",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/17.html#preprocessing",
    "title": "Eclat",
    "section": "preprocessing",
    "text": "preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ndataset = pd.read_csv('_data/16.csv', header=None)\ntransactions = []\nfor i in range(0, len(dataset)):\n    transactions.append([str(dataset.values[i, j]) for j in range(0, len(dataset.columns))])",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Eclat"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/notes/17.html#modeling",
    "href": "posts/03_archives/stored_categories/machine_learning/notes/17.html#modeling",
    "title": "Eclat",
    "section": "Modeling",
    "text": "Modeling\n\nfrom apyori import apriori\n\nrules = apriori(transactions=transactions, min_support=0.003, min_confidence=0.2, min_lift=3, min_length=2, max_length=2)\n\n\nresults = list(rules)\nresults\n\n[RelationRecord(items=frozenset({'light cream', 'chicken'}), support=0.004532728969470737, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'chicken'}), confidence=0.29059829059829057, lift=4.84395061728395)]),\n RelationRecord(items=frozenset({'mushroom cream sauce', 'escalope'}), support=0.005732568990801226, ordered_statistics=[OrderedStatistic(items_base=frozenset({'mushroom cream sauce'}), items_add=frozenset({'escalope'}), confidence=0.3006993006993007, lift=3.790832696715049)]),\n RelationRecord(items=frozenset({'pasta', 'escalope'}), support=0.005865884548726837, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'escalope'}), confidence=0.3728813559322034, lift=4.700811850163794)]),\n RelationRecord(items=frozenset({'fromage blanc', 'honey'}), support=0.003332888948140248, ordered_statistics=[OrderedStatistic(items_base=frozenset({'fromage blanc'}), items_add=frozenset({'honey'}), confidence=0.2450980392156863, lift=5.164270764485569)]),\n RelationRecord(items=frozenset({'herb & pepper', 'ground beef'}), support=0.015997866951073192, ordered_statistics=[OrderedStatistic(items_base=frozenset({'herb & pepper'}), items_add=frozenset({'ground beef'}), confidence=0.3234501347708895, lift=3.2919938411349285)]),\n RelationRecord(items=frozenset({'ground beef', 'tomato sauce'}), support=0.005332622317024397, ordered_statistics=[OrderedStatistic(items_base=frozenset({'tomato sauce'}), items_add=frozenset({'ground beef'}), confidence=0.3773584905660377, lift=3.840659481324083)]),\n RelationRecord(items=frozenset({'light cream', 'olive oil'}), support=0.003199573390214638, ordered_statistics=[OrderedStatistic(items_base=frozenset({'light cream'}), items_add=frozenset({'olive oil'}), confidence=0.20512820512820515, lift=3.1147098515519573)]),\n RelationRecord(items=frozenset({'whole wheat pasta', 'olive oil'}), support=0.007998933475536596, ordered_statistics=[OrderedStatistic(items_base=frozenset({'whole wheat pasta'}), items_add=frozenset({'olive oil'}), confidence=0.2714932126696833, lift=4.122410097642296)]),\n RelationRecord(items=frozenset({'pasta', 'shrimp'}), support=0.005065991201173177, ordered_statistics=[OrderedStatistic(items_base=frozenset({'pasta'}), items_add=frozenset({'shrimp'}), confidence=0.3220338983050847, lift=4.506672147735896)])]\n\n\n\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    return list(zip(lhs, rhs, supports))\nresultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Product 1', 'Product 2', 'Support'])\nresultsinDataFrame\n\n\n\n\n\n\n\n\nProduct 1\nProduct 2\nSupport\n\n\n\n\n0\nlight cream\nchicken\n0.004533\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n\n\n2\npasta\nescalope\n0.005866\n\n\n3\nfromage blanc\nhoney\n0.003333\n\n\n4\nherb & pepper\nground beef\n0.015998\n\n\n5\ntomato sauce\nground beef\n0.005333\n\n\n6\nlight cream\nolive oil\n0.003200\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n\n\n8\npasta\nshrimp\n0.005066\n\n\n\n\n\n\n\n\nresultsinDataFrame.nlargest(n=10, columns='Support')\n\n\n\n\n\n\n\n\nProduct 1\nProduct 2\nSupport\n\n\n\n\n4\nherb & pepper\nground beef\n0.015998\n\n\n7\nwhole wheat pasta\nolive oil\n0.007999\n\n\n2\npasta\nescalope\n0.005866\n\n\n1\nmushroom cream sauce\nescalope\n0.005733\n\n\n5\ntomato sauce\nground beef\n0.005333\n\n\n8\npasta\nshrimp\n0.005066\n\n\n0\nlight cream\nchicken\n0.004533\n\n\n3\nfromage blanc\nhoney\n0.003333\n\n\n6\nlight cream\nolive oil\n0.003200",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning",
      "Notes",
      "Eclat"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/index.html",
    "href": "posts/03_archives/stored_categories/machine_learning/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "machine learning 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/index.html#details",
    "href": "posts/03_archives/stored_categories/machine_learning/index.html#details",
    "title": "Machine Learning",
    "section": "",
    "text": "machine learning 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/index.html#tasks",
    "href": "posts/03_archives/stored_categories/machine_learning/index.html#tasks",
    "title": "Machine Learning",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/index.html#참고-자료",
    "href": "posts/03_archives/stored_categories/machine_learning/index.html#참고-자료",
    "title": "Machine Learning",
    "section": "참고 자료",
    "text": "참고 자료\n\n이 책\nudemy machine learning 강의",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/03_archives/stored_categories/machine_learning/index.html#related-posts",
    "href": "posts/03_archives/stored_categories/machine_learning/index.html#related-posts",
    "title": "Machine Learning",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Stored Categories",
      "Machine Learning"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/index.html",
    "href": "posts/02_categories/block_chain/index.html",
    "title": "Block Chain",
    "section": "",
    "text": "block chain 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/index.html#details",
    "href": "posts/02_categories/block_chain/index.html#details",
    "title": "Block Chain",
    "section": "",
    "text": "block chain 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/index.html#tasks",
    "href": "posts/02_categories/block_chain/index.html#tasks",
    "title": "Block Chain",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/index.html#참고-자료",
    "href": "posts/02_categories/block_chain/index.html#참고-자료",
    "title": "Block Chain",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/index.html#related-posts",
    "href": "posts/02_categories/block_chain/index.html#related-posts",
    "title": "Block Chain",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/block_chain_basic/00.html#what-is-block-chain",
    "href": "posts/02_categories/block_chain/notes/block_chain_basic/00.html#what-is-block-chain",
    "title": "block chain basic",
    "section": "What is Block Chain?",
    "text": "What is Block Chain?\n\n우리가 돈을 관리하는 방식부터 집단적 합의를 이루는 방식에 이르기까지, 중앙집중식 시스템에 내재된 취약점과 한계에 대한 대응책\n중앙집중식 시스템: 종종 효율적이지만, 중앙 권력에 대한 의존은 통제, 허가, 그리고 신뢰와 관련된 심각한 문제들을 야기\n\n절대적인 통제권\n배제(검열) 가능성\n프라이버시 부재\n\n신뢰가 최소화된 시스템:\n\n공유된 탈중앙화 원장(The Shared, Decentralized Ledger)\n\n노드들은 탈중앙화된 P2P 네트워크를 형성하고, 중앙 서버나 단일 실패 지점이 없다.\n새로운 거래가 발생하면 네트워크에 전파되고, 마치 소문이 퍼지듯 노드에서 노드로 전달되어 결국 모든 참여자가 동일한 최신 원장 사본을 갖게 된다.\n\nBlocks\n\n거래들은 블록이라는 그룹으로 모이고 묶인다. 블록을 글로벌 기록 장부의 한 페이지라고 생각하면 됨.\n한 페이지가 검증된 거래 목록으로 채워지면, 장부에 추가될 준비가 된 것.\n\n해시: 블록이 생성될 때, 내부의 모든 거래 데이터는 암호화 알고리즘을 거쳐 해시라는 고유한 코드를 생성.\n이 해시는 해당 블록에 대한 디지털 지문이나 승인 도장 역할을 함. 거래 데이터의 글자 하나만 바뀌어도 해시는 완전히 달라짐.\n체인 연결: 각 블록은 이전 블록의 해시를 포함하여, 블록들이 시간 순서대로 연결되고 암호화 기술로 보안이 유지되어, 영구적이고 끊어지지 않는 기록을 만듦.\n\n\nChain\n\n블록들은 시간 순서대로 연결되고 암호화 기술로 보안이 유지되어, 영구적이고 끊어지지 않는 기록을 만듦.\n\n\n\n\nHistory\n\n이중지불문제: 디지털 화폐 한 단위가 한 번 이상 사용될 수 있는 내재적 위험.\n일부 노드가 악의적이고 거짓 정보(이중 지불 시도 등)를 퍼뜨리려 하더라도, 네트워크는 어떤 거래가 유효하고 어떤 순서로 발생했는지에 대해 집단적으로 합의해야 함.\n2008년, 사토시 나카모토는 비트코인 백서를 발표하여, 탈중앙화된 디지털 화폐 시스템을 제안.\n2015년, 돈 뿐만 아니라 합의에도 적용할 수 있는 스마트 계약 개념이 도입됨.(이더리움)\n\n\n\nChain in Block Chain\n\n거래 수요가 처리 능력을 훨씬 초과하면 네트워크는 혼잡해짐.\n혼잡한 블록체인에서는 거래 속도가 급격히 떨어지고, 거래 수수료 비싸짐\nLayer 1 (L1): 기초적이고 독립적인 블록체인\n\n각각이 독립적인 개발 커뮤니티, 생태계를 가지고 있음\n\nLayer 2 (L2): L1 블록체인 위에 구축된 확장 솔루션\n\n대량의 거래 묶음을 매우 저렴한 비용으로 오프체인에서 처리하고, 이를 압축된 요약본으로 묶은 뒤, 그 요약본을 다시 L1에 제출\n\n\n\n\nProblem\n\nOracale Problem: 블록체인 외부의 데이터를 신뢰할 수 있는 방식으로 블록체인에 제공하는 문제\n\n스마트 계약은 블록체인 내부의 데이터에만 접근할 수 있음\n외부 데이터를 필요로 하는 스마트 계약은 신뢰할 수 있는 제3자(오라클)를 필요로 함\n오라클이 악의적이거나 오류가 있을 경우, 스마트 계약이 잘못된 데이터를 기반으로 실행될 수 있음\n\nDON(Decentralized Oracle Network): 여러 오라클로부터 데이터를 수집하고, 이를 집계하여 스마트 계약에 제공하는 네트워크\n\n네트워크 내의 여러 독립적인 노드가 현실 세계에서 동일한 데이터를 가져온다. (예: 여러 프리미엄 데이터 소스에서 이더리움 가격을 수집)\n노드들은 서로 데이터를 교차 검증.\n합의 프로토콜을 실행하여 하나의 정확한 값에 동의.\n검증되고 합의된 이 값이 스마트 계약을 위해 온체인에 제출.\n\nChain link: 오라클 문제를 해결하기 위한 핵심 인프라를 제공하는 업계 표준의 모듈식 탈중앙화 오라클 네트워크\n\n\n\nSmart Contract\n\n모든 노드가 동일한 스마트 계약 코드를 실행하여, 동일한 출력을 생성\n\nEVM을 통해 deterministic하게 실행됨\n\n등가성: EVM 등가성을 가진 체인은 모든 면에서 이더리움 메인넷과 똑같이 작동하도록 설계\n호환성: 내부적으로 이더리움 스마트 계약과 작동하는 방식에서 차이가 있음",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "Block Chain Basic",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/block_chain_basic/00.html#architecture",
    "href": "posts/02_categories/block_chain/notes/block_chain_basic/00.html#architecture",
    "title": "block chain basic",
    "section": "Architecture",
    "text": "Architecture\n\nConsensus Mechanism\n\n하나의 공유된 ’진실의 원천’을 만들기 위해 아래 세 가지 주요 과제를 해결해야 함\n\n\n시빌 저항성(Sybil Resistance)으로 조작 방어하기\n\n각 노드가 거래 내역에 대해 하나의 “투표권”을 가진다고 볼 수 있는 블록체인에서, 시빌 공격은 치명적인 위협.\n네트워크를 위협할 만큼 큰 규모로 운영하는 것을 엄청나게 비싸게 만들어서 시빌 공격을 방어\n\n작업 증명(PoW): 네트워크 합의에 참여하려면 노드는 막대한 컴퓨팅 파워와 전기를 소모하는 복잡한 수학 퍼즐을 풀어야 한다.\n지분 증명(PoS): 검증자로 참여하려면 사용자는 네트워크의 기축 암호화폐 상당량을 담보로 예치해야 한다.\n\n검증자가 일정 양의 코인을 예치\n무작위로 선택된 검증자가 블록 제안\n다른 검증자들이 블록을 검증하고 투표\n\n\n\n완결성(Finality)으로 영구적 보장하기\n\n일단 거래가 확정되어 블록체인에 추가되면, 되돌릴 수 없으며 결코 변경되거나 취소되거나 삭제될 수 없다는 보장\n블록체인마다 이 보장을 달성하기 위한 규칙과 소요 시간이 다름\n비트코인에서는 6 컨펌 이후 완결성 보장\n이더리움에서는 네트워크 전체 예치된 지분의 최소 2/3이 블록을 포함하는 체인을 지지할 때 정당화 되고, 2/3의 검증자가 합의하면 블록이 최종 확정\n\n합의 문제(Consensus Problem) 해결하기\n\n거래의 올바른 순서, 어떤 거래가 유효하고 어떤 것이 사기인지, 블록체인의 어떤 버전이 유일한 진짜 정본(canonical) 체인인지\n\n보통 제일 긴 체인(longest chain)을 진짜 체인으로 간주\n\n\n\n\n\n블록체인의 취약점\n\nSybil Attack\n\n공격자가 네트워크에서 다수의 가짜 신원을 만들어, 네트워크의 합의를 조작하거나 방해하는 공격\n방어 방법: PoW, PoS 등 시빌 저항성 메커니즘 도입\n\n51% Attack\n\n공격자가 네트워크의 해시 파워 또는 스테이킹된 코인의 51% 이상을 통제하여, 거래를 되돌리거나 이중 지불을 수행하는 공격\n\nMEV와 샌드위치 공격\n\nMEV(Maximal Extractable Value): 블록 생산자가 거래 순서를 조작하여 얻을 수 있는 추가 수익\n샌드위치 공격: 공격자가 피해자의 거래 전후에 자신의 거래를 삽입하여 가격을 조작하고 이익을 얻는 전략\n\n클라이언트 소프트웨어 버그\n리플레이 공격\n\n공격자가 이전에 유효했던 거래 데이터를 다른 네트워크에서 재전송하여, 동일한 거래를 두 번 이상 실행하는 공격\n방어 방법: chain ID, nonce\n\n\n\n\nHard Fork\n\n이더리움에서 프로토콜 업그레이드를 구현하는 데 사용되는 기술적 과정\n하드 포크가 활성화되면 새로운 규칙이 이전 규칙과 너무 달라져서 더 이상 호환되지 않음.\n즉, 새로운 소프트웨어로 업데이트하지 않은 노드(블록체인 소프트웨어를 실행하는 컴퓨터)는 새로운 블록과 거래를 검증할 수 없게 됨.\nEIP(Ethereum Improvement Proposal) 작성 및 승인으로 업데이트가 이루어짐\n\nERC(Ethereum Request for Comments): 애플리케이션 레벨의 상호작용을 위한 규칙을 만드는 EIP의 필수적인 부분집합으로, 생태계가 번영할 수 있도록 하는 상호운용성을 촉진\n\n종류\n\n비경합적 하드 포크: 커뮤니티 전체가 제안된 변경 사항이 유익하다는 데 동의하고, 모두가 소프트웨어를 업그레이드하기로 협력한다.\n경합적 하드 포크: 커뮤니티 내에서 의견이 갈려 일부 노드가 업그레이드를 거부하고 이전 규칙을 계속 따르는 경우\n\n개발자가 코드를 작성하지만, 새 소프트웨어를 실행할지 말지 최종적으로 선택하는 것은 노드를 운영하는 수천 명의 개인과 조직. 그들이 업데이트하지 않으면 업그레이드는 실패.\n\n\n\nGas Fee\n\n이더리움에서 작업을 실행하는 데 필요한 연산 작업량을 측정하는 단위\n필요성:\n\n검증자에 대한 보상: 가스비는 필수적인 작업을 수행한 대가로 받는 보상이며, 정직하게 참여하고 네트워크를 계속 가동하게 만드는 직접적인 경제적 유인을 제공함.\n스팸 방지: 모든 연산에 수수료를 요구함으로써, 가스는 스팸 공격 비용을 엄두도 못 낼 만큼 비싸게 만드는 금전적 장벽을 형성하여 네트워크의 제한된 블록 공간을 보호함.\n\n각 블록의 공간은 유한함. 더 많은 사용자가 거래를 제출하려고 하면 이 제한된 공간에 대한 수요가 증가하고, 결과적으로 블록에 포함되기 위해 필요한 가격(가스비)이 올라감.\n변화:\n\n래거시 거래(type 0): 최고가 입찰 경매 모델\n\n사용자가 gasPrice, gasLimit 설정. 거래를 빨리 포함시키려면 사용자는 다른 사람들이 gasPrice로 얼마를 입찰하고 있는지 추측해야 하고, 이로 인해 비용을 과도하게 지불하거나, 너무 적게 지불하여 사용자 경험에 어려움을 줌.\n\nEIP-1559 거래(type 2): 기본 수수료 + 팁 모델\n\n기본 수수료: 네트워크 혼잡에 따라 자동으로 조정되는 수수료\n팁: 거래를 우선 처리하도록 채굴자에게 주는 추가 수수료\n기본 수수료는 소각되어 네트워크에서 영구적으로 제거됨\n\n\n\n\n\nAccount Abstraction\n\nEOA(Externally Owned Accounts): 개인 키로 제어되는 계정\n스마트 계약 계정(Smart Contract Accounts): 스마트 계약을 계정처럼 사용하는 것. smart contract 주소가 계정 주소 역할\n\ntransaction을 실행할 수 없으나, 프로그래밍 가능한 계정처럼 사용할 수 있다.\nauthentication이나 account recovery, gas fee automation 같은 기능을 스마트 계약 코드로 구현 가능\n\nAbstracted Account: EOA와 스마트 계약 계정의 기능을 결합한 계정\n\nsmart contract를 main account로 사용\n\n구현(EIP-4337):\n\nUser Operation: 사용자가 서명한 거래 요청\nEntry Point Contract: User Operation을 수신하고 처리하는 스마트 계약\nBundler: 여러 User Operation을 Alt Mempool에 모아서 Entry Point Contract에 제출하는 역할\nPaymaster: 사용자의 가스비를 대신 지불하는 역할",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "Block Chain Basic",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/block_chain_basic/00.html#blockchain-use-cases",
    "href": "posts/02_categories/block_chain/notes/block_chain_basic/00.html#blockchain-use-cases",
    "title": "block chain basic",
    "section": "Blockchain Use Cases",
    "text": "Blockchain Use Cases\n\nDeFi\n\n사용자가 탈중앙화된 방식으로 금융 서비스와 상호작용할 수 있게 해주는 모든 프로토콜, 서비스, 애플리케이션을 포괄\nDeFi 애플리케이션은 본질적으로 블록체인에 배포된 스마트 계약, 혹은 스마트 계약들의 시스템.\n결정론적 실행, 결합성\n사례:\n\n탈중앙화 거래소 (DEX): 용자가 중앙 중개인 없이 서로 직접 디지털 자산을 거래할 수 있게 함\n대출 및 차입: 사용자가 자산을 빌려주고 이자를 얻거나, 기존 암호화폐 보유량을 담보로 사용하여 자산을 빌릴 수 있게 함\n이자 농사(Yield Farming) 및 유동성 채굴: 많은 프로토콜이 사용자에게 플랫폼에 유동성(자금)을 제공한 대가로 추가 토큰을 보상하여 참여를 장려함\n\n토큰:\n\n네이티브 토큰: 블록체인 프로토콜의 기본 암호화폐 (예: 이더리움의 ETH, 비트코인의 BTC)\n대체 가능 토큰(ERC20): 상호 교환 가능하고 동일한 가치가 있는 토큰 (예: USDC, DAI)\n대체 불가능 토큰(ERC721): 고유한 디지털 자산을 나타내며, 예술 작품, 수집품, 게임 아이템 등에 사용됨\n준 대체 가능 토큰(ERC155): 여러 개의 동일한 복사본이 존재할 수 있지만, 각 복사본이 여전히 개별적으로 추적되고 소유되는 자산에 사용됨\n\n\n\n\n중앙화 거래소 vs 탈중앙화 거래소\n\n중앙화 거래소:\n\n자금을 입금할 때 자산의 통제권을 거래소로 넘기게 됨. 회사가 자신의 지갑에 코인을 보관하며, 이는 곧 그들이 개인키를 통제한다는 뜻.\n법정 화폐를 코인으로 바꾸게 해주고, 코인을 다시 법정 화폐로 바꿔 은행으로 출금할 수 있게 해줌.\n거래소의 지급 능력과 보안에 완전히 의존하게 됨\n\n탈중앙화 거래소(DEX):\n\n자동화된 시장 조성자 모델\n\n\n\n\nDAO\n\n참여자들이 자신의 금융 자산을 걸고 투표\n탈중앙화 금융(DeFi)에서 가장 큰 프로토콜들을 관리하는 데 실제로 사용됨.",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "Block Chain Basic",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/block_chain_basic/00.html#scalability",
    "href": "posts/02_categories/block_chain/notes/block_chain_basic/00.html#scalability",
    "title": "block chain basic",
    "section": "Scalability",
    "text": "Scalability\n\nRoll up: L2 체인에서 트랜잭션을 실행한 다음, 수백 개의 트랜잭션을 하나의 압축된 트랜잭션 배치로 묶거나 말아 올려서, L1 네트워크에 게시하는 방식으로 작동\n\nOptimistic Rollup: 트랜잭션이 유효하다고 가정. L2 운영자가 L1에 배치를 게시하면, 일반적으로 약 일주일 동안 지속되는 이의 제기 기간이 시작\nZK Rollups: 모든 트랜잭션 배치의 유효성을 선제적으로 증명. 증명이 유효하다면, 배치는 즉시 수락되고 확정됨.\n\nsequencer: L2 네트워크에서 트랜잭션의 순서를 정하고, 이를 롤업 배치로 묶어 L1에 제출하는 역할.\n\n현재 대부분의 시퀀서가 중앙화되어 있음.\n평판이 좋은 프로젝트라면 장기 로드맵에 점진적 탈중앙화 전략을 반드시 포함하고 있음.\n롤업 시퀀서의 상태는 그 프로젝트의 성숙도, 보안, 그리고 Web3 핵심 원칙에 대한 의지를 보여주는 가장 중요한 지표 중 하나\n\n\n\nRollup Stage\n\n완전한 보조 바퀴\n\n신뢰할 수 있는 운영자 팀과 보안 위원회에 크게 의존\n오픈 소스 데이터 재구성: 롤업 소프트웨어는 오픈 소스여야 하며, 누구나 L1 체인에 게시된 데이터를 사용하여 롤업의 상태를 재구성할 수 있어야 함.\n제한된 사용자 탈출: 원치 않는 업그레이드나 시스템 장애 발생 시, 운영자의 협조와 함께 사용자가 시스템을 빠져나가 자금을 출금할 수 있는 짧은 기간이 주어짐.\n\n강화된 롤업 거버넌스\n\n보안 위원회는 여전히 존재하지만, 그 역할은 종종 긴급 버그 수정 등으로 축소\n운영 증명 시스템: 완전히 기능하는 탈중앙화된 사기 증명 또는 유효성 증명 시스템을 갖추고 있음.\n무허가 사용자 탈출: 롤업 운영자의 허가나 조정 없이 출금할 수 있는 더 긴 기간이 주어짐\n\n보조 바퀴 제거\n\n완전한 스마트 계약 거버넌스: 롤업은 중앙화된 운영자에 대한 의존 없이 온체인 스마트 계약에 의해 완전히 관리됨.\n무허가 증명 시스템: 사기 또는 유효성 증명 시스템은 완전히 무허가형이며, 누구나 체인 검증에 참여할 수 있음.\n제한된 보안 위원회: 보안 위원회가 여전히 존재한다면, 그 권한은 온체인에서 판결된 오류를 해결하는 것으로 엄격히 제한됨.\n충분한 탈출 기간: 제안된 업그레이드가 실행되기 전에 사용자에게 시스템을 빠져나갈 수 있는 상당한 시간이 제공되어, 변경 사항에 동의하지 않을 경우 자산을 안전하게 출금할 수 있도록 보장",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "Block Chain Basic",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/01.html",
    "href": "posts/02_categories/block_chain/notes/01.html",
    "title": "chain link",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "chain link"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/00.html#what-is-block-chain",
    "href": "posts/02_categories/block_chain/notes/00.html#what-is-block-chain",
    "title": "block chain basic",
    "section": "What is Block Chain?",
    "text": "What is Block Chain?\n\n우리가 돈을 관리하는 방식부터 집단적 합의를 이루는 방식에 이르기까지, 중앙집중식 시스템에 내재된 취약점과 한계에 대한 대응책\n중앙집중식 시스템: 종종 효율적이지만, 중앙 권력에 대한 의존은 통제, 허가, 그리고 신뢰와 관련된 심각한 문제들을 야기\n\n절대적인 통제권\n배제(검열) 가능성\n프라이버시 부재\n\n신뢰가 최소화된 시스템:\n\n공유된 탈중앙화 원장(The Shared, Decentralized Ledger)\n\n노드들은 탈중앙화된 P2P 네트워크를 형성하고, 중앙 서버나 단일 실패 지점이 없다.\n새로운 거래가 발생하면 네트워크에 전파되고, 마치 소문이 퍼지듯 노드에서 노드로 전달되어 결국 모든 참여자가 동일한 최신 원장 사본을 갖게 된다.\n\nBlocks\n\n거래들은 블록이라는 그룹으로 모이고 묶인다. 블록을 글로벌 기록 장부의 한 페이지라고 생각하면 됨.\n한 페이지가 검증된 거래 목록으로 채워지면, 장부에 추가될 준비가 된 것.\n\n해시: 블록이 생성될 때, 내부의 모든 거래 데이터는 암호화 알고리즘을 거쳐 해시라는 고유한 코드를 생성.\n이 해시는 해당 블록에 대한 디지털 지문이나 승인 도장 역할을 함. 거래 데이터의 글자 하나만 바뀌어도 해시는 완전히 달라짐.\n체인 연결: 각 블록은 이전 블록의 해시를 포함하여, 블록들이 시간 순서대로 연결되고 암호화 기술로 보안이 유지되어, 영구적이고 끊어지지 않는 기록을 만듦.\n\n\nChain\n\n블록들은 시간 순서대로 연결되고 암호화 기술로 보안이 유지되어, 영구적이고 끊어지지 않는 기록을 만듦.\n\n\n\n\nHistory\n\n이중지불문제: 디지털 화폐 한 단위가 한 번 이상 사용될 수 있는 내재적 위험.\n일부 노드가 악의적이고 거짓 정보(이중 지불 시도 등)를 퍼뜨리려 하더라도, 네트워크는 어떤 거래가 유효하고 어떤 순서로 발생했는지에 대해 집단적으로 합의해야 함.\n2008년, 사토시 나카모토는 비트코인 백서를 발표하여, 탈중앙화된 디지털 화폐 시스템을 제안.\n2015년, 돈 뿐만 아니라 합의에도 적용할 수 있는 스마트 계약 개념이 도입됨.(이더리움)\n\n\n\nChain in Block Chain\n\n거래 수요가 처리 능력을 훨씬 초과하면 네트워크는 혼잡해짐.\n혼잡한 블록체인에서는 거래 속도가 급격히 떨어지고, 거래 수수료 비싸짐\nLayer 1 (L1): 기초적이고 독립적인 블록체인\n\n각각이 독립적인 개발 커뮤니티, 생태계를 가지고 있음\n\nLayer 2 (L2): L1 블록체인 위에 구축된 확장 솔루션\n\n대량의 거래 묶음을 매우 저렴한 비용으로 오프체인에서 처리하고, 이를 압축된 요약본으로 묶은 뒤, 그 요약본을 다시 L1에 제출\n\n\n\n\nProblem\n\nOracale Problem: 블록체인 외부의 데이터를 신뢰할 수 있는 방식으로 블록체인에 제공하는 문제\n\n스마트 계약은 블록체인 내부의 데이터에만 접근할 수 있음\n외부 데이터를 필요로 하는 스마트 계약은 신뢰할 수 있는 제3자(오라클)를 필요로 함\n오라클이 악의적이거나 오류가 있을 경우, 스마트 계약이 잘못된 데이터를 기반으로 실행될 수 있음\n\nDON(Decentralized Oracle Network): 여러 오라클로부터 데이터를 수집하고, 이를 집계하여 스마트 계약에 제공하는 네트워크\n\n네트워크 내의 여러 독립적인 노드가 현실 세계에서 동일한 데이터를 가져온다. (예: 여러 프리미엄 데이터 소스에서 이더리움 가격을 수집)\n노드들은 서로 데이터를 교차 검증.\n합의 프로토콜을 실행하여 하나의 정확한 값에 동의.\n검증되고 합의된 이 값이 스마트 계약을 위해 온체인에 제출.\n\nChain link: 오라클 문제를 해결하기 위한 핵심 인프라를 제공하는 업계 표준의 모듈식 탈중앙화 오라클 네트워크\n\n\n\nSmart Contract\n\n모든 노드가 동일한 스마트 계약 코드를 실행하여, 동일한 출력을 생성\n\nEVM을 통해 deterministic하게 실행됨\n\n등가성: EVM 등가성을 가진 체인은 모든 면에서 이더리움 메인넷과 똑같이 작동하도록 설계\n호환성: 내부적으로 이더리움 스마트 계약과 작동하는 방식에서 차이가 있음",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/00.html#architecture",
    "href": "posts/02_categories/block_chain/notes/00.html#architecture",
    "title": "block chain basic",
    "section": "Architecture",
    "text": "Architecture\n\nConsensus Mechanism\n\n하나의 공유된 ’진실의 원천’을 만들기 위해 아래 세 가지 주요 과제를 해결해야 함\n\n\n시빌 저항성(Sybil Resistance)으로 조작 방어하기\n\n각 노드가 거래 내역에 대해 하나의 “투표권”을 가진다고 볼 수 있는 블록체인에서, 시빌 공격은 치명적인 위협.\n네트워크를 위협할 만큼 큰 규모로 운영하는 것을 엄청나게 비싸게 만들어서 시빌 공격을 방어\n\n작업 증명(PoW): 네트워크 합의에 참여하려면 노드는 막대한 컴퓨팅 파워와 전기를 소모하는 복잡한 수학 퍼즐을 풀어야 한다.\n지분 증명(PoS): 검증자로 참여하려면 사용자는 네트워크의 기축 암호화폐 상당량을 담보로 예치해야 한다.\n\n검증자가 일정 양의 코인을 예치\n무작위로 선택된 검증자가 블록 제안\n다른 검증자들이 블록을 검증하고 투표\n\n\n\n완결성(Finality)으로 영구적 보장하기\n\n일단 거래가 확정되어 블록체인에 추가되면, 되돌릴 수 없으며 결코 변경되거나 취소되거나 삭제될 수 없다는 보장\n블록체인마다 이 보장을 달성하기 위한 규칙과 소요 시간이 다름\n비트코인에서는 6 컨펌 이후 완결성 보장\n이더리움에서는 네트워크 전체 예치된 지분의 최소 2/3이 블록을 포함하는 체인을 지지할 때 정당화 되고, 2/3의 검증자가 합의하면 블록이 최종 확정\n\n합의 문제(Consensus Problem) 해결하기\n\n거래의 올바른 순서, 어떤 거래가 유효하고 어떤 것이 사기인지, 블록체인의 어떤 버전이 유일한 진짜 정본(canonical) 체인인지\n\n보통 제일 긴 체인(longest chain)을 진짜 체인으로 간주\n\n\n\n\n\n블록체인의 취약점\n\nSybil Attack\n\n공격자가 네트워크에서 다수의 가짜 신원을 만들어, 네트워크의 합의를 조작하거나 방해하는 공격\n방어 방법: PoW, PoS 등 시빌 저항성 메커니즘 도입\n\n51% Attack\n\n공격자가 네트워크의 해시 파워 또는 스테이킹된 코인의 51% 이상을 통제하여, 거래를 되돌리거나 이중 지불을 수행하는 공격\n\nMEV와 샌드위치 공격\n\nMEV(Maximal Extractable Value): 블록 생산자가 거래 순서를 조작하여 얻을 수 있는 추가 수익\n샌드위치 공격: 공격자가 피해자의 거래 전후에 자신의 거래를 삽입하여 가격을 조작하고 이익을 얻는 전략\n\n클라이언트 소프트웨어 버그\n리플레이 공격\n\n공격자가 이전에 유효했던 거래 데이터를 다른 네트워크에서 재전송하여, 동일한 거래를 두 번 이상 실행하는 공격\n방어 방법: chain ID, nonce\n\n\n\n\nHard Fork\n\n이더리움에서 프로토콜 업그레이드를 구현하는 데 사용되는 기술적 과정\n하드 포크가 활성화되면 새로운 규칙이 이전 규칙과 너무 달라져서 더 이상 호환되지 않음.\n즉, 새로운 소프트웨어로 업데이트하지 않은 노드(블록체인 소프트웨어를 실행하는 컴퓨터)는 새로운 블록과 거래를 검증할 수 없게 됨.\nEIP(Ethereum Improvement Proposal) 작성 및 승인으로 업데이트가 이루어짐\n\nERC(Ethereum Request for Comments): 애플리케이션 레벨의 상호작용을 위한 규칙을 만드는 EIP의 필수적인 부분집합으로, 생태계가 번영할 수 있도록 하는 상호운용성을 촉진\n\n종류\n\n비경합적 하드 포크: 커뮤니티 전체가 제안된 변경 사항이 유익하다는 데 동의하고, 모두가 소프트웨어를 업그레이드하기로 협력한다.\n경합적 하드 포크: 커뮤니티 내에서 의견이 갈려 일부 노드가 업그레이드를 거부하고 이전 규칙을 계속 따르는 경우\n\n개발자가 코드를 작성하지만, 새 소프트웨어를 실행할지 말지 최종적으로 선택하는 것은 노드를 운영하는 수천 명의 개인과 조직. 그들이 업데이트하지 않으면 업그레이드는 실패.\n\n\n\nGas Fee\n\n이더리움에서 작업을 실행하는 데 필요한 연산 작업량을 측정하는 단위\n필요성:\n\n검증자에 대한 보상: 가스비는 필수적인 작업을 수행한 대가로 받는 보상이며, 정직하게 참여하고 네트워크를 계속 가동하게 만드는 직접적인 경제적 유인을 제공함.\n스팸 방지: 모든 연산에 수수료를 요구함으로써, 가스는 스팸 공격 비용을 엄두도 못 낼 만큼 비싸게 만드는 금전적 장벽을 형성하여 네트워크의 제한된 블록 공간을 보호함.\n\n각 블록의 공간은 유한함. 더 많은 사용자가 거래를 제출하려고 하면 이 제한된 공간에 대한 수요가 증가하고, 결과적으로 블록에 포함되기 위해 필요한 가격(가스비)이 올라감.\n변화:\n\n래거시 거래(type 0): 최고가 입찰 경매 모델\n\n사용자가 gasPrice, gasLimit 설정. 거래를 빨리 포함시키려면 사용자는 다른 사람들이 gasPrice로 얼마를 입찰하고 있는지 추측해야 하고, 이로 인해 비용을 과도하게 지불하거나, 너무 적게 지불하여 사용자 경험에 어려움을 줌.\n\nEIP-1559 거래(type 2): 기본 수수료 + 팁 모델\n\n기본 수수료: 네트워크 혼잡에 따라 자동으로 조정되는 수수료\n팁: 거래를 우선 처리하도록 채굴자에게 주는 추가 수수료\n기본 수수료는 소각되어 네트워크에서 영구적으로 제거됨\n\n\n\n\n\nAccount Abstraction\n\nEOA(Externally Owned Accounts): 개인 키로 제어되는 계정\n스마트 계약 계정(Smart Contract Accounts): 스마트 계약을 계정처럼 사용하는 것. smart contract 주소가 계정 주소 역할\n\ntransaction을 실행할 수 없으나, 프로그래밍 가능한 계정처럼 사용할 수 있다.\nauthentication이나 account recovery, gas fee automation 같은 기능을 스마트 계약 코드로 구현 가능\n\nAbstracted Account: EOA와 스마트 계약 계정의 기능을 결합한 계정\n\nsmart contract를 main account로 사용\n\n구현(EIP-4337):\n\nUser Operation: 사용자가 서명한 거래 요청\nEntry Point Contract: User Operation을 수신하고 처리하는 스마트 계약\nBundler: 여러 User Operation을 Alt Mempool에 모아서 Entry Point Contract에 제출하는 역할\nPaymaster: 사용자의 가스비를 대신 지불하는 역할",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/00.html#blockchain-use-cases",
    "href": "posts/02_categories/block_chain/notes/00.html#blockchain-use-cases",
    "title": "block chain basic",
    "section": "Blockchain Use Cases",
    "text": "Blockchain Use Cases\n\nDeFi\n\n사용자가 탈중앙화된 방식으로 금융 서비스와 상호작용할 수 있게 해주는 모든 프로토콜, 서비스, 애플리케이션을 포괄\nDeFi 애플리케이션은 본질적으로 블록체인에 배포된 스마트 계약, 혹은 스마트 계약들의 시스템.\n결정론적 실행, 결합성\n사례:\n\n탈중앙화 거래소 (DEX): 용자가 중앙 중개인 없이 서로 직접 디지털 자산을 거래할 수 있게 함\n대출 및 차입: 사용자가 자산을 빌려주고 이자를 얻거나, 기존 암호화폐 보유량을 담보로 사용하여 자산을 빌릴 수 있게 함\n이자 농사(Yield Farming) 및 유동성 채굴: 많은 프로토콜이 사용자에게 플랫폼에 유동성(자금)을 제공한 대가로 추가 토큰을 보상하여 참여를 장려함\n\n토큰:\n\n네이티브 토큰: 블록체인 프로토콜의 기본 암호화폐 (예: 이더리움의 ETH, 비트코인의 BTC)\n대체 가능 토큰(ERC20): 상호 교환 가능하고 동일한 가치가 있는 토큰 (예: USDC, DAI)\n대체 불가능 토큰(ERC721): 고유한 디지털 자산을 나타내며, 예술 작품, 수집품, 게임 아이템 등에 사용됨\n준 대체 가능 토큰(ERC155): 여러 개의 동일한 복사본이 존재할 수 있지만, 각 복사본이 여전히 개별적으로 추적되고 소유되는 자산에 사용됨\n\n\n\n\n중앙화 거래소 vs 탈중앙화 거래소\n\n중앙화 거래소:\n\n자금을 입금할 때 자산의 통제권을 거래소로 넘기게 됨. 회사가 자신의 지갑에 코인을 보관하며, 이는 곧 그들이 개인키를 통제한다는 뜻.\n법정 화폐를 코인으로 바꾸게 해주고, 코인을 다시 법정 화폐로 바꿔 은행으로 출금할 수 있게 해줌.\n거래소의 지급 능력과 보안에 완전히 의존하게 됨\n\n탈중앙화 거래소(DEX):\n\n자동화된 시장 조성자 모델\n\n\n\n\nDAO\n\n참여자들이 자신의 금융 자산을 걸고 투표\n탈중앙화 금융(DeFi)에서 가장 큰 프로토콜들을 관리하는 데 실제로 사용됨.",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/00.html#scalability",
    "href": "posts/02_categories/block_chain/notes/00.html#scalability",
    "title": "block chain basic",
    "section": "Scalability",
    "text": "Scalability\n\nRoll up: L2 체인에서 트랜잭션을 실행한 다음, 수백 개의 트랜잭션을 하나의 압축된 트랜잭션 배치로 묶거나 말아 올려서, L1 네트워크에 게시하는 방식으로 작동\n\nOptimistic Rollup: 트랜잭션이 유효하다고 가정. L2 운영자가 L1에 배치를 게시하면, 일반적으로 약 일주일 동안 지속되는 이의 제기 기간이 시작\nZK Rollups: 모든 트랜잭션 배치의 유효성을 선제적으로 증명. 증명이 유효하다면, 배치는 즉시 수락되고 확정됨.\n\nsequencer: L2 네트워크에서 트랜잭션의 순서를 정하고, 이를 롤업 배치로 묶어 L1에 제출하는 역할.\n\n현재 대부분의 시퀀서가 중앙화되어 있음.\n평판이 좋은 프로젝트라면 장기 로드맵에 점진적 탈중앙화 전략을 반드시 포함하고 있음.\n롤업 시퀀서의 상태는 그 프로젝트의 성숙도, 보안, 그리고 Web3 핵심 원칙에 대한 의지를 보여주는 가장 중요한 지표 중 하나\n\n\n\nRollup Stage\n\n완전한 보조 바퀴\n\n신뢰할 수 있는 운영자 팀과 보안 위원회에 크게 의존\n오픈 소스 데이터 재구성: 롤업 소프트웨어는 오픈 소스여야 하며, 누구나 L1 체인에 게시된 데이터를 사용하여 롤업의 상태를 재구성할 수 있어야 함.\n제한된 사용자 탈출: 원치 않는 업그레이드나 시스템 장애 발생 시, 운영자의 협조와 함께 사용자가 시스템을 빠져나가 자금을 출금할 수 있는 짧은 기간이 주어짐.\n\n강화된 롤업 거버넌스\n\n보안 위원회는 여전히 존재하지만, 그 역할은 종종 긴급 버그 수정 등으로 축소\n운영 증명 시스템: 완전히 기능하는 탈중앙화된 사기 증명 또는 유효성 증명 시스템을 갖추고 있음.\n무허가 사용자 탈출: 롤업 운영자의 허가나 조정 없이 출금할 수 있는 더 긴 기간이 주어짐\n\n보조 바퀴 제거\n\n완전한 스마트 계약 거버넌스: 롤업은 중앙화된 운영자에 대한 의존 없이 온체인 스마트 계약에 의해 완전히 관리됨.\n무허가 증명 시스템: 사기 또는 유효성 증명 시스템은 완전히 무허가형이며, 누구나 체인 검증에 참여할 수 있음.\n제한된 보안 위원회: 보안 위원회가 여전히 존재한다면, 그 권한은 온체인에서 판결된 오류를 해결하는 것으로 엄격히 제한됨.\n충분한 탈출 기간: 제안된 업그레이드가 실행되기 전에 사용자에게 시스템을 빠져나갈 수 있는 상당한 시간이 제공되어, 변경 사항에 동의하지 않을 경우 자산을 안전하게 출금할 수 있도록 보장",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "block chain basic"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html",
    "href": "posts/01_projects/정보처리기사/notes/00.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "00"
    ]
  },
  {
    "objectID": "posts/01_projects/진로준비/notes/포트폴리오/00.html",
    "href": "posts/01_projects/진로준비/notes/포트폴리오/00.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Projects",
      "진로 준비",
      "Notes",
      "포트폴리오",
      "00"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/03.html#인터페이스-요구사항-검증",
    "href": "posts/01_projects/정보처리기사/notes/03.html#인터페이스-요구사항-검증",
    "title": "소프트웨어 설계 - 인터페이스 설계",
    "section": "인터페이스 요구사항 검증",
    "text": "인터페이스 요구사항 검증\n\n검토 계획 수립\n검토 및 오류 수정\n\npeer review\n워크스루: 요구사항 명세서 배포 후, 짧게 회의 진행\n인스펙션: 다른 전문가가 검토하는거\n\n베이스라인 설정",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 인터페이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/03.html#인터페이스-방법-명세화",
    "href": "posts/01_projects/정보처리기사/notes/03.html#인터페이스-방법-명세화",
    "title": "소프트웨어 설계 - 인터페이스 설계",
    "section": "인터페이스 방법 명세화",
    "text": "인터페이스 방법 명세화\n\n연계 매커니즘:\n\n송신 시스템\n연계 서버\n수신 시스템\n\n\n\n주요 시스템 연계 기술\n\nDBLink\nAPI/Open API\n연계 솔루션(EAI)\nSocket\nWeb Service(WSDL, SOAP, UDDI)",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 인터페이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/03.html#미들웨어-솔루션-명세",
    "href": "posts/01_projects/정보처리기사/notes/03.html#미들웨어-솔루션-명세",
    "title": "소프트웨어 설계 - 인터페이스 설계",
    "section": "미들웨어 솔루션 명세",
    "text": "미들웨어 솔루션 명세\n\nDB\nRPC\nMOM: message queue. 온라인 업무 x\nTP-Monitor(OLTP)\nORB: 객체 지향 미들웨어, CORBA 표준 스펙 구현\nWAS",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 인터페이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/05.html#분류",
    "href": "posts/01_projects/정보처리기사/notes/05.html#분류",
    "title": "소프트웨어 개발 - 어플리케이션 테스트 관리",
    "section": "분류",
    "text": "분류\n\n실행 여부\n\n정적 테스트\n동적 테스트:\n\n화이트박스: 모든 로직을 테스트 (기초 경로 검사, 제어 구조 검사)\n블랙 박스: 기능이 작동하는 것을 입증(동치 분할 검사, 경계값 분석, 원인-효과 그래프 검사, 오류 예측 검사, 비교 검사)\n\n\n테스트 기반\n\n명세 기반 테스트\n구조 기반 테스트\n경험 기반 테스트: 체크 리스트\n\n시각\n\n검증 테스트: 개발자 시각\n확인 테스트: 사용자 시각\n\n목적\n\n회복\n안전(security)\n강도(stress)\n성능\n구조: 복잡도 평가\n회귀: 수정 코드에 결함이 없는지 확인\n병행: 입력에 따른 결과가 동일한지 비교",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 어플리케이션 테스트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/05.html#개발-단계에-따른-테스트",
    "href": "posts/01_projects/정보처리기사/notes/05.html#개발-단계에-따른-테스트",
    "title": "소프트웨어 개발 - 어플리케이션 테스트 관리",
    "section": "개발 단계에 따른 테스트",
    "text": "개발 단계에 따른 테스트\n\n요구사항\n분석\n설계\n구현\n단위 테스트\n통합 테스트\n\n하향식: 아직 개발 안된 모듈은 스텁 사용\n상향식: 아직 개발 안된 모듈은 드라이버 사용\n\n시스템 테스트\n인수 테스트",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 어플리케이션 테스트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/05.html#테스트-오라클",
    "href": "posts/01_projects/정보처리기사/notes/05.html#테스트-오라클",
    "title": "소프트웨어 개발 - 어플리케이션 테스트 관리",
    "section": "테스트 오라클",
    "text": "테스트 오라클\n\n테스트 결과가 올바른지 판단하기 위해 사전에 정의된 참 값을 대입하여 비교하는 기법\n참 오라클: 모든 테이스 케이스의 기댓값을 제공\n샘플링 오라클: 몇몇 케이스의 기댓값을 제공\n추정 오라클: 몇몇 케이스의 기댓값 제공. 나머지는 추정\n일관성 검사 오라클: 어플리케이션 변경 시 변경 전 후의 결과값 비교",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 어플리케이션 테스트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/05.html#복잡도",
    "href": "posts/01_projects/정보처리기사/notes/05.html#복잡도",
    "title": "소프트웨어 개발 - 어플리케이션 테스트 관리",
    "section": "복잡도",
    "text": "복잡도\n\n순환 복잡도(cyclomatic): E - N + 2",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 어플리케이션 테스트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/04.html#디지털-저작권-관리drm",
    "href": "posts/01_projects/정보처리기사/notes/04.html#디지털-저작권-관리drm",
    "title": "소프트웨어 개발 - 제품 소프트웨어 패키징",
    "section": "디지털 저작권 관리(DRM)",
    "text": "디지털 저작권 관리(DRM)\n\n클리어링 하우스: 디지털 저작권 라이선스 중개 및 발급 수행 기관.\n사용자가 콘텐츠를 사용하려면 클리어링 하우스를 통해 라이선스를 발급받아야 함.\n관리 기술 요소:\n\n암호화\n키 관리\n암호화 파일 생성\n식별 기술\n저작권 표현\n정책 관리\n크랙 방지\n인증",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 제품 소프트웨어 패키징"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html#소프트웨어-생명수명-주기",
    "href": "posts/01_projects/정보처리기사/notes/00.html#소프트웨어-생명수명-주기",
    "title": "소프트웨어 설계 - 요구사항 확인",
    "section": "소프트웨어 생명(수명) 주기",
    "text": "소프트웨어 생명(수명) 주기\n\n소프트웨어 개발 방법론[^1]의 바탕이 되는 것으로, 소프트웨어를 개발하기 위해 정의하고 운용, 유지보수 등의 과정을 각 단계별로 나눈 것\n소프트웨어 공학\n\n소프트웨어의 위기를 극복하기 위한 방안으로 연구된 학문\n기본 원칙:\n\n현대적인 프로그래밍 기술을 계속 적용해야 함\n개발된 소프트웨어 품질이 유지되도록 지속적으로 검증해야 함\n소프트웨어 개발 관련 사항 및 결과에 대한 명확한 기록을 유지해야 함\n\n\n\n\n대표적 모형\n\n폭포수 모형\n\n이전 단계로 돌아갈 수 없다는 전제하에 각 단계를 확실히 매듭짓고 그 결과를 철저하게 검토하여 승인 과정을 거친 후 다음 단계로 진행하는 방식\n가장 오래되고 폭넓게 사용된 모형. 적용 경험과 성공 사례가 많음\n결과물이 명확하게 산출되어야 함\n병행 작업 불가\n\n프로토타입(원형) 모형\n\n의뢰자나 개발자 모두에게 공동의 참조 모델이 되는 시제품 개발\n새로운 요구사항이 도출될 때마다 이를 반영한 프로토타입을 새롭게 만듦\n단기간 제작을 목적으로 함\n비효율적인 언어나 알고리즘이 사용될 수 있음\n\n나선형(점진적) 모형\n\n위험 관리에 중점을 둔 모형\n핵심 기술에 문제가 있거나 사용자의 요구사항이 이해하기 어려운 경우 적합\n각 단계마다 계획, 분석, 개발, 평가의 활동을 반복\n\n애자일 모형\n\n스크럼, XP, 칸반, 린, 크리스탈, ASD, 기능 중심 개발(FDD), DSDM, DAD 등\n4가지 핵심 가치\n\n프로세스와 도구보다 개인과 상호작용을 중시\n포괄적인 문서화보다 작동하는 소프트웨어를 중시\n계약 협상보다 고객과의 협력을 중시\n계획을 따르기보다 변화에 대응하는 것을 중시",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 요구사항 확인"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html#스크럼-기법",
    "href": "posts/01_projects/정보처리기사/notes/00.html#스크럼-기법",
    "title": "소프트웨어 설계 - 요구사항 확인",
    "section": "스크럼 기법",
    "text": "스크럼 기법\n\n럭비에서 반칙으로 경기가 중단된 경우, 양 팀의 선수들이 럭비공을 가운데 두고 상대팀을 밀치기 위해 서로 대치해 있는 대형\n\n팀의 중요성을 강조하는 용어\n\n팀원 스스로가 스크럼 팀을 구성해야 하며, 개발 작업에 관한 모든 것을 스스로 해결할 수 있어야 한다.\n\n\n구성\n\n제품 책임자\n\n이해관계자들 중 개발될 제품에 대한 이해도가 높고, 요구사항을 책임지고 의사 결정할 사람\n주로 개발 의뢰자나 사용자가 담당\n백로그 작성(팀원들도 가능) 및 우선순위 지정\n\n스크럼 마스터\n\n고문 담당\n일일 스크럼 회의를 주관\n\n개발 팀\n\n개발자, 디자이너, 테스터 등 실제 개발 작업을 수행하는 팀원 통칭\n7~8 명\n\n\n\n\n프로세스\n\n제품 백로그: 제품 개발에 필요한 모든 요구사항(user story)을 우선순위에 따라 정리한 목록\n스프린트 계획 회의\n\n백로그 중 이번 스프린트에서 수행할 작업을 대상으로 단기 일정 수립(2~4주)\n백로그를 task 단위로 세분화 해서 개발자 별로 수행할 작업 목록인 스프린트 백로그 작성\n\n스프린트\n\ntask의 velocity를 기반으로 개발자에게 작업 할당 혹은 개발자가 스스로 작업 선택\n할일, 진행 중, 완료의 상태를 가짐.\n\n일일 스크럼 회의\n\n모든 팀원이 매일약속된 시간에 15분 정도의 짧은 시간동안 진행 상황을 점검\n남은 작업 시간은 소멸 차트에 표시\n\n스프린트 검토 회의\n\n부분 또는 전체 완성 제품이 요구사항에 잘 부합하는지 사용자가 포함된 참석자 앞에서 테스팅\n\n스프린트 회고",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 요구사항 확인"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html#xp익스트림-프로그래밍-기법",
    "href": "posts/01_projects/정보처리기사/notes/00.html#xp익스트림-프로그래밍-기법",
    "title": "소프트웨어 설계 - 요구사항 확인",
    "section": "XP(익스트림 프로그래밍) 기법",
    "text": "XP(익스트림 프로그래밍) 기법\n\n릴리즈의 기간을 짧게 반복하면서 고객의 요구사항 반영에 대한 가시성을 높이는 기법\n핵심 가치: 의사소통, 단순성, 용기, 존중, 피드백\n\n\n\n\n개발 프로세스\n\n\n\n스파이크: 잘 모르는 프로그램 그냥 간단하게 데모로 만들어 보는 것\niteration: 하나의 릴리즈를 세분화 한 단위. 일반적으로 1~3주\n승인 검사: iteration 완료 후 고객이 직접 수행\n\n\n주요 실천 방법\n\npair programming: 혼자 개발하지 마라\ncollective ownership: 코드 권한, 책임 공유\ncontinuous integration\ndesign improvement / refactoring",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 요구사항 확인"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html#현행-시스템-파악",
    "href": "posts/01_projects/정보처리기사/notes/00.html#현행-시스템-파악",
    "title": "소프트웨어 설계 - 요구사항 확인",
    "section": "현행 시스템 파악",
    "text": "현행 시스템 파악\n\n단계\n\n시스템 구성 파악: 기간 업무, 지원 업무로 구분하여 기술\n시스템 기능 파악\n시스템 인터페이스 파악: 데이터 형식, 통신 규약, 연계 유형(EAI, FEP 등) 파악\n\n단계\n\n아키텍처 구성 파악\n소프트웨어 구성 파악: 소프트웨어들의 제품명, 용도, 라이선스 적용 방식, 라이선스 수 등 파악\n\n단계\n\n하드웨어 구성 파악: 서버의 주요 사양, 수량, 이중화 적용 여부 명시\n네트워크 구성 파악",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 요구사항 확인"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html#개발-기술-환경-파악",
    "href": "posts/01_projects/정보처리기사/notes/00.html#개발-기술-환경-파악",
    "title": "소프트웨어 설계 - 요구사항 확인",
    "section": "개발 기술 환경 파악",
    "text": "개발 기술 환경 파악\n\n운영체제 관련 요구사항 식별 시 고려사항\n\n가용성, 성능, 기술 지원, 주변 기기, 구축 비용\n\nDBMS 관련 요구사항 식별 시 고려사항\n\n가용성, 성능, 기술지원, 상호 호환성, 구축 비용\n\nWAS 관련 요구사항 식별 시 고려사항\n\n가용성, 성능, 기술지원, 구축 비용",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 요구사항 확인"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html#요구사항-정의",
    "href": "posts/01_projects/정보처리기사/notes/00.html#요구사항-정의",
    "title": "소프트웨어 설계 - 요구사항 확인",
    "section": "요구사항 정의",
    "text": "요구사항 정의\n\n유형\n\n기능 요구사항: 사용자가 시스템을 통해 제공받기 원하는 기능\n비기능 요구사항: 품질, 제약사항 등\n\n\n\n개발 프로세스\n\n타당성 조사\n도출\n\n관련 사람들이 서로 의견을 교환하며 요구사항이 어디에 있는지, 어떻게 수집할 것인지 식별\n청취, 인터뷰, 브레인 스토밍, 프로토타이핑, 유스케이스 등\n\n분석\n\n소프트웨어 개발의 실제적인 첫 단계, 사용자의 요구사항을 이해하고 문서화하는 활동을 의미\n타당성 조사, 제약 설정, 중복 제거, 소프트웨어 범위 파악 등\nAgile, UML, 자료 흐름도(DFD), 자료 사전(DD) 등의 도구가 사용됨\n\n자료 흐름도(자료 흐름 그래프, 버블 차트)\n\n프로세스: 원, 자료 흐름: 화살표, 자료 저장소: top-bottom border, 단말: 네모\n처리를 거칠 때 마다 새로운 자료 흐름 이름 부여\n출력은 반드시 입력이 필요. 역은 성립하지 않음\n\n자료 사전: 자료 흐름도에 있는 자료를 더 자세히 정의하고 기록한 것(메타 데이터)\n\n=: 자료의 정의(구성 요소)\n\\(\\{\\}_n\\): n번 이상 반복, \\(\\{\\}^n\\): 최대 n 번 반복\n(): optional\n+: and, []: or\n* *: 주석\n\n\n\n명세\n\n정형 명세 기법\n\n수학적 원리, 모델 기반\n일관성 있고 간결하지만 이해하기 어려움\nVDM, Z, Petri net, CSP 등\n\n비정형 명세 기법\n\n상태 / 기능 / 객체 중심\n자연어로 작성되어 이해하기 쉬우나, 모호하고 일관성이 떨어짐\nFSM, Decision Table, ER 모델링, State Chart 등\n\n\n확인\n\n\n\n요구사항 분석 CASE(자동화 도구)\n\nSADT(Structured Analysis and Design Technique): 구조적 요구 분석을 위해 블록 다이어그램을 채택한 도구\nHIPO(Hierarchy Input Process Output): 입력, 처리, 출력으로 구성되며 하향식 소프트웨어 개발을 위한 문서화 도구\n\nHIPO Chart 종류:\n\n가시적 도표: 목차\n총체적 도표: 프로그램을 구성하는 기능들 기술\n세부적 도표: 기능의 구성 요소 기술",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 요구사항 확인"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html#umlunified-modeling-language",
    "href": "posts/01_projects/정보처리기사/notes/00.html#umlunified-modeling-language",
    "title": "소프트웨어 설계 - 요구사항 확인",
    "section": "UML(Unified Modeling Language)",
    "text": "UML(Unified Modeling Language)\n\n모두를 위한 객체지향 모델링 언어\n\n\n사물\n\n구조 사물: 노드, 클래스, 컴포넌트 등\n행동 사물: 시간과 공간에 따른 요소들의 행위 (상호작용, 상태 머신 등)\n그룹 사물\n주해 사물: 부가 설명\n\n\n\n관계\n\n연관 관계: 실선으로 표현. 양방향의 경우 화살표 생략 가능\n\n0, 1, n: 연관된 객체의 갯수\n*: 연관된 객체가 다수일 수 있음을 의미\n1..n: 최소 1개에서 최대 n개\n\n의존 관계: 점선 화살표. 짧은 시간 동안만 연관있는 관계\n일반화 간계: 속이 빈 화살표\n실체화 관계: 점선 속이 빈 화살표. 사물의 기능 표현\n\n\n\n다이어그램\n\n구조적 다이어 그램: 정적 모델링\n\n클래스 다이어그램\n\nclass: 일반적으로 3개의 획으로 나눠 클래스 이름, 속성, 오퍼레이션을 표기함\n제약 조건: 입력될 값에 대한 조건, 오퍼레이션(함수) 전후에 지정해야 할 조건\n관계: 연관, 포함, 집합, 일반화, 의존\n\n객체 다이어그램: 인스턴스를 특정 시점의 객체와 객체 사이의 관계로 표현\n컴포넌트 다이어그램: 구현 단계에서 실제 컴포넌트 간의 관계나 인터페이스 표현\n배치(deployment) 다이어그램: 결과물, 프로세스, 컴포넌트 등 물리적 요소들의 위치를 표현\n패키지 다이어그램: 유스케이스나 클래스 등의 모델 요소들을 그룹화한 패키지들의 관계 표현\n\n행동 다이어그램: 동적 모델링\n\n유스케이스 다이어그램: 사용자, 사용 사례로 구성. 사용 사례 간 여러 관계 표현\n\n시스템: 시스템 내부에서 수행되는 기능들을 사각형으로 묶어 표현\n액터: 시스템과 상호작용 하는 모든 외부 요소\n\n주액터: 시스템을 사용함으로써 이득을 얻는 대상\n부액터: 주액터의 목적 달성을 위해 서비스를 제공하는 외부 시스템\n\n유스케이스: 사용자가 보는 관점에서 시스템이 액터에게 제공하는 서비스\n관계: 연관, 포함, 확장, 일반화\n\n순차 다이어그램: 객체 간 메세지 표현\n\n액터: 외부 시스템\n객체: 메시지를 주고받는 주체\n생명선: 객체가 메모리에 존재하는 기간\n실행 상자: 객체가 메시지를 주고받고 있음을 표현\n메시지\n회귀 메시지(reply message)\n제어 블록(loop)\n\n상태 다이어그램: 이벤트에 의한 객체들의 변화 표현\n활동 다이어그램: 객체의 처리 흐름 표현",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 요구사항 확인"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/00.html#footnotes",
    "href": "posts/01_projects/정보처리기사/notes/00.html#footnotes",
    "title": "소프트웨어 설계 - 요구사항 확인",
    "section": "각주",
    "text": "각주\n\n\n소프트웨어 개발과 유지보수 등에 필요한 작업들의 수행 방법 및 필요 기법을 표준화한 것↩︎",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 요구사항 확인"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/01.html#종류",
    "href": "posts/02_categories/block_chain/notes/01.html#종류",
    "title": "ZKPs",
    "section": "종류",
    "text": "종류\n\n상호작용형 영지식 증명:\n\nZKP의 초기 형태.\n증명자와 검증자 사이에 여러 라운드의 상호작용이 필요.\n증명자는 단순히 증거 하나를 제시하는 것이 아니라, 검증자가 제시하는 일련의 도전에 참여하고 이에 대한 응답을 제공.\n단점:\n\n시간 소모적\n블록체인에 비실용적: 분산 시스템에서 서로 다른 라운드에 걸쳐 상호작용형 증명의 상태를 유지하는 것은 복잡함.\n검증자 종속적: 상호작용 과정을 통해 생성된 증명은 일반적으로 그 상호작용에 참여한 특정 검증자만 납득시킬 수 있음. 다른 사람에게 증명하려면 다시 해야 함\n\n\n비상호작용형 영지식 증명:\n\n단 한 번의 통신 라운드만 필요로 함.\n종류:\n\nSNARKs: 증명하려는 진술의 복잡성과 관계없이 증명의 크기가 매우 작고 검증 속도가 빠름.\nSTARKs: 일부 SNARK의 잠재적 취약점인 trusted setup 단계가 필요 없음\nBulletproofs: 신뢰 설정이 필요 없으며, 범위 증명에 효율적임",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "ZKPs"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/01.html#용어",
    "href": "posts/02_categories/block_chain/notes/01.html#용어",
    "title": "ZKPs",
    "section": "용어",
    "text": "용어\n\n주장, 진술: 어떤 것이 사실이라는 단언\n비공개 입력: 증명자가 알고 있지만 검증자에게 공개하지 않는 정보\n공개 입력: 증명자와 검증자 모두가 알고 있는 정보\n제약조건: 주장이 유효한 것으로 간주되기 위해 반드시 충족되어야 하는 수학적 조건이나 방정식. 제약 조건이 표현되는 방식은 R1CS나 Plonkish 시스템과 같이 사용되는 특정 ZKP 제약 시스템에 따라 다름\n회로: 제약 조건들의 집합\n위트니스: 증명자가 자신의 주장이나 진술이 참이며 정의된 모든 제약 조건을 충족함을 보여줄 수 있게 하는 비밀 값들의 집합",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "ZKPs"
    ]
  },
  {
    "objectID": "posts/02_categories/block_chain/notes/01.html#신뢰-설정",
    "href": "posts/02_categories/block_chain/notes/01.html#신뢰-설정",
    "title": "ZKPs",
    "section": "신뢰 설정",
    "text": "신뢰 설정\n\n어떤 암호화 프로토콜이 실행될 때마다 사용되어야 하는 데이터를 생성하기 위해 단 한 번 수행되는 절차\n\n\n비밀 값 생성: 한 명 이상의 참가자가 비밀 무작위 값을 생성\n암호학적 데이터로 변환: 이 비밀 값은 공개 암호학적 매개변수 세트를 생성하는 수학적 과정의 입력값으로 사용됨.\n비밀의 되돌릴 수 없는 폐기: 공개 암호학적 매개변수가 생성되면, 원래의 비밀 값은 반드시 완전하고 복구 불가능하게 파기되어야 함.\n증명 생성: 파기된 비밀로부터 생성된 공개 암호학적 데이터는 증명자가 자신의 진술에 대한 영지식 증명을 구성하는 데 사용됨",
    "crumbs": [
      "Home",
      "Categories",
      "Block Chain",
      "Notes",
      "ZKPs"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/01.html#사용자-인터페이스",
    "href": "posts/01_projects/정보처리기사/notes/01.html#사용자-인터페이스",
    "title": "소프트웨어 설계 - 화면 설계",
    "section": "사용자 인터페이스",
    "text": "사용자 인터페이스\n\n구분\n\nCLI, GUI\nNUI(Natural User Interface): 사용자의 말이나 행동으로 조작.\n\nTap, Double Tap, Drag, Pan, Press, Flick, Pinch\n\n\n\n\n원칙\n\n직관성, 유효성, 학습성, 유연성\n\n\n\n설계 지침\n\n사용자 중심, 사용성, 심미성, 오류 발생 해결",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 화면 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/01.html#ui-설계-도구",
    "href": "posts/01_projects/정보처리기사/notes/01.html#ui-설계-도구",
    "title": "소프트웨어 설계 - 화면 설계",
    "section": "UI 설계 도구",
    "text": "UI 설계 도구\n\n와이어프레임: UI 스케치 해보는 거\n목업: 와이어프레임보다 더 구체적이지만 여전히 스케치 해보는 거\n스토리보드: 와이어프레임 + 콘텐츠에 대한 설명, 페이지 간 이동 흐름\n프로토타입: 실제 동작하는 UI\n유스케이스: 유스케이스 다이어그램 + 명세서",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 화면 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/01.html#품질-요구사항",
    "href": "posts/01_projects/정보처리기사/notes/01.html#품질-요구사항",
    "title": "소프트웨어 설계 - 화면 설계",
    "section": "품질 요구사항",
    "text": "품질 요구사항\n\nISO/IEC 9126 표준에 따름\nISO/IEC 25010으로 개정됨\n\n\n기능성: 적절성, 상호 운용성, 보안성, 준수성\n신뢰성: 성숙성, 고장 허용성, 회복성\n사용성: 이해성, 친밀성, 학습성\n이식성: 적용성, 설치성, 대체성, 공존성",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 화면 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/02.html#소프트웨어-아키텍처",
    "href": "posts/01_projects/정보처리기사/notes/02.html#소프트웨어-아키텍처",
    "title": "소프트웨어 설계 - 어플리케이션 설계",
    "section": "소프트웨어 아키텍처",
    "text": "소프트웨어 아키텍처\n\n모듈화\n추상화: 제어(이벤트), 과정, 자료(데이터) 추상화가 있다.\n단계적 분해\n정보 은닉\n\n\n설계 과정\n\n설계 목표 설정\n\n협약에 의한 설계: 컴포넌트를 설계할 때 클래스에 대한 가정\n\n선행 조건, 결과 조건, 불변 조건\n\n\n시스템 타입 결정\n아키텍처 패턴 적용: 시스템 타입에 맞는 표준 아키텍처 설계\n\n파이프-필터 패턴\n모델-뷰-컨트롤러(MVC) 패턴\n마스터-슬레이브 패턴: 기능은 마스터, 슬레이브 같음\n\n서브시스템 구체화\n검토",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 어플리케이션 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/02.html#객체지향-분석",
    "href": "posts/01_projects/정보처리기사/notes/02.html#객체지향-분석",
    "title": "소프트웨어 설계 - 어플리케이션 설계",
    "section": "객체지향 분석",
    "text": "객체지향 분석\n\nCoad / Yourdon: ER Diagram\n럼바우(OMT):\n\n객체 모델링\n동적 모델링: 시간의 흐름에 따른 객체들 간의 변화 모델링\n기능 모델링: DFD를 이용하여 프로세스들 간의 자료 흐름을 중심으로 처리 과정 모델링\n\n설계 원칙(SOLID):\n\n단일 책임 원칙: 객체는 단 하나의 책임만 가져야 함\n개방-폐쇄 원칙: 확장에는 열려 있고, 변경에는 닫혀 있어야 함\n리스코프 치환 원칙: 자식 클래스는 부모 클래스를 확장만 함(무시, 재정의 x)\n인터페이스 분리 원칙: 사용하지 않는 인터페이스와 의존 관계를 맺지 않도록 함\n의존성 역전 원칙: 추상성이 높은 클래스에 의존해야 함",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 어플리케이션 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/02.html#모듈",
    "href": "posts/01_projects/정보처리기사/notes/02.html#모듈",
    "title": "소프트웨어 설계 - 어플리케이션 설계",
    "section": "모듈",
    "text": "모듈\n\n결합도\n\n아래로 갈 수록 결합도 강함(안 좋음)\n\n\n자료 결합도: 매개변수로만 데이터 전달\n스탬프(검인) 결합도: 배열이나 레코드 등의 자료 구조가 전달됨\n제어 결합도: flag 변수를 전달함\n외부 결합도: 모듈에서 선언한 변수를 다른 모듈에서 참조\n공통 결합도: 공통의 영역을 여러 모듈이 참조\n내용 결합도: 한 모듈의 내부 기능이나 자료를 직접 참조\n\n\n\n응집도\n\n아래로 갈 수록 응집도 강함(좋음)\n\n\n우연적 응집도: 모듈의 구성 요소들이 관련 없음\n논리적 응집도: 유사한 기능들을 묶음\n시간적 응집도: 특정 시간에 실행되는 기능들을 묶음(ex 프로그램 초기화)\n절차적 응집도: 모듈 안의 구성 요소들이 순차적으로 수행됨. 데이터를 주고받지는 않음\n교환적 응집도: 동일한 입력 데이터 혹은 출력 데이터를 사용하여 서로 다른 기능을 수행함\n순차적 응집도: 하나의 기능에서 나온 출력 데이터를 다음 기능의 입력 데이터로 사용\n기능적 응집도: 모듈 내 모든 기능들이 단일 문제와 연관됨\n\n\n팬인: 모듈이 다른 모듈로부터 호출되는 횟수\n팬아웃: 모듈이 다른 모듈을 호출하는 횟수\n팬인이 높고 팬아웃이 낮은 모듈이 이상적",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 어플리케이션 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/02.html#디자인-패턴",
    "href": "posts/01_projects/정보처리기사/notes/02.html#디자인-패턴",
    "title": "소프트웨어 설계 - 어플리케이션 설계",
    "section": "디자인 패턴",
    "text": "디자인 패턴\n\n생성 패턴\n\n팩토리 메소드(가상 생성자):\n추상 팩토리\n빌더\n프로토타입\n싱글톤: 인스턴스가 하나만 생성되도록 보장. 생성자 함수를 private으로 선언\n\n\n\n구조 패턴\n\n어뎁터\n브릿지\n컴포지트: 트리 구조\n데코레이터\n퍼사드: 인터페이스 wrapper\n플라이웨이트: 메모리 공유\n프록시\n\n\n\n행위 패턴\n\n책임 연쇄: 한 객체가 요청을 처리하지 못하면 다음 객체로 넘김\n커맨드: 요청을 객체의 형태로 캡슐화\n인터프리터: 언어 parser\n반복자: 같은 인터페이스를 이용해 집합체의 요소들을 순차적으로 접근\n중재자\n메멘토: 특정 상태의 객체를 저장하고 복원\n옵서버: 한 객체의 상태 변화가 있을 때 의존하는 다른 객체들에게 통지\n상태: 객체의 상태에 따라 다른 행동을 하도록 함\n전략: 알고리즘 캡슐화해서 쓰는거\n템플릿 메소드\n방문자: 처리 기능을 별도의 클래스로 분리",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 설계 - 어플리케이션 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#개발-방법론",
    "href": "posts/01_projects/정보처리기사/notes/06.html#개발-방법론",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "개발 방법론",
    "text": "개발 방법론\n\n구조적 방법론: 정형화된 분석 절차에 따라 요구사항을 문서화 하는 처리 중심 방법론\n정보공학 방법론: 데이터 중심 방법론 (ERD)\n객체지향 방법론\n컴포넌트 기반 방법론\n애자일 방법론\n제품 개열 방법론",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#소프트웨어-재사용",
    "href": "posts/01_projects/정보처리기사/notes/06.html#소프트웨어-재사용",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "소프트웨어 재사용",
    "text": "소프트웨어 재사용\n\n합성 중심: 모듈 끼워 맞추는거\n생성 중심: 추상화된 컴포넌트를 구체화\n\n\n소프트웨어 재공학\n\n분석\n재구성: 기능, 외적인 동작은 유지하면서 내부 구조 변경\n역공학: 기존 코드를 분석\n이식: 기존 소프트웨어를 다른 환경에 이식할 수 있게 변경\n\nCASE(Computer-Aided Software Engineering): 구조적 기법, 프로토타이핑, 분산 처리 등의 기법으로 소프트웨어 개발 생산성을 향상시키기 위한 자동화 도구",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#상향식-비용-산정-기법",
    "href": "posts/01_projects/정보처리기사/notes/06.html#상향식-비용-산정-기법",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "상향식 비용 산정 기법",
    "text": "상향식 비용 산정 기법\n\nLOC\n\n예측치: \\(\\frac{\\text{낙관치} + 4\\text{중간치} + \\text{비관치}}{6}\\)\n노력: LOC / 1인당 월 평균 생산 코드 라인 수\n개발 비용: 노력 * 단위 비용(인건비 등)\n개발 기간: 노력 / 투입 인원\n생산성: LOC / 노력\n\n수학적 산정 기법\n\nCOCOMO: LOC에 기초한 경험적 모델\n\n조직형: 5만 라인 이하의 소프트웨어\n반분리형: 30만 라인 이하의 소프트웨어\n임베디드형: 대규모 소프트웨어\n\nPutnam: Rayleigh-Norden 곡선에 기초하여 노력 산정\n기능 점수(FP): 소프트웨어 기능 별 가중치를 부여하고 합산하여 노력 산출",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#프로젝트-일정-계획",
    "href": "posts/01_projects/정보처리기사/notes/06.html#프로젝트-일정-계획",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "프로젝트 일정 계획",
    "text": "프로젝트 일정 계획\n\nPERT(Program Evaluation and Review Technique): 결정 경로, 경계 시간, 작업 간 상호 관련성 파악 가능\nCPM(Critical Path Method): 프로젝트 일정 계획 및 관리 기법\n간트 차트: 작업 일정과 진행 상황을 막대 그래프로 표현",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#소프트웨어-개발-표준",
    "href": "posts/01_projects/정보처리기사/notes/06.html#소프트웨어-개발-표준",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "소프트웨어 개발 표준",
    "text": "소프트웨어 개발 표준\n\nISO/IEC 12207: 소프트웨어 생명주기 프로세스\nCMMI(Capability Maturity Model Integration): 개발 조직 업무 능력 및 성숙도 평가 모델\n\n초기, 관리, 정의, 정량적 관리, 최적화\n\nSPICE(Software Process Improvement and Capability Determination): 소프트웨어 프로세스 평가 및 개선 모델",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/05.html#어플리케이션-테스트",
    "href": "posts/01_projects/정보처리기사/notes/05.html#어플리케이션-테스트",
    "title": "소프트웨어 개발 - 어플리케이션 테스트 관리",
    "section": "어플리케이션 테스트",
    "text": "어플리케이션 테스트\n\n확인(validation): 사용자 입장에서 요구사항이 충족되었는지 확인하는 것\n검증(verification): 개발자 입장에서 설계/코딩이 제대로 되었는지 확인하는 것",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 어플리케이션 테스트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/05.html#테스트-기법",
    "href": "posts/01_projects/정보처리기사/notes/05.html#테스트-기법",
    "title": "소프트웨어 개발 - 어플리케이션 테스트 관리",
    "section": "테스트 기법",
    "text": "테스트 기법\n\n정적 테스트\n동적 테스트:\n\n화이트 박스: 모든 로직을 테스트\n\n기초 경로 검사\n제어 구조 검사\n\n조건, 루프, 데이터(변수) 흐름 검사\n\n\n블랙 박스: 기능이 작동하는 것을 입증\n\n동치 분할 검사: 타당한 입력, 타당하지 않은 입력 균등하게 테스트\n경계값 분석\n원인-효과 그래프 검사: 효용성 높은 테스트 케이스 선정하여 검사\n오류 예측 검사: 경험 기반 오류 케이스 테스트",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 어플리케이션 테스트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/05.html#개발-단계에-따른-어플리케이션-테스트",
    "href": "posts/01_projects/정보처리기사/notes/05.html#개발-단계에-따른-어플리케이션-테스트",
    "title": "소프트웨어 개발 - 어플리케이션 테스트 관리",
    "section": "개발 단계에 따른 어플리케이션 테스트",
    "text": "개발 단계에 따른 어플리케이션 테스트\n\n통합 테스트\n\n하향식: stub 사용\n상향식: driver 사용\n\n인수 테스트:\n\n알파 테스트: 개발사에서 실제 사용자 환경과 유사한 환경에서 테스트\n베타 테스트: 다수의 실제 사용자가 실제 환경에서 테스트",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 어플리케이션 테스트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/05.html#인터페이스-구현-검증-도구",
    "href": "posts/01_projects/정보처리기사/notes/05.html#인터페이스-구현-검증-도구",
    "title": "소프트웨어 개발 - 어플리케이션 테스트 관리",
    "section": "인터페이스 구현 검증 도구",
    "text": "인터페이스 구현 검증 도구\n\nXUnit: Unit 테스트\nSTAF: 서비스 호출 및 컴포넌트 재사용 등 다양한 환경을 지원\nFitNesse: 웹 기반 테스트 결과 확인 도구\nNTAF: FitNesse + STAF\nwatir: ruby 기반 테스트 프레임 워크",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 어플리케이션 테스트 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/04.html#정렬",
    "href": "posts/01_projects/정보처리기사/notes/04.html#정렬",
    "title": "소프트웨어 개발 - 데이터 입 출력 구현",
    "section": "정렬",
    "text": "정렬\n\n삽입 정렬: 2, 3, …번째 값을 앞과 비교해서 swap\n선택 정렬: n개 중에서 제일 작은거 앞으로, n-1 중에서 제일 작은거 앞으로, …\n버블 정렬: 인접한 값끼리 비교해서 swap. 끝에 제일 큰게 옴\n퀵 정렬\n힙 정렬",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 데이터 입 출력 구현"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/04.html#해싱",
    "href": "posts/01_projects/정보처리기사/notes/04.html#해싱",
    "title": "소프트웨어 개발 - 데이터 입 출력 구현",
    "section": "해싱",
    "text": "해싱\n\n제산법: 소수로 나눈거\n제곱법: 제곱해서 가운데 숫자\n폴딩법: 값을 일정 길이로 나눠서 연산\n숫자 분석법: 분포를 분석해서 고르게 나뉘도록 기준 정하는거",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 데이터 입 출력 구현"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/04.html#개발-지원-도구",
    "href": "posts/01_projects/정보처리기사/notes/04.html#개발-지원-도구",
    "title": "소프트웨어 개발 - 데이터 입 출력 구현",
    "section": "개발 지원 도구",
    "text": "개발 지원 도구\n\n빌드 도구: Ant, Maven, Gradle",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "소프트웨어 개발 - 데이터 입 출력 구현"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/14.html#secure-sdlc",
    "href": "posts/01_projects/정보처리기사/notes/14.html#secure-sdlc",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 보안 구축",
    "section": "Secure SDLC",
    "text": "Secure SDLC\n\nSeven Touch Point: 소프트웨어 보안의 모범 사례를 SDLC에 통합한 방법론",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 보안 구축"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/14.html#입력-데이터-검증-및-표현",
    "href": "posts/01_projects/정보처리기사/notes/14.html#입력-데이터-검증-및-표현",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 보안 구축",
    "section": "입력 데이터 검증 및 표현",
    "text": "입력 데이터 검증 및 표현\n\nSQL Injection\nXss\n메모리 버퍼 오버플로우",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 보안 구축"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/14.html#암호-알고리즘",
    "href": "posts/01_projects/정보처리기사/notes/14.html#암호-알고리즘",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 보안 구축",
    "section": "암호 알고리즘",
    "text": "암호 알고리즘\n\n개인키 암호화\n공개키 암호화",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 보안 구축"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/11.html#파티션-설계",
    "href": "posts/01_projects/정보처리기사/notes/11.html#파티션-설계",
    "title": "데이터베이스 구축 - 물리 데이터베이스 설계",
    "section": "파티션 설계",
    "text": "파티션 설계\n\n범위 분할: 열의 값을 기준으로 분할(일별, 월별 등)\n해시 분할: 해시 함수를 이용해서 데이터를 고르게 분할\n조합 분할: 범위 분할 후 해시 분할 적용\n라운드 로빈 분할: 순차적으로 데이터 분할",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 물리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/11.html#분산-데이터베이스-설계",
    "href": "posts/01_projects/정보처리기사/notes/11.html#분산-데이터베이스-설계",
    "title": "데이터베이스 구축 - 물리 데이터베이스 설계",
    "section": "분산 데이터베이스 설계",
    "text": "분산 데이터베이스 설계\n\n목표: 위치, 중복, 병행, 장애 투명성",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 물리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/11.html#접근-통제",
    "href": "posts/01_projects/정보처리기사/notes/11.html#접근-통제",
    "title": "데이터베이스 구축 - 물리 데이터베이스 설계",
    "section": "접근 통제",
    "text": "접근 통제\n\n임의 접근 통제(DAC): 소유자가 접근 권한 결정\n강제 접근 통제(MAC): 객체와 사용자에게 보안 등급 할당\n\n벨 라파듈라 모델\n\n역할 기반 접근 통제(RBAC)",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 물리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/11.html#스토리지",
    "href": "posts/01_projects/정보처리기사/notes/11.html#스토리지",
    "title": "데이터베이스 구축 - 물리 데이터베이스 설계",
    "section": "스토리지",
    "text": "스토리지\n\nDAS(Direct Attached Storage): 서버에 직접 연결\n\nSATA, eSATA, SCSI, SAS\n\nSAN(Storage Area Network): 고속 네트워크로 연결된 스토리지\n\niSCSI, FC, FCoE",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 물리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/15.html#서비스-공격-유형",
    "href": "posts/01_projects/정보처리기사/notes/15.html#서비스-공격-유형",
    "title": "정보시스템 구축 관리 - 시스템 보안 구축",
    "section": "서비스 공격 유형",
    "text": "서비스 공격 유형\n\nPing of Death: 비정상적으로 큰 패킷을 전송하여 시스템 다운 유발\nSmurfing: ICMP Echo 요청을 라우터의 브로드캐스트 주소로 전송하여 다수의 응답을 유발, 네트워크 과부하 초래\nSYN Flooding: SYN 패킷을 대량으로 전송하고 중단하여 서버의 연결 자원을 고갈시킴\nLand: 출발지와 목적지 IP 주소가 동일한 패킷을 전송하여 시스템 혼란 유발\nDDoS\nPhising\nWorm\nKey logger\nRansomware",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 시스템 보안 구축"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/15.html#보안-솔루션",
    "href": "posts/01_projects/정보처리기사/notes/15.html#보안-솔루션",
    "title": "정보시스템 구축 관리 - 시스템 보안 구축",
    "section": "보안 솔루션",
    "text": "보안 솔루션\n\n침입 탐지 시스템(IDS): 네트워크 트래픽을 모니터링하여 악의적인 활동이나 정책 위반을 탐지\n\n오용 탐지: 알려진 공격 패턴 기반 탐지\n이상 탐지: 정상적인 활동과의 차이점 기반 탐지",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 시스템 보안 구축"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/12.html#소프트웨어-재사용",
    "href": "posts/01_projects/정보처리기사/notes/12.html#소프트웨어-재사용",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "소프트웨어 재사용",
    "text": "소프트웨어 재사용\n\n합성 중심: 모듈 끼워 맞추는거\n생성 중심: 추상화된 컴포넌트를 구체화\n\n\n소프트웨어 재공학\n\n분석\n재구성: 기능, 외적인 동작은 유지하면서 내부 구조 변경\n역공학: 기존 코드를 분석\n이식: 기존 소프트웨어를 다른 환경에 이식할 수 있게 변경\n\nCASE(Computer-Aided Software Engineering): 구조적 기법, 프로토타이핑, 분산 처리 등의 기법으로 소프트웨어 개발 생산성을 향상시키기 위한 자동화 도구",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/12.html#상향식-비용-산정-기법",
    "href": "posts/01_projects/정보처리기사/notes/12.html#상향식-비용-산정-기법",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "상향식 비용 산정 기법",
    "text": "상향식 비용 산정 기법\n\nLOC\n\n예측치: 낙관치, 비관치, 기대치 이용해서 계산\n노력: LOC / 1인당 월 평균 생산 코드 라인 수\n개발 비용: 노력 * 단위 비용(인건비 등)\n개발 기간: 노력 / 투입 인원\n생산성: LOC / 노력\n\n수학적 산정 기법\n\nCOCOMO: LOC에 기초한 경험적 모델\n\n조직형: 5만 라인 이하의 소프트웨어\n반분리형: 30만 라인 이하의 소프트웨어\n임베디드형: 대규모 소프트웨어\n\nPutnam: Rayleigh-Norden 곡선에 기초하여 노력 산정\n기능 점수(FP): 소프트웨어 기능 별 가중치를 부여하고 합산하여 노력 산출",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/12.html#프로젝트-일정-계획",
    "href": "posts/01_projects/정보처리기사/notes/12.html#프로젝트-일정-계획",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "프로젝트 일정 계획",
    "text": "프로젝트 일정 계획\n\nPERT(Program Evaluation and Review Technique): 결정 경로, 경계 시간, 작업 간 상호 관련성 파악 가능\nCPM(Critical Path Method): 프로젝트 일정 계획 및 관리 기법\n간트 차트: 작업 일정과 진행 상황을 막대 그래프로 표현",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/12.html#소프트웨어-개발-표준",
    "href": "posts/01_projects/정보처리기사/notes/12.html#소프트웨어-개발-표준",
    "title": "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용",
    "section": "소프트웨어 개발 표준",
    "text": "소프트웨어 개발 표준\n\nISO/IEC 12207: 소프트웨어 생명주기 프로세스\n\n기본, 지원, 조직 생명 주기 프로세스\n\nCMMI(Capability Maturity Model Integration): 개발 조직 업무 능력 및 성숙도 평가 모델\n\n초기, 관리, 정의, 정량적 관리, 최적화\n\nSPICE(Software Process Improvement and Capability Determination): 소프트웨어 프로세스 평가 및 개선 모델\n\n불완전, 수행, 관리, 확립, 예측, 최적화",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - 소프트웨어 개발 방법론 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#네트워크-관련-신기술",
    "href": "posts/01_projects/정보처리기사/notes/13.html#네트워크-관련-신기술",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "네트워크 관련 신기술",
    "text": "네트워크 관련 신기술\n\n소프트웨어 정의 기술\n\n네트워크, 데이터 센터 등에서 소유한 자원을 가상화해서 갭려 사용자에게 제어하고 중앙에서 관리하는 기술\nSDN(Software Defined Networking): 네트워크를 가상화하고 소프트웨어로 제어\nSDDC(Software Defined Data Center): 데이터 센터의 모든 자원을 가상화하고 소프트웨어로 제어\nSDS(Software Defined Storage): 스토리지 자원을 가상화하고 소프트웨어로 관리\n\n\n\n기타 용어\n\n올조인(AllJoyn): 다양한 제조사의 기기들이 상호 운용될 수 있도록 지원하는 오픈 소스 프레임워크\n메시 네트워크: 대규모 무선 네트워크\n피코넷: 여러 블루투스 기술이나 UWB 통신 기술을 쓰는 장치가 무선 망을 형성\nZing: 저전력, 저비용 무선 통신 기술(NFC)\nWDM: 광섬유 통신에서 여러 파장의 빛을 동시에 전송",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#네트워크-구축",
    "href": "posts/01_projects/정보처리기사/notes/13.html#네트워크-구축",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "네트워크 구축",
    "text": "네트워크 구축\n\nCAMS/CD: 데이터 프레임 충돌 감지, 재전송 기능\nCSMA/CA: 무선 랜에서 충돌 회피",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#경로-제어",
    "href": "posts/01_projects/정보처리기사/notes/13.html#경로-제어",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "경로 제어",
    "text": "경로 제어\n\nIGP(Interior Gateway Protocol)\n\nRIP(Routing Information Protocol):\n\n벨만 포드 알고리즘 기반 라우팅 프로토콜\n최대 홉 수 15로 제한(소규모 전용)\n\nOSPF(Open Shortest Path First)\n\n다익스트라 알고리즘 기반 라우팅 프로토콜\n대규모 네트워크에 적합",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#트래픽-제어",
    "href": "posts/01_projects/정보처리기사/notes/13.html#트래픽-제어",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "트래픽 제어",
    "text": "트래픽 제어\n\n정지-대기: 수신 측의 확인 신호를 받은 후에 패킷을 전송하는 방식",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#sw-신기술",
    "href": "posts/01_projects/정보처리기사/notes/13.html#sw-신기술",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "SW 신기술",
    "text": "SW 신기술\n\nVaporware: 발표되었지만 아직 배포되지 않은 소프트웨어",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#보안-관련-신기술",
    "href": "posts/01_projects/정보처리기사/notes/13.html#보안-관련-신기술",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "보안 관련 신기술",
    "text": "보안 관련 신기술\n\n비트로커: windows 전용 볼륨 암호화 기능\nOWASP: 보안 관련 취약점 연구하는 비영리 단체\nTCP wrapper: 외부 컴퓨터의 접근을 제어하는 리눅스 보안 프로그램\n허니팟: 해커의 공격을 유인하여 분석하는 보안 시스템\nDPI(Deep Packet Inspection): OSI 7 계층의 전 층 포로토콜과 패킷 내부를 검사",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#회복",
    "href": "posts/01_projects/정보처리기사/notes/13.html#회복",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "회복",
    "text": "회복\n\n즉각 갱신 기법: Redo, Undo 모두 가능",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#병행-제어",
    "href": "posts/01_projects/정보처리기사/notes/13.html#병행-제어",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "병행 제어",
    "text": "병행 제어\n\n로킹(locking)\n\n로킹 단위가 작으면 관리하기 어려우나 병행도가 높아짐\n\n타임 스탬프 순서",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/13.html#교착-상태",
    "href": "posts/01_projects/정보처리기사/notes/13.html#교착-상태",
    "title": "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리",
    "section": "교착 상태",
    "text": "교착 상태\n\n필요 충분 조건\n\n상호 배제: 자원을 한 프로세스가 독점\n점유와 대기: 프로세스가 점유된 자원을 기다림\n비선점: 자원을 강제로 빼앗을 수 없음\n환형 대기: 프로세스들이 원형으로 자원을 기다림\n\n\n\n해결 방법\n\n회피 기법: 은행원 알고리즘",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "정보시스템 구축 관리 - IT 프로젝트 정보시스템 구축 관리"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/10.html#데이터베이스-설계",
    "href": "posts/01_projects/정보처리기사/notes/10.html#데이터베이스-설계",
    "title": "데이터베이스 구축 - 논리 데이터베이스 설계",
    "section": "데이터베이스 설계",
    "text": "데이터베이스 설계\n\n개념적 설계\n\nER Diagram:\n\n개체: 사각형\n속성: 타원\n관계: 마름모\n다중 값: 이중 타원\n\n\n논리적 설계\n\n데이터 모델: 구조, 연산, 제약 조건 표시\n\n물리적 설계\n\n\n수퍼키: 모든 튜플에 대해서 유일성은 만족하지만 최소성은 만족하지 않는 키",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 논리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/10.html#무결성",
    "href": "posts/01_projects/정보처리기사/notes/10.html#무결성",
    "title": "데이터베이스 구축 - 논리 데이터베이스 설계",
    "section": "무결성",
    "text": "무결성\n\n개체 무결성: 기본 키를 구성하는 어떤 속성도 NULL이나 중복값을 가질 수 없다.\n도메인 무결성: 속성 값이 도메이에 속한 값이여야 한다.\n참조 무결성: 외래 키 값은 NULL이거나 참조되는 릴레이션의 기본 키 값과 일치해야 한다.",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 논리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/10.html#관계대수",
    "href": "posts/01_projects/정보처리기사/notes/10.html#관계대수",
    "title": "데이터베이스 구축 - 논리 데이터베이스 설계",
    "section": "관계대수",
    "text": "관계대수\n\n절차적 언어\nselect (σ): 릴레이션에서 특정 조건을 만족하는 튜플을 선택\nproject (π): 릴레이션에서 특정 속성만을 추출\njoin (⨝): 두 릴레이션을 공통 속성을 기준으로 결합\ndevision (÷): s의 값을 가진 r의 relation에서 특정 속성을 제외한 나머지 속성으로 구성된 튜플을 추출",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 논리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/10.html#관계해석",
    "href": "posts/01_projects/정보처리기사/notes/10.html#관계해석",
    "title": "데이터베이스 구축 - 논리 데이터베이스 설계",
    "section": "관계해석",
    "text": "관계해석\n\n비절차적 특성",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 논리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/10.html#정규화",
    "href": "posts/01_projects/정보처리기사/notes/10.html#정규화",
    "title": "데이터베이스 구축 - 논리 데이터베이스 설계",
    "section": "정규화",
    "text": "정규화\n\n1NF: 도메인 원자값\n2NF: 부분적 함수 종속 제거\n3NF: 이행적 함수 종속 제거\nBCNF: 결정자가 후보 키\n4NF: 다치 종속 제거\n5NF: 조인 종속 제거",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 논리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/10.html#반정규화",
    "href": "posts/01_projects/정보처리기사/notes/10.html#반정규화",
    "title": "데이터베이스 구축 - 논리 데이터베이스 설계",
    "section": "반정규화",
    "text": "반정규화\n\n중복 테이블 추가: 집계 테이블, 진행(로그) 테이블, 특정 부분만 포함하는 테이블",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 논리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/10.html#시스템-카탈로그",
    "href": "posts/01_projects/정보처리기사/notes/10.html#시스템-카탈로그",
    "title": "데이터베이스 구축 - 논리 데이터베이스 설계",
    "section": "시스템 카탈로그",
    "text": "시스템 카탈로그\n\n데이터 사전이라고도 함\n일반 사용자도 검색할 수 있지만 갱신은 불가",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "데이터베이스 구축 - 논리 데이터베이스 설계"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#기억장치-관리의-개요",
    "href": "posts/01_projects/정보처리기사/notes/06.html#기억장치-관리의-개요",
    "title": "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용",
    "section": "기억장치 관리의 개요",
    "text": "기억장치 관리의 개요\n\n반입(fetch) 전략\n배치(placement) 전략\n\n최초 적합, 최적 적합, 최악 적합\n\n교체(replacement) 전략",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#가상기억장치",
    "href": "posts/01_projects/정보처리기사/notes/06.html#가상기억장치",
    "title": "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용",
    "section": "가상기억장치",
    "text": "가상기억장치\n\n페이징 기법: 일정 크기로 나누는 기법.\n\n내부 단편화 발생\n페이지 크기가 작아질수록 맵 테이블이 커지고, 디스크 접근 횟수가 많아짐.\n\n세그멘테이션 기법: 논리 단위로 나누는 기법.\n\n외부 단편화 발생\n\n\n\n페이지 교체 알고리즘\n\nOPT: 가장 오랫동안 참조되지 않을 페이지 교체\nFIFO: 가장 먼저 들어온 페이지 교체\nLRU: 가장 오랫동안 사용되지 않은 페이지 교체\nLFU: 가장 적게 사용된 페이지 교체\nNUR: 최근에 사용된 페이지는 교체하지 않는 알고리즘\n\n참조 비트, 변형 비트. 변형 비트 우선 고려\n\n\n\n\n기타 고려사항\n\nworking set: 일정 시간 동안 자주 참조하는 페이지 집합\nthrashing: 프로세스 처리 시간보다 교체 시간이 더 많이 걸리는 현상",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#프로세스-스케줄링",
    "href": "posts/01_projects/정보처리기사/notes/06.html#프로세스-스케줄링",
    "title": "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용",
    "section": "프로세스 스케줄링",
    "text": "프로세스 스케줄링\n\nSJF: 실행 시간이 가장 짧은 프로세스 우선\nHRN: (대기시간 + 서비스 시간) / 서비스 시간",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#인터넷",
    "href": "posts/01_projects/정보처리기사/notes/06.html#인터넷",
    "title": "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용",
    "section": "인터넷",
    "text": "인터넷\n\nA class: 0~127\nB class: 128~191\nC class: 192~223\n\n\nIPv6\n\n16 * 8 = 128비트\n패킷 크기 제한 없음. 헤더는 40byte\nIPv4 전환 전략: 듀얼 스택, 터널링, 그냥 변환\n주소 체계: 글로벌 유니캐스트, 멀티캐스트, 애니캐스트",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#osi-참조-모델",
    "href": "posts/01_projects/정보처리기사/notes/06.html#osi-참조-모델",
    "title": "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용",
    "section": "OSI 참조 모델",
    "text": "OSI 참조 모델\n\n물리 계층\n데이터 링크 계층: 랜카드, 브리지, 스위치 등을 통해 프레임 전달\n\n인접한 개방 시스템들 간에 신뢰성 있고 효율적인 데이터 전송 제공\n\n네트워크 계층: 라우터를 통해 패킷 전달\n전송 계층: 게이트웨이를 통한 TCP, UDP 통신으로 전달\n\n종단 간에 신뢰성 있고 효율적인 데이터 전송 제공\n\n세션 계층\n표현 계층: 응용 계층과 세션 계층간의 데이터 변환\n응용 계층",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/06.html#네트워크-관련-장비",
    "href": "posts/01_projects/정보처리기사/notes/06.html#네트워크-관련-장비",
    "title": "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용",
    "section": "네트워크 관련 장비",
    "text": "네트워크 관련 장비\n\n브릿지: LAN과 LAN 연결\n스위치: LAN과 LAN을 연결하여 더 큰 LAN 구성\n라우터: LAN과 LAN, LAN과 WAN 연결. 최적 경로 선택\n게이트웨이: 서로 다른 프로토콜을 사용하는 네트워크 연결",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "프로그래밍 언어 활용 - 응용 SW 기초 기술 활용"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/16.html#서론",
    "href": "posts/01_projects/정보처리기사/notes/16.html#서론",
    "title": "필기 후기",
    "section": "서론",
    "text": "서론\n정보처리기사 필기 시험을 보고 왔습니다. 수원 삼일 공고에서 시험을 봤는데 고등학교가 시설이 이렇게 좋을 수가 있구나 라는 생각이 들었다가 변기가 푸세식인걸 보고 기묘한 느낌이 들었습니다. 오전 9시로 기사 시험 중 제일 빠른 시간으로 예약해서 왔는데 시험 전 날 출근 시간이었다는 점이 생각나서 아차 싶었지만 수원 방향으로 가는 버스는 사람이 별로 없었습니다. 어쩌면 20살의 김형훈이 경기권 대학을 다니는 선택을 했다면 죽음의 출근열차를 경험하지 않았을지도 모르겠다는 생각이 들었습니다.",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "필기 후기"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/16.html",
    "href": "posts/01_projects/정보처리기사/notes/16.html",
    "title": "필기 후기",
    "section": "",
    "text": "삼일 공고 정문",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "필기 후기"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/16.html#공부",
    "href": "posts/01_projects/정보처리기사/notes/16.html#공부",
    "title": "필기 후기",
    "section": "공부",
    "text": "공부\n참고서를 한 권 사서 2주 정도 공부했습니다. 보통 기출문제만 풀고 시험을 보는 경우가 많은 것 같은데, 저는 NCS도 대비할 겸 꼼꼼하게 공부하는게 좋다고 판단했습니다. 근데 생각보다 당연한 내용들이 많아서 굳이 이렇게 공부할 필요는 없었던것 같습니다.\n그리고 오늘 시험을 보니 확실히 기출문제 그대로 나오는 경향이 있는 것 같아서 빠르게 취득하는게 목표라면 개념 공부는 생략해도 괜찮을 것 같습니다. 참고로 필기 결과는 당일 바로 나옵니다. 저는 무난하게 합격했습니다.",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "필기 후기"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/16.html#결과",
    "href": "posts/01_projects/정보처리기사/notes/16.html#결과",
    "title": "필기 후기",
    "section": "결과",
    "text": "결과\n시험장에서 바로 채점 결과가 나와서 합격 여부를 알 수 있었습니다. 당연히 붙었고요. 난이도는 크게 어렵다고 느껴지지 않았습니다. 은근히 기출 문제 그대로 나와줘서 더 쉽게 느껴졌던 것 같습니다.",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "필기 후기"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/16.html#결론",
    "href": "posts/01_projects/정보처리기사/notes/16.html#결론",
    "title": "필기 후기",
    "section": "결론",
    "text": "결론\n방학동안 참 많이 늘어지는 것 같습니다. 정보처리기사 자격증 시험이 저를 각성시켜주는 계기가 될줄 알았는데 so easy 했네요. 뉴런의 30%도 활성화되지 않은 기분입니다. 시험날 가장 큰 고난은 시험 문제도 아니고 수원의 살인적인 칼바람 뿐이었던것 같습니다. 실기는 합격률이 꽤 낮다고 하던데 저를 성장시켜주는 시련이 되어줄 수 있을까요? 근데 왜 후기글을 이렇게 건방지게 쓰고 있는거죠? 하하 필기 시험이 제 자존감은 채워줬군요! 어쨌든 저는 실기 준비로 돌아오겠습니다.",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "필기 후기"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/금융/00.html#한국은행",
    "href": "posts/02_categories/진로준비/notes/금융/00.html#한국은행",
    "title": "관심 분야 JD",
    "section": "한국은행",
    "text": "한국은행\n\n\n\n채용 절차\n\n\n\n채용 인원: 컴퓨터공학 15명 이내\n어학 성적 우대: TOEIC, TOEFL(iBT), TEPS 중\n필기 출제:\n\n전공 학술: 소프트웨어공학, 데이터베이스, 컴퓨터구조, 데이터통신, 정보보호, 운영체제, 자료구조, 인공지능, 기계학습\n논술: 주요 경제 / 금융 이슈, 인문학 등\n기출\n\n면접 전형:\n\n1차 실무 면접: 집단 토론, 심층 면접\n2차 집행 간부 면접",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/금융/00.html#한국-투자증권",
    "href": "posts/02_categories/진로준비/notes/금융/00.html#한국-투자증권",
    "title": "관심 분야 JD",
    "section": "한국 투자증권",
    "text": "한국 투자증권\n\n자기소개서 문항\n\n성장과정:가족사항, 학창시절, 교우관계, 생활습관, 자신에게 크게 영향을 미친 사건 등을 포함하여 구체적으로 작성 (최대 500자)\n실패 혹은 좌절을 극복한 사례와 이를 통해 얻은 교훈은? (최대 300자)\n증권업을 선택하게된 이유와 증권사 중 당사를 선택한 이유 (최대 300자)\n지원한 분야는 어떤 일을 한다고 생각하는가? (최대 200자)\n지원분야에 본인이 적합한 이유를 증명하시오. (최대 500자)\n당신의 인생계획(Life Plan)에서 꿈은 무엇이고, 그 꿈을 이루기 위해 회사가 어떻게 도움을 줄 수 있는지 서술하시오. (최대 500자)",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/금융/00.html#금융-감독원",
    "href": "posts/02_categories/진로준비/notes/금융/00.html#금융-감독원",
    "title": "관심 분야 JD",
    "section": "금융 감독원",
    "text": "금융 감독원\n\n채용 인원: IT 7명\n\n - 어학 성적 우대: TOEIC 730, TOEFL(iBT) 79, TEPS 368 만점 - 1차 필기 시험: NCS 직업 기초 시험 - 2차 필기 시험 - 전공 시험 - 논술 - 1차 면접 전형 - 실무진, 외부위원 개별 면접, 집단 토론 - 2차 면접 전형 - 임원, 외부위원 개별 면접",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/금융/00.html#ibk-기업은행",
    "href": "posts/02_categories/진로준비/notes/금융/00.html#ibk-기업은행",
    "title": "관심 분야 JD",
    "section": "IBK 기업은행",
    "text": "IBK 기업은행\n  - ADP를 꼭 따자 - 산업공학 석사 우대 - 금융권 공동채용 박람회 우수면접자 우대\n\n\n\n직무 소개\n\n\n\n\n\n전형 방법\n\n\n\n필기 시험\n\n직업 기초(객관식 40문항): 의사소통, 문제 해결, 자원관리, 조직 이해, 수리, 정보\n직무 수행(객관식 30문항, 주관식 5문항): 데이터베이스, 데이터분석, 인공지능 모델링, 블록체인, 시사 등\n\n실기 시험\n\n실기시험 전, AI역량검사 및 인성검사(온라인) 실시 예정\n(공통) 개인발표, 토론, 인터뷰\n(디지털, IT) 코딩테스트\n\n면접 시험\n\n多대多 질의응답을 통해 인성, 윤리의식, 직무·조직적합도 등의 평가항목을 기준으로 종합평가(평가위원 점수합계)\n\n\n\n자기소개 문항\n\nIBK와 미래를 함께해 나갈 지원자님을 환영합니다! 다양한 회사들 중에서 IBK를 선택하신 이유가 궁금한데요, 지원동기에 대해 편안하게 이야기해 주세요. (1500자 이내)\n지원자님의 여러 장점 중 “팀웍”에 대해 듣고 싶어요. 최선의 결과를 이끌어내기 위해 팀원으로서 했던 역할에 대해 구체적인 경험을 들어 말씀해 주세요. (1500자 이내)\n지원자님이 생각하는 본인의 단점에 대해 알고 싶어요. 그리고 그것을 극복하기 위해 기울이셨던 노력에 대해서도 자유롭게 전달해 주세요. (1500자 이내)\n은행원이라는 직업이 지원자님께 왜 어울리는지 궁금합니다. 지원자님만이 갖고 있는 차별화된 스토리를 저희에게 들려 주세요. (1500자 이내)",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/금융/00.html#우리-은행",
    "href": "posts/02_categories/진로준비/notes/금융/00.html#우리-은행",
    "title": "관심 분야 JD",
    "section": "우리 은행",
    "text": "우리 은행\n\n\n\n직무 내용\n\n\n\n신입 행원 연수: 약 1년 이내 영업점 근무 후, 유관 부서 및 본부 부서 배치. 인력 운영 및 업무상 필요에 따라 변동될 수 있음\n\n(이거 잘못하면 내 커리어 개같이 멸망할 수도 있겠는데)\n\nTECH 직무 관련 전공 우대 (구체적으로 무슨 전공인지 모르겠음)\n2025 금융권 공동채용 박람회 우수면접자 우대\n\n\n\n\n우대 자격증\n\n\n\n\n\n공인 영어\n\n\n\n\n\n전형 방법\n\n\n\n서류 전형: 입행 지원서 심사 및 AI 역량검사\n\nAI 역량검사: 자기보고, 전략게임, 영상 면접\n\n1차 면접: 기본 역량 면접\n2차 면접:\n\n인사이트 면접(PT, 포트폴리오), 참여형 팀워크 프로그램, 직무 / 인성 면접\n코딩테스트: 알고리즘 및 SQL\n\n최종 면접: 심화 역량 면접 (blind 면접. 그럼 어떻게 본다는거지?)\n\n\n자기소개 문항\n\n우리은행 및 해당 부문에 지원한 동기와 입행 후 이루고 싶은 목표를 구체적으로 서술해 주세요. (800자 이내)\n본인 성격 중 은행원 업무와 가장 맞지 않는 부분이 무엇이라고 생각하며, 그 이유를 구체적으로 설명해 주세요. (800자 이내)\n과거에 했던 선택이나 행동 중, 후회하는 사례를 구체적으로 작성해 주세요.(800자 이내)\n본인이 IT 직무에서 다른 지원자와 차별화된 경쟁력을 갖추었다고 생각하는 부분을 구체적인 사례를 근거로 설명해 주세요. (800자 이내)",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "금융",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/01.html",
    "href": "posts/02_categories/진로준비/notes/대학원/01.html",
    "title": "연구실",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "연구실"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/00.html#인프라-엔지니어",
    "href": "posts/02_categories/진로준비/notes/대학원/00.html#인프라-엔지니어",
    "title": "관심 분야 JD",
    "section": "인프라 / 엔지니어",
    "text": "인프라 / 엔지니어",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/00.html#computer-vision-품질검사",
    "href": "posts/02_categories/진로준비/notes/대학원/00.html#computer-vision-품질검사",
    "title": "관심 분야 JD",
    "section": "Computer Vision / 품질검사",
    "text": "Computer Vision / 품질검사\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 현대오토에버\n\n\n\n\n\n\n2025 상반기 현대트랜시스\n\n\n\n\n\n\n2025 상반기 에스엘",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/00.html#자율주행",
    "href": "posts/02_categories/진로준비/notes/대학원/00.html#자율주행",
    "title": "관심 분야 JD",
    "section": "자율주행",
    "text": "자율주행\n \n\n\n\n\n2025 상반기 KAI\n\n\n\n\n\n\n2025 상반기 현대트랜시스",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/00.html#llm",
    "href": "posts/02_categories/진로준비/notes/대학원/00.html#llm",
    "title": "관심 분야 JD",
    "section": "LLM",
    "text": "LLM\n\n\n\n2024 LG화학 기반기술 연구소",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/00.html#기타",
    "href": "posts/02_categories/진로준비/notes/대학원/00.html#기타",
    "title": "관심 분야 JD",
    "section": "기타",
    "text": "기타\n\n\n\n2025 상반기 HD한국조선해양\n\n\n\n \n\n \n\n \n\n\n\n\n2025 상반기 현대로템\n\n\n\n\n\n\n2025 상반기 삼성물산\n\n\n\n\n\n\n2024 하반기 LG U+\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 kt\n\n\n\n\n\n\n2025 상반기 네이버\n\n\n\n\n\n\n2025 상반기 네이버",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "관심 분야 JD"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/02.html",
    "href": "posts/02_categories/진로준비/notes/대학원/02.html",
    "title": "대학원 준비",
    "section": "",
    "text": "숭실대 성적 확정 7월 5일",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/02.html#서울대",
    "href": "posts/02_categories/진로준비/notes/대학원/02.html#서울대",
    "title": "대학원 준비",
    "section": "서울대",
    "text": "서울대\n\n중복지원 불가\nTeps 327 이상\n\n\n데이터사이언스\n\n입학지원서 접수: 10.07 - 10.11 17:00 까지\n서류 제출: 10.14 17:00 까지\n1차 합격자 발표: 10.28 18:00\n면접: 11.01\n최종 합격자 발표: 11.21 18:00\n\n\n면접이 제일 중요해 보임\n데이터사이언스 면접은 데이터사이언스에 관련된 3가지 주제 중 지원자가 2가지 주제를 선택해 품.\n데이터사이언스는 1차 서류에서 탈락할 가능성이 높아 보임\n그렇다고 서울대 산업공학 대학원이 만만한건 절대 아님",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/02.html#카이스트",
    "href": "posts/02_categories/진로준비/notes/대학원/02.html#카이스트",
    "title": "대학원 준비",
    "section": "카이스트",
    "text": "카이스트\n\n온라인 원서접수: 6.27 - 7.9 17:30 까지\n서류 제출: 7.11 18:00 까지\n1단계 전형 합격자 발표: 8.07 14:00 이후\n면접: 8.12 - 8.14 중 진행\n최종 합격자 발표: 9.18 14:00 이후\n\n\n2 지망 산업및시스템공학과 지원 가능\nTeps 326 이상, TOEIC 720 이상\n전형료 10만원\n산업공학 컨택은 면접 다 끝나고 있다\n데이터사이언스 사전 컨택은 강력히 권장된다고 한다.",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/02.html#포항공대",
    "href": "posts/02_categories/진로준비/notes/대학원/02.html#포항공대",
    "title": "대학원 준비",
    "section": "포항공대",
    "text": "포항공대",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/02.html#연세대",
    "href": "posts/02_categories/진로준비/notes/대학원/02.html#연세대",
    "title": "대학원 준비",
    "section": "연세대",
    "text": "연세대\n\n1개 학과에만 지원해야 함\n영어 성적 안봄\n\n\n원서접수: 10. 8 10:00 ~ 10. 16 17:00\n구술/실기시험대상자 발표: 11. 8 17:00\n구술시험 및 실기시험: 11. 16\n최종합격자 발표: 12. 6. 17:00\n\n\n스마트시스템 연구실 - 김우주",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/notes/대학원/02.html#고려대",
    "href": "posts/02_categories/진로준비/notes/대학원/02.html#고려대",
    "title": "대학원 준비",
    "section": "고려대",
    "text": "고려대",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비",
      "Notes",
      "대학원",
      "대학원 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/index.html",
    "href": "posts/02_categories/진로준비/index.html",
    "title": "진로 준비",
    "section": "",
    "text": "진로 준비 관련 노트",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/index.html#details",
    "href": "posts/02_categories/진로준비/index.html#details",
    "title": "진로 준비",
    "section": "",
    "text": "진로 준비 관련 노트",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/index.html#tasks",
    "href": "posts/02_categories/진로준비/index.html#tasks",
    "title": "진로 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/진로준비/index.html#related-posts",
    "href": "posts/02_categories/진로준비/index.html#related-posts",
    "title": "진로 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Categories",
      "진로 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/rust/notes/00.html",
    "href": "posts/02_categories/rust/notes/00.html",
    "title": "rust",
    "section": "",
    "text": "rust 언어의 마스코트\n\n\nfn main() {\n    println!(\"Hello, Rust!\");\n}\n\n\n\n 맨 위로",
    "crumbs": [
      "Home",
      "Categories",
      "Rust",
      "Notes",
      "rust"
    ]
  },
  {
    "objectID": "posts/02_categories/rust/index.html",
    "href": "posts/02_categories/rust/index.html",
    "title": "Rust",
    "section": "",
    "text": "Rust 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Rust"
    ]
  },
  {
    "objectID": "posts/02_categories/rust/index.html#details",
    "href": "posts/02_categories/rust/index.html#details",
    "title": "Rust",
    "section": "",
    "text": "Rust 관련 노트입니다.",
    "crumbs": [
      "Home",
      "Categories",
      "Rust"
    ]
  },
  {
    "objectID": "posts/02_categories/rust/index.html#tasks",
    "href": "posts/02_categories/rust/index.html#tasks",
    "title": "Rust",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Categories",
      "Rust"
    ]
  },
  {
    "objectID": "posts/02_categories/rust/index.html#참고-자료",
    "href": "posts/02_categories/rust/index.html#참고-자료",
    "title": "Rust",
    "section": "참고 자료",
    "text": "참고 자료\n \n\n유튜브에 한국말로 강의하는 외국인이 있다.",
    "crumbs": [
      "Home",
      "Categories",
      "Rust"
    ]
  },
  {
    "objectID": "posts/02_categories/rust/index.html#related-posts",
    "href": "posts/02_categories/rust/index.html#related-posts",
    "title": "Rust",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Categories",
      "Rust"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/index.html",
    "href": "posts/01_projects/adp_실기2/index.html",
    "title": "ADP 실기 준비 - try 2",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2026-02-05\n        종료일: 2026-05-22\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        자격증데이터 분석python",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/index.html#details",
    "href": "posts/01_projects/adp_실기2/index.html#details",
    "title": "ADP 실기 준비 - try 2",
    "section": "Details",
    "text": "Details\n에 이어서 다시 해봅시다.",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/index.html#참고-자료",
    "href": "posts/01_projects/adp_실기2/index.html#참고-자료",
    "title": "ADP 실기 준비 - try 2",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/index.html#related-posts",
    "href": "posts/01_projects/adp_실기2/index.html#related-posts",
    "title": "ADP 실기 준비 - try 2",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/index.html",
    "href": "posts/01_projects/bs_4_1/index.html",
    "title": "Toeic 준비",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2026-01-12\n        종료일: 2026-02-12\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/index.html#details",
    "href": "posts/01_projects/bs_4_1/index.html#details",
    "title": "Toeic 준비",
    "section": "Details",
    "text": "Details\n빠르게 끝내 봅시다.",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/index.html#tasks",
    "href": "posts/01_projects/bs_4_1/index.html#tasks",
    "title": "Toeic 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/index.html#참고-자료",
    "href": "posts/01_projects/bs_4_1/index.html#참고-자료",
    "title": "Toeic 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/index.html#related-posts",
    "href": "posts/01_projects/bs_4_1/index.html#related-posts",
    "title": "Toeic 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/02_categories/rust/notes/00.html#문법",
    "href": "posts/02_categories/rust/notes/00.html#문법",
    "title": "basic",
    "section": "문법",
    "text": "문법\n\n문자는 기본적으로 ascii가 아닌 유니코드로 처리된다.\n구문과 표현식이 구분되어 있다.\n\nlet x = let y = 6 -&gt; 오류\n표현식은 ;가 없어야 한다.\n\nrust는 bool type 자동 변환을 하지 않는다.\nloop에 레이블을 붙일 수 있다.\nbreak로 값을 반환하거나 레이블로 탈출할 수 있다.\nscope를 벗어나는 동적 변수들은 drop 함수가 호출되어 자동으로 메모리가 해제된다.\n\n얕은 복사가 일어날 경우 double free가 발생할 수 있기 때문에 rust에서는 move가 일어남. (이전 변수는 더이상 사용 불가)\n\n이로 인해 참조자 없이 method에 인자로 넘겨줄 경우, 넘긴 변수는 그 다음부터 사용 불가\n\n참조자(reference)를 사용하면 됨. 그러나 참조자를 사용하면 변수 수정 불가\n\n가변 참조자(mut)를 사용하면 변수 수정 가능\n\n단, 가변 참조자는 한 번에 하나만 존재할 수 있음.\n\n\n\n\n\n구조체의 properties는 기본적으로 private. method를 정의해줌으로써 공개함.\nenum의 variant는 기본적으로 public",
    "crumbs": [
      "Home",
      "Categories",
      "Rust",
      "Notes",
      "basic"
    ]
  },
  {
    "objectID": "posts/02_categories/rust/notes/00.html#패키지-관리",
    "href": "posts/02_categories/rust/notes/00.html#패키지-관리",
    "title": "basic",
    "section": "패키지 관리",
    "text": "패키지 관리\n\n바이너리, 라이브러리 크래이트를 만들 수 있다.\nsrc/bin/ 디렉토리에 여러 바이너리 크래이트를 넣을 수 있다.",
    "crumbs": [
      "Home",
      "Categories",
      "Rust",
      "Notes",
      "basic"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/notes/00.html#명동형부",
    "href": "posts/01_projects/bs_4_1/notes/00.html#명동형부",
    "title": "Toeic 문법",
    "section": "명동형부",
    "text": "명동형부\n\n명사 / 대명사\n\n기본 명사 생김새\n\n명사 1초 공식 TOP 5\n\n관사(a/an/the) + 명사 + 전치사\n전치사 + 명사 + 전치사\n관사/소유격 + (부사) + (형용사) + 명사\n형용사(지시형용사, 수량형용사) + 명사\n명사 + 명사\n\n명사 자리\n\n주어\n타/목\n전/목\n보어\n\n가산명사 vs. 불가산명사 &gt; 관사 a/an는 가산 단수 명사 앞에만 쓸 수 있다. &gt; the는 가산 단수 복수, 불가산 명사 앞에 쓸 수 있다.\n\n불가산 명사: a/an, -s 안 붙음 information: 정보 advice: 조언 merchandise: 상품 access: 접근 assistance: 지원 equipment: 장비 luggage/ baggage: 수하물 stationery: 문구류 funding: 자금 제공 furniture: 가구 produce: 농산물 news: 소식 \n\n~ing는 대부분 불가산 명사\n\n\n사람명사 vs. 사물명사\n\n사람명사: 가산 명사\n사물명사: 가산/불가산 명사\n\n특이 어미 명사: -al(proposal, arrival, approval), -ive(initiative),\n\n기본 인칭대명사 표\n\n소유격-소유대명사\n\n소유격 뒤에만 명사가 올 수 있다.\nhis: 소유격이면서 소유대명사 \n\n목적격-재귀대명사\n\n재귀대명사는 재귀용법, 강조용법(부사 자리)으로 사용 가능\n\n지시대명사\n\nThis, That + 단수동사/ These, Those + 복수동사\nThose who + 복수동사\n\n부정대명사\n\none vs another\nsome vs any\n\nsome: 긍정문(not이 없음), 권유나 제안을 나타내는 의문문\nany: 부정문, 의문문, 조건문, 어떤 ~라도\n\n\n\n\n\n\n동사 (수, 태, 시제)\n\nS질량의 보존의 법칙: 단수 주어 + 단수 동사 / 복수 주어 + 복수 동사\n\n고유명사(대문자)는 단수\n\n\n주어 + 거품(부사 / 전치사구) + 동사\n무조건 동사원형 자리\n\n조동사(will, can, should, may) + 동사원형\nplease + 동사원형\n\n무조건 단수 동사\n\n단수 명사\n불가산 명사\nto 부정사구\n동명사구\nthat 절\n\n\n기본 동사 보는 방법\n\npp + be: 수동\n나머지: 능동\n\n\n\n\n\n12가지 시제\n\n\n\n\n형용사 / 부사\n\n형용사 위치와 공식\n\n주격 보어자리(s + 2v + sc), 2v: be, become, remain, seem, Stay\n목적격 보어자리(s + 5v + o + oc), 5v: Keep, Find, Consider, Make\n\n수량형용사 수일치\n\neach, every, another + 단수 명사\nmany, (a) few, a number of, several + 복수명사\n\na number of + 주어: 주어에 맞춘 동사\nthe number of + 주어: the number of에 맞춘 동사(단수)\n\nmuch, little, a little, less + 불가산 명사\n\na little(few) vs litte(few): a 가 붙으면 긍정, 아니면 부정\n\nall, some, most, a lot of + 복수 명사/ 불가산 명사",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/notes/00.html#전접부",
    "href": "posts/01_projects/bs_4_1/notes/00.html#전접부",
    "title": "Toeic 문법",
    "section": "전접부",
    "text": "전접부\n\n전치사 / 등위접속사 / 부사절 접속사\n\n빈출 접속사\n\nat: 시각(at 7 a.m., at the end of this year), 정확한 지점 / 주소\non: 몇 월 몇 일, 일, 접촉(on the table, on the wall, on the ceiling)\nin: 연도, 월, 계절, 행정 구역, 3차원 공간\nuntil: (동사가) 지속성\nby: (동사가) 일회성\nduring: 특정 기간 명사\nfor: 숫자, 숙어, ~을 위해\nthrough: ~을 통해(관통해서), 수단 / 매개\nthroughout: 장소, 시간\nbetween / among: 복수명사가 온다. between은 2개, among은 3개 이상\nsince: 부사로도 쓸 수 있긴 함\nbecause of / due to / owing to / on account of: ~때문에\n\n\n등위접속사\n\nBoth A and B\n\n복수 동사\n\nEither A or B\nNeither A nor B\nNot only A but also B\nB As well as A\nNot A but B\n\nB에 맞춘 동사\n\n\n부사절 접속사\n\n조건: if, unless, provided (that), as long as, in case (that), in event (that)\n\n\n\n\n명사절 접속사 / 형용사절 접속사\n\nif 미래내용: 현재동사\n형용사절 접속사\n\n소유격 + 완전한 문장\n그 외 + 불완전한 문장\n, 다음에 that 안 씀\n\n\n\n\n접속부사 / 전치사 접속사 부사\n\nnotwithstanding: ~에도 불구하고",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/notes/00.html#준동사",
    "href": "posts/01_projects/bs_4_1/notes/00.html#준동사",
    "title": "Toeic 문법",
    "section": "준동사",
    "text": "준동사\n\nTo부정사 / 동명사 / 분사\n\nTo부정사: 명사, 형용사, 부사(~하기 위해서 so as to, in order to) 역할. 미래 지향적인 느낌\n동명사: 명사 역할. 과거 지향적이고 부정적인 느낌\n\n\n\n\n분사: 형용사 역할\n\n자동사: ing",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/bs_4_1/notes/00.html#기타",
    "href": "posts/01_projects/bs_4_1/notes/00.html#기타",
    "title": "Toeic 문법",
    "section": "기타",
    "text": "기타\n\n분사 구문 / 비교 구문",
    "crumbs": [
      "Home",
      "Projects",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/notes/00.html#명동형부",
    "href": "posts/01_projects/adp_실기2/notes/00.html#명동형부",
    "title": "Toeic 문법",
    "section": "명동형부",
    "text": "명동형부\n\n명사 / 대명사\n\n기본 명사 생김새\n\n명사 1초 공식 TOP 5\n\n관사(a/an/the) + 명사 + 전치사\n전치사 + 명사 + 전치사\n관사/소유격 + (부사) + (형용사) + 명사\n형용사(지시형용사, 수량형용사) + 명사\n명사 + 명사\n\n명사 자리\n\n주어\n타/목\n전/목\n보어\n\n가산명사 vs. 불가산명사 &gt; 관사 a/an는 가산 단수 명사 앞에만 쓸 수 있다. &gt; the는 가산 단수 복수, 불가산 명사 앞에 쓸 수 있다.\n\n불가산 명사: a/an, -s 안 붙음 information: 정보 advice: 조언 merchandise: 상품 access: 접근 assistance: 지원 equipment: 장비 luggage/ baggage: 수하물 stationery: 문구류 funding: 자금 제공 furniture: 가구 produce: 농산물 news: 소식 \n\n~ing는 대부분 불가산 명사\n\n\n사람명사 vs. 사물명사\n\n사람명사: 가산 명사\n사물명사: 가산/불가산 명사\n\n특이 어미 명사: -al(proposal, arrival, approval), -ive(initiative),\n\n기본 인칭대명사 표\n\n소유격-소유대명사\n\n소유격 뒤에만 명사가 올 수 있다.\nhis: 소유격이면서 소유대명사 \n\n목적격-재귀대명사\n\n재귀대명사는 재귀용법, 강조용법(부사 자리)으로 사용 가능\n\n지시대명사\n\nThis, That + 단수동사/ These, Those + 복수동사\nThose who + 복수동사\n\n부정대명사\n\none vs another\nsome vs any\n\nsome: 긍정문(not이 없음), 권유나 제안을 나타내는 의문문\nany: 부정문, 의문문, 조건문, 어떤 ~라도\n\n\n\n\n\n\n동사 (수, 태, 시제)\n\nS질량의 보존의 법칙: 단수 주어 + 단수 동사 / 복수 주어 + 복수 동사\n\n고유명사(대문자)는 단수\n\n\n주어 + 거품(부사 / 전치사구) + 동사\n무조건 동사원형 자리\n\n조동사(will, can, should, may) + 동사원형\nplease + 동사원형\n\n무조건 단수 동사\n\n단수 명사\n불가산 명사\nto 부정사구\n동명사구\nthat 절\n\n\n기본 동사 보는 방법\n\npp + be: 수동\n나머지: 능동\n\n\n\n\n\n12가지 시제\n\n\n\n\n형용사 / 부사\n\n형용사 위치와 공식\n\n주격 보어자리(s + 2v + sc), 2v: be, become, remain, seem, Stay\n목적격 보어자리(s + 5v + o + oc), 5v: Keep, Find, Consider, Make\n\n수량형용사 수일치\n\neach, every, another + 단수 명사\nmany, (a) few, a number of, several + 복수명사\n\na number of + 주어: 주어에 맞춘 동사\nthe number of + 주어: the number of에 맞춘 동사(단수)\n\nmuch, little, a little, less + 불가산 명사\n\na little(few) vs litte(few): a 가 붙으면 긍정, 아니면 부정\n\nall, some, most, a lot of + 복수 명사/ 불가산 명사",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/notes/00.html#전접부",
    "href": "posts/01_projects/adp_실기2/notes/00.html#전접부",
    "title": "Toeic 문법",
    "section": "전접부",
    "text": "전접부\n\n전치사 / 등위접속사 / 부사절 접속사\n\n빈출 접속사\n\nat: 시각(at 7 a.m., at the end of this year), 정확한 지점 / 주소\non: 몇 월 몇 일, 일, 접촉(on the table, on the wall, on the ceiling)\nin: 연도, 월, 계절, 행정 구역, 3차원 공간\nuntil: (동사가) 지속성\nby: (동사가) 일회성\nduring: 특정 기간 명사\nfor: 숫자, 숙어, ~을 위해\nthrough: ~을 통해(관통해서), 수단 / 매개\nthroughout: 장소, 시간\nbetween / among: 복수명사가 온다. between은 2개, among은 3개 이상\nsince: 부사로도 쓸 수 있긴 함\nbecause of / due to / owing to / on account of: ~때문에\n\n\n등위접속사\n\nBoth A and B\n\n복수 동사\n\nEither A or B\nNeither A nor B\nNot only A but also B\nB As well as A\nNot A but B\n\nB에 맞춘 동사\n\n\n부사절 접속사\n\n조건: if, unless, provided (that), as long as, in case (that), in event (that)\n\n\n\n\n명사절 접속사 / 형용사절 접속사\n\nif 미래내용: 현재동사\n형용사절 접속사\n\n소유격 + 완전한 문장\n그 외 + 불완전한 문장\n, 다음에 that 안 씀\n\n\n\n\n접속부사 / 전치사 접속사 부사\n\nnotwithstanding: ~에도 불구하고",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/notes/00.html#준동사",
    "href": "posts/01_projects/adp_실기2/notes/00.html#준동사",
    "title": "Toeic 문법",
    "section": "준동사",
    "text": "준동사\n\nTo부정사 / 동명사 / 분사\n\nTo부정사: 명사, 형용사, 부사(~하기 위해서 so as to, in order to) 역할. 미래 지향적인 느낌\n동명사: 명사 역할. 과거 지향적이고 부정적인 느낌\n\n\n\n\n분사: 형용사 역할\n\n자동사: ing",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/notes/00.html#기타",
    "href": "posts/01_projects/adp_실기2/notes/00.html#기타",
    "title": "Toeic 문법",
    "section": "기타",
    "text": "기타\n\n분사 구문 / 비교 구문",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/01_projects/정보처리기사/notes/17.html",
    "href": "posts/01_projects/정보처리기사/notes/17.html",
    "title": "김형훈의 학습 블로그",
    "section": "",
    "text": "맨 위로",
    "crumbs": [
      "Home",
      "Projects",
      "정보처리기사",
      "Notes",
      "17"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/00.html#서론",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/00.html#서론",
    "title": "3학년 2학기 후기",
    "section": "서론",
    "text": "서론\n\n\n\n학기 점수\n\n\n\n\n\n과목 점수\n\n\n\n\n\n시간표\n\n\n이번 학기는 온라인 강의 없이 21학점을 다 채워서 들었다. 일부러 그런건 아니고, 그냥 수강신청을 실패해서 어쩔 수 없이 그렇게 됐다. 가끔씩 점심 먹는 시간이 애매했다는 점만 제외하면 그래도 꽤 할만 했었다. 물론 두번 다시 이런 학기를 맞이하고 싶지는 않다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "3학년 2학기 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/00.html#점수",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/00.html#점수",
    "title": "3학년 2학기 후기",
    "section": "점수",
    "text": "점수\n이번 학기를 통해서 비로소 알게된 점은 확실히 통계, 프로그래밍 관련된 과목들은 점수가 잘 나오는 편인 것 같다. 발표나 과제 중심 과목들은 상대적으로 물을 먹는 편인거 같은데, 노력을 안한건 절대 아니니... 이런쪽엔 소질이 없다고 봐야하는 걸까? 그래도 뭐... A0가 나와버린건 운이 없어서라고 친다고 하더라도, A-가 나온 과목들은 확실히 내가 부족한 부분이 있었던 것 같다.\n글로벌소통과언어 과목에서 제일 컸던 문제는 주제 선정을 너무 어려운걸로 했던게 아닐까 싶다. 양자컴퓨팅의 미래에 관해서 발표를 했었는데, 당시에는 충분히 잘 해낼 수 있을거라는 믿음이 있었던 것 같다. 하지만 근거 자료 찾기도, 영어로 발표를 준비하는 것도 쉽지 않았었다. 이런 일이 발생한건 아마 나의 능력에 대해서 너무 과신 하는것이 문제가 된 걸 수도 있다. 상대적으로 배경지식이 있는 다른 과목들에서라면 몰라도, 그렇지 못한 과목에서도 똑같은 행동을 해버리니 점수가 낮게 나오는 걸수도.\n아 참고로 기초공학수학 과목에서 A-가 나온건... 민망하지만 기말고사 시험 시간을 착각해서 30분을 지각했던 것이 원인이다. 50분 짜리 시험에서 말이다. 하하.. 시간 관리 잘 해야지.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "3학년 2학기 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/00.html#그래도-잘-했던-점",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/00.html#그래도-잘-했던-점",
    "title": "3학년 2학기 후기",
    "section": "그래도 잘 했던 점",
    "text": "그래도 잘 했던 점\n너무 부족했던 점만 생각하면서 우울해 할 필요는 없을 것 같다. 사실 학기 초만 하더라도 압도적인 스케쥴을 보며 절망적인 stance였는데, 이걸 문제없이 해냈다고 생각하니 감격이 물밀듯 밀려온다. 사실 스케쥴 뿐만 아니라 압도적인 팀과제의 양도 나의 숨을 조여왔는데, 모든걸 마무리했을 때의 그 감격은 이루 말할 수 없다. 시스템시뮬레이션, 프로세스 경영 과목 모두 보통의 사람이였다면 절대 이 정도로 잘 해내지 못했을 것이다.\n자존감을 계속 붙잡자. 결국 모든 일의 시작은 나에 대한 믿음에서 시작되니까.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "3학년 2학기 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/bs_3_2/notes/00.html#결론",
    "href": "posts/03_archives/completed_project/bs_3_2/notes/00.html#결론",
    "title": "3학년 2학기 후기",
    "section": "결론",
    "text": "결론\n복학을 처음 할 때 생각했던 것들이 기억난다. 나는 내 트라우마를 극복하고 싶었다. 나는 신입생 때 지금과 같은 퍼포먼스를 보이지 못했다. 지금 내가 재수강의 늪에 빠져서 고통받고 있는 원인이기도 하다. 그때 점수가 낮았던 자세한 이유를 말하기에는 이제는 너무 오래된 일이라 잘 기억이 나지 않는다. 다만 ’김형훈은 대학 생활이랑 맞지 않아.’라는 생각에서 벗어나고 싶었다.\n이정도면 그래도 이제는 충분히 잘 생활하고 있다고 볼 수 있지 않을까? 아직 1년의 시간이 남았지만 내가 할 수 있는 최선을 다해서 잘 마무리해보려 한다. 다음학기부터는 드디어 전액 장학금도 받게 되니, 부담없이 열심히 해보자.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "학부 3학년 2학기",
      "Notes",
      "3학년 2학기 후기"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/notes/00.html#명동형부",
    "href": "posts/03_archives/completed_project/토익/notes/00.html#명동형부",
    "title": "Toeic 문법",
    "section": "명동형부",
    "text": "명동형부\n\n명사 / 대명사\n\n기본 명사 생김새\n\n명사 1초 공식 TOP 5\n\n관사(a/an/the) + 명사 + 전치사\n전치사 + 명사 + 전치사\n관사/소유격 + (부사) + (형용사) + 명사\n형용사(지시형용사, 수량형용사) + 명사\n명사 + 명사\n\n명사 자리\n\n주어\n타/목\n전/목\n보어\n\n가산명사 vs. 불가산명사 &gt; 관사 a/an는 가산 단수 명사 앞에만 쓸 수 있다. &gt; the는 가산 단수 복수, 불가산 명사 앞에 쓸 수 있다.\n\n불가산 명사: a/an, -s 안 붙음 information: 정보 advice: 조언 merchandise: 상품 access: 접근 assistance: 지원 equipment: 장비 luggage/ baggage: 수하물 stationery: 문구류 funding: 자금 제공 furniture: 가구 produce: 농산물 news: 소식 \n\n~ing는 대부분 불가산 명사\n\n\n사람명사 vs. 사물명사\n\n사람명사: 가산 명사\n사물명사: 가산/불가산 명사\n\n특이 어미 명사: -al(proposal, arrival, approval), -ive(initiative),\n\n기본 인칭대명사 표\n\n소유격-소유대명사\n\n소유격 뒤에만 명사가 올 수 있다.\nhis: 소유격이면서 소유대명사 \n\n목적격-재귀대명사\n\n재귀대명사는 재귀용법, 강조용법(부사 자리)으로 사용 가능\n\n지시대명사\n\nThis, That + 단수동사/ These, Those + 복수동사\nThose who + 복수동사\n\n부정대명사\n\none vs another\nsome vs any\n\nsome: 긍정문(not이 없음), 권유나 제안을 나타내는 의문문\nany: 부정문, 의문문, 조건문, 어떤 ~라도\n\n\n\n\n\n\n동사 (수, 태, 시제)\n\nS질량의 보존의 법칙: 단수 주어 + 단수 동사 / 복수 주어 + 복수 동사\n\n고유명사(대문자)는 단수\n\n\n주어 + 거품(부사 / 전치사구) + 동사\n무조건 동사원형 자리\n\n조동사(will, can, should, may) + 동사원형\nplease + 동사원형\n\n무조건 단수 동사\n\n단수 명사\n불가산 명사\nto 부정사구\n동명사구\nthat 절\n\n\n기본 동사 보는 방법\n\npp + be: 수동\n나머지: 능동\n\n\n\n\n\n12가지 시제\n\n\n\n\n형용사 / 부사\n\n형용사 위치와 공식\n\n주격 보어자리(s + 2v + sc), 2v: be, become, remain, seem, Stay\n목적격 보어자리(s + 5v + o + oc), 5v: Keep, Find, Consider, Make\n\n수량형용사 수일치\n\neach, every, another + 단수 명사\nmany, (a) few, a number of, several + 복수명사\n\na number of + 주어: 주어에 맞춘 동사\nthe number of + 주어: the number of에 맞춘 동사(단수)\n\nmuch, little, a little, less + 불가산 명사\n\na little(few) vs litte(few): a 가 붙으면 긍정, 아니면 부정\n\nall, some, most, a lot of + 복수 명사/ 불가산 명사",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/notes/00.html#전접부",
    "href": "posts/03_archives/completed_project/토익/notes/00.html#전접부",
    "title": "Toeic 문법",
    "section": "전접부",
    "text": "전접부\n\n전치사 / 등위접속사 / 부사절 접속사\n\n빈출 접속사\n\nat: 시각(at 7 a.m., at the end of this year), 정확한 지점 / 주소\non: 몇 월 몇 일, 일, 접촉(on the table, on the wall, on the ceiling)\nin: 연도, 월, 계절, 행정 구역, 3차원 공간\nuntil: (동사가) 지속성\nby: (동사가) 일회성\nduring: 특정 기간 명사\nfor: 숫자, 숙어, ~을 위해\nthrough: ~을 통해(관통해서), 수단 / 매개\nthroughout: 장소, 시간\nbetween / among: 복수명사가 온다. between은 2개, among은 3개 이상\nsince: 부사로도 쓸 수 있긴 함\nbecause of / due to / owing to / on account of: ~때문에\n\n\n등위접속사\n\nBoth A and B\n\n복수 동사\n\nEither A or B\nNeither A nor B\nNot only A but also B\nB As well as A\nNot A but B\n\nB에 맞춘 동사\n\n\n부사절 접속사\n\n조건: if, unless, provided (that), as long as, in case (that), in event (that)\n\n\n\n\n명사절 접속사 / 형용사절 접속사\n\nif 미래내용: 현재동사\n형용사절 접속사\n\n소유격 + 완전한 문장\n그 외 + 불완전한 문장\n, 다음에 that 안 씀\n\n\n\n\n접속부사 / 전치사 접속사 부사\n\nnotwithstanding: ~에도 불구하고",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/notes/00.html#준동사",
    "href": "posts/03_archives/completed_project/토익/notes/00.html#준동사",
    "title": "Toeic 문법",
    "section": "준동사",
    "text": "준동사\n\nTo부정사 / 동명사 / 분사\n\nTo부정사: 명사, 형용사, 부사(~하기 위해서 so as to, in order to) 역할. 미래 지향적인 느낌\n동명사: 명사 역할. 과거 지향적이고 부정적인 느낌\n\n\n\n\n분사: 형용사 역할\n\n자동사: ing",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/notes/00.html#기타",
    "href": "posts/03_archives/completed_project/토익/notes/00.html#기타",
    "title": "Toeic 문법",
    "section": "기타",
    "text": "기타\n\n분사 구문 / 비교 구문",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비",
      "Notes",
      "Toeic 문법"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/index.html",
    "href": "posts/03_archives/completed_project/토익/index.html",
    "title": "Toeic 준비",
    "section": "",
    "text": "ON-GOING\n    \n    \n        시작일: 2026-01-12\n        종료일: 2026-02-12\n    \n    \n        \n            \n        \n        계산 중...\n    \n    \n    \n        영어자격증",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/index.html#details",
    "href": "posts/03_archives/completed_project/토익/index.html#details",
    "title": "Toeic 준비",
    "section": "Details",
    "text": "Details\n빠르게 끝내 봅시다.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/index.html#tasks",
    "href": "posts/03_archives/completed_project/토익/index.html#tasks",
    "title": "Toeic 준비",
    "section": "Tasks",
    "text": "Tasks\n\n\nNo tasks defined.",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/index.html#참고-자료",
    "href": "posts/03_archives/completed_project/토익/index.html#참고-자료",
    "title": "Toeic 준비",
    "section": "참고 자료",
    "text": "참고 자료",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/03_archives/completed_project/토익/index.html#related-posts",
    "href": "posts/03_archives/completed_project/토익/index.html#related-posts",
    "title": "Toeic 준비",
    "section": "Related Posts",
    "text": "Related Posts",
    "crumbs": [
      "Home",
      "Archives",
      "Completed Project",
      "Toeic 준비"
    ]
  },
  {
    "objectID": "posts/01_projects/adp_실기2/index.html#tasks",
    "href": "posts/01_projects/adp_실기2/index.html#tasks",
    "title": "ADP 실기 준비 - try 2",
    "section": "Tasks",
    "text": "Tasks\n\n\n\n    \n    \n    \n            \n                \n                    \n                    원서 접수 (2026.03.30 10 am)\n                \n                \n            \n\n            \n            \n                \n                    \n                    사전 점수 공개 (2026.05.22)",
    "crumbs": [
      "Home",
      "Projects",
      "ADP 실기 준비 - try 2"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/05.html#intro",
    "href": "posts/02_categories/42_seoul/notes/05.html#intro",
    "title": "inception-of-things part 2",
    "section": "Intro",
    "text": "Intro\nPart 1에 이어서 진행해보겠습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 2"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/05.html#개념-설명",
    "href": "posts/02_categories/42_seoul/notes/05.html#개념-설명",
    "title": "inception-of-things part 2",
    "section": "개념 설명",
    "text": "개념 설명\n\n\n\nPart 1 구조\n\n\nPart 1에서 클러스터가 여러 노드들을 논리적으로 묶어주는 기술임을 설명했습니다.\n그렇다면 구체적으로 각각의 노드들이 어떻게 협업을 한다는 것일까요?",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 2"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/05.html#코드-설명",
    "href": "posts/02_categories/42_seoul/notes/05.html#코드-설명",
    "title": "inception-of-things part 2",
    "section": "코드 설명",
    "text": "코드 설명\n\nProvision\n\n\nVagrantfile\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"bento/ubuntu-24.04\"\n  config.vm.box_version = \"202404.26.0\"\n\n  config.vm.define \"hyunghkiS\" do |control|\n    control.vm.hostname = \"hyunghkiS\"\n    control.vm.network \"private_network\", ip: \"192.168.56.110\"\n    control.vm.provider \"virtualbox\" do |v|\n      v.customize [\"modifyvm\", :id, \"--name\", \"hyunghkiS\"]\n      v.memory = \"8192\"\n      v.cpus = \"5\"\n    end\n    config.vm.synced_folder \"confs\", \"/etc/vagrant/confs\"\n    control.vm.provision \"shell\", path: \"scripts/server.sh\"\n  end\nend\n\nVagrant 파일을 이용해서 가상머신을 생성해줍니다. 이번 파트에서는 k3s 클러스터 안에 하나의 노드만 provision하고 있는데, 이 경우 노드 하나가 master, worker 역할을 모두 수행합니다.\n\n\nk3s config file\n\n\ndeploy.yml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-one\n  labels:\n    app: app1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app1\n  template:\n    metadata:\n      labels:\n        app: app1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-two\n  labels:\n    app: app2\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-three\n  labels:\n    app: app3\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app3\n  template:\n    metadata:\n      labels:\n        app: app3\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n\n\n\ning.yml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress\nspec:\n  ingressClassName: traefik\n  rules:\n  - host: \"app1.com\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: app-one\n            port:\n              number: 80\n  - host: \"app2.com\"\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: app-two\n            port:\n              number: 80\n\n\n\nroute.yml\n\napiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: default-backend\nspec:\n  routes:\n    - match: PathPrefix(`/`)\n      kind: Rule\n      services:\n        - name: app-three\n          port: 80\n\n\n\nsvc.yml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-one\nspec:\n  selector:\n    app: app1\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-two\nspec:\n  selector:\n    app: app2\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: app-three\nspec:\n  selector:\n    app: app3\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n\n\n\n실행 스크립트\n\n\nscripts/server.sh\n\n#!/bin/bash\n\necho 'alias k=kubectl' &gt;&gt; /home/vagrant/.bashrc\nsource /home/vagrant/.bashrc\ncurl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=\"644\" sh -s - server\nuntil kubectl get crd | grep -q 'ingressroutes.traefik.containo.us'; do\n    echo 'waiting for CRD...'\n    sleep 1\ndone\n\n# 작성한 설정 파일을 적용해줍니다.\nkubectl apply -f /etc/vagrant/confs\n\n# pod이 정상적으로 올라올 때 까지 기다려줍니다.\nuntil [ ! -z \"$(kubectl get pods -o jsonpath='{.items[*].metadata.name}')\" ]; do\n    sleep 1\ndone\nfor pod in $(kubectl get pods -o jsonpath='{.items[*].metadata.name}'); do\n    until kubectl get pods $pod | grep -q 'Running'; do\n        sleep 1\n    done\n    app_name=$(kubectl get pods $pod -o jsonpath='{.metadata.labels.app}')\n\n    # 과제에서 제시된 HTML 파일을 생성된 pod에 저장해줍니다.\n    HTML=$(cat &lt;&lt;EOF\n&lt;!DOCTYPE html&gt;\n&lt;html&gt; \n&lt;head&gt; \n    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n    &lt;div class=\"main\"&gt;\n        &lt;div class=\"content\"&gt;\n            &lt;div id=\"message\"&gt; \n    Hello from ${app_name}.\n&lt;/div&gt;\n&lt;div id=\"info\"&gt;\n    &lt;table&gt; \n        &lt;tr&gt;\n            &lt;th&gt;pod:&lt;/th&gt; \n            &lt;td&gt;${pod}&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;th&gt;node:&lt;/th&gt; \n            &lt;td&gt;$(uname -s) ($(uname -r))&lt;/td&gt; \n        &lt;/tr&gt; \n    &lt;/table&gt;\n\n&lt;/div&gt; \n    &lt;/div&gt; \n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nEOF\n)\n    echo $\"${HTML}\" &gt; /home/vagrant/index.html\n    kubectl cp /home/vagrant/index.html $pod:/usr/share/nginx/html/index.html &gt; /dev/null\ndone\n\nProvision이 완료되고 자동으로 실행되는 script를 작성해줍니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 2"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/05.html#outro",
    "href": "posts/02_categories/42_seoul/notes/05.html#outro",
    "title": "inception-of-things part 2",
    "section": "Outro",
    "text": "Outro\nPart 3와 Bonus는 다음 포스팅에서 작성하겠습니다.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 2"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/06.html#결과",
    "href": "posts/02_categories/42_seoul/notes/06.html#결과",
    "title": "inception-of-things part 3",
    "section": "결과",
    "text": "결과\n\n\n\n최종 점수\n\n\n\n\n\n\n최종 평가",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 3"
    ]
  },
  {
    "objectID": "posts/02_categories/42_seoul/notes/06.html#outro",
    "href": "posts/02_categories/42_seoul/notes/06.html#outro",
    "title": "inception-of-things part 3",
    "section": "outro",
    "text": "outro\nk8s의 개략적인 개념을 적용해보기 좋은 과제였던것 같습니다. 왠지 이 과제를 발판으로 삼아 더 응용할만한 과제가 있을법 한데, 의외로 후속 과제가 없는것이 조금 아쉬웠습니다. 개인적으로 조금 더 깊이 있는 infra 분야의 심화 과제가 많이 나왔으면 좋겠네요.",
    "crumbs": [
      "Home",
      "Categories",
      "42 Seoul",
      "Notes",
      "inception-of-things part 3"
    ]
  }
]